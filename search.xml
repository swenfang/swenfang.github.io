<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[CloudToolkit之Arthas]]></title>
    <url>%2F2020%2F03%2F21%2F%E5%BC%80%E5%8F%91%E5%B7%A5%E5%85%B7%2FCloudToolkit%E4%B9%8BArthas%2F</url>
    <content type="text"><![CDATA[CloudToolkit之Arthas通过 Cloud Toolkit 插件，您可以在本地 IDE 中使用 Arthas 来实现本地诊断或远程诊断。 3 分钟了解 Alibaba Cloud Toolkithttps://yq.aliyun.com/articles/689345?spm=a2c4e.11153940.0.0.33fa2708lyTUps 下载 Cloud Toolkit 插件https://cn.aliyun.com/product/cloudtoolkit https://plugins.jetbrains.com/plugin/11386-alibaba-cloud-toolkit?spm=5176.11997469.1327965.1.3a622fa85iZLDD 安装教程 点击 InteIIi IDEA 可以查看 IDEA 怎么使用 Cloud Toolkit 。 我这里直接使用从阿里官网下载的 toolkit-intellij-2020.3.1.zip 导入到 IDEA 的插件中。 1.Ctrl+Alt+s 打开 Settings 面板 2.选择从阿里官网下载下来的 toolkit-intellij-2020.3.1.zip 。 3.重启IDEA 4.重启之后会重新这个提示，并在 IDEA 的工具栏上多了一个 Alibaba Cloud Explorer 可能遇到的问题 查询 C:\Users\shenw.arthas\lib 目录是否是如下内容。 使用手册代码检查https://help.aliyun.com/document_detail/145927.html?spm=a2c4g.11186623.6.593.48dec928DUf3GE Arthas 诊断https://help.aliyun.com/document_detail/112975.html?spm=a2c4g.11186623.4.4.6bda7e11QVU8Jv]]></content>
      <categories>
        <category>IntelliJ IDEA</category>
      </categories>
      <tags>
        <tag>开发工具</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[skywalking是什么]]></title>
    <url>%2F2020%2F03%2F16%2F%E9%93%BE%E8%B7%AF%E8%B7%9F%E8%B8%AA%E7%B3%BB%E7%BB%9F%2Fskywalking%E6%98%AF%E4%BB%80%E4%B9%88%2F</url>
    <content type="text"><![CDATA[skywalking是什么分布式追踪系统，也是APM（Application Performance Monitoring），即应用性能监控系统。尤其是为服务，云原生，以及基于容器（k8s, Mesos）架构而设计，skywalking就是这样一个为了观察和监控分布式系统的解决方案。 skywalking是一个收集，分析，聚合和可视化来自服务和云原生架构数据的开源观察平台，它提供了一个非常简单的方法，让你清晰认识你的分布式系统。 skywalking完全由中国人开发，项目发起人是吴晟(https://github.com/wu-sheng)，并已经进入apache孵化。Github地址：https://github.com/apache/incubator-skywalking。 skywalking与其他链路跟踪系统对比 cat pinpoint zipkin jaeger skywalking OpenTracing兼容 否 否 是 是 是 客户端支持语言 java java、php java,c#,go,php等 java,c#,go,php等 Java, .NET Core, NodeJS and PHP 存储 mysql , hdfs hbase ES，mysql,Cassandra,内存 ES，kafka,Cassandra,内存 ES，H2,mysql,TIDB,sharding sphere 传输协议支持 thrift http,MQ udp/http gRPC ui丰富程度 低 高 低 中 中 实现方式-代码侵入性 拦截请求，侵入 字节码注入，无侵入 拦截请求，侵入 拦截请求，侵入 字节码注入，无侵入 扩展性 低 低 高 高 中 trace查询 不支持 不支持 支持 支持 支持 警告支持 支持 支持 不支持 不支持 支持 jvm监控 不支持 支持 不支持 不支持 支持 性能损失 高 中 中 低 skywalking架构图 架构图解读： 架构图左边部分就是skywalking的UI，即提供给相关人员操作使用的界面。在UI上可以看到服务拓扑图，接口调用链路信息等。并且skywaling6.x通过新的GraphQL查询协议，不再与UI强绑定； 架构图的右边是skywalking支持的底层存储。由图可知，skywalking支持es，mysql, TiDB, H2, ShardingSphere这些存储方式，用户可以根据自己的能力选择适合自己的存储（其中，H2仅限调研阶段使用，禁止使用在生产环境）。同时存储是开放的，用户可以自定义实现，例如使用HBase作为底层存储（如果你的自定义实现经过生产环境的考验，还可以将源码贡献给skywalking，成为apache顶级项目skywalking的contributor甚至committer）。 架构图的上面部分，表示skywalking的探针（probe）。由图可知，skywalking支持采集zipkin（v1/v2）格式，skywalking自己的格式，Istio遥测格式等。 skywalking通过探针收集tracing和metrics数据并格式化，然后通过协议（gRPC或者HTTP）发送给skywalking核心部分即OAP观察分析平台。 架构图中间部分就是skywalking的核心，称为skywalking可观测分析平台。并且由图可知，skywalking支持通过gRPC和http两种协议接收数据。且收集的数据主要分为两部分：tracing和metrics。 skywalking的主要特性 服务，实例和端点的metrics分析 根因分析 服务拓扑图分析 服务，实例和端点的依赖分析 慢服务和接口探测 性能优化 分布式追踪以及上下文广播 告警 根因分析 服务拓扑图分析 服务，实例和端点的依赖分析 性能优化 服务，实例和端点的metrics分析 概念说明 服务（service）：表示提供相同行为请求的一个服务集合或服务组工作负载，例如用户服务，评论服务等。当你用语言探针或SDK时，你可以定义服务名称。或是SkyWalking使用你在Istio这类平台中定义的名字。 服务实例（service instance）：服务组中的每个工作负载都称作服务实例，例如部署的若干用户服务中的某个JVM实例。就像pods在Kubernetes(k8s)中一样，它不需要是OS中的单个进程。然而，如果您正在使用语言探针，一个服务实例则实际上是OS中的单个进程。 端点（endpoint）：表示某个服务传入请求的路径，例如HTTP URI路径（/user/api/userinfo）或gRPC服务类+方法签名。 目标设计skywalking核心设计目标如下： 保持可观察性，无论待监控的目标系统如何部署，skywalking都能提供一个解决方案，或者集成方案保持对目标系统的观察。基于这个原因，skywalking提供了几个运行时形态和探针。 拓扑图，Metric和Trace，了解分布式系统的第一步，应该是从拓扑图开始。它把整个复杂的系统可视化为一个简单的拓扑图。在拓扑图之下，我们需要更多关于服务，实例，端点和调用的metrics信息。例如当端点延迟变长，你需要查看最慢的地方从而找出延迟原因。正如你所见，它们是从一个大的图片到细节，这些都是必须的。SkyWalking整合并提供了许多特性，从而让其成为可能，并易于理解。 轻量级. 主要有两个部分需要轻量级。 (1) 在探针上，我们仅依赖网络通信框架，优先gRPC。如此依赖，探针能尽可能的小，从而避免包冲突，和对VM的压力。 (2)作为一个可观察性平台，它在你的工程环境中是二级和三级系统。因此，skywalking用它们自己的轻量级框架构建后端核心，这样的话用户不需要部署大数据技术平台并维护它们，skywalking在技术栈上应该是简单的。 可插拔，skywalking核心团队提供了很多默认实现，但是肯定还不够，也不可能覆盖每一种场景。所以，许多特性支持可插拔，例如底层存储。 可移植性，skywalking能够运行在许多环境上，包括（1）使用传统的注册中心，比如Eureka。（2）用RPC框架包括服务发现，例如SpringCloud，dubbo。（3）使用ServiceMesh这样的现代基础架构。（4）使用云服务。（5）跨云部署。在这些环境下skywalking都应该能运行良好。 互操作，可观察性是一个很大的landscape，skywalking不可能支持所有，即使通过它的社区也不行。所以，skywalking支持和其他的OSS系统相互协作。比如Zipkin，Jaeger，OpenTracing, OpenCensus。skywalking要能够接收并理解它们的数据格式。 为什么选择skywalking探针的性能消耗 APM组件服务的影响应该做到足够小。服务调用埋点本身会带来性能损耗，这就需要调用跟踪的低损耗，实际中还会通过配置采样率的方式，选择一部分请求去分析请求路径。 代码的侵入性即也作为业务组件，应当尽可能少入侵或者无入侵其他业务系统，对于使用方透明，减少开发人员的负担。 可扩展性一个优秀的调用跟踪系统必须支持分布式部署，具备良好的可扩展性。 数据的分析数据的分析要快 ，分析的维度尽可能多。界面能呈现出来的数据和报表需要强大。 支持多种语言微服务经常不只有一种语言，比如公司就有PHP，Java，NodeJS等语言，支持多语言的就显得很必要。 探针的性能消耗、代码的侵入性、可扩展性、数据的分析、支持多种语言 这几个都可以作为链路跟踪系统的选型指标。其中标记颜色的几个指标就是我决定使用skywalking的原因。]]></content>
      <categories>
        <category>skywalking</category>
      </categories>
      <tags>
        <tag>链路跟踪系统</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[skywalking 的简单安装]]></title>
    <url>%2F2020%2F03%2F14%2F%E9%93%BE%E8%B7%AF%E8%B7%9F%E8%B8%AA%E7%B3%BB%E7%BB%9F%2Fskywalking%E7%9A%84%E7%AE%80%E5%8D%95%E5%AE%89%E8%A3%85%2F</url>
    <content type="text"><![CDATA[skywalking 的简单安装win10 环境下来搭建 skywalking 基于 elasticsearch 。 安装 JDK由于本次使用的 elasticsearch 版本是 elasticsearch-7.6 ，skywalking 版本是 skywalking-6.6 ，所 JDK 版本要 1.8 或以上。 安装 elasticsearch1）下载 https://www.elastic.co/downloads/ 2) 解压 3）修改配置文件 进入 elasticsearch 解压文件夹的 config目录,编辑 elasticsearch.yml 改成服务地址 1network.host: 127.0.0.1 服务名和agent的service_name相同 1cluster.name: jccrc-pre-sales-service elasticsearch的cluster.name和agent的service_name相同原因？假如是多个agent的service_name怎么配置？ 4）启动 进入到解压文件夹的 bin 目录，双击 elasticsearch.bat 文件即可。 4）访问 http://localhost:9200/ 这样说明启动 elasticsearch 成功。 5）关闭 安装 skywalking1）下载： http://skywalking.apache.org/zh/downloads/ 2）解压 apache-skywalking-apm-es7-6.6.0.zip 3）修改配置文件 在 skywalking 解压后 config 目录找到 elasticsearch.yml 文件，做如下修改： skywalking 支持 h2、mysql、ElasticSearch 作为数据存储。官网好像是推荐使用ElasticSearch，为什么推荐？应该是ElasticSearch快。需要注意的是，ElasticSearch不是自带的，需要安装。 Skywalking启用ElasticSearch，只需要配置文件设置如下： 放开 elasticsearch7 的注释同时记得注释掉 h2 。 1234567891011121314151617181920212223242526 elasticsearch7: #nameSpace: $&#123;SW_NAMESPACE:""&#125; clusterNodes: $&#123;SW_STORAGE_ES_CLUSTER_NODES:localhost:9200&#125; protocol: $&#123;SW_STORAGE_ES_HTTP_PROTOCOL:"http"&#125; trustStorePath: $&#123;SW_SW_STORAGE_ES_SSL_JKS_PATH:"../es_keystore.jks"&#125; trustStorePass: $&#123;SW_SW_STORAGE_ES_SSL_JKS_PASS:""&#125; user: $&#123;SW_ES_USER:""&#125; password: $&#123;SW_ES_PASSWORD:""&#125; indexShardsNumber: $&#123;SW_STORAGE_ES_INDEX_SHARDS_NUMBER:2&#125; indexReplicasNumber: $&#123;SW_STORAGE_ES_INDEX_REPLICAS_NUMBER:0&#125; # Those data TTL settings will override the same settings in core module. recordDataTTL: $&#123;SW_STORAGE_ES_RECORD_DATA_TTL:7&#125; # Unit is day otherMetricsDataTTL: $&#123;SW_STORAGE_ES_OTHER_METRIC_DATA_TTL:45&#125; # Unit is day monthMetricsDataTTL: $&#123;SW_STORAGE_ES_MONTH_METRIC_DATA_TTL:18&#125; # Unit is month # Batch process setting, refer to https://www.elastic.co/guide/en/elasticsearch/client/java-api/5.5/java-docs-bulk-processor.html bulkActions: $&#123;SW_STORAGE_ES_BULK_ACTIONS:1000&#125; # Execute the bulk every 1000 requests flushInterval: $&#123;SW_STORAGE_ES_FLUSH_INTERVAL:10&#125; # flush the bulk every 10 seconds whatever the number of requests concurrentRequests: $&#123;SW_STORAGE_ES_CONCURRENT_REQUESTS:2&#125; # the number of concurrent requests resultWindowMaxSize: $&#123;SW_STORAGE_ES_QUERY_MAX_WINDOW_SIZE:10000&#125; metadataQueryMaxSize: $&#123;SW_STORAGE_ES_QUERY_MAX_SIZE:5000&#125; segmentQueryMaxSize: $&#123;SW_STORAGE_ES_QUERY_SEGMENT_SIZE:200&#125;# h2:# driver: $&#123;SW_STORAGE_H2_DRIVER:org.h2.jdbcx.JdbcDataSource&#125;# url: $&#123;SW_STORAGE_H2_URL:jdbc:h2:mem:skywalking-oap-db&#125;# user: $&#123;SW_STORAGE_H2_USER:sa&#125;# metadataQueryMaxSize: $&#123;SW_STORAGE_H2_QUERY_MAX_SIZE:5000&#125; 配置文件config/application.yml中的存储配置需要注意–clusterName的值一定要与es集群的cluster.name一致，还有clusterNodes的地址： 12elasticsearch7: #nameSpace: $&#123;SW_NAMESPACE:""&#125; 4）启动 skywalking 在bin目录下执行 startup.bat 执行startup.bat之后会启动如下两个服务：（1）Skywalking-Collector：追踪信息收集器，通过 gRPC/Http 收集客户端的采集信息 ，Http默认端口 12800，gRPC默认端口 11800。（2）Skywalking-Webapp：管理平台页面 默认端口 8080 5）访问管理后台： http://localhost:8080/ 6）可能会遇到的问题 在bin目录下执行 startup.bat 时，startup.bat 闪退，不会启动 Skywalking-Collector 和 Skywalking-Webapp。 解决方案：将 skywalkinge 解压之后的文件夹放到 JDK 安装目录的上一级 7）关闭 配置要监控的服务演示 jccrc-pre-sales-service 服务调用 jccrc-vehicle-service 服务的监控配置过程。 添加参数java启动参数，如下： 123-javaagent:C:\skywalking-6.6\apache-skywalking-apm-bin-es7\agent\skywalking-agent.jar-Dskywalking.agent.service_name=jccrc-vehicle-service-Dskywalking.collector.backend_service=127.0.0.1:11800 skywalking-agent.jar 这个 jar 在 skywalking 解压目录 agent 下。 service_name 指定是你的服务名称（AppId） backend_service 指的是 skywalking 收集客服端信息地址 再次skywalking访问管理后台 http://localhost:8080/]]></content>
      <categories>
        <category>skywalking</category>
      </categories>
      <tags>
        <tag>链路跟踪系统</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[yum安装RabbitMQ]]></title>
    <url>%2F2020%2F03%2F08%2F%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97%2FRabbitMQ%2Fyum%E5%AE%89%E8%A3%85RabbitMQ%2F</url>
    <content type="text"><![CDATA[yum安装RabbitMQ 进入/etc/yum.repos.d/ 文件夹 创建rabbitmq-erlang.repo 文件 内容如下： 1234567[rabbitmq-erlang] name=rabbitmq-erlangbaseurl=https://dl.bintray.com/rabbitmq-erlang/rpm/erlang/21/el/7gpgcheck=1gpgkey=https://dl.bintray.com/rabbitmq/Keys/rabbitmq-release-signing-key.ascrepo_gpgcheck=0enabled=1 创建rabbitmq.repo 文件 内容如下： 123456[bintray-rabbitmq-server]name=bintray-rabbitmq-rpmbaseurl=https://dl.bintray.com/rabbitmq/rpm/rabbitmq-server/v3.8.x/el/8/gpgcheck=0repo_gpgcheck=0enabled=1 安装命令 1yum install rabbitmq-server rabbitmq相关命令 开启 1service rabbitmq-server start 关闭 1service rabbitmq-server stop 查看状态 1service rabbitmq-server status 重启 1service rabbitmq-server restart 启用插件页面管理 1rabbitmq-plugins enable rabbitmq_management 创建用户 1rabbitmqctl add_user admin mypassword 赋予权限 12rabbitmqctl set_user_tags admin administratorrabbitmqctl set_permissions -p / admin &quot;.*&quot; &quot;.*&quot; &quot;.*&quot; 浏览器访问http://机器IP:15672打开管理界面，使用上一步配置好的admin账号登录 访问不到管理后台的原因 如果无法访问到界面，那么有可能是服务器防火墙没有关闭的问题，解决这个问题有良好总方式： 关闭防火墙或者配置15672和5672 端口可以通过 关闭防火墙：systemctl stop firewalld 或者禁用 systemctl disable firewalld 开发或者测试环境。 配置防火墙端口： 15672（ui管理端口）：firewall-cmd –add-port=15672/tcp –permanent 5672（远程连接端口）：firewall-cmd –add-port=5672/tcp –permanent 最后 执行 firewall-cmd –reload]]></content>
      <categories>
        <category>RabbitMQ</category>
      </categories>
      <tags>
        <tag>消息队列</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java 工程师要学什么]]></title>
    <url>%2F2019%2F07%2F28%2F%E5%AD%A6%E4%B9%A0%E5%86%85%E5%AE%B9%E6%B1%87%E6%80%BB%2FJava%20%E5%B7%A5%E7%A8%8B%E5%B8%88%E8%A6%81%E5%AD%A6%E4%BB%80%E4%B9%88%2F</url>
    <content type="text"><![CDATA[Java 工程师要学什么参考语法：https://www.jianshu.com/p/af48cc77b57a 作为一个 java 程序员，总是觉得要学的东西非常多，但又不知从何学起从何入手。所以针对这种情况，我整理了一个比较系统的知识学习框架。希望可以帮到和我有同样烦恼的同学。希望一下的内容，可以你在脑海中有一个整体的学习框架。 数据结构和算法数据结构和算法，特别是算法，是程序员的一个分水岭。可能在刚开始工作的时候不是很明显，但是当你有一定的工作经验之后，你接触的业务越来越多也越来复杂。这时候，假如你会算法，就可以派上用场了；假如你不会算法，那么别人一句话的代码，你就可能要写上好几个 if else 了。大家都知道，算法写出来程序效率是比你写很多if else 要高很多倍的。进一步讲，如果你能算法用到实际的业务中，说明你的思维逻辑也非常不错。这在我们公司，对于 review code 的人来说，是一个有很高的加分项。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879graph LRjichu(&quot;基础&quot;)jichu--&gt;shujujichu(&quot;数据结构&quot;)shujujichu--&gt;duilie(&quot;队列&quot;)shujujichu--&gt;zhan(&quot;栈&quot;)shujujichu--&gt;lianbiao(&quot;链表&quot;)shujujichu--&gt;shuzu(&quot;数组&quot;)shujujichu--&gt;zidian(&quot;字典&quot;)shujujichu--&gt;tu(&quot;图&quot;)shujujichu--&gt;erchashu(&quot;二叉树&quot;)shujujichu--&gt;pinghengerchashu(&quot;平衡二叉树&quot;)shujujichu--&gt;hongheishu(&quot;红黑树&quot;)shujujichu--&gt;Bshu(&quot;B+树&quot;)shujujichu--&gt;LSMshu(&quot;LSM树&quot;)jichu--&gt;jibensuanfa(&quot;基本算法&quot;)jibensuanfa--&gt;paixusuanfa(&quot;排序算法&quot;)paixusuanfa--&gt;bijiaopaixu(&quot;比较类排序&quot;)bijiaopaixu--&gt;jiaohuanpaixu(&quot;交换排序&quot;)jiaohuanpaixu--&gt;maopaopaixu(&quot;冒泡排序&quot;)jiaohuanpaixu--&gt;kuasupaixu(&quot;快速排序&quot;)bijiaopaixu--&gt;charupaixu(&quot;插入排序&quot;)charupaixu--&gt;jiandancharupaixu(&quot;简单插入排序&quot;)charupaixu--&gt;xierpaixu(&quot;希尔排序&quot;)bijiaopaixu--&gt;xuanzepaixu(&quot;选择排序&quot;)xuanzepaixu--&gt;jiandanxuanzepaixu(&quot;简单选择排序&quot;)xuanzepaixu--&gt;duipaixu(&quot;堆排序&quot;)bijiaopaixu--&gt;guibingpaixu(&quot;归并排序&quot;)guibingpaixu--&gt;erluguibingpaixu(&quot;二路归并排序&quot;)guibingpaixu--&gt;duoluguibingpaixu(&quot;多路归并排序&quot;)paixusuanfa--&gt;feibijiaopaixu(&quot;非比较类排序&quot;)feibijiaopaixu--&gt;jishupaixu(&quot;计数排序&quot;)feibijiaopaixu--&gt;tongpaixu(&quot;桶排序&quot;)feibijiaopaixu--&gt;jiishupaixu(&quot;基数排序&quot;)jibensuanfa--&gt;chazhaosuanfa(&quot;查找算法&quot;)jibensuanfa--&gt;diguisuanfa(&quot;递归算法&quot;)jibensuanfa--&gt;tanxinsuanfa(&quot;贪心算法&quot;)jibensuanfa--&gt;dongtaiguihua(&quot;动态规划&quot;)jibensuanfa--&gt;fenzhisuanfa(&quot;分治算法&quot;)jibensuanfa--&gt;huisusuanfa(&quot;回溯算法&quot;)jibensuanfa--&gt;jianzhisuanfa(&quot;剪枝算法&quot;)jibensuanfa--&gt;tusuanfa(&quot;图算法&quot;)jibensuanfa--&gt;dikesitelasuanfa(&quot;狄克斯特拉算法&quot;)jibensuanfa--&gt;bulongguolvqi(&quot;布隆过滤器算法&quot;)jibensuanfa--&gt;xuehuaxuanfa(&quot;雪花算法&quot;)jibensuanfa--&gt;Kzuijinling(&quot;K最近邻算法&quot;)jichu--&gt;shujukujichu(&quot;数据库基础&quot;)shujukujichu--&gt; MySQl(&quot;MySQL&quot;)MySQl--&gt; innoDB(&quot;InnoDB&quot;)MySQl--&gt; MyISSAM(&quot;MyISSAM&quot;)MySQl--&gt; zifiji(&quot;字符集&quot;)MySQl--&gt; suoyin(&quot;索引&quot;)MySQl--&gt; shujukufanshi(&quot;数据库范式&quot;)MySQl--&gt; shujukugelijibie(&quot;数据库隔离级别&quot;)MySQl--&gt; huandu(&quot;幻读&quot;)MySQl--&gt; mvcc(&quot;MVCC&quot;)MySQl--&gt; shujukusuo(&quot;数据库锁&quot;)shujukujichu--&gt; Oracle(&quot;Oracle&quot;)jichu--&gt;wangluojichu(&quot;网路基础&quot;)wangluojichu--&gt;wangluoxieyi(&quot;网络协议&quot;)wangluojichu--&gt;https(&quot;HTTP/S&quot;)wangluojichu--&gt;wangluozhangtai(&quot;网络状态&quot;)wangluojichu--&gt;changlianjie(&quot;长连接&quot;)wangluojichu--&gt;wangluowoshou(&quot;网络握手&quot;)wangluojichu--&gt;huadongchuankou(&quot;滑动窗口&quot;)wangluojichu--&gt;wangluocanshu(&quot;网络参数&quot;)wangluojichu--&gt;tongxingmoxing(&quot;通信模型&quot;)wangluojichu--&gt;xuliehua(&quot;序列化&quot;)wangluojichu--&gt;pacong(&quot;爬虫&quot;)wangluojichu--&gt;netty(&quot;Netty&quot;)jichu--&gt;caozuoxitong(&quot;操作系统&quot;)caozuoxitong--&gt;cpu(&quot;CPU&quot;)caozuoxitong--&gt;neicun(&quot;内存&quot;)caozuoxitong--&gt;wangluo(&quot;网络&quot;)caozuoxitong--&gt;io(&quot;I/O&quot;)caozuoxitong--&gt;jincheng(&quot;进程&quot;)caozuoxitong--&gt;diaodusuanfa(&quot;调度算法&quot;)caozuoxitong--&gt;zhengze(&quot;正则&quot;)caozuoxitong--&gt;Shellbiancheng(&quot;Shell 编程&quot;)caozuoxitong--&gt;jisuanjizuchengjigou(&quot;计算机组成结构&quot;) 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162graph LRjavajichu(&quot;Java 基础&quot;)javajichu--&gt;jdk(&quot;JDK&quot;)jdk--&gt;jihe(&quot;集合&quot;)jdk--&gt;wenjiancaozuo(&quot;文件操作&quot;)jdk--&gt;duoxiancheng(&quot;多线程&quot;)jdk--&gt;nio(&quot;NIO&quot;)jdk--&gt;fanshe(&quot;反射&quot;)jdk--&gt;lamdba(&quot;Lambda&quot;)jdk--&gt;jbdc(&quot;JDBC&quot;)jdk--&gt;jndi(&quot;JNDI&quot;)jdk--&gt;rmi(&quot;RMI&quot;)jdk--&gt;jmx(&quot;JMX&quot;)javajichu--&gt;jvm(&quot;JVM&quot;)jvm--&gt;lajishoujiqi(&quot;垃圾收集器&quot;)jvm--&gt;leijiazaijizhi(&quot;类加载机制&quot;)jvm--&gt;classwenjianjiegou(&quot;Class 文件结构&quot;)jvm--&gt;canshutiaoyou(&quot;参数调优&quot;)jvm--&gt;zijiema(&quot;字节码&quot;)jvm--&gt;suoshengji(&quot;锁升级&quot;)jvm--&gt;jvmbingfa(&quot;JVM 并发&quot;)jvm--&gt;jit(&quot;JIT&quot;)jvm--&gt;jmm(&quot;JMM&quot;)jvm--&gt;zhichigongju(&quot;支持工具&quot;)javajichu--&gt;bingfabiancheng(&quot;并发编程&quot;)bingfabiancheng--&gt;xianchenganquan(&quot;线程安全&quot;)bingfabiancheng--&gt;xianchenchi(&quot;线程池&quot;)bingfabiancheng--&gt;aqs(&quot;AQS&quot;)bingfabiancheng--&gt;suo(&quot;锁&quot;)bingfabiancheng--&gt;leguanbeiguansuo(&quot;乐观/悲观锁&quot;)bingfabiancheng--&gt;fei/gongpingsuo(&quot;非/公平锁&quot;)bingfabiancheng--&gt;Concurrentgongjubao(&quot;Concurrent 工具包&quot;)bingfabiancheng--&gt;wusuoduilie(&quot;无锁队列&quot;)bingfabiancheng--&gt;ABA(&quot;ABA 问题&quot;)bingfabiancheng--&gt;weigongxiang(&quot;伪共享&quot;)bingfabiancheng--&gt;sisuo(&quot;死锁&quot;)javajichu--&gt;shejimoshi(&quot;设计模式&quot;)javajichu--&gt;ssm(&quot;SSM&quot;)ssm--&gt;ioc(&quot;IOC&quot;)ssm--&gt;aop(&quot;AOP&quot;)ssm--&gt;springboot(&quot;SpringBoot&quot;)ssm--&gt;mybaties(&quot;Mybaties&quot;)ssm--&gt;tomcat(&quot;Tomcat&quot;)ssm--&gt;rizhizujian(&quot;日志组件&quot;)ssm--&gt;gofshejimoshi(&quot;GoF设计模式23种&quot;)ssm--&gt;uml(&quot;UML&quot;)ssm--&gt;ddd(&quot;DDD&quot;)ssm--&gt;restful(&quot;Restful&quot;)javajichu--&gt;guzhangpaicha(&quot;故障排查&quot;)guzhangpaicha--&gt;neicunyichupaicha(&quot;内存溢出排查&quot;)guzhangpaicha--&gt;duiwaineicunpaicha(&quot;堆外内存排查&quot;)guzhangpaicha--&gt;wangluopaicha(&quot;网络排查&quot;)guzhangpaicha--&gt;iopaicha(&quot;I/O排查&quot;)guzhangpaicha--&gt;gaofuzaipaicha(&quot;高负载排查&quot;)guzhangpaicha--&gt;liuliangxianzhi(&quot;流量限制&quot;)javajichu--&gt;xingnengyouhua(&quot;性能优化&quot;)xingnengyouhua--&gt;neihecanshuyouhua(&quot;内核参数优化&quot;)xingnengyouhua--&gt;jvmyouhua(&quot;JVM 优化&quot;)xingnengyouhua--&gt;wangluocanshuyouhua(&quot;网络参数优化&quot;)xingnengyouhua--&gt;shiwuyouhua(&quot;事务优化&quot;)xingnengyouhua--&gt;shujukuyouhua(&quot;数据库优化&quot;)xingnengyouhua--&gt;chihua(&quot;池化&quot;) 123456789101112131415graph LRfenbushi(&quot;分布式&quot;)fenbushi--&gt; caphebase(&quot;CAP/BASE&quot;)fenbushi--&gt; raft(&quot;Raft&quot;)fenbushi--&gt; paxos(&quot;Paxos&quot;)fenbushi--&gt; gossip(&quot;Gossip&quot;)fenbushi--&gt; zookeeper(&quot;Zookeeper&quot;)fenbushi--&gt; liangjieduanjiaohuan(&quot;两阶段交换&quot;)fenbushi--&gt; tcc(&quot;TCC&quot;)fenbushi--&gt; fenpian(&quot;分片&quot;)fenbushi--&gt; fuben(&quot;副本&quot;)fenbushi--&gt; quorunhenwr(&quot;Quorum/NWR&quot;)fenbushi--&gt; mideng(&quot;幂等&quot;)fenbushi--&gt; yizhixinghash(&quot;一致性哈希&quot;)fenbushi--&gt; idchengchengqi(&quot;ID生成器&quot;) 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849graph LRfenbushixitong(&quot;分布式系统&quot;)fenbushixitong--&gt;zhongjianjian(&quot;中间件&quot;)zhongjianjian--&gt;fenkufenbiao(&quot;分库分表&quot;)fenkufenbiao--&gt;shardingJDBC(&quot;ShardingJDBC&quot;)shardingJDBC--&gt;qierenchengci(&quot;切人层次&quot;)shardingJDBC--&gt;cuizhichaifen(&quot;垂直拆分&quot;)shardingJDBC--&gt;shuipingchaifen(&quot;水平拆分&quot;)shardingJDBC--&gt;duxiefenli(&quot;读写分离&quot;)shardingJDBC--&gt;chaifenliucheng(&quot;拆分流程&quot;)shardingJDBC--&gt;shujutongbu(&quot;数据同步&quot;)shardingJDBC--&gt;butingjiqiehuan(&quot;不停机切换&quot;)shardingJDBC--&gt;hahefailover(&quot;HA &amp; FailOver&quot;)zhongjianjian--&gt;xiaoxiduilie(&quot;消息队列&quot;)xiaoxiduilie--&gt;kafka(&quot;Kafka&quot;)kafka--&gt;mqyingyongtu(&quot;MQ 用途&quot;)kafka--&gt;jibenganian(&quot;基本概念&quot;)kafka--&gt;shengchanzhezuijiashijian(&quot;生产者最佳实践&quot;)kafka--&gt;xiaofeizhezuijiashijian(&quot;消费者最佳实践&quot;)kafka--&gt;jiquangaokeyong(&quot;集群高可用&quot;)kafka--&gt;ackjibie(&quot;ACK 级别&quot;)kafka--&gt;shiwuxiaoxi(&quot;事务消息&quot;)kafka--&gt;jiankongjingbao(&quot;监控警报&quot;)kafka--&gt;KafkaStream(&quot;KafkaStream&quot;)xiaoxiduilie--&gt;RabbitMQ(&quot;RabbitMQ&quot;)zhongjianjian--&gt;huancun(&quot;缓存&quot;)huancun--&gt;redis(&quot;Redis&quot;)redis--&gt;leixing(&quot;String/Hash/List/Set/Zset&quot;)redis--&gt;jiqungaokeyong(&quot;集群高可用&quot;)redis--&gt;zuijiashijian(&quot;最佳实践&quot;)redis--&gt;qianwanjipaihanbang(&quot;千万级排行榜&quot;)redis--&gt;fenbushisuo(&quot;分布式锁&quot;)redis--&gt;xianliu(&quot;限流&quot;)redis--&gt;huancuntongbu(&quot;缓存同步&quot;)redis--&gt;shiyongguifan(&quot;使用规范&quot;)fenbushixitong--&gt;weifuwu(&quot;微服务&amp;中间件&quot;)weifuwu--&gt;springcloud(&quot;SpringCloud&quot;)weifuwu--&gt;zhucezhongxin(&quot;注册中心&quot;)weifuwu--&gt;wangguan(&quot;网关&quot;)weifuwu--&gt;feignrpc(&quot;Feign rpc&quot;)weifuwu--&gt;rongduanhexianliu(&quot;熔断&amp;限流&quot;)weifuwu--&gt;fuzaijunheng(&quot;负载均衡&quot;)weifuwu--&gt;huidu(&quot;灰度&quot;)weifuwu--&gt;jiankongbaojing(&quot;监控报警&quot;)weifuwu--&gt;rizhishouji(&quot;日志收集 ELKB&quot;)weifuwu--&gt;diaoyonglian(&quot;调用链&quot;)weifuwu--&gt;peizhizhongxin(&quot;配置中心&quot;)weifuwu--&gt;wendanghezhili(&quot;文档 &amp; 治理&quot;)weifuwu--&gt;job(&quot;任务调度 Job&quot;) 123456789101112131415161718192021222324graph LRyunwei(&quot;运维&quot;)yunwei--&gt;yunweijichu(&quot;运维基础&quot;)yunweijichu--&gt;ansiable(&quot;Ansiable&quot;)yunweijichu--&gt;xunihua(&quot;虚拟化&quot;)yunweijichu--&gt;rongqi(&quot;容器&quot;)yunweijichu--&gt;zidonghuajiaoben(&quot;自动化脚本&quot;)yunweijichu--&gt;cihecd(&quot;CI/CD&quot;)yunweijichu--&gt;jiankongxitong(&quot;监控系统&quot;)yunweijichu--&gt;devops(&quot;DevOps&quot;)yunweijichu--&gt;liuchengguifan(&quot;流程规范&quot;)yunweijichu--&gt;chengbenguanli(&quot;成本管理&quot;)yunweijichu--&gt;nginx(&quot;nginx&quot;)yunweijichu--&gt;k8s(&quot;k8s&quot;)yunwei--&gt;anquan(&quot;安全&quot;)anquan--&gt;sso(&quot;SSO&quot;)anquan--&gt;sqlzhuru(&quot;SQl注入&quot;)anquan--&gt;xss(&quot;XSS&quot;)anquan--&gt;csrf(&quot;CSRF&quot;)anquan--&gt;ddos(&quot;DDos&quot;)anquan--&gt;jiamijiemi(&quot;加密解密&quot;)anquan--&gt;zhengshutixi(&quot;证书体系&quot;)anquan--&gt;wangluogeli(&quot;网络隔离&quot;)anquan--&gt;oauth(&quot;OAuth&quot;) 在这里说明一下，本次的内容整理参考了 https://github.com/sayhiai/javaok 文章的很多内容，因为这里提到的很多内容是我有接触过的。那么我就以这个框架为基础，展开来整理。]]></content>
      <categories>
        <category>知识总结</category>
      </categories>
      <tags>
        <tag>Java 知识总结</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[线程面试题]]></title>
    <url>%2F2019%2F05%2F12%2F%E9%9D%A2%E8%AF%95%E6%80%BB%E7%BB%93%2F%E7%BA%BF%E7%A8%8B%E9%9D%A2%E8%AF%95%E9%A2%98%2F</url>
    <content type="text"><![CDATA[1、多线程有什么用？ 1）发挥多核CPU的优势 随着工业的进步，现在的笔记本、台式机乃至商用的应用服务器至少也都是双核的，4核、8核甚至16核的也都不少见，如果是单线程的程序，那么在双核CPU上就浪费了50%，在4核CPU上就浪费了75%。单核CPU上所谓的”多线程”那是假的多线程，同一时间处理器只会处理一段逻辑，只不过线程之间切换得比较快，看着像多个线程”同时”运行罢了。多核CPU上的多线程才是真正的多线程，它能让你的多段逻辑同时工作，多线程，可以真正发挥出多核CPU的优势来，达到充分利用CPU的目的。 2）防止阻塞 从程序运行效率的角度来看，单核CPU不但不会发挥出多线程的优势，反而会因为在单核CPU上运行多线程导致线程上下文的切换，而降低程序整体的效率。但是单核CPU我们还是要应用多线程，就是为了防止阻塞。试想，如果单核CPU使用单线程，那么只要这个线程阻塞了，比方说远程读取某个数据吧，对端迟迟未返回又没有设置超时时间，那么你的整个程序在数据返回回来之前就停止运行了。多线程可以防止这个问题，多条线程同时运行，哪怕一条线程的代码执行读取数据阻塞，也不会影响其它任务的执行。 3）便于建模 这是另外一个没有这么明显的优点了。假设有一个大的任务A，单线程编程，那么就要考虑很多，建立整个程序模型比较麻烦。但是如果把这个大的任务A分解成几个小任务，任务B、任务C、任务D，分别建立程序模型，并通过多线程分别运行这几个任务，那就简单很多了。 2、创建线程的方式 继承 Thread 类创建线程 实现 Runnable 接口创建线程 实现 Callable 和 Future 创建线程 比较： 实现 Runnable 接口，还可以继承其他类，在这种方式下多线程共享一个 target 对象，所以适合多个相同的线程来处理同一份资源的情况；但是它编程稍微复杂，若想访问当前线程必须使用 Thread.currentThread() 方法。 继承 Thread 类，代码编写简单，当前 Thread 可用 this 获取，但是它不能再继承其他类。 3、start()方法和run()方法的区别 start() 可以启动一个新线程，run()不能 start()不能被重复调用，run()可以 start()中的run代码可以不执行完就继续执行下面的代码，即进行了线程切换。直接调用run方法必须等待其代码全部执行完才能继续执行下面的代码。 start() 实现了多线程，run()没有实现多线程。 4、Runnable接口和Callable接口的区别 1)Runnable是自从java1.1就有了，而Callable是1.5之后才加上去的 2)Callable规定的方法是call(),Runnable规定的方法是run() 3)Callable的任务执行后可返回值，而Runnable的任务是不能返回值(是void) 4)call方法可以抛出异常，run方法不可以 5)运行Callable任务可以拿到一个Future对象，表示异步计算的结果。它提供了检查计算是否完成的方法，以等待计算的完成，并检索计算的结果。通过Future对象可以了解任务执行情况，可取消任务的执行，还可获取执行结果。 6)加入线程池运行，Runnable使用ExecutorService的execute方法，Callable使用submit方法。Callable接口也是位于java.util.concurrent包中。Callable接口的定义为： 1234&gt;public interface Callable&lt;V&gt;&#123;&gt; V call() throws Exception; &gt;&#125;&gt; Callable中的call()方法类似Runnable的run()方法，就是前者有返回值，后者没有。当将一个Callable的对象传递给ExecutorService的submit方法，则该call方法自动在一个线程上执行，并且会返回执行结果Future对象。同样，将Runnable的对象传递给ExecutorService的submit方法，则该run方法自动在一个线程上执行，并且会返回执行结果Future对象，但是在该Future对象上调用get方法，将返回null。 5、CyclicBarrier和CountDownLatch的区别 两个看上去有点像的类，都在java.util.concurrent下，都可以用来表示代码运行到某个点上，二者的区别在于： CyclicBarrier的某个线程运行到某个点上之后，该线程即停止运行，直到所有的线程都到达了这个点，所有线程才重新运行；CountDownLatch则不是，某线程运行到某个点上之后，只是给某个数值-1而已，该线程继续运行。 CyclicBarrier只能唤起一个任务，CountDownLatch可以唤起多个任务。 CyclicBarrier可重用，CountDownLatch不可重用，计数值为0该CountDownLatch就不可再用了。 6、volatile的作用和原理 作用： volatile保证变量对所有线程的可见性：当volatile变量被修改，新值对所有线程会立即更新。或者理解为多线程环境下使用volatile修饰的变量的值一定是最新的。 jdk1.5以后volatile完全避免了指令重排优化。 7、什么是线程安全 又是一个理论的问题，各式各样的答案有很多，我给出一个个人认为解释地最好的：如果你的代码在多线程下执行和在单线程下执行永远都能获得一样的结果，那么你的代码就是线程安全的。 这个问题有值得一提的地方，就是线程安全也是有几个级别的： 1）不可变 像String、Integer、Long这些，都是final类型的类，任何一个线程都改变不了它们的值，要改变除非新创建一个，因此这些不可变对象不需要任何同步手段就可以直接在多线程环境下使用 2）绝对线程安全 不管运行时环境如何，调用者都不需要额外的同步措施。要做到这一点通常需要付出许多额外的代价，Java中标注自己是线程安全的类，实际上绝大多数都不是线程安全的，不过绝对线程安全的类，Java中也有，比方说CopyOnWriteArrayList、CopyOnWriteArraySet 3）相对线程安全 相对线程安全也就是我们通常意义上所说的线程安全，像Vector这种，add、remove方法都是原子操作，不会被打断，但也仅限于此，如果有个线程在遍历某个Vector、有个线程同时在add这个Vector，99%的情况下都会出现ConcurrentModificationException，也就是fail-fast机制。 4）线程非安全 这个就没什么好说的了，ArrayList、LinkedList、HashMap等都是线程非安全的类，点击这里了解为什么不安全。 8、Java中如何获取到线程dump文件 死循环、死锁、阻塞、页面打开慢等问题，打线程dump是最好的解决问题的途径。所谓线程dump也就是线程堆栈，获取到线程堆栈有两步： 1）获取到线程的pid，可以通过使用jps命令，在Linux环境下还可以使用ps -ef | grep java 2）打印线程堆栈，可以通过使用jstack pid命令，在Linux环境下还可以使用kill -3 pid 另外提一点，Thread类提供了一个getStackTrace()方法也可以用于获取线程堆栈。这是一个实例方法，因此此方法是和具体线程实例绑定的，每次获取获取到的是具体某个线程当前运行的堆栈。 9、一个线程如果出现了运行时异常会怎么样 如果这个异常没有被捕获的话，这个线程就停止执行了。另外重要的一点是：如果这个线程持有某个某个对象的监视器，那么这个对象监视器会被立即释放 10、如何在两个线程之间共享数据 通过在线程之间共享对象就可以了，然后通过wait/notify/notifyAll、await/signal/signalAll进行唤起和等待，比方说阻塞队列BlockingQueue就是为线程之间共享数据而设计的 11、sleep方法和wait方法有什么区别 这个问题常问，sleep方法和wait方法都可以用来放弃CPU一定的时间，不同点在于如果线程持有某个对象的监视器，sleep方法不会放弃这个对象的监视器，wait方法会放弃这个对象的监视器 12、生产者消费者模型的作用是什么 这个问题很理论，但是很重要： 1）通过平衡生产者的生产能力和消费者的消费能力来提升整个系统的运行效率，这是生产者消费者模型最重要的作用 2）解耦，这是生产者消费者模型附带的作用，解耦意味着生产者和消费者之间的联系少，联系越少越可以独自发展而不需要收到相互的制约 13、ThreadLocal有什么用 简单说ThreadLocal就是一种以空间换时间的做法，在每个Thread里面维护了一个以开地址法实现的ThreadLocal.ThreadLocalMap，把数据进行隔离，数据不共享，自然就没有线程安全方面的问题了 14、为什么wait()方法和notify()/notifyAll()方法要在同步块中被调用 这是JDK强制的，wait()方法和notify()/notifyAll()方法在调用前都必须先获得对象的锁 15、wait()方法和notify()/notifyAll()方法在放弃对象监视器时有什么区别 wait()方法和notify()/notifyAll()方法在放弃对象监视器的时候的区别在于：wait()方法立即释放对象监视器，notify()/notifyAll()方法则会等待线程剩余代码执行完毕才会放弃对象监视器。 16、为什么要使用线程池 避免频繁地创建和销毁线程，达到线程对象的重用。另外，使用线程池还可以根据项目灵活地控制并发的数目。点击这里学习线程池详解。 17、怎么检测一个线程是否持有对象监视器 我也是在网上看到一道多线程面试题才知道有方法可以判断某个线程是否持有对象监视器：Thread类提供了一个holdsLock(Object obj)方法，当且仅当对象obj的监视器被某条线程持有的时候才会返回true，注意这是一个static方法，这意味着”某条线程”指的是当前线程。 18、synchronized和ReentrantLock的区别 synchronized是和if、else、for、while一样的关键字，ReentrantLock是类，这是二者的本质区别。既然ReentrantLock是类，那么它就提供了比synchronized更多更灵活的特性，可以被继承、可以有方法、可以有各种各样的类变量，ReentrantLock比synchronized的扩展性体现在几点上： （1）ReentrantLock可以对获取锁的等待时间进行设置，这样就避免了死锁 （2）ReentrantLock可以获取各种锁的信息 （3）ReentrantLock可以灵活地实现多路通知 另外，二者的锁机制其实也是不一样的。ReentrantLock底层调用的是Unsafe的park方法加锁，synchronized操作的应该是对象头中mark word，这点我不能确定。 19、ConcurrentHashMap的并发度是什么 ConcurrentHashMap的并发度就是segment的大小，默认为16，这意味着最多同时可以有16条线程操作ConcurrentHashMap，这也是ConcurrentHashMap对Hashtable的最大优势，任何情况下，Hashtable能同时有两条线程获取Hashtable中的数据吗？ 20、ReadWriteLock是什么 首先明确一下，不是说ReentrantLock不好，只是ReentrantLock某些时候有局限。如果使用ReentrantLock，可能本身是为了防止线程A在写数据、线程B在读数据造成的数据不一致，但这样，如果线程C在读数据、线程D也在读数据，读数据是不会改变数据的，没有必要加锁，但是还是加锁了，降低了程序的性能。 因为这个，才诞生了读写锁ReadWriteLock。ReadWriteLock是一个读写锁接口，ReentrantReadWriteLock是ReadWriteLock接口的一个具体实现，实现了读写的分离，读锁是共享的，写锁是独占的，读和读之间不会互斥，读和写、写和读、写和写之间才会互斥，提升了读写的性能。 21、FutureTask是什么 这个其实前面有提到过，FutureTask表示一个异步运算的任务。FutureTask里面可以传入一个Callable的具体实现类，可以对这个异步运算的任务的结果进行等待获取、判断是否已经完成、取消任务等操作。当然，由于FutureTask也是Runnable接口的实现类，所以FutureTask也可以放入线程池中。 22、Linux环境下如何查找哪个线程使用CPU最长 这是一个比较偏实践的问题，这种问题我觉得挺有意义的。可以这么做： （1）获取项目的pid，jps或者ps -ef | grep java，这个前面有讲过 （2）top -H -p pid，顺序不能改变 这样就可以打印出当前的项目，每条线程占用CPU时间的百分比。注意这里打出的是LWP，也就是操作系统原生线程的线程号，我笔记本山没有部署Linux环境下的Java工程，因此没有办法截图演示，网友朋友们如果公司是使用Linux环境部署项目的话，可以尝试一下。 使用”top -H -p pid”+”jps pid”可以很容易地找到某条占用CPU高的线程的线程堆栈，从而定位占用CPU高的原因，一般是因为不当的代码操作导致了死循环。 最后提一点，”top -H -p pid”打出来的LWP是十进制的，”jps pid”打出来的本地线程号是十六进制的，转换一下，就能定位到占用CPU高的线程的当前线程堆栈了。 23、Java编程写一个会导致死锁的程序 第一次看到这个题目，觉得这是一个非常好的问题。很多人都知道死锁是怎么一回事儿：线程A和线程B相互等待对方持有的锁导致程序无限死循环下去。当然也仅限于此了，问一下怎么写一个死锁的程序就不知道了，这种情况说白了就是不懂什么是死锁，懂一个理论就完事儿了，实践中碰到死锁的问题基本上是看不出来的。 真正理解什么是死锁，这个问题其实不难，几个步骤： 1）两个线程里面分别持有两个Object对象：lock1和lock2。这两个lock作为同步代码块的锁； 2）线程1的run()方法中同步代码块先获取lock1的对象锁，Thread.sleep(xxx)，时间不需要太多，50毫秒差不多了，然后接着获取lock2的对象锁。这么做主要是为了防止线程1启动一下子就连续获得了lock1和lock2两个对象的对象锁 3）线程2的run)(方法中同步代码块先获取lock2的对象锁，接着获取lock1的对象锁，当然这时lock1的对象锁已经被线程1锁持有，线程2肯定是要等待线程1释放lock1的对象锁的 这样，线程1”睡觉”睡完，线程2已经获取了lock2的对象锁了，线程1此时尝试获取lock2的对象锁，便被阻塞，此时一个死锁就形成了。代码就不写了，占的篇幅有点多，Java多线程7：死锁这篇文章里面有，就是上面步骤的代码实现。 点击这里提供了一个死锁的案例。 24、怎么唤醒一个阻塞的线程 如果线程是因为调用了wait()、sleep()或者join()方法而导致的阻塞，可以中断线程，并且通过抛出InterruptedException来唤醒它；如果线程遇到了IO阻塞，无能为力，因为IO是操作系统实现的，Java代码并没有办法直接接触到操作系统。 25、不可变对象对多线程有什么帮助 前面有提到过的一个问题，不可变对象保证了对象的内存可见性，对不可变对象的读取不需要进行额外的同步手段，提升了代码执行效率。 26、什么是多线程的上下文切换 多线程的上下文切换是指CPU控制权由一个已经正在运行的线程切换到另外一个就绪并等待获取CPU执行权的线程的过程。 27、如果你提交任务时，线程池队列已满，这时会发生什么 这里区分一下： 1）如果使用的是无界队列LinkedBlockingQueue，也就是无界队列的话，没关系，继续添加任务到阻塞队列中等待执行，因为LinkedBlockingQueue可以近乎认为是一个无穷大的队列，可以无限存放任务 2）如果使用的是有界队列比如ArrayBlockingQueue，任务首先会被添加到ArrayBlockingQueue中，ArrayBlockingQueue满了，会根据maximumPoolSize的值增加线程数量，如果增加了线程数量还是处理不过来，ArrayBlockingQueue继续满，那么则会使用拒绝策略RejectedExecutionHandler处理满了的任务，默认是AbortPolicy 28、Java中用到的线程调度算法是什么 抢占式。一个线程用完CPU之后，操作系统会根据线程优先级、线程饥饿情况等数据算出一个总的优先级并分配下一个时间片给某个线程执行。 29、Thread.sleep(0)的作用是什么 这个问题和上面那个问题是相关的，我就连在一起了。由于Java采用抢占式的线程调度算法，因此可能会出现某条线程常常获取到CPU控制权的情况，为了让某些优先级比较低的线程也能获取到CPU控制权，可以使用Thread.sleep(0)手动触发一次操作系统分配时间片的操作，这也是平衡CPU控制权的一种操作。 30、什么是自旋 很多synchronized里面的代码只是一些很简单的代码，执行时间非常快，此时等待的线程都加锁可能是一种不太值得的操作，因为线程阻塞涉及到用户态和内核态切换的问题。既然synchronized里面的代码执行得非常快，不妨让等待锁的线程不要被阻塞，而是在synchronized的边界做忙循环，这就是自旋。如果做了多次忙循环发现还没有获得锁，再阻塞，这样可能是一种更好的策略。 31、什么是Java内存模型 Java内存模型定义了一种多线程访问Java内存的规范。Java内存模型要完整讲不是这里几句话能说清楚的，我简单总结一下Java内存模型的几部分内容： 1）Java内存模型将内存分为了主内存和工作内存。类的状态，也就是类之间共享的变量，是存储在主内存中的，每次Java线程用到这些主内存中的变量的时候，会读一次主内存中的变量，并让这些内存在自己的工作内存中有一份拷贝，运行自己线程代码的时候，用到这些变量，操作的都是自己工作内存中的那一份。在线程代码执行完毕之后，会将最新的值更新到主内存中去 2）定义了几个原子操作，用于操作主内存和工作内存中的变量 3）定义了volatile变量的使用规则 4）happens-before，即先行发生原则，定义了操作A必然先行发生于操作B的一些规则，比如在同一个线程内控制流前面的代码一定先行发生于控制流后面的代码、一个释放锁unlock的动作一定先行发生于后面对于同一个锁进行锁定lock的动作等等，只要符合这些规则，则不需要额外做同步措施，如果某段代码不符合所有的happens-before规则，则这段代码一定是线程非安全的 32、什么是CAS CAS，全称为Compare and Swap，即比较-替换。假设有三个操作数：内存值V、旧的预期值A、要修改的值B，当且仅当预期值A和内存值V相同时，才会将内存值修改为B并返回true，否则什么都不做并返回false。当然CAS一定要volatile变量配合，这样才能保证每次拿到的变量是主内存中最新的那个值，否则旧的预期值A对某条线程来说，永远是一个不会变的值A，只要某次CAS操作失败，永远都不可能成功。更多CAS详情请点击这里学习。 33、什么是乐观锁和悲观锁 1）乐观锁：就像它的名字一样，对于并发间操作产生的线程安全问题持乐观状态，乐观锁认为竞争不总是会发生，因此它不需要持有锁，将比较-替换这两个动作作为一个原子操作尝试去修改内存中的变量，如果失败则表示发生冲突，那么就应该有相应的重试逻辑。 2）悲观锁：还是像它的名字一样，对于并发间操作产生的线程安全问题持悲观状态，悲观锁认为竞争总是会发生，因此每次对某资源进行操作时，都会持有一个独占的锁，就像synchronized，不管三七二十一，直接上了锁就操作资源了。 点击这里了解更多乐观锁与悲观锁详情。 34、什么是AQS 简单说一下AQS，AQS全称为AbstractQueuedSychronizer，翻译过来应该是抽象队列同步器。 如果说java.util.concurrent的基础是CAS的话，那么AQS就是整个Java并发包的核心了，ReentrantLock、CountDownLatch、Semaphore等等都用到了它。AQS实际上以双向队列的形式连接所有的Entry，比方说ReentrantLock，所有等待的线程都被放在一个Entry中并连成双向队列，前面一个线程使用ReentrantLock好了，则双向队列实际上的第一个Entry开始运行。 AQS定义了对双向队列所有的操作，而只开放了tryLock和tryRelease方法给开发者使用，开发者可以根据自己的实现重写tryLock和tryRelease方法，以实现自己的并发功能。 35、单例模式的线程安全性 老生常谈的问题了，首先要说的是单例模式的线程安全意味着：某个类的实例在多线程环境下只会被创建一次出来。单例模式有很多种的写法，我总结一下： 1）饿汉式单例模式的写法：线程安全 2）懒汉式单例模式的写法：非线程安全 3）双检锁单例模式的写法：线程安全 36、Semaphore有什么作用 Semaphore就是一个信号量，它的作用是限制某段代码块的并发数。Semaphore有一个构造函数，可以传入一个int型整数n，表示某段代码最多只有n个线程可以访问，如果超出了n，那么请等待，等到某个线程执行完毕这段代码块，下一个线程再进入。由此可以看出如果Semaphore构造函数中传入的int型整数n=1，相当于变成了一个synchronized了。 37、Hashtable的size()方法中明明只有一条语句”return count”，为什么还要做同步？ 这是我之前的一个困惑，不知道大家有没有想过这个问题。某个方法中如果有多条语句，并且都在操作同一个类变量，那么在多线程环境下不加锁，势必会引发线程安全问题，这很好理解，但是size()方法明明只有一条语句，为什么还要加锁？ 关于这个问题，在慢慢地工作、学习中，有了理解，主要原因有两点： 1）同一时间只能有一条线程执行固定类的同步方法，但是对于类的非同步方法，可以多条线程同时访问。所以，这样就有问题了，可能线程A在执行Hashtable的put方法添加数据，线程B则可以正常调用size()方法读取Hashtable中当前元素的个数，那读取到的值可能不是最新的，可能线程A添加了完了数据，但是没有对size++，线程B就已经读取size了，那么对于线程B来说读取到的size一定是不准确的。而给size()方法加了同步之后，意味着线程B调用size()方法只有在线程A调用put方法完毕之后才可以调用，这样就保证了线程安全性 2）CPU执行代码，执行的不是Java代码，这点很关键，一定得记住。Java代码最终是被翻译成机器码执行的，机器码才是真正可以和硬件电路交互的代码。即使你看到Java代码只有一行，甚至你看到Java代码编译之后生成的字节码也只有一行，也不意味着对于底层来说这句语句的操作只有一个。一句”return count”假设被翻译成了三句汇编语句执行，一句汇编语句和其机器码做对应，完全可能执行完第一句，线程就切换了。 38、线程类的构造方法、静态块是被哪个线程调用的 这是一个非常刁钻和狡猾的问题。请记住：线程类的构造方法、静态块是被new这个线程类所在的线程所调用的，而run方法里面的代码才是被线程自身所调用的。 如果说上面的说法让你感到困惑，那么我举个例子，假设Thread2中new了Thread1，main函数中new了Thread2，那么： 1）Thread2的构造方法、静态块是main线程调用的，Thread2的run()方法是Thread2自己调用的 2）Thread1的构造方法、静态块是Thread2调用的，Thread1的run()方法是Thread1自己调用的 39、同步方法和同步块，哪个是更好的选择 同步块，这意味着同步块之外的代码是异步执行的，这比同步整个方法更提升代码的效率。请知道一条原则：同步的范围越小越好。 借着这一条，我额外提一点，虽说同步的范围越少越好，但是在Java虚拟机中还是存在着一种叫做锁粗化的优化方法，这种方法就是把同步范围变大。这是有用的，比方说StringBuffer，它是一个线程安全的类，自然最常用的append()方法是一个同步方法，我们写代码的时候会反复append字符串，这意味着要进行反复的加锁-&gt;解锁，这对性能不利，因为这意味着Java虚拟机在这条线程上要反复地在内核态和用户态之间进行切换，因此Java虚拟机会将多次append方法调用的代码进行一个锁粗化的操作，将多次的append的操作扩展到append方法的头尾，变成一个大的同步块，这样就减少了加锁–&gt;解锁的次数，有效地提升了代码执行的效率。 40、高并发、任务执行时间短的业务怎样使用线程池？并发不高、任务执行时间长的业务怎样使用线程池？并发高、业务执行时间长的业务怎样使用线程池？ 这是我在并发编程网上看到的一个问题，把这个问题放在最后一个，希望每个人都能看到并且思考一下，因为这个问题非常好、非常实际、非常专业。关于这个问题，个人看法是： 1）高并发、任务执行时间短的业务，线程池线程数可以设置为CPU核数+1，减少线程上下文的切换 2）并发不高、任务执行时间长的业务要区分开看： a）假如是业务时间长集中在IO操作上，也就是IO密集型的任务，因为IO操作并不占用CPU，所以不要让所有的CPU闲下来，可以加大线程池中的线程数目，让CPU处理更多的业务 b）假如是业务时间长集中在计算操作上，也就是计算密集型任务，这个就没办法了，和（1）一样吧，线程池中的线程数设置得少一些，减少线程上下文的切换 c）并发高、业务执行时间长，解决这种类型任务的关键不在于线程池而在于整体架构的设计，看看这些业务里面某些数据是否能做缓存是第一步，增加服务器是第二步，至于线程池的设置，设置参考其他有关线程池的文章。最后，业务执行时间长的问题，也可能需要分析一下，看看能不能使用中间件对任务进行拆分和解耦。]]></content>
      <categories>
        <category>基础面试题</category>
      </categories>
      <tags>
        <tag>基础面试题</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[日志框架面试题]]></title>
    <url>%2F2019%2F05%2F12%2F%E9%9D%A2%E8%AF%95%E6%80%BB%E7%BB%93%2F%E6%97%A5%E5%BF%97%E6%A1%86%E6%9E%B6%E9%9D%A2%E8%AF%95%E9%A2%98%2F</url>
    <content type="text"><![CDATA[更快的执行速度基于我们先前在log4j上的工作，logback 重写了内部的实现，在某些特定的场景上面，甚至可以比之前的速度快上10倍。在保证logback的组件更加快速的同时，同时所需的内存更加少。 充分的测试Logback 历经了几年，数不清小时数的测试。尽管log4j也是测试过的，但是Logback的测试更加充分，跟log4j不在同一个级别。我们认为，这正是人们选择Logback而不是log4j的最重要的原因。人们都希望即使在恶劣的条件下，你的登录依然稳定而可靠。 logback-classic 非常自然的实现了SLF4Jlogback-classic中的登陆类自然的实现了SLF4J。当你使用logback-classic作为底层实现时，涉及到LF4J日记系统的问题你完全不需要考虑。更进一步来说，由于 logback-classic强烈建议使用SLF4J作为客户端日记系统实现，如果需要切换到log4j或者其他，你只需要替换一个jar包即可，不需要去改变那些通过 SLF4J API 实现的代码。这可以大大减少更换日记系统的工作量。 自动重新载入配置文件Logback-classic可以在配置文件被修改后，自动重新载入。这个扫描过程很快，无资源争用，并且可以动态扩展支持在上百个线程之间每秒上百万个调用。它和应用服务器结合良好，并且在JEE环境通用，因为它不会调用创建一个单独的线程来做扫描。 优雅地从I/O错误中恢复FileAppender和它的子类，包括RollingFileAppender，可以优雅的从I/O错误中恢复。所以，如果一个文件服务器临时宕机，你再也不需要重启你的应用，而日志功能就能正常工作。当文件服务器恢复工作，logback相关的appender就会透明地和快速的从上一个错误中恢复。 自动清除旧的日志归档文件通过设置TimeBasedRollingPolicy 或者 SizeAndTimeBasedFNATP的 maxHistory 属性，你就可以控制日志归档文件的最大数量。如果你的回滚策略是每月回滚的，并且你希望保存一年的日志，那么只需简单的设置maxHistory属性为12。对于12个月之前的归档日志文件将被自动清除。 自动压缩归档日志文件RollingFileAppender可以在回滚操作中，自动压缩归档日志文件。压缩通常是异步执行的，所以即使是很大的日志文件，你的应用都不会因此而被阻塞。 谨慎模式在谨慎模式中，在多个JVM中运行的多个FileAppender实例，可以安全的写入统一个日志文件。谨慎模式可以在一定的限制条件下应用于RollingFileAppender。 LilithLilith是logback的一个记录和访问事件查看器。它相当于log4j的 chainsaw，但是Lilith设计的目的是处理大量的日志记录。 配置文件中的条件处理开发者通常需要在不同的目标环境中变换logback的配置文件，例如开发环境，测试环境和生产环境。这些配置文件大体是一样的，除了某部分会有不同。为了避免重复，logback支持配置文件中的条件处理，只需使用,和，那么同一个配置文件就可以在不同的环境中使用了。 过滤Logback拥有远比log4j更丰富的过滤能力。例如，让我们假设，有一个相当重要的商业应用部署在生产环境。考虑到大量的交易数据需要处理，记录级别被设置为WARN，那么只有警告和错误信息才会被记录。现在，想象一下，你在开发环境遇到了一个臭虫，但是在测试平台中却很难发现，因为一些环境之间(生产环境/测试环境)的未知差异。 使用log4j，你只能选择在生产系统中降低记录的级别到DEBUG，来尝试发现问题。但是很不幸，这会生成大量的日志记录，让分析变得困难。更重要的是，多余的日志记录会影响到生产环境的性能。 使用logback，你可以选择保留只所有用户的WARN级别的日志，而除了某个用户，例如Alice，而她就是问题的相关用户。当Alice登录系统，她就会以DEBUG级别被记录，而其他用户仍然是以WARN级别来记录日志。这个功能，可以通过在配置文件的XML中添加4行。请在相关章节中查找MDCFilter SiftingAppenderSiftingAppender是一个全能的追加器。它可以基于任何给定的实时属性分开（或者筛选）日志。例如，SiftingAppender可以基于用户会话分开日志事件，这样，可以为每一个用户建立一个独立的日志文件。 堆栈轨迹信息包含包的数据当logback打印一个异常，堆栈轨迹信息将包含包的相关数据。下面是一个通过 logback-demo 生成的堆栈信息： 1234567891011121314:28:48.835 [btpool0-7] INFO c.q.l.demo.prime.PrimeAction - 99 is not a valid valuejava.lang.Exception: 99 is invalid at ch.qos.logback.demo.prime.PrimeAction.execute(PrimeAction.java:28) [classes/:na] at org.apache.struts.action.RequestProcessor.processActionPerform(RequestProcessor.java:431) [struts-1.2.9.jar:1.2.9] at org.apache.struts.action.RequestProcessor.process(RequestProcessor.java:236) [struts-1.2.9.jar:1.2.9] at org.apache.struts.action.ActionServlet.doPost(ActionServlet.java:432) [struts-1.2.9.jar:1.2.9] at javax.servlet.http.HttpServlet.service(HttpServlet.java:820) [servlet-api-2.5-6.1.12.jar:6.1.12] at org.mortbay.jetty.servlet.ServletHolder.handle(ServletHolder.java:502) [jetty-6.1.12.jar:6.1.12] at ch.qos.logback.demo.UserServletFilter.doFilter(UserServletFilter.java:44) [classes/:na] at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1115) [jetty-6.1.12.jar:6.1.12] at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:361) [jetty-6.1.12.jar:6.1.12] at org.mortbay.jetty.webapp.WebAppContext.handle(WebAppContext.java:417) [jetty-6.1.12.jar:6.1.12] at org.mortbay.jetty.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:230) [jetty-6.1.12.jar:6.1.12] 从上面的信息，你可以发现这个应用使用Struts 1.2.9 而且是使用 jetty 6.1.12部署的。所以，堆栈轨迹信息将快速的告诉读者，关于异常发生的类还有包和包的版本。当你的客户发送一个堆栈轨迹信息给你，作为一个开发人员，你就不需要让他们告诉你他们正在使用的包的版本。这项信息已经包括在堆栈轨迹信息中。详细请参考 “%xThrowable” conversion word. 这项功能可以非常有帮助的说明，有些用户误以为这是IDE的功能。 Log-back-access模块，提供了通过HTTP访问日志的能力，是logback不可或缺的部分最后但绝非最不重要的是，作为logback发布包的一部分，logback-access模块可与Jetty或者Tomcat进行集成，提供了非常丰富而强大的通过HTTP访问日志的功能。因为logback-access模块是logback初期设计方案中的一部分，因此，所有你所喜欢的logback-classic模块所提供的全部特性logback-access同样也具备。 logback的优点 内核重写、测试充分、初始化内存加载更小，这一切让logback性能和log4j相比有诸多倍的提升 logback非常自然地直接实现了slf4j，这个严格来说算不上优点，只是这样，再理解slf4j的前提下会很容易理解logback，也同时很容易用其他日志框架替换logback logback有比较齐全的200多页的文档 logback当配置文件修改了，支持自动重新加载配置文件，扫描过程快且安全，它并不需要另外创建一个扫描线程 支持自动去除旧的日志文件，可以控制已经产生日志文件的最大数量 logback加载我们简单分析一下logback加载过程，当我们使用logback-classic.jar时，应用启动，那么logback会按照如下顺序进行扫描： 在系统配置文件System Properties中寻找是否有logback.configurationFile对应的value 在classpath下寻找是否有logback.groovy（即logback支持groovy与xml两种配置方式） 在classpath下寻找是否有logback-test.xml 在classpath下寻找是否有logback.xml 以上任何一项找到了，就不进行后续扫描，按照对应的配置进行logback的初始化，具体代码实现可见ch.qos.logback.classic.util.ContextInitializer类的findURLOfDefaultConfigurationFile方法。 当所有以上四项都找不到的情况下，logback会调用ch.qos.logback.classic.BasicConfigurator的configure方法，构造一个ConsoleAppender用于向控制台输出日志，默认日志输出格式为”%d{HH:mm:ss.SSS} [%thread] %-5level %logger{36} - %msg%n”。 logback的configurationlogback的重点应当是Appender、Logger、Pattern，在这之前先简单了解一下logback的，只有三个属性： scan：当scan被设置为true时，当配置文件发生改变，将会被重新加载，默认为true scanPeriod：检测配置文件是否有修改的时间间隔，如果没有给出时间单位，默认为毫秒，当scan=true时这个值生效，默认时间间隔为1分钟 debug：当被设置为true时，将打印出logback内部日志信息，实时查看logback运行信息，默认为false 与先从最基本的与开始。 用来设置某一个包或者具体某一个类的日志打印级别、以及指定。可以包含零个或者多个元素，标识这个appender将会添加到这个logger。仅有一个name属性、一个可选的level属性和一个可选的additivity属性： name：用来指定受此logger约束的某一个包或者具体的某一个类 level：用来设置打印级别，五个常用打印级别从低至高依次为TRACE、DEBUG、INFO、WARN、ERROR，如果未设置此级别，那么当前logger会继承上级的级别 additivity：是否向上级logger传递打印信息，默认为true 也是元素，但是它是根logger，只有一个level属性，因为它的name就是root。]]></content>
      <categories>
        <category>基础面试题</category>
      </categories>
      <tags>
        <tag>基础面试题</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Jdk1.7与 jdk1.8的区别和最新的特征]]></title>
    <url>%2F2019%2F05%2F12%2F%E9%9D%A2%E8%AF%95%E6%80%BB%E7%BB%93%2FJdk1.7%E4%B8%8E%20jdk1.8%E7%9A%84%E5%8C%BA%E5%88%AB%E5%92%8C%E6%9C%80%E6%96%B0%E7%9A%84%E7%89%B9%E5%BE%81%2F</url>
    <content type="text"><![CDATA[jdk7的新特性jdk7的新特性方面主要有下面几方面的增强 二进制变量的表示 二进制变量的表示,支持将整数类型用二进制来表示，用0b开头。 所有整数int、short、long、byte都可以用二进制表示： 12&gt;byte aByte = (byte) 0b00100001;&gt; Switch语句支持String类型Try-with-resource语句参考博客：try-with-resources语句 try-with-resources语句是一种声明了一种或多种资源的try语句。资源是指在程序用完了之后必须要关闭的对象。try-with-resources语句保证了每个声明了的资源在语句结束的时候都会被关闭。任何实现了java.lang.AutoCloseable接口的对象，和实现了java.io.Closeable接口的对象，都可以当做资源使用。 Catch多个异常 在Java 7中，catch代码块得到了升级，用以在单个catch块中处理多个异常。如果你要捕获多个异常并且它们包含相似的代码，使用这一特性将会减少代码重复度。下面用一个例子来理解。 12345&gt;catch(IOException | SQLException | Exception ex)&#123;&gt; logger.error(ex);&gt; throw new MyException(ex.getMessage());&gt;&#125;&gt; 数字类型的下划线表示 数字类型的下划线表示 更友好的表示方式，不过要注意下划线添加的一些标准。 字面常量数字里加下划线的规则：下划线只能在数字之间，在数字的开始或结束一定不能使用下划线。 12345678910111213141516&gt;public class UsingUnderscoreInNumericLiterals &#123;&gt; public static void main(String[] args) &#123;&gt; int int_num = 1_00_00_000;&gt; System.out.println("int num:" + int_num);&gt;&gt; long long_num = 1_00_00_000;&gt; System.out.println("long num:" + long_num);&gt;&gt; float float_num = 2.10_001F;&gt; System.out.println("float num:" + float_num);&gt;&gt; double double_num = 2.10_12_001;&gt; System.out.println("double num:" + double_num);&gt; &#125;&gt;&#125;&gt; 泛型实例的创建简化 泛型实例的创建可以通过类型推断来简化 可以去掉后面new部分的泛型类型，只用&lt;&gt;就可以了。 并发工具增强 并发工具增强： fork-join框架最大的增强，充分利用多核特性，将大问题分解成各个子问题，由多个cpu可以同时解决多个子问题，最后合并结果，继承RecursiveTask，实现compute方法，然后调用fork计算，最后用join合并结果。 参考自己写的例子：Java7 Fork-Join 框架：任务切分，并行处理 JDK1.8的新特性 接口的默认和静态方法 Java 8允许我们给接口添加一个非抽象的方法实现，只需要使用 default关键字即可，这个特征又叫做扩展方法。 1234567891011&gt;public interface JDK8Interface &#123;&gt; // static修饰符定义静态方法 &gt; static void staticMethod() &#123; &gt; System.out.println("接口中的静态方法"); &gt; &#125; &gt; // default修饰符定义默认方法 &gt; default void defaultMethod() &#123; &gt; System.out.println("接口中的默认方法"); &gt; &#125; &gt;&#125; &gt; Lambda 表达式 Lambda 表达式：(例如： (x, y) -&gt; { return x + y; } ;λ表达式有三部分组成：参数列表，箭头（-&gt;），以及一个表达式或语句块。) 参考博客: lambda表达式详解; 函数式接口和Lambda表达式深入理解 在Java 8 中你就没必要使用这种传统的匿名对象的方式了，Java 8提供了更简洁的语法，lambda表达式： 1234&gt;Collections.sort(names, (String a, String b) -&gt; &#123;&gt; return b.compareTo(a);&gt;&#125;);&gt; 方法与构造函数引用 Java 8 允许你使用 : 关键字来传递方法或者构造函数引用，上面的代码展示了如何引用一个静态方法，我们也可以引用一个对象的方法： 1234&gt;converter = something::startsWith;&gt;String converted = converter.convert("Java");&gt;System.out.println(converted);&gt; 函数式接口 所谓的函数式接口，当然首先是一个接口，然后就是在这个接口里面只能有一个抽象方法。 Annotation 注解 Annotation 注解：支持多重注解： 很多时候一个注解需要在某一位置多次使用。 123456&gt;YourAnnotation&gt;@YourAnnotation&gt;public void test()&#123;&gt; //TODO&gt;&#125;&gt; 新的日期时间 API Java 8新的Date-Time API (JSR 310)受Joda-Time的影响，提供了新的java.time包，可以用来替代 java.util.Date和java.util.Calendar。一般会用到Clock、LocaleDate、LocalTime、LocaleDateTime、ZonedDateTime、Duration这些类，对于时间日期的改进还是非常不错的。 Base64编码 Base64编码是一种常见的字符编码，可以用来作为电子邮件或Web Service附件的传输编码。 在Java 8中，Base64编码成为了Java类库的标准。Base64类同时还提供了对URL、MIME友好的编码器与解码器。 JavaScript引擎Nashorn Nashorn允许在JVM上开发运行JavaScript应用，允许Java与JavaScript相互调用。 Stream的使用 Stream API是把真正的函数式编程风格引入到Java中。其实简单来说可以把Stream理解为MapReduce，当然Google的MapReduce的灵感也是来自函数式编程。她其实是一连串支持连续、并行聚集操作的元素。从语法上看，也很像linux的管道、或者链式编程，代码写起来简洁明了，非常酷帅！ Optional Java 8引入Optional类来防止空指针异常，Optional类最先是由Google的Guava项目引入的。Optional类实际上是个容器：它可以保存类型T的值，或者保存null。使用Optional类我们就不用显式进行空指针检查了。 扩展注解的支持 Java 8扩展了注解的上下文，几乎可以为任何东西添加注解，包括局部变量、泛型类、父类与接口的实现，连方法的异常也能添加注解。 并行（parallel）数组 支持对数组进行并行处理，主要是parallelSort()方法，它可以在多核机器上极大提高数组排序的速度。 编译器优化 Java 8将方法的参数名加入了字节码中，这样在运行时通过反射就能获取到参数名，只需要在编译时使用-parameters参数。 新的Java1.8对IO做了升级： 关于IO/NIO 新IO的对比，请参考：Java NIO：IO与NIO的区别 -阿里面试题 还对CurrentHashMap做了升级，请参考：ConcurrentHashMap原理分析（1.7与1.8）]]></content>
      <categories>
        <category>基础面试题</category>
      </categories>
      <tags>
        <tag>基础面试题</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mybaties 面试题]]></title>
    <url>%2F2019%2F05%2F12%2F%E9%9D%A2%E8%AF%95%E6%80%BB%E7%BB%93%2FMybaties%20%E9%9D%A2%E8%AF%95%E9%A2%98%2F</url>
    <content type="text"><![CDATA[Hibernate与MyBatis的异同相同点：Hibernate与MyBatis都可以是通过SessionFactoryBuider由XML配置文件生成SessionFactory，然后由SessionFactory 生成Session，最后由Session来开启执行事务和SQL语句。其中SessionFactoryBuider，SessionFactory，Session的生命周期都是差不多的。Hibernate和MyBatis都支持JDBC和JTA事务处理。 Mybatis优势：MyBatis可以进行更为细致的SQL优化，可以减少查询字段。MyBatis容易掌握，而Hibernate门槛较高。Hibernate优势：Hibernate的DAO层开发比MyBatis简单，Mybatis需要维护SQL和结果映射。Hibernate对对象的维护和缓存要比MyBatis好，对增删改查的对象的维护要方便。Hibernate数据库移植性很好，MyBatis的数据库移植性不好，不同的数据库需要写不同SQL。Hibernate有更好的二级缓存机制，可以使用第三方缓存。MyBatis本身提供的缓存机制不佳。 Hibernate与MyBatis在sql优化方面异同Hibernate的查询会将表中的所有字段查询出来，这一点会有性能消耗。Hibernate也可以自己写SQL来指定需要查询的字段，但这样就破坏了Hibernate开发的简洁性。而Mybatis的SQL是手动编写的，所以可以按需求指定查询的字段。Hibernate HQL语句的调优需要将SQL打印出来，而Hibernate的SQL被很多人嫌弃因为太丑了。MyBatis的SQL是自己手动写的所以调整方便。但Hibernate具有自己的日志统计。Mybatis本身不带日志统计，使用Log4j进行日志记录。 Hibernate与MyBatis对象管理对比Hibernate 是完整的对象/关系映射解决方案，它提供了对象状态管理（state management）的功能，使开发者不再需要理会底层数据库系统的细节。也就是说，相对于常见的 JDBC/SQL 持久层方案中需要管理 SQL 语句，Hibernate采用了更自然的面向对象的视角来持久化 Java 应用中的数据。换句话说，使用 Hibernate 的开发者应该总是关注对象的状态（state），不必考虑 SQL 语句的执行。这部分细节已经由 Hibernate 掌管妥当，只有开发者在进行系统性能调优的时候才需要进行了解。而MyBatis在这一块没有文档说明，用户需要对对象自己进行详细的管理。 Mybatis分层结构 MyBatis 编程步骤 创建 SqlSessionFactory 通过 SqlSessionFactory 获取 SqlSession 通过 SqlSession 执行数据库操作 提交事务 关闭会话 讲下MyBatis的缓存MyBatis的缓存分为一级缓存和二级缓存,一级缓存放在session里面,默认就有,二级缓存放在它的命名空间里,默认是不打开的,使用二级缓存属性类需要实现Serializable序列化接口(可用来保存对象的状态),可在它的映射文件中配置 Mybatis的一级、二级缓存 一级缓存: 基于 PerpetualCache 的 HashMap 本地缓存，其存储作用域为 Session，当 Session flush 或 close 之后，该 Session 中的所有 Cache 就将清空，默认打开一级缓存。 二级缓存与一级缓存其机制相同，默认也是采用 PerpetualCache，HashMap 存储，不同在于其存储作用域为 Mapper(Namespace)，并且可自定义存储源，如 Ehcache。默认不打开二级缓存，要开启二级缓存，使用二级缓存属性类需要实现Serializable序列化接口(可用来保存对象的状态),可在它的映射文件中配置 ； 对于缓存数据更新机制，当某一个作用域(一级缓存 Session/二级缓存Namespaces)的进行了C/U/D 操作后，默认该作用域下所有 select 中的缓存将被 clear。 Mybatis是如何进行分页的？分页插件的原理是什么？ Mybatis使用RowBounds对象进行分页，也可以直接编写sql实现分页，也可以使用Mybatis的分页插件。 分页插件的原理：实现Mybatis提供的接口，实现自定义插件，在插件的拦截方法内拦截待执行的sql，然后重写sql。 举例：select from student，拦截sql后重写为：select t. from （select * from student）t limit 0，10 Mybatis都有哪些Executor执行器？它们之间的区别是什么？Mybatis有三种基本的Executor执行器，SimpleExecutor、ReuseExecutor、BatchExecutor。 SimpleExecutor：每执行一次update或select，就开启一个Statement对象，用完立刻关闭Statement对象。 ReuseExecutor：执行update或select，以sql作为key查找Statement对象，存在就使用，不存在就创建，用完后，不关闭Statement对象，而是放置于Map&lt;String, Statement&gt;内，供下一次使用。简言之，就是重复使用Statement对象。 BatchExecutor：执行update（没有select，JDBC批处理不支持select），将所有sql都添加到批处理中（addBatch()），等待统一执行（executeBatch()），它缓存了多个Statement对象，每个Statement对象都是addBatch()完毕后，等待逐一执行executeBatch()批处理。与JDBC批处理相同。 作用范围：Executor的这些特点，都严格限制在SqlSession生命周期范围内 简述Mybatis的插件运行原理，以及如何编写一个插件？ Mybatis仅可以编写针对ParameterHandler、ResultSetHandler、StatementHandler、Executor这4种接口的插件，Mybatis通过动态代理，为需要拦截的接口生成代理对象以实现接口方法拦截功能，每当执行这4种接口对象的方法时，就会进入拦截方法，具体就是InvocationHandler的invoke()方法，当然，只会拦截那些你指定需要拦截的方法。 实现Mybatis的Interceptor接口并复写intercept()方法，然后在给插件编写注解，指定要拦截哪一个接口的哪些方法即可，记住，别忘了在配置文件中配置你编写的插件。 Mybatis动态sql是做什么的？都有哪些动态sql？能简述一下动态sql的执行原理不？ Mybatis动态sql可以让我们在Xml映射文件内，以标签的形式编写动态sql，完成逻辑判断和动态拼接sql的功能。 Mybatis提供了9种动态sql标签：trim|where|set|foreach|if|choose|when|otherwise|bind。 其执行原理为，使用OGNL从sql参数对象中计算表达式的值，根据表达式的值动态拼接sql，以此来完成动态sql的功能。 #{}和${}的区别是什么？1）#{}是预编译处理，${}是字符串替换。 2）Mybatis在处理#{}时，会将sql中的#{}替换为?号，调用PreparedStatement的set方法来赋值； 3）Mybatis在处理${}时，就是把${}替换成变量的值。 4）使用#{}可以有效的防止SQL注入，提高系统安全性。 字段与数据库列名不一致 为查询出的字段起别名，保证名称与类中属性名相同(缺点：在查询字段过多的情况下会降低代码可读性且导致重复代码增多) 因为Mybatis映射的原理就是通过返回的数据库字段名找实体类的setXxx方法进行对应注入，故可以采用让实体类set方法名对应数据库字段名 (推荐)利用XxxMapper.xml的sql标签(同解决方案一原理相同,将方案一提取封装,简化操作) (推荐)使用resultMap手动映射一个数据结果的封装规则 Statement和PrepareStatement的区别PreparedStatement：表示预编译的 SQL 语句的对象。 （1）PrepareStatement可以使用占位符，是预编译的，批处理比Statement效率高 （2）在对数据库只执行一次性存取的时侯，用 Statement 对象进行处理。 （3）PreparedStatement的第一次执行消耗是很高的. 它的性能体现在后面的重复执行 为什么说Mybatis是半自动ORM映射工具？它与全自动的区别在哪里？Hibernate属于全自动ORM映射工具，使用Hibernate查询关联对象或者关联集合对象时，可以根据对象关系模型直接获取，所以它是全自动的。而Mybatis在查询关联对象或关联集合对象时，需要手动编写sql来完成，所以，称之为半自动ORM映射工具。 Mybatis是否支持延迟加载？如果支持，它的实现原理是什么？1）Mybatis仅支持association关联对象和collection关联集合对象的延迟加载，association指的就是一对一，collection指的就是一对多查询。在Mybatis配置文件中，可以配置是否启用延迟加载lazyLoadingEnabled=true|false。 2）它的原理是，使用CGLIB创建目标对象的代理对象，当调用目标方法时，进入拦截器方法，比如调用a.getB().getName()，拦截器invoke()方法发现a.getB()是null值，那么就会单独发送事先保存好的查询关联B对象的sql，把B查询上来，然后调用a.setB(b)，于是a的对象b属性就有值了，接着完成a.getB().getName()方法的调用。这就是延迟加载的基本原理。 MyBatis与Hibernate有哪些不同？ Mybatis和hibernate不同，它不完全是一个ORM框架，因为MyBatis需要程序员自己编写Sql语句，不过mybatis可以通过XML或注解方式灵活配置要运行的sql语句，并将java对象和sql语句映射生成最终执行的sql，最后将sql执行的结果再映射生成java对象。 Mybatis学习门槛低，简单易学，程序员直接编写原生态sql，可严格控制sql执行性能，灵活度高，非常适合对关系数据模型要求不高的软件开发，例如互联网软件、企业运营类软件等，因为这类软件需求变化频繁，一但需求变化要求成果输出迅速。但是灵活的前提是mybatis无法做到数据库无关性，如果需要实现支持多种数据库的软件则需要自定义多套sql映射文件，工作量大。 Hibernate对象/关系映射能力强，数据库无关性好，对于关系模型要求高的软件（例如需求固定的定制化软件）如果用hibernate开发可以节省很多代码，提高效率。但是Hibernate的缺点是学习门槛高，要精通门槛更高，而且怎么设计O/R映射，在性能和对象模型之间如何权衡，以及怎样用好Hibernate需要具有很强的经验和能力才行。 总之，按照用户的需求在有限的资源环境下只要能做出维护性、扩展性良好的软件架构都是好架构，所以框架只有适合才是最好。 MyBatis的好处是什么？ MyBatis把sql语句从Java源程序中独立出来，放在单独的XML文件中编写，给程序的维护带来了很大便利。 MyBatis封装了底层JDBC API的调用细节，并能自动将结果集转换成Java Bean对象，大大简化了Java数据库编程的重复工作。 因为MyBatis需要程序员自己去编写sql语句，程序员可以结合数据库自身的特点灵活控制sql语句，因此能够实现比Hibernate等全自动orm框架更高的查询效率，能够完成复杂查询。 简述Mybatis的Xml映射文件和Mybatis内部数据结构之间的映射关系？Mybatis将所有Xml配置信息都封装到All-In-One重量级对象Configuration内部。在Xml映射文件中，标签会被解析为ParameterMap对象，其每个子元素会被解析为ParameterMapping对象。标签会被解析为ResultMap对象，其每个子元素会被解析为ResultMapping对象。每一个、、、标签均会被解析为MappedStatement对象，标签内的sql会被解析为BoundSql对象。 什么是MyBatis的接口绑定,有什么好处接口映射就是在MyBatis中任意定义接口,然后把接口里面的方法和SQL语句绑定,我们直接调用接口方法就可以,这样比起原来了SqlSession提供的方法我们可以有更加灵活的选择和设置. 接口绑定有几种实现方式,分别是怎么实现的?接口绑定有两种实现方式,一种是通过注解绑定,就是在接口的方法上面加上@Select@Update等注解里面包含Sql语句来绑定,另外一种就是通过xml里面写SQL来绑定,在这种情况下,要指定xml映射文件里面的namespace必须为接口的全路径名. 什么情况下用注解绑定,什么情况下用xml绑定？当Sql语句比较简单时候,用注解绑定；当SQL语句比较复杂时候,用xml绑定,一般用xml绑定的比较多 MyBatis实现一对一有几种方式?具体怎么操作的？有联合查询和嵌套查询,联合查询是几个表联合查询,只查询一次,通过在resultMap里面配置association节点配置一对一的类就可以完成;嵌套查询是先查一个表,根据这个表里面的结果的外键id,去再另外一个表里面查询数据,也是通过association配置,但另外一个表的查询通过select属性配置。 Mybatis能执行一对一、一对多的关联查询吗？都有哪些实现方式，以及它们之间的区别？能，Mybatis不仅可以执行一对一、一对多的关联查询，还可以执行多对一，多对多的关联查询，多对一查询，其实就是一对一查询，只需要把selectOne()修改为selectList()即可；多对多查询，其实就是一对多查询，只需要把selectOne()修改为selectList()即可。 关联对象查询，有两种实现方式，一种是单独发送一个sql去查询关联对象，赋给主对象，然后返回主对象。另一种是使用嵌套查询，嵌套查询的含义为使用join查询，一部分列是A对象的属性值，另外一部分列是关联对象B的属性值，好处是只发一个sql查询，就可以把主对象和其关联对象查出来。 MyBatis里面的动态Sql是怎么设定的?用什么语法?MyBatis里面的动态Sql一般是通过if节点来实现,通过OGNL语法来实现,但是如果要写的完整,必须配合where,trim节点,where节点是判断包含节点有内容就插入where,否则不插入,trim节点是用来判断如果动态语句是以and 或or开始,那么会自动把这个and或者or取掉。 Mybatis是如何将sql执行结果封装为目标对象并返回的？都有哪些映射形式？第一种是使用标签，逐一定义列名和对象属性名之间的映射关系。 第二种是使用sql列的别名功能，将列别名书写为对象属性名，比如T_NAME AS NAME，对象属性名一般是name，小写，但是列名不区分大小写，Mybatis会忽略列名大小写，智能找到与之对应对象属性名，你甚至可以写成T_NAME AS NaMe，Mybatis一样可以正常工作。 有了列名与属性名的映射关系后，Mybatis通过反射创建对象，同时使用反射给对象的属性逐一赋值并返回，那些找不到映射关系的属性，是无法完成赋值的。 Xml映射文件中，除了常见的select|insert|updae|delete标签之外，还有哪些标签？还有很多其他的标签，、、、、，加上动态sql的9个标签，trim|where|set|foreach|if|choose|when|otherwise|bind等，其中为sql片段标签，通过标签引入sql片段，为不支持自增的主键生成策略标签。 当实体类中的属性名和表中的字段名不一样，如果将查询的结果封装到指定pojo？ 通过在查询的sql语句中定义字段名的别名。 通过来映射字段名和实体类属性名的一一对应的关系。 模糊查询like语句该怎么写 在java中拼接通配符，通过#{}赋值 在Sql语句中拼接通配符 （不安全 会引起Sql注入） 通常一个Xml映射文件，都会写一个Dao接口与之对应, Dao的工作原理，是否可以重载？不能重载，因为通过Dao寻找Xml对应的sql的时候全限名+方法名的保存和寻找策略。接口工作原理为jdk动态代理原理，运行时会为dao生成proxy，代理对象会拦截接口方法，去执行对应的sql返回数据。 Mybatis映射文件中，如果A标签通过include引用了B标签的内容，请问，B标签能否定义在A标签的后面，还是说必须定义在A标签的前面？虽然Mybatis解析Xml映射文件是按照顺序解析的，但是，被引用的B标签依然可以定义在任何地方，Mybatis都可以正确识别。原理是，Mybatis解析A标签，发现A标签引用了B标签，但是B标签尚未解析到，尚不存在，此时，Mybatis会将A标签标记为未解析状态，然后继续解析余下的标签，包含B标签，待所有标签解析完毕，Mybatis会重新解析那些被标记为未解析的标签，此时再解析A标签时，B标签已经存在，A标签也就可以正常解析完成了。 Mybatis的Xml映射文件中，不同的Xml映射文件，id是否可以重复？不同的Xml映射文件，如果配置了namespace，那么id可以重复；如果没有配置namespace，那么id不能重复；毕竟namespace不是必须的，只是最佳实践而已。原因就是namespace+id是作为Map&lt;String, MappedStatement&gt;的key使用的，如果没有namespace，就剩下id，那么，id重复会导致数据互相覆盖。有了namespace，自然id就可以重复，namespace不同，namespace+id自然也就不同。 Mybatis中如何执行批处理？使用BatchExecutor完成批处理。 Mybatis都有哪些Executor执行器？它们之间的区别是什么？Mybatis有三种基本的Executor执行器，SimpleExecutor、ReuseExecutor、BatchExecutor。1）SimpleExecutor：每执行一次update或select，就开启一个Statement对象，用完立刻关闭Statement对象。2）ReuseExecutor：执行update或select，以sql作为key查找Statement对象，存在就使用，不存在就创建，用完后，不关闭Statement对象，而是放置于Map3）BatchExecutor：完成批处理。 Mybatis中如何指定使用哪一种Executor执行器？在Mybatis配置文件中，可以指定默认的ExecutorType执行器类型，也可以手动给DefaultSqlSessionFactory的创建SqlSession的方法传递ExecutorType类型参数。 Mybatis执行批量插入，能返回数据库主键列表吗？能，JDBC都能，Mybatis当然也能。 Mybatis是否可以映射Enum枚举类？Mybatis可以映射枚举类，不单可以映射枚举类，Mybatis可以映射任何对象到表的一列上。映射方式为自定义一个TypeHandler，实现TypeHandler的setParameter()和getResult()接口方法。TypeHandler有两个作用，一是完成从javaType至jdbcType的转换，二是完成jdbcType至javaType的转换，体现为setParameter()和getResult()两个方法，分别代表设置sql问号占位符参数和获取列查询结果。 如何获取自动生成的(主)键值？配置文件设置usegeneratedkeys 为true 在mapper中如何传递多个参数？1）直接在方法中传递参数，xml文件用#{0} #{1}来获取 2）使用 @param 注解:这样可以直接在xml文件中通过#{name}来获取 resultType resultMap的区别？ 类的名字和数据库相同时，可以直接设置resultType参数为Pojo类 若不同，需要设置resultMap 将结果名字和Pojo名字进行转换 使用MyBatis的mapper接口调用时有哪些要求？ Mapper接口方法名和mapper.xml中定义的每个sql的id相同 Mapper接口方法的输入参数类型和mapper.xml中定义的每个sql 的parameterType的类型相同 Mapper接口方法的输出参数类型和mapper.xml中定义的每个sql的resultType的类型相同 Mapper.xml文件中的namespace即是mapper接口的类路径。 JDBC 编程有哪些不足？MyBatis 是如何解决的？ 数据库的连接创建、释放频繁，从而影响性能，mybatis 通过配置 SqlMapConfig.xml 文件使用连接池的方式解决了数据库连接创建和释放频繁所造成的性能影响。 大量的 sql 存在于代码之中，造成代码的可维护性低。mybatis 只用 xml 文件对 sql 进行统一管理，方便维护。 jdbc 操作中存在参数时，需要准确的定位参数的位置和对应占位符的个数，否则会出错。mybatis 通过提供参数对象的方式解决了该问题。 sql 语句在编写时，如果存在动态条件则不容易处理。mybatis 提供动态 sql 编写机制，时用户可以根据自己传入参数的情况进行 sql 语句的动态编写。 对查询时返回的结果集处理是一件比较麻烦的事情，而随着查询结果的改变，处理结果集的代码也随之改变。mybatis 通过把结果集映射成对应的 java 对象，解决了结果集处理麻烦的问题。 MyBatis 和 Hibernate 的区别总体来说 mybatis 和 hibernate 都属于持久层框架。就目前主流的 ORM 框架而言，hibernate 数据全自动 ORM 实现，而 mybatis 属于半自动 ORM 实现。他们最直观的区别在于：hibernate 完整的实现了对象到数据库结构的映射，也就是说在使用 hibernate 的时候，你可以不需要关心数据库结构，而仅仅只用知道数据库结构对应的对象即可，hibernate 提供了一系列的操作使你所有的数据库操作转换成对象的操作，而 hibernate 在执行操作的时候会自动的帮你生成对应的 sql 并执行，并把返回的结果集封装成对应的对象返回。而 mybatis 在实现是仅仅是对结果集进行了对象的封装返回，而执行的 sql 需要开发人员自己去实现。 虽然 hibernate 是全自动 ORM 框架，但是在某些情况下并不是那么好用，比如： 在某些情况下，数据库权限没有全部开放，只能执行部分 sql 操作的时候。 数据库查询操作十分繁琐，sql 执行很复杂，为了提供效率，需要对 sql 进行优化。 数据库查询需要用到存储过程时。 等等以上情况在使用 hibernate 时就不能应付了。这时使用 mybatis 则是一个不错的选择。但是如果存在多数据库访问的情况下，hibernate 会是一个更好的选择，而 mybatis 在操作多数据库时，同一个数据库操作需要实现不同的 sql。 Mybatis中设计模式 Builder模式，例如SqlSessionFactoryBuilder、XMLConfigBuilder、XMLMapperBuilder、XMLStatementBuilder、CacheBuilder； 工厂模式，例如SqlSessionFactory、ObjectFactory、MapperProxyFactory； 单例模式，例如ErrorContext和LogFactory； 代理模式，Mybatis实现的核心，比如MapperProxy、ConnectionLogger，用的jdk的动态代理；还有executor.loader包使用了cglib或者javassist达到延迟加载的效果； 组合模式，例如SqlNode和各个子类ChooseSqlNode等； 模板方法模式，例如BaseExecutor和SimpleExecutor，还有BaseTypeHandler和所有的子类例如IntegerTypeHandler； 适配器模式，例如Log的Mybatis接口和它对jdbc、log4j等各种日志框架的适配实现； 装饰者模式，例如Cache包中的cache.decorators子包中等各个装饰者的实现； 迭代器模式，例如迭代器模式PropertyTokenizer详MyBatis设计模式 Mybatis比IBatis比较大的几个改进是什么？ 有接口绑定,包括注解绑定sql和xml绑定Sql 动态sql由原来的节点配置变成OGNL表达式3） 在一对一,一对多的时候引进了association,在一对多的时候引入了collection节点,不过都是在resultMap里面配置 IBatis和MyBatis在核心处理类分别叫什么？IBatis里面的核心处理类交SqlMapClient,MyBatis里面的核心处理类叫做SqlSession。 IBatis和MyBatis在细节上的不同有哪些？ 在sql里面变量命名有原来的#变量# 变成了#{变量} 原来的 1$变量$ 变成了${变量} 原来在sql节点里面的class都换名字交type 原来的queryForObject queryForList 变成了selectOne selectList5）原来的别名设置在映射文件里面放在了核心配置文件里]]></content>
      <categories>
        <category>基础面试题</category>
      </categories>
      <tags>
        <tag>基础面试题</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[全文检索面试题]]></title>
    <url>%2F2019%2F05%2F12%2F%E9%9D%A2%E8%AF%95%E6%80%BB%E7%BB%93%2F%E5%85%A8%E6%96%87%E6%A3%80%E7%B4%A2%E9%9D%A2%E8%AF%95%E9%A2%98%2F</url>
    <content type="text"><![CDATA[为什么要用全文搜索搜索引擎我们的所有数据在数据库里面都有，而且 Oracle、SQL Server 等数据库里也能提供查询检索或者聚类分析功能，直接通过数据库查询不就可以了吗？确实，我们大部分的查询功能都可以通过数据库查询获得，如果查询效率低下，还可以通过建数据库索引，优化SQL等方式进行提升效率，甚至通过引入缓存来加快数据的返回速度。如果数据量更大，就可以分库分表来分担查询压力。 那为什么还要全文搜索引擎呢？我们主要从以下几个原因分析： 数据类型 全文索引搜索支持非结构化数据的搜索，可以更好地快速搜索大量存在的任何单词或单词组的非结构化文本。 例如 Google，百度类的网站搜索，它们都是根据网页中的关键字生成索引，我们在搜索的时候输入关键字，它们会将该关键字即索引匹配到的所有网页返回；还有常见的项目中应用日志的搜索等等。对于这些非结构化的数据文本，关系型数据库搜索不是能很好的支持。 索引的维护 一般传统数据库，全文检索都实现的很鸡肋，因为一般也没人用数据库存文本字段。进行全文检索需要扫描整个表，如果数据量大的话即使对SQL的语法优化，也收效甚微。建立了索引，但是维护起来也很麻烦，对于 insert 和 update 操作都会重新构建索引。 什么时候使用全文搜索引擎： 搜索的数据对象是大量的非结构化的文本数据。 文件记录量达到数十万或数百万个甚至更多。 支持大量基于交互式文本的查询。 需求非常灵活的全文搜索查询。 对高度相关的搜索结果的有特殊需求，但是没有可用的关系数据库可以满足。 对不同记录类型、非文本数据操作或安全事务处理的需求相对较少的情况。 倒排索引Lucene，Solr， ElasticSearch它们的索引建立都是根据倒排索引的方式生成索引，何谓倒排索引？ 维基百科倒排索引（英语：Inverted index），也常被称为反向索引、置入档案或反向档案，是一种索引方法，被用来存储在全文搜索下某个单词在一个文档或者一组文档中的存储位置的映射。它是文档检索系统中最常用的数据结构。 何为 Lucene ?Lucene是一个Java全文搜索引擎，完全用Java编写。Lucene不是一个完整的应用程序，而是一个代码库和API，可以很容易地用于向应用程序添加搜索功能。 Lucene通过简单的API提供强大的功能： 可扩展的高性能索引 在现代硬件上超过150GB /小时 小RAM要求 - 只有1MB堆 增量索引与批量索引一样快 索引大小约为索引文本大小的20-30％ 强大，准确，高效的搜索算法 排名搜索 - 首先返回最佳结果 许多强大的查询类型：短语查询，通配符查询，邻近查询，范围查询等 现场搜索（例如标题，作者，内容） 按任何字段排序 使用合并结果进行多索引搜索 允许同时更新和搜索 灵活的分面，突出显示，连接和结果分组 快速，内存效率和错误容忍的建议 可插拔排名模型，包括矢量空间模型和Okapi BM25 可配置存储引擎（编解码器） 跨平台解决方案 作为Apache许可下的开源软件提供 ，允许您在商业和开源程序中使用Lucene 100％-pure Java 可用的其他编程语言中的实现是索引兼容的 Apache软件基金会在Apache软件基金会提供的开源软件项目的Apache社区的支持。 但是Lucene只是一个框架，要充分利用它的功能，需要使用JAVA，并且在程序中集成Lucene。需要很多的学习了解，才能明白它是如何运行的，熟练运用Lucene确实非常复杂。 何为 SolrApache Solr是一个基于名为Lucene的Java库构建的开源搜索平台。它以用户友好的方式提供Apache Lucene的搜索功能。作为一个行业参与者近十年，它是一个成熟的产品，拥有强大而广泛的用户社区。它提供分布式索引，复制，负载平衡查询以及自动故障转移和恢复。如果它被正确部署然后管理得好，它就能够成为一个高度可靠，可扩展且容错的搜索引擎。很多互联网巨头，如Netflix，eBay，Instagram和亚马逊（CloudSearch）都使用Solr，因为它能够索引和搜索多个站点。 主要功能列表包括： 全文搜索 突出 分面搜索 实时索引 动态群集 数据库集成 NoSQL功能和丰富的文档处理（例如Word和PDF文件） 何为 ElasticSearchElasticsearch是一个开源（Apache 2许可证），是一个基于Apache Lucene库构建的RESTful搜索引擎。 Elasticsearch是在Solr之后几年推出的。它提供了一个分布式，多租户能力的全文搜索引擎，具有HTTP Web界面（REST）和无架构JSON文档。Elasticsearch的官方客户端库提供Java，Groovy，PHP，Ruby，Perl，Python，.NET和Javascript。 分布式搜索引擎包括可以划分为分片的索引，并且每个分片可以具有多个副本。每个Elasticsearch节点都可以有一个或多个分片，其引擎也可以充当协调器，将操作委派给正确的分片。 Elasticsearch可通过近实时搜索进行扩展。其主要功能之一是多租户。 主要功能列表包括： 分布式搜索 多租户 分析搜索 分组和聚合 Lucene 是什么？Lucene即全文检索。全文检索是计算机程序通过扫描文章中的每一个词，对每一个词建立一个索引，指明该词在文章中出现的次数和位置。当用户查询时根据建立的索引查找，类似于通过字典的检索字表查字的过程。 Lucene是一个基于Java的全文信息检索工具包，它不是一个完整的搜索应用程序，而是为你的应用程序提供索引和搜索功能。Lucene 目前是 Apache Jakarta(雅加达) 家族中的一个开源项目。也是目前最为流行的基于Java开源全文检索工具包。目前已经有很多应用程序的搜索功能是基于 Lucene ，比如Eclipse 帮助系统的搜索功能。Lucene能够为文本类型的数据建立索引，所以你只要把你要索引的数据格式转化的文本格式，Lucene 就能对你的文档进行索引和搜索。 Solr 是什么？Apache Solr是一个高性能，基于Lucene的全文搜索服务器。Solr是Apache下的一个开源项目，采用Java开发，只需要进行配置就可以实现全文检索服务。Solr提供了比Lucene更为丰富的查询语言，同时实现了可配置、可扩展，并对索引、搜索性能进行了优化。做为一款搜索引擎，solr不具备爬虫一样采集信息的能力，而是专注于信息的存储和检索。许多朋友误认为solr是数据库，从广义上讲也可认为是数据库，但是它和传统意义上的数据库还是有些区别的。相信使用过关系型数据库的朋友们一定都做过搜索的功能，比如：有100条记录，我想搜索记录中含有“云计算”字段的记录，可以使用关系型数据库提供的“模糊搜索”的功能。“模糊搜索”能不能满足你的要求呢？如果记录数小，100条、1000条记录当然没问题。但是，如果有100万条、1000万条甚至上亿，那么“模糊搜索”的效果就会大大折扣。而这时，我们就需要用到solr等搜索引擎了。 Solr易于加入到 Web 应用程序中。Solr 提供了层面搜索(就是统计)、命中醒目显示并且支持多种输出格式（包括XML/XSLT 和JSON等格式）。它易于安装和配置，而且附带了一个基于HTTP 的管理界面。可以使用 Solr 的表现优异的基本搜索功能，也可以对它进行扩展从而满足企业的需要。Solr的特性包括：• 高级的全文搜索功能• 专为高通量的网络流量进行的优化• 基于开放接口（XML和HTTP）的标准• 综合的HTML管理界面• 可伸缩性－能够有效地复制到另外一个Solr搜索服务器• 使用XML配置达到灵活性和适配性• 可扩展的插件体系 Solr 和 Lucene的区别Solr与Lucene的关系 Solr与Lucene 并不是竞争对立关系，恰恰相反Solr 依存于Lucene，因为Solr底层的核心技术是使用Lucene 来实现的，Solr和Lucene的本质区别有以下三点：搜索服务器，企业级和管理。Lucene本质上是搜索库，不是独立的应用程序，而Solr是。Lucene专注于搜索底层的建设，而Solr专注于企业应用。Lucene不负责支撑搜索服务所必须的管理，而Solr负责。所以说，一句话概括 Solr: Solr是Lucene面向企业搜索应用的扩展。 Solr与Lucene的开发成本\1. 使用Lucene实现单独使用Lucene实现站内搜索需要开发的工作量较大，主要表现在：索引维护、索引性能优化、搜索性能优化等，因此不建议采用。\2. 使用solr实现基于Solr实现站内搜索扩展性较好并且可以减少程序员的工作量，因为Solr提供了较为完备的搜索引擎解决方案，因此在门户、论坛等系统中常用此方案。 Solr在Lucene上的扩展• 一个真正的拥有动态字段(Dynamic Field)和唯一键(Unique Key)的数据模式(Data Schema)• 对Lucene查询语言的强大扩展！• 支持对结果进行动态的分组和过滤• 高级的，可配置的文本分析• 高度可配置和可扩展的缓存机制• 性能优化• 支持通过XML进行外部配置• 拥有一个管理界面• 可监控的日志• 支持高速增量式更新(Fast incremental Updates)和快照发布(Snapshot Distribution) lucene 针对它的搜索方式有哪些词项查询(TermQuery)/布尔查询(BooleanQuery)/短语查询(PhraseQuery)/范围查询(RangeQuery)/百搭查询(WildardQuery)/FuzzQuery(模糊) 什么是Lucene数据结构 (倒排索引) Lucene如何计算文档的相关性向量空间模型,布尔模型 什么是细分市场指数的一部分 如何将文本编入索引 (分析器,标记器) Lucene查询语法是什么样的布尔查询,提升,模糊搜索 Lucene和Solr和Elasticsearch的区别Lucene： Lucene是apache下的一个子项目，是一个开放源代码的全文检索引擎工具包，但它不是一个完整的全文检索引擎，而是一个全文检索引擎的架构，提供了完整的查询引擎和索引引擎，部分文本分析引擎。官网地址：https://lucene.apache.org/ Solr Solr是一个高性能，采用Java5开发，基于Lucene的全文搜索服务器。同时对其进行了扩展，提供了比Lucene更为丰富的查询语言，同时实现了可配置、可扩展并对查询性能进行了优化，并且提供了一个完善的功能管理界面，是一款非常优秀的全文搜索引擎。官网地址：http://lucene.apache.org/solr/ Elasticsearch Elasticsearch跟Solr一样，也是一个基于Lucene的搜索服务器，它提供了一个分布式多用户能力的全文搜索引擎，基于RESTful web接口。官网地址：https://www.elastic.co/products/elasticsearch Solr的优缺点优点 1.Solr有一个更大、更成熟的用户、开发和贡献者社区。 2.支持添加多种格式的索引，如：HTML、PDF、微软 Office 系列软件格式以及 JSON、XML、CSV 等纯文本格式。 3.Solr比较成熟、稳定。 4.不考虑建索引的同时进行搜索，速度更快。 缺点 1.建立索引时，搜索效率下降，实时索引搜索效率不高。 Elasticsearch 与 Solr 的比较1.二者安装都很简单； 2.Solr 利用 Zookeeper 进行分布式管理，而 Elasticsearch 自身带有分布式协调管理功能; 3.Solr 支持更多格式的数据，而 Elasticsearch 仅支持json文件格式； 4.Solr 官方提供的功能更多，而 Elasticsearch 本身更注重于核心功能，高级功能多有第三方插件提供； 5.Solr 在传统的搜索应用中表现好于 Elasticsearch，但在处理实时搜索应用时效率明显低于 Elasticsearch。 6.Solr 是传统搜索应用的有力解决方案，但 Elasticsearch 更适用于新兴的实时搜索应用。 使用案例： 1.维基百科使用Elasticsearch来进行全文搜做并高亮显示关键词，以及提供search-as-you-type、did-you-mean等搜索建议功能。 2.英国卫报使用Elasticsearch来处理访客日志，以便能将公众对不同文章的反应实时地反馈给各位编辑。 3.StackOverflow将全文搜索与地理位置和相关信息进行结合，以提供more-like-this相关问题的展现。 4.GitHub使用Elasticsearch来检索超过1300亿行代码。 5.每天，Goldman Sachs使用它来处理5TB数据的索引，还有很多投行使用它来分析股票市场的变动。 solr如何实现搜索的倒排索引，先抽取文档中词，并建立词与文档id的映射关系，然后查询的时候会根据词去查询文档id，并查询出文档 Solr过滤器Solr的过滤器对接收到的标记流（TokenStream ）做额外的处理过滤查询，在查询时设置 Solr原理Solr是基于Lucene开发的全文检索服务器，而Lucene就是一套实现了全文检索的api，其本质就是一个全文检索的过程。全文检索就是把原始文档根据一定的规则拆分成若干个关键词，然后根据关键词创建索引，当查询时先查询索引找到对应的关键词，并根据关键词找到对应的文档，也就是查询结果，最终把查询结果展示给用户的过程 Solr基于什么基于lucene搜索库的一个搜索引擎框架，lucene是一个开放源码的全文检索引擎工具包 solr怎么设置搜索结果排名靠前设置文档中域的boost值，值越高相关性越高，排名就靠前 IK分词器原理本质上是词典分词，在内存中初始化一个词典，然后在分词过程中逐个读取字符，和字典中的字符相匹配，把文档中的所有词语拆分出来的过程 solr的索引查询为什么比数据库要快Solr使用的是Lucene API实现的全文检索。全文检索本质上是查询的索引。而数据库中并不是所有的字段都建立的索引，更何况如果使用like查询时很大的可能是不使用索引，所以使用solr查询时要比查数据库快 solr索引库个别数据索引丢失怎么办首先Solr是不会丢失个别数据的。如果索引库中缺少数据，那就向索引库中添加 Lucene索引优化直接使用Lucene实现全文检索已经是过时的方案，推荐使用solr。Solr已经提供了完整的全文检索解决方案 多张表的数据导入solr(解决id冲突)在schema.xml中添加uuid，然后solrconfig那边修改update的部分，改为使用uuid生成 solr如何分词，新增词和禁用词如何解决schema.xml文件中配置一个IK分词器，然后域指定分词器为IK 新增词添加到词典配置文件中ext.dic，禁用词添加到禁用词典配置文件中stopword.dic，然后在schema.xml文件中配置禁用词典： solr多条件组合查询创建多个查询对象，指定他们的组合关系，Occur.MUST（必须满足and），Occur.SHOULD（应该满足or），Occur.MUST_NOT（必须不满足not） elasticsearch 了解多少，说说你们公司 es 的集群架构，索引数据大小，分片有多少，以及一些调优手段。elasticsearch 的倒排索引是什么。ElasticSearch（简称ES）是一个分布式、Restful的搜索及分析服务器，设计用于分布式计算；能够达到实时搜索，稳定，可靠，快速。和Apache Solr一样，它也是基于Lucence的索引服务器，而ElasticSearch对比Solr的优点在于： 轻量级：安装启动方便，下载文件之后一条命令就可以启动。 Schema free：可以向服务器提交任意结构的JSON对象，Solr中使用schema.xml指定了索引结构。 多索引文件支持：使用不同的index参数就能创建另一个索引文件，Solr中需要另行配置。 分布式：Solr Cloud的配置比较复杂。 倒排索引是实现“单词-文档矩阵”的一种具体存储形式，通过倒排索引，可以根据单词快速获取包含这个单词的文档列表。倒排索引主要由两个部分组成：“单词词典”和“倒排文件”。 elasticsearch 索引数据多了怎么办，如何调优，部署。使用bulk API 初次索引的时候，把 replica 设置为 0 增大 threadpool.index.queue_size 增大 indices.memory.index_buffer_size 增大 index.translog.flush_threshold_ops 增大 index.translog.sync_interval 增大 index.engine.robin.refresh_interval http://www.jianshu.com/p/5eeeeb4375d4 lucence 内部结构是什么索引(Index)： 在Lucene中一个索引是放在一个文件夹中的。 如上图，同一文件夹中的所有的文件构成一个Lucene索引。 段(Segment)： 一个索引可以包含多个段，段与段之间是独立的，添加新文档可以生成新的段，不同的段可以合并。 segments.gen和segments_X是段的元数据文件，也即它们保存了段的属性信息。 文档(Document)： 文档是我们建索引的基本单位，不同的文档是保存在不同的段中的，一个段可以包含多篇文档。 新添加的文档是单独保存在一个新生成的段中，随着段的合并，不同的文档合并到同一个段中。 域(Field)： 一篇文档包含不同类型的信息，可以分开索引，比如标题，时间，正文，作者等，都可以保存在不同的域里。 不同域的索引方式可以不同，在真正解析域的存储的时候，我们会详细解读。 词(Term)： 词是索引的最小单位，是经过词法分析和语言处理后的字符串。 solr和lucene的区别Solr和Lucene的本质区别有以下三点：搜索服务器，企业级和管理。Lucene本质上是搜索库，不是独立的应用程序，而Solr是。Lucene专注于搜索底层的建设，而Solr专注于企业应用。Lucene不负责支撑搜索服务所必须的管理，而Solr负责。所以说，一句话概括Solr: Solr是Lucene面向企业搜索应用的扩展 Lucene: 是一个索引与搜索类库，而不是完整的程序。 Solr：是一个高性能，采用Java5开发，基于Lucene的一个独立的企业级搜索应用服务器，它对外提供类似于Web-service的API接口。 solr 实现全文检索 索引流程：客户端—》solr 服务器(发送post请求,xml文档包含filed，solr实现对索引的维护) 搜索流程：客户端---》solr 服务器(发送get 请求，服务器返回一个xml 文档) solr和lucene之间的区别​ lucene全文检索的工具包,jar包 solr 全文检索服务器,单独运行的servlet容器 Elasticsearch vs. Solr的选择由于Lucene的复杂性，一般很少会考虑它作为搜索的第一选择，排除一些公司需要自研搜索框架，底层需要依赖Lucene。所以这里我们重点分析 Elasticsearch 和 Solr。 Elasticsearch vs. Solr。哪一个更好？他们有什么不同？你应该使用哪一个？ 历史比较 Apache Solr是一个成熟的项目，拥有庞大而活跃的开发和用户社区，以及Apache品牌。Solr于2006年首次发布到开源，长期以来一直占据着搜索引擎领域，并且是任何需要搜索功能的人的首选引擎。它的成熟转化为丰富的功能，而不仅仅是简单的文本索引和搜索； 如分面，分组，强大的过滤，可插入的文档处理，可插入的搜索链组件，语言检测等。 Solr 在搜索领域占据了多年的主导地位。然后，在2010年左右，Elasticsearch成为市场上的另一种选择。那时候，它远没有Solr那么稳定，没有Solr的功能深度，没有思想分享，品牌等等。 Elasticsearch虽然很年轻，但它也自己的一些优势，Elasticsearch 建立在更现代的原则上，针对更现代的用例，并且是为了更容易处理大型索引和高查询率而构建的。此外，由于它太年轻，没有社区可以合作，它可以自由地向前推进，而不需要与其他人（用户或开发人员）达成任何共识或合作，向后兼容，或任何其他更成熟的软件通常必须处理。 因此，它在Solr之前就公开了一些非常受欢迎的功能(例如，接近实时搜索，英文：Near Real-Time Search)。从技术上讲，NRT搜索的能力确实来自Lucene，它是 Solr 和 Elasticsearch 使用的基础搜索库。具有讽刺意味的是，因为 Elasticsearch 首先公开了NRT搜索，所以人们将NRT搜索与Elasticsearch 联系在一起，尽管 Solr 和 Lucene 都是同一个 Apache 项目的一部分，因此，人们会首先期望 Solr 具有如此高要求的功能。 特征差异比较 这两个搜索引擎都是流行的，先进的的开源搜索引擎。它们都是围绕核心底层搜索库 - Lucene构建的 - 但它们又是不同的。像所有东西一样，每个都有其优点和缺点，根据您的需求和期望，每个都可能更好或更差。Solr和Elasticsearch都在快速发展，所以，话不多说，先来看下它们的差异清单： 特征 Solr/SolrCloud Elasticsearch 社区和开发者 Apache 软件基金和社区支持 单一商业实体及其员工 节点发现 Apache Zookeeper，在大量项目中成熟且经过实战测试 Zen内置于Elasticsearch本身，需要专用的主节点才能进行分裂脑保护 碎片放置 本质上是静态，需要手动工作来迁移分片，从Solr 7开始 - Autoscaling API允许一些动态操作 动态，可以根据群集状态按需移动分片 高速缓存 全局，每个段更改无效 每段，更适合动态更改数据 分析引擎性能 非常适合精确计算的静态数据 结果的准确性取决于数据放置 全文搜索功能 基于Lucene的语言分析，多建议，拼写检查，丰富的高亮显示支持 基于Lucene的语言分析，单一建议API实现，高亮显示重新计算 全文搜索功能 基于Lucene的语言分析，多建议，拼写检查，丰富的高亮显示支持 基于Lucene的语言分析，单一建议API实现，高亮显示重新计算 DevOps支持 尚未完全，但即将到来 非常好的API 非平面数据处理 嵌套文档和父-子支持 嵌套和对象类型的自然支持允许几乎无限的嵌套和父-子支持 查询DSL JSON（有限），XML（有限）或URL参数 JSON 索引/收集领导控制 领导者安置控制和领导者重新平衡甚至可以节点上的负载 不可能 机器学习 内置 - 在流聚合之上，专注于逻辑回归和学习排名贡献模块 商业功能，专注于异常和异常值以及时间序列数据 综合比较 另外，我们在从以下几个方面来分析下： 近几年的流行趋势 我们查看一下这两种产品的Google搜索趋势。谷歌趋势表明，与 Solr 相比，Elasticsearch具有很大的吸引力，但这并不意味着Apache Solr已经死亡。虽然有些人可能不这么认为，但Solr仍然是最受欢迎的搜索引擎之一，拥有强大的社区和开源支持。 安装和配置 与Solr相比，Elasticsearch易于安装且非常轻巧。此外，您可以在几分钟内安装并运行Elasticsearch。 但是，如果Elasticsearch管理不当，这种易于部署和使用可能会成为一个问题。基于JSON的配置很简单，但如果要为文件中的每个配置指定注释，那么它不适合您。 总的来说，如果您的应用使用的是JSON，那么Elasticsearch是一个更好的选择。否则，请使用Solr，因为它的schema.xml和solrconfig.xml都有很好的文档记录。 社区 Solr拥有更大，更成熟的用户，开发者和贡献者社区。ES虽拥有的规模较小但活跃的用户社区以及不断增长的贡献者社区。 Solr是真正的开源社区代码。任何人都可以为Solr做出贡献，并且根据优点选出新的Solr开发人员（也称为提交者）。Elasticsearch在技术上是开源的，但在精神上却不那么重要。任何人都可以看到来源，任何人都可以更改它并提供贡献，但只有Elasticsearch的员工才能真正对Elasticsearch进行更改。 Solr贡献者和提交者来自许多不同的组织，而Elasticsearch提交者来自单个公司。 成熟度 Solr更成熟，但ES增长迅速，我认为它稳定。 文档 Solr在这里得分很高。它是一个非常有据可查的产品，具有清晰的示例和API用例场景。 Elasticsearch的文档组织良好，但它缺乏好的示例和清晰的配置说明。 总结 那么，到底是Solr还是Elasticsearch？有时很难找到明确的答案。无论您选择Solr还是Elasticsearch，首先需要了解正确的用例和未来需求。总结他们的每个属性。 记住： 由于易于使用，Elasticsearch在新开发者中更受欢迎。但是，如果您已经习惯了与Solr合作，请继续使用它，因为迁移到Elasticsearch没有特定的优势。 如果除了搜索文本之外还需要它来处理分析查询，Elasticsearch是更好的选择。 如果需要分布式索引，则需要选择Elasticsearch。对于需要良好可伸缩性和性能的云和分布式环境，Elasticsearch是更好的选择。 两者都有良好的商业支持（咨询，生产支持，整合等） 两者都有很好的操作工具，尽管Elasticsearch因其易于使用的API而更多地吸引了DevOps人群，因此可以围绕它创建一个更加生动的工具生态系统。 Elasticsearch在开源日志管理用例中占据主导地位，许多组织在Elasticsearch中索引它们的日志以使其可搜索。虽然Solr现在也可以用于此目的，但它只是错过了这一想法。 Solr仍然更加面向文本搜索。另一方面，Elasticsearch 通常用于过滤和分组 - 分析查询工作负载 - 而不一定是文本搜索。Elasticsearch 开发人员在 Lucene 和 Elasticsearch 级别上投入了大量精力使此类查询更高效(降低内存占用和CPU使用)。因此，对于不仅需要进行文本搜索，而且需要复杂的搜索时间聚合的应用程序，Elasticsearch是一个更好的选择。 Elasticsearch更容易上手，一个下载和一个命令就可以启动一切。Solr传统上需要更多的工作和知识，但Solr最近在消除这一点上取得了巨大的进步，现在只需努力改变它的声誉。 在性能方面，它们大致相同。我说“大致”，因为没有人做过全面和无偏见的基准测试。对于95％的用例，任何一种选择在性能方面都会很好，剩下的5％需要用它们的特定数据和特定的访问模式来测试这两种解决方案。 从操作上讲，Elasticsearch使用起来比较简单 - 它只有一个进程。Solr在其类似Elasticsearch的完全分布式部署模式SolrCloud中依赖于Apache ZooKeeper。ZooKeeper是超级成熟，超级广泛使用等等，但它仍然是另一个活跃的部分。也就是说，如果您使用的是Hadoop，HBase，Spark，Kafka或其他一些较新的分布式软件，您可能已经在组织的某个地方运行ZooKeeper。 虽然Elasticsearch内置了类似ZooKeeper的组件Xen，但ZooKeeper可以更好地防止有时在Elasticsearch集群中出现的可怕的裂脑问题。公平地说，Elasticsearch开发人员已经意识到这个问题，并致力于改进Elasticsearch的这个方面。 如果您喜欢监控和指标，那么使用Elasticsearch，您将会进入天堂。这个东西比新年前夜在时代广场可以挤压的人有更多的指标！Solr暴露了关键指标，但远不及Elasticsearch那么多。 总之，两者都是功能丰富的搜索引擎，只要设计和实现得当，它们或多或少都能提供相同的性能。本篇文章的总体内容大致如下图，该图由园友ReyCG精心绘制并提供。]]></content>
      <categories>
        <category>基础面试题</category>
      </categories>
      <tags>
        <tag>基础面试题</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据库面试题]]></title>
    <url>%2F2019%2F05%2F12%2F%E9%9D%A2%E8%AF%95%E6%80%BB%E7%BB%93%2F%E6%95%B0%E6%8D%AE%E5%BA%93%E9%9D%A2%E8%AF%95%E9%A2%98%2F</url>
    <content type="text"><![CDATA[什么是事务事务指的是满足 ACID 特性的一组操作，可以通过 Commit 提交一个事务，也可以使用 Rollback 进行回滚。 数据库ACID1. 原子性（Atomicity） 原子性是指事务是一个不可分割的工作单位，事务中的操作要么全部成功，要么全部失败。比如在同一个事务中的SQL语句，要么全部执行成功，要么全部执行失败。 回滚可以用日志来实现，日志记录着事务所执行的修改操作，在回滚时反向执行这些修改操作即可。 2. 一致性（Consistency） 事务必须使数据库从一个一致性状态变换到另外一个一致性状态。 以转账为例子，A向B转账，假设转账之前这两个用户的钱加起来总共是2000，那么A向B转账之后，不管这两个账户怎么转，A用户的钱和B用户的钱加起来的总额还是2000，这个就是事务的一致性。 3. 隔离性（Isolation） 隔离性是当多个用户并发访问数据库时，比如操作同一张表时，数据库为每一个用户开启的事务，不能被其他事务的操作所干扰，多个并发事务之间要相互隔离。 即要达到这么一种效果：对于任意两个并发的事务 T1 和 T2，在事务 T1 看来，T2 要么在 T1 开始之前就已经结束，要么在 T1 结束之后才开始，这样每个事务都感觉不到有其他事务在并发地执行。 4. 持久性（Durability） 一旦事务提交，则其所做的修改将会永远保存到数据库中。即使系统发生崩溃，事务执行的结果也不能丢失。 1可以通过数据库备份和恢复来实现，在系统发生奔溃时，使用备份的数据库进行数据恢复。 事务的 ACID 特性概念简单，但不是很好理解，主要是因为这几个特性不是一种平级关系： 只有满足一致性，事务的执行结果才是正确的。 在无并发的情况下，事务串行执行，隔离性一定能够满足。此时要只要能满足原子性，就一定能满足一致性。 在并发的情况下，多个事务并发执行，事务不仅要满足原子性，还需要满足隔离性，才能满足一致性。 事务满足持久化是为了能应对数据库奔溃的情况。 事务隔离级别1. 串行化 (Serializable) 所有事务一个接着一个的执行，这样可以避免幻读 (phantom read)，对于基于锁来实现并发控制的数据库来说，串行化要求在执行范围查询的时候，需要获取范围锁，如果不是基于锁实现并发控制的数据库，则检查到有违反串行操作的事务时，需回滚该事务。 2. 可重复读 (Repeated Read) 所有被 Select 获取的数据都不能被修改，这样就可以避免一个事务前后读取数据不一致的情况。但是却没有办法控制幻读，因为这个时候其他事务不能更改所选的数据，但是可以增加数据，即前一个事务有读锁但是没有范围锁，为什么叫做可重复读等级呢？那是因为该等级解决了下面的不可重复读问题。 引申：现在主流数据库都使用 MVCC 并发控制，使用之后RR（可重复读）隔离级别下是不会出现幻读的现象。 3. 读已提交 (Read Committed) 被读取的数据可以被其他事务修改，这样可能导致不可重复读。也就是说，事务读取的时候获取读锁，但是在读完之后立即释放(不需要等事务结束)，而写锁则是事务提交之后才释放，释放读锁之后，就可能被其他事务修改数据。该等级也是 SQL Server 默认的隔离等级。 4. 读未提交 (Read Uncommitted) 最低的隔离等级，允许其他事务看到没有提交的数据，会导致脏读。 总结 四个级别逐渐增强，每个级别解决一个问题，每个级别解决一个问题，事务级别遇到，性能越差，大多数环境(Read committed 就可以用了) 隔离级别 脏读 不可重复读 幻影读 未提交读 √ √ √ 提交读 × √ √ 可重复读 × × √ 可串行化 × × × 存储引擎 对于初学者来说我们通常不关注存储引擎，但是 MySQL 提供了多个存储引擎，包括处理事务安全表的引擎和处理非事务安全表的引擎。在 MySQL 中，不需要在整个服务器中使用同一种存储引擎，针对具体的要求，可以对每一个表使用不同的存储引擎。 简介 MySQL 中的数据用各种不同的技术存储在文件（或者内存）中。这些技术中的每一种技术都使用不同的存储机制、索引技巧、锁定水平并且最终提供广泛的不同的功能和能力。通过选择不同的技术，你能够获得额外的速度或者功能，从而改善你的应用的整体功能。存储引擎说白了就是如何存储数据、如何为存储的数据建立索引和如何更新、查询数据等技术的实现方法。 例如，如果你在研究大量的临时数据，你也许需要使用内存存储引擎。内存存储引擎能够在内存中存储所有的表格数据。又或者，你也许需要一个支持事务处理的数据库（以确保事务处理不成功时数据的回退能力）。 在MySQL中有很多存储引擎，每种存储引擎大相径庭，那么又改如何选择呢？ MySQL 5.5 以前的默认存储引擎是 MyISAM, MySQL 5.5 之后的默认存储引擎是 InnoDB 不同存储引起都有各自的特点，为适应不同的需求，需要选择不同的存储引擎，所以首先考虑这些存储引擎各自的功能和兼容。 1. MyISAMMySQL 5.5 版本之前的默认存储引擎，在 5.0 以前最大表存储空间最大 4G，5.0 以后最大 256TB。 Myisam 存储引擎由 .myd（数据）和 .myi（索引文件）组成，.frm文件存储表结构（所以存储引擎都有） 特性 并发性和锁级别 （对于读写混合的操作不好，为表级锁，写入和读互斥） 表损坏修复 Myisam 表支持的索引类型（全文索引） Myisam 支持表压缩（压缩后，此表为只读，不可以写入。使用 myisampack 压缩） 应用场景 没有事务 只读类应用（插入不频繁，查询非常频繁） 空间类应用（唯一支持空间函数的引擎） 做很多 count 的计算 2. InnoDBMySQL 5.5 及之后版本的默认存储引擎 特性 InnoDB为事务性存储引擎 完全支持事物的 ACID 特性 Redo log （实现事务的持久性） 和 Undo log（为了实现事务的原子性，存储未完成事务log，用于回滚） InnoDB支持行级锁 行级锁可以最大程度的支持并发 行级锁是由存储引擎层实现的 应用场景 可靠性要求比较高，或者要求事务 表更新和查询都相当的频繁，并且行锁定的机会比较大的情况。 3. CSV文件系统存储特点 数据以文本方式存储在文件中 .csv文件存储表内容 .csm文件存储表的元数据，如表状态和数据量 .frm存储表的结构 CSV存储引擎特点 以 CSV 格式进行数据存储 所有列必须都是不能为 NULL 不支持索引 可以对数据文件直接编辑（其他引擎是二进制存储，不可编辑） 引用场景 作为数据交换的中间表 4. Archive特性 以 zlib 对表数据进行压缩，磁盘 I/O 更少 数据存储在ARZ为后缀的文件中（表文件为 a.arz，a.frm） 只支持 insert 和 select 操作（不可以 delete 和 update，会提示没有这个功能） 只允许在自增ID列上加索引 应用场景 日志和数据采集类应用 5. Memory特性 也称为 HEAP 存储引擎，所以数据保存在内存中（数据库重启后会导致数据丢失） 支持 HASH 索引（等值查找应选择 HASH）和 BTree 索引（范围查找应选择） 所有字段都为固定长度，varchar(10) == char(10) 不支持 BLOG 和 TEXT 等大字段 Memory 存储使用表级锁（性能可能不如 innodb） 最大大小由 max_heap_table_size 参数决定 Memory存储引擎默认表大小只有 16M，可以通过调整 max_heap_table_size 参数 应用场景 用于查找或是映射表，例如右边和地区的对应表 用于保存数据分析中产生的中间表 用于缓存周期性聚合数据的结果表 注意： Memory 数据易丢失，所以要求数据可再生 6. Federated特性 提供了访问远程 MySQL 服务器上表的方法 本地不存储数据，数据全部放在远程服务器上 使用 Federated 默认是禁止的。如果需要启用，需要在启动时增加Federated参数 问：独立表空间和系统表空间应该如何抉择两者比较 系统表空间：无法简单的收缩大小（这很恐怖，会导致 ibdata1 一直增大，即使删除了数据也不会变小） 独立表空间：可以通过 optimize table 命令收缩系统文件 系统表空间：会产生I/O瓶颈（因为只有一个文件） 独立表空间：可以向多个文件刷新数据 总结 强烈建议：对Innodb引擎使用独立表空间（mysql5.6版本以后默认是独立表空间） 系统表转移为独立表的步骤（非常繁琐） 使用 mysqldump 导出所有数据库表数据 停止 mysql 服务，修改参数，并且删除Innodb相关文件 重启 mysql 服务，重建mysql系统表空间 重新导入数据 问：如何选择存储引擎参考条件： 是否需要事务 是否可以热备份 崩溃恢复 存储引擎的特有特性 重要一点： 不要混合使用存储引擎 强烈推荐： Innodb 问：MyISAM和InnoDB引擎的区别区别： MyISAM 不支持外键，而 InnoDB 支持 MyISAM 是非事务安全型的，而 InnoDB 是事务安全型的。 MyISAM 锁的粒度是表级，而 InnoDB 支持行级锁定。 MyISAM 支持全文类型索引，而 InnoDB 不支持全文索引。 MyISAM 相对简单，所以在效率上要优于 InnoDB，小型应用可以考虑使用 MyISAM。 MyISAM 表是保存成文件的形式，在跨平台的数据转移中使用 MyISAM 存储会省去不少的麻烦。 InnoDB 表比 MyISAM 表更安全，可以在保证数据不会丢失的情况下，切换非事务表到事务表（alter table tablename type=innodb）。 应用场景： MyISAM 管理非事务表。它提供高速存储和检索，以及全文搜索能力。如果应用中需要执行大量的 SELECT 查询，那么 MyISAM 是更好的选择。 InnoDB 用于事务处理应用程序，具有众多特性，包括 ACID 事务支持。如果应用中需要执行大量的 INSERT 或 UPDATE 操作，则应该使用 InnoDB，这样可以提高多用户并发操作的性能。 问：为什么不建议 InnoDB 使用亿级大表仅作拓展延伸，详情请转向：为什么不建议innodb使用亿级大表 | 峰云就她了 索引索引分类 特性 说明 InnoDB MyISAM MEMORY B树索引 (B-tree indexes) 自增ID物理连续性更高， 二叉树，红黑树高度不可控 √ √ √ R树索引 (R-tree indexes) 空间索引 √ 哈希索引 (Hash indexes) 无法做范围查询 √ √ 全文索引 (Full-text indexes) √ √ B+Tree 索引B+Tree 索引是大多数 MySQL 存储引擎的默认索引类型。 因为不再需要进行全表扫描，只需要对树进行搜索即可，因此查找速度快很多。除了用于查找，还可以用于排序和分组。 可以指定多个列作为索引列，多个索引列共同组成键。 哈希索引InnoDB 引擎有一个特殊的功能叫 “自适应哈希索引”，当某个索引值被使用的非常频繁时，会在 B+Tree 索引之上再创建一个哈希索引，这样就让 B+Tree 索引具有哈希索引的一些优点，比如快速的哈希查找 全文索引MyISAM 存储引擎支持全文索引，用于查找文本中的关键词，而不是直接比较是否相等。查找条件使用 MATCH AGAINST，而不是普通的 WHERE 空间数据索引（R-Tree）MyISAM 存储引擎支持空间数据索引，可以用于地理数据存储。空间数据索引会从所有维度来索引数据，可以有效地使用任意维度来进行组合查询。 mysql中的索引类型有哪些，可以从哪些角度来看？从数据结构角度 1、B+树索引(O(log(n)))：关于B+树索引，可以参考 MySQL索引背后的数据结构及算法原理 2、hash索引：a 仅仅能满足”=”,”IN”和”&lt;=&gt;”查询，不能使用范围查询b 其检索效率非常高，索引的检索可以一次定位，不像B-Tree 索引需要从根节点到枝节点，最后才能访问到页节点这样多次的IO访问，所以 Hash 索引的查询效率要远高于 B-Tree 索引c 只有Memory存储引擎显示支持hash索引 3、FULLTEXT索引（现在MyISAM和InnoDB引擎都支持了） 4、R-Tree索引（用于对GIS数据类型创建SPATIAL索引） 从物理存储角度 1、聚集索引（clustered index）聚集索引一个表只能有一个，而非聚集索引一个表可以存在多个 2、非聚集索引（non-clustered index） 聚集索引存储记录是物理上连续存在，而非聚集索引是逻辑上的连续，物理存储并不连续 从逻辑角度 1、主键索引：主键索引是一种特殊的唯一索引，不允许有空值 2、普通索引或者单列索引 : 即一个索引只包含单个列，一个表可以有多个单列索引 3、多列索引（复合索引）：复合索引指多个字段上创建的索引，只有在查询条件中使用了创建索引时的第一个字段，索引才会被使用。使用复合索引时遵循最左前缀集合 4、唯一索引或者非唯一索引 5、空间索引：空间索引是对空间数据类型的字段建立的索引，MYSQL中的空间数据类型有4种，分别是GEOMETRY、POINT、LINESTRING、POLYGON。MYSQL使用SPATIAL关键字进行扩展，使得能够用于创建正规索引类型的语法创建空间索引。创建空间索引的列，必须将其声明为NOT NULL，空间索引只能在存储引擎为MYISAM的表中创建 索引的缺点 创建索引和维护索引要耗费时间，这种时间随着数据量的增加而增加 索引需要占用物理空间，除了数据表占用数据空间之外，每一个索引还要占一定的物理空间，如果建立聚簇索引，那么需要的空间就会更大 当对表中的数据进行增加、删除和修改的时候，索引也需要维护，降低数据维护的速度 索引的优点 创建唯一性索引，保证数据库表中每一行数据的唯一性 大大加快数据的检索速度，这是创建索引的最主要的原因 加速数据库表之间的连接，特别是在实现数据的参考完整性方面特别有意义 在使用分组和排序子句进行数据检索时，同样可以显著减少查询中分组和排序的时间 通过使用索引，可以在查询中使用优化隐藏器，提高系统的性能 在什么情况下适合建立索引 为经常出现在关键字order by、group by、distinct后面的字段，建立索引。 在union等集合操作的结果集字段上，建立索引。其建立索引的目的同上。 为经常用作查询选择 where 后的字段，建立索引。 在经常用作表连接 join 的属性上，建立索引。 考虑使用索引覆盖。对数据很少被更新的表，如果用户经常只查询其中的几个字段，可以考虑在这几个字段上建立索引，从而将表的扫描改变为索引的扫描。 索引失效 如果MySQL估计使用全表扫秒比使用索引快，则不适用索引。 例如，如果列key均匀分布在1和100之间，下面的查询使用索引就不是很好：select * from table_name where key&gt;1 and key&lt;90; 如果条件中有or，即使其中有条件带索引也不会使用 例如：select * from table_name where key1=’a’ or key2=’b’;如果在key1上有索引而在key2上没有索引，则该查询也不会走索引 复合索引，如果索引列不是复合索引的第一部分，则不使用索引（即不符合最左前缀） 例如，复合索引为(key1,key2),则查询select * from table_name where key2=’b’;将不会使用索引 如果like是以 % 开始的，则该列上的索引不会被使用。 例如select * from table_name where key1 like ‘%a’；该查询即使key1上存在索引，也不会被使用如果列类型是字符串，那一定要在条件中使用引号引起来，否则不会使用索引 如果列为字符串，则where条件中必须将字符常量值加引号，否则即使该列上存在索引，也不会被使用。 例如,select * from table_name where key1=1;如果key1列保存的是字符串，即使key1上有索引，也不会被使用。 如果使用MEMORY/HEAP表，并且where条件中不使用“=”进行索引列，那么不会用到索引，head表只有在“=”的条件下才会使用索引 左连接和右连接区别 左连接where只影向右表，右连接where只影响左表。 MySQL 主从复制原理 主库将变更写入 binlog 日志，然后从库连接到主库之后，从库有一个 IO 线程，将主库的 binlog 日志拷贝到自己本地，写入一个 relay 中继日志中。接着从库中有一个 SQL 线程会从中继日志读取 binlog，然后执行 binlog 日志中的内容，也就是在自己本地再次执行一遍 SQL，这样就可以保证自己跟主库的数据是一样的。 MysqlMysql 的分页 SQL 语句select * from tablename limit m,n(n是指从第m+1条开始，取n条) 下面提供几个查询优化的建议。使用explain分析查询语句前面已经演示过如何使用explain命令分析查询语句了，这里再解释一下其中几个有参考价值的字段的含义： select_typeselect_type表示查询中每个select子句的类型，一般有下面几个值: simple 简单SELECT,不使用UNION或子查询等。 primary 查询中若包含任何复杂的子部分,最外层的select被标记为PRIMARY。 union union中的第二个或后面的SELECT语句。 dependent union union 中的第二个或后面的SELECT语句，取决于外面的查询。 union result union 的结果。 subquery子查询中的第一个SELECT。 dependent subquery子查询中的第一个SELECT，取决于外面的查询。 derived派生表的SELECT, FROM子句的子查询。 uncacheable subquery一个子查询的结果不能被缓存，必须重新评估外链接的第一行。 typetype表示MySQL在表中找到所需行的方式，又称“访问类型”，常用的类型有： ALL, index, range, ref, eq_ref, const, system, NULL。 从左到右，性能从差到好。 ALL： Full Table Scan，MySQL将遍历全表以找到匹配的行。 index: Full Index Scan，index与ALL区别为index类型只遍历索引树。 range: 只检索给定范围的行，使用一个索引来选择行。 ref: 表示上述表的连接匹配条件，即哪些列或常量被用于查找索引列上的值。 eq_ref: 类似ref，区别就在使用的索引是唯一索引，对于每个索引键值，表中只有一条记录匹配，简单来说，就是多表连接中使用primary key或者 unique key作为关联条件。 const: 当MySQL对查询某部分进行优化，并转换为一个常量时，使用这些类型访问。 如将主键置于where列表中，MySQL就能将该查询转换为一个常量。 NULL: MySQL在优化过程中分解语句，执行时甚至不用访问表或索引，例如从一个索引列里选取最小值可以通过单独索引查找完成。 Keykey列显示MySQL实际决定使用的键（索引），如果没有选择索引，键是NULL。 possible_keyspossible_keys指出MySQL能使用哪个索引在表中找到记录，查询涉及到的字段上如果存在索引则该索引将被列出，但不一定被查询使用。 refref表示上述表的连接匹配条件，即哪些列或常量被用于查找索引列上的值。 rowsrows表示MySQL根据表统计信息，以及索引选用的情况，找到所需记录需要读取的行数。这个行数是估算的值，实际行数可能不同。 声明NOT NULL当数据列被声明为NOT NULL以后，在查询的时候就不需要判断是否为NULL，由于减少了判断，可以降低复杂性，提高查询速度。 如果要表示数据列为空，可以使用0等代替。 考虑使用数值类型代替字符串MySQL对数值类型的处理速度要远远快于字符串，而且数值类型往往更加节省空间。 例如对于“Male”和“Female”可以用“0”和“1”进行代替。 考虑使用ENUM类型如果你的数据列的取值是确定有限的，可以使用ENUM类型代替字符串。因为MySQL会把这些值表示为一系列对应的数字，这样处理的速度会提高很多。 123456789CREATE TABLE shirts ( name VARCHAR(40), size ENUM('x-small', 'small', 'medium', 'large', 'x-large'));INSERT INTO shirts (name, size) VALUES ('dress shirt','large'), ('t-shirt','medium'), ('polo shirt','small');SELECT name, size FROM shirts WHERE size = 'medium'; 总结索引是一个单独的，存储在磁盘上的数据结构，索引对数据表中一列或者多列值进行排序，索引包含着对数据表中所有数据的引用指针。 本教程从MySQL开始讲起，又介绍了MySQL中索引的使用，最后提供了使用索引的几条原则和优化查询的几个方法。 Oracleoracle中row_id理解ORACLE的row_id是一个伪列,其个是为18个字节可将这18个字节用6363来划分,分别表示段编号,数据文件编号,数据块 嵌入式数据库和传统数据库的区别嵌入式数据库主要像：SQLite、 传统数据库服务器：SQL Server、Oracle、MySQL 嵌入式数据库：SQLite的主要特点： 支持事件，不需要配置，不需要安装，也不需要管理员； 支持大部分SQL92； 一个完整的数据库保存在磁盘上面一个文件，同一个数据库文件可以在不同机器上面使用，最大支持数据库到2T，字符和BLOB的支持仅限制于可用内存； 整个系统少于3万行代码，少于250KB的内存占用(gcc)，大部分应用比目前常见的客户端/服务端的数据库快，没有其它依赖 源代码开放，代码95%有较好的注释，简单易用的API。官方带有TCL的编译版本。 关系数据库特点： 更好的安全性、多用户管理 强大的数据管理能力，如索引、视图等关系对象 强大的数据库编程式的设计，像T-SQL、存储过程、游标 丰富的数据类型 Inserted和deleted的含义inserted表反映插入或更新操作时插入的记录 deleted表反映删除或更新操作时删除的记录 函数和过程的区别存储过程： 一般用于在数据库中完成特定的业务或任务 可以定义返回类型，也可以不定义返回类型 SQL语句中不可以调用 函数： 一般用于特定的数据查询或数据转转换处理 申请时必须要定义返回类型，且程序体中必须定义return语句 不能独立执行，必须作为表达式的一部分调用 SQL语句中可以调用 数据库优化的方案建立主键，为数据库创建索引，建立存储过程，触发器，可提高查询速度。 Oracle中有哪几种索引 单列索引与复合索引：一个索引可以由一个或多个列组成，用来创建索引的列被称为“索引列”。单列索引是基于单列所创建的索引，复合索引是基于两列或者多列所创建的索引。 唯一索引与非唯一索引：唯一索引是索引列值不能重复的索引，非唯一索引是索引列可以重复的索引。无论是唯一索引还是非唯一索引，索引列都允许取NULL值。默认情况下，Oracle创建的索引是不唯一索引。 B树索引：B树索引是按B树算法组织并存放索引数据的，所以B树索引主要依赖其组织并存放索引数据的算法来实现快速检索功能。 位图索引：位图索引在多列查询时，可以对两个列上的位图进行AND和OR操作，达到更好的查询效果。 函数索引：Oracle中不仅能够直接对表中的列创建索引，还可以对包含列的函数或表达式创建索引，这种索引称为“位图索引 数据库索引的优点和缺点优点： 通过创建唯一性索引，可以保证数据库表中每一行数据的唯一性。 可以大大加快数据的检索速度，这也是创建索引的最主要的原因。 可以加速表和表之间的连接，特别是在实现数据的参考完整性方面特别有意义。 在使用分组和排序子句进行数据检索时，同样可以显著减少查询中分组和排序的时间。 通过使用索引，可以在查询的过程中，使用优化隐藏器，提高系统的性能。 缺点： 创建索引和维护索引要耗费时间，这种时间随着数据量的增加而增加。 索引需要占物理空间，除了数据表占数据空间之外，每一个索引还要占一定的物理空间，如果要建立聚簇索引，那么需要的空间就会更大。 当对表中的数据进行增加、删除和修改的时候，索引也要动态的维护，这样就降低了数据的维护速度。 触发器有几种共2种，一种DML触发，就是遇到DML事件时触发执行，像insert\update\delete。一种DDL触发，遇到DDL事件时触发，像Login Datatabase、更改数据库状态、create语句等。 oracle中除了数据库备份，还有什么方法备份Oracle数据库有三种标准的备份方法，它们分别是导出/导入(EXP/IMP)、热备份和冷备份。导出备份是一种逻辑备份，冷备份和热备份是物理备份。 10G有几种新功能进行备份，像数据磅 写出删除表中重复记录的语句oracle12delete from peoplewhere peopleId in (select peopleId from people group by peopleId having count(peopleId) &gt; 1) and rowid not in (select min(rowid) from people group by]]></content>
      <categories>
        <category>基础面试题</category>
      </categories>
      <tags>
        <tag>基础面试题</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SpringBoot和SpringCloud面试题]]></title>
    <url>%2F2019%2F05%2F12%2F%E9%9D%A2%E8%AF%95%E6%80%BB%E7%BB%93%2FSpringBoot%E5%92%8CSpringCloud%E9%9D%A2%E8%AF%95%E9%A2%98%2F</url>
    <content type="text"><![CDATA[Spring Boot 与 Spring 的区别 Spring Boot可以建立独立的Spring应用程序； 内嵌了如Tomcat，Jetty和Undertow这样的容器，也就是说可以直接跑起来，用不着再做部署工作了。 无需再像Spring那样搞一堆繁琐的xml文件的配置； 可以自动配置Spring； 提供了一些现有的功能，如度量工具，表单数据验证以及一些外部配置这样的一些第三方功能； 提供的POM可以简化Maven的配置； SpringBoot 的自动配置是怎么做的 Spring boot 的所有自动化配置的实现都在 spring-boot-autoconfigure 依赖中，通过@EnableAutoConfiguration 核心注解初始化，并扫描 ClassPath 目录中自动配置类对应依赖。并对对应的组件依赖按一定规则获取默认配置并自动初始化所需要的 Bean。 先答为什么需要自动配置？顾名思义，自动配置的意义是利用这种模式代替了配置 XML 繁琐模式。以前使用 Spring MVC ，需要进行配置组件扫描、调度器、视图解析器等，使用 Spring Boot 自动配置后，只需要添加 MVC 组件即可自动配置所需要的 Bean。所有自动配置的实现都在 spring-boot-autoconfigure 依赖中，包括 Spring MVC 、Data 和其它框架的自动配置。 接着答spring-boot-autoconfigure 依赖的工作原理?spring-boot-autoconfigure 依赖的工作原理很简单，通过 @EnableAutoConfiguration 核心注解初始化，并扫描 ClassPath 目录中自动配置类对应依赖。比如工程中有木有添加 Thymeleaf 的 Starter 组件依赖。如果有，就按按一定规则获取默认配置并自动初始化所需要的 Bean。 ioc的思想最核心的地方在于，资源不由使用资源的双方管理，而由不使用资源的第三方管理，这可以带来很多好处： 第一，资源集中管理，实现资源的可配置和易管理。 第二，降低了使用资源双方的依赖程度，也就是我们说的耦合度。 什么是springboot用来简化spring应用的初始搭建以及开发过程 使用特定的方式来进行配置（properties或yml文件） 创建独立的spring引用程序 main方法运行 嵌入的Tomcat 无需部署war文件 简化maven配置 自动配置spring添加对应功能starter自动化配置 springboot常用的starter有哪些spring-boot-starter-web 嵌入tomcat和web开发需要servlet与jsp支持 spring-boot-starter-data-jpa 数据库支持 spring-boot-starter-data-redis redis数据库支持 spring-boot-starter-data-solr solr支持 mybatis-spring-boot-starter 第三方的mybatis集成starter springboot自动配置的原理在spring程序main方法中 添加@SpringBootApplication或者@EnableAutoConfiguration 会自动去maven中读取每个starter中的spring.factories文件 该文件里配置了所有需要被创建spring容器中的bean springboot读取配置文件的方式springboot默认读取配置文件为application.properties或者是application.yml springboot集成mybatis的过程12345678添加mybatis的starter maven依赖 &lt;dependency&gt; &lt;groupId&gt;org.mybatis.spring.boot&lt;/groupId&gt; &lt;artifactId&gt;mybatis-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;1.2.0&lt;/version&gt; &lt;/dependency&gt;在mybatis的接口中 添加@Mapper注解在application.yml配置数据源信息 ​ springboot如何添加【修改代码】自动重启功能添加开发者工具集=====spring-boot-devtools 1234567&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-devtools&lt;/artifactId&gt; &lt;optional&gt;true&lt;/optional&gt; &lt;/dependency&gt;&lt;/dependencies&gt; SpringBoot2.0新增什么 12345678910111.支持 Java92.基于 Spring5 构建3.Tomcat 升级到 8.54.Flyway 升级到 55.Hibernate 升级到 5.26.Thymeleaf 升级到 3 ​ 什么是微服务以前的模式是 所有的代码在同一个工程中 部署在同一个服务器中 同一个项目的不同模块不同功能互相抢占资源 微服务 将工程根据不同的业务规则拆分成微服务 微服务部署在不同的机器上 服务之间进行相互调用 Java微服务的框架有 dubbo（只能用来做微服务），spring cloud（提供了服务的发现，断路器等） springcloud如何实现服务的注册和发现服务在发布时 指定对应的服务名（服务名包括了IP地址和端口） 将服务注册到注册中心（eureka或者zookeeper） 这一过程是springcloud自动实现 只需要在main方法添加@EnableDisscoveryClient 同一个服务修改端口就可以启动多个实例调用方法：传递服务名称通过注册中心获取所有的可用实例 通过负载均衡策略调用（ribbon和feign）对应的服务 ribbon和feign区别Ribbon添加maven依赖 spring-starter-ribbon 使用@RibbonClient(value=&quot;服务名称&quot;) 使用RestTemplate调用远程服务对应的方法feign添加maven依赖 spring-starter-feign 服务提供方提供对外接口 调用方使用 在接口上使用@FeignClient(&quot;指定服务名&quot;) Ribbon和Feign的区别：Ribbon和Feign都是用于调用其他服务的，不过方式不同。 1.启动类使用的注解不同，Ribbon用的是@RibbonClient，Feign用的是@EnableFeignClients。 2.服务的指定位置不同，Ribbon是在@RibbonClient注解上声明，Feign则是在定义抽象方法的接口中使用@FeignClient声明。 3.调用方式不同，Ribbon需要自己构建http请求，模拟http请求然后使用RestTemplate发送给其他服务，步骤相当繁琐。Feign则是在Ribbon的基础上进行了一次改进，采用接口的方式，将需要调用的其他服务的方法定义成抽象方法即可， 不需要自己构建http请求。不过要注意的是抽象方法的注解、方法签名要和提供服务的方法完全一致。 springcloud断路器的作用当一个服务调用另一个服务由于网络原因或者自身原因出现问题时 调用者就会等待被调用者的响应 当更多的服务请求到这些资源时导致更多的请求等待 这样就会发生连锁效应（雪崩效应） 断路器就是解决这一问题 断路器有完全打开状态 一定时间内 达到一定的次数无法调用 并且多次检测没有恢复的迹象 断路器完全打开，那么下次请求就不会请求到该服务 半开 短时间内 有恢复迹象 断路器会将部分请求发给该服务 当能正常调用时 断路器关闭 关闭 当服务一直处于正常状态 能正常调用 断路器关闭 服务注册发现组件Eureka工作原理 服务网关组件Zuul工作原理 跨域时序图 Eureka与Ribbon整合工作原理 解决分布式一致性 级联故障流程 断路器组件Hystrix工作原理 分布式追踪Sleuth工作原理 SpringBoot自动配置工作原理 什么是SpringCloudSpring cloud流应用程序启动器是基于Spring Boot的Spring集成应用程序，提供与外部系统的集成。Spring cloud Task，一个生命周期短暂的微服务框架，用于快速构建执行有限数据处理的应用程序。 使用Spring Cloud有什么优势使用Spring Boot开发分布式微服务时，我们面临以下问题 与分布式系统相关的复杂性-这种开销包括网络问题，延迟开销，带宽问题，安全问题。 服务发现-服务发现工具管理群集中的流程和服务如何查找和互相交谈。它涉及一个服务目录，在该目录中注册服务，然后能够查找并连接到该目录中的服务。 冗余-分布式系统中的冗余问题。 负载平衡 –负载平衡改善跨多个计算资源的工作负荷，诸如计算机，计算机集群，网络链路，中央处理单元，或磁盘驱动器的分布。 性能-问题 由于各种运营开销导致的性能问题。 部署复杂性-Devops技能的要求。 服务注册和发现是什么意思？Spring Cloud如何实现？当我们开始一个项目时，我们通常在属性文件中进行所有的配置。随着越来越多的服务开发和部署，添加和修改这些属性变得更加复杂。有些服务可能会下降，而某些位置可能会发生变化。手动更改属性可能会产生问题。 Eureka服务注册和发现可以在这种情况下提供帮助。由于所有服务都在Eureka服务器上注册并通过调用Eureka服务器完成查找，因此无需处理服务地点的任何更改和处理。 负载平衡的意义什么在计算中，负载平衡可以改善跨计算机，计算机集群，网络链接，中央处理单元或磁盘驱动器等多种计算资源的工作负载分布。负载平衡旨在优化资源使用，最大化吞吐量，最小化响应时间并避免任何单一资源的过载。使用多个组件进行负载平衡而不是单个组件可能会通过冗余来提高可靠性和可用性。负载平衡通常涉及专用软件或硬件，例如多层交换机或域名系统服务器进程。 什么是Hystrix？它如何实现容错？Hystrix是一个延迟和容错库，旨在隔离远程系统，服务和第三方库的访问点，当出现故障是不可避免的故障时，停止级联故障并在复杂的分布式系统中实现弹性。 通常对于使用微服务架构开发的系统，涉及到许多微服务。这些微服务彼此协作。 思考以下微服务 假设如果上图中的微服务9失败了，那么使用传统方法我们将传播一个异常。但这仍然会导致整个系统崩溃。 随着微服务数量的增加，这个问题变得更加复杂。微服务的数量可以高达1000.这是hystrix出现的地方 我们将使用Hystrix在这种情况下的Fallback方法功能。我们有两个服务employee-consumer使用由employee-consumer公开的服务。 简化图如下所示 现在假设由于某种原因，employee-producer公开的服务会抛出异常。我们在这种情况下使用Hystrix定义了一个回退方法。这种后备方法应该具有与公开服务相同的返回类型。如果暴露服务中出现异常，则回退方法将返回一些值。 什么是Hystrix断路器？我们需要它吗？由于某些原因，employee-consumer公开服务会引发异常。在这种情况下使用Hystrix我们定义了一个回退方法。如果在公开服务中发生异常，则回退方法返回一些默认值。 如果firstPage method() 中的异常继续发生，则Hystrix电路将中断，并且员工使用者将一起跳过firtsPage方法，并直接调用回退方法。 断路器的目的是给第一页方法或第一页方法可能调用的其他方法留出时间，并导致异常恢复。可能发生的情况是，在负载较小的情况下，导致异常的问题有更好的恢复机会 。 什么是Netflix Feign？它的优点是什么？Feign是受到Retrofit，JAXRS-2.0和WebSocket启发的java客户端联编程序。Feign的第一个目标是将约束分母的复杂性统一到http apis，而不考虑其稳定性。在employee-consumer的例子中，我们使用了employee-producer使用REST模板公开的REST服务。 但是我们必须编写大量代码才能执行以下步骤 使用功能区进行负载平衡。 获取服务实例，然后获取基本URL。 利用REST模板来使用服务。 前面的代码如下 123456789101112131415161718192021222324252627@Controllerpublic class ConsumerControllerClient &#123; @Autowiredprivate LoadBalancerClient loadBalancer; public void getEmployee() throws RestClientException, IOException &#123; ServiceInstance serviceInstance=loadBalancer.choose("employee-producer"); System.out.println(serviceInstance.getUri()); String baseUrl=serviceInstance.getUri().toString(); baseUrl=baseUrl+"/employee"; RestTemplate restTemplate = new RestTemplate(); ResponseEntity&lt;String&gt; response=null; try&#123; response=restTemplate.exchange(baseUrl, HttpMethod.GET, getHeaders(),String.class); &#125;catch (Exception ex) &#123; System.out.println(ex); &#125; System.out.println(response.getBody());&#125; 之前的代码，有像NullPointer这样的例外的机会，并不是最优的。我们将看到如何使用Netflix Feign使呼叫变得更加轻松和清洁。如果Netflix Ribbon依赖关系也在类路径中，那么Feign默认也会负责负载平衡。 什么是Spring Cloud Bus？我们需要它吗？考虑以下情况：我们有多个应用程序使用Spring Cloud Config读取属性，而Spring Cloud Config从GIT读取这些属性。 下面的例子中多个员工生产者模块从 什么是微服务架构1234567891011 1.微服务是系统架构上的一种设计风格，他的主旨是将一个原本独立的系统拆分成多个小型服务2.这些小型服务都在各自独立的进程中运行3.服务之间通过基于HTTP的RESTful API进行通信协作4.被拆分的每个微服务都围绕系统中的某一项或一些耦合度较高的业务功能进行构建5.并且每个微服务都维护着自身的数据存储，业务开发，自动化测试案例以及独立部署机制6.由于有了轻量级的通信协作基础，所以这些微服务可以使用不同的语言来编写 微服务之间是如何独立通讯的？12345同步：RPC,REST等；使用HTTP的RESTful API或轻量级的消息发送协议，实现信息传递与服务调用的触发。 异步：消息队列，要考虑可靠传输，高性能，以及编程模型的编号等；通过在轻量级的消息总线上传递消息，类似Rabbitmq等一些提供可靠异步 交换的中间件。 谈谈你对SpringBoot,SpringCloud的理解什么是服务熔断，什么是服务降级？扇出与雪崩响应： 多位微服务之间调用的时候，假设微服务A调用微服务B和微服务C，微服务B和微服务C又调用其他微服务，这就是所谓的”扇处”。如果扇处的链路上某个微服务的调用响应时间过长或者不可用，对微服务A的调用就会占用越来越多的系统资源，进而引起系统崩溃—所谓的”雪崩效应”。 服务熔断： 熔断机制是应对雪崩效应的一种微服务链路保护机制！！！当扇出链路的某个微服务不可用或者响应时间太长时，会进行服务的降级，进而熔断该节点微服务的调用，快速返回“错误”的响应信息。当检测到该节点微服务调用响应正常后恢复链路。在SpringCloud框架里熔断机制通过Hystrix实现。Hystrix会监控微服务调用的状况，当失败的调用到一定阈值就会启动熔断机制(默认是5秒内20次调用失败就会启动熔断机制)。但服务熔断来处理雪崩效应是非常可怕的，不可取的。第一如果有多个方法，对应的FallBack方法也会随之膨胀；第二异常处理与业务逻辑高耦合，完全不符合Spring AOP面向切面的思想。 服务降级： 整体资源快不够了，忍痛将某些服务先关掉，待渡过难关，再开启回来。服务降级处理是在客户端(消费者)完成的与服务端没有关系。 个人感觉：多个微服务之间进行调用，假设微服务A调用微服务B和微服务C,微服务B和微服务C又调用其他微服务，这就是所谓的“扇出”。如果扇出链路上某个微服务的调用响应时间过长或者不可用，对微服务A的调用就会占用越来越多的系统资源，进而引起系统崩溃—所谓的“雪崩效应”。 服务熔断和服务降级都是为了解决雪崩效应，只是二者实现方式不同： 服务熔断在服务提供端，进行服务降级，进而熔断该节点微服务的调用，快速返回“失败”的响应信息。当检测到 该节点微服务调用响应正常恢复链路。当失败的次数达到一定阈值就会启动熔断机制服务降级，处理方式在客户端完成与服务端有没有关系，整体资源快不够了，忍痛将某些服务先关掉，待度过难关再开启回来。 服务监控：htstrix dashbord 微服务的优缺点？优点： 每个服务足够内聚，足够小，代码容易理解这样能聚焦一个指定的业务功能或业务需求 开发简单、开发效率提高，一个服务可能就是专一的只干一件事。 微服务能够被小团队独立开发，这个团队可以使2到5人的开发人员组成。 微服务是松耦合的，是有功能意义的服务，无论实在开发阶段或部署阶段都是独立的。 微服务能使用不同的语言开发。 微服务只是业务逻辑的代码，不会和HTML,CSS或其他界面组件混合。 每个微服务都有自己的存储能力，可以有自己的数据库。也可以有统一的数据库。 缺点： 开发人员要处理分布式系统的复杂性 多服务运维难度，随着服务的增加，运维的压力也在增加 系统部署依赖 服务间通信成本 数据一致性 系统集成测试 性能监控 分布式事务？使用lcn 谈谈你所知道的微服务技术栈有哪些？什么是springbootSpring Boot的宗旨并非要重写Spring或是替代Spring,而是希望通过设计大量的自动化配置等方式来简化Spring原有样板化的配置，用来简化Spring初始构建以及开发工程的，使用特定的方式来进行配置 大量的自动化配置来简化Spring原有样板化的配置， 简化Maven配置，是依赖管理变得更加简单(添加对应功能的starter)， 快速开发(创建独立的Spring引用程序，main方法运行)， 轻松部署(内嵌的tomcat，不需要部署war文件，直接运行jar文件) SpringBoot自动配置的原理在spring程序main方法中 添加@SpringBootApplication或者@EnableAutoConfiguration会自动去maven中读取每个starter中的spring.factories文件 该文件里配置了所有需要被创建spring容器中的bean springboot读取配置文件的方式springboot默认读取配置文件为application.properties或者是application.yml springCloud如何实现服务的注册和发现的服务在发布时 指定对应的服务名（服务名包括了IP地址和端口） 将服务注册到注册中心（eureka或者zookeeper）这一过程是springcloud自动实现 只需要在main方法添加@EnableDisscoveryClient 同一个服务修改端口就可以启动多个实例调用方法：传递服务名称通过注册中心获取所有的可用实例 通过负载均衡策略调用（ribbon和feign）对应的服务 对外服务接口带有Request参数的请求，带有Header信息的请求，带有RequestBody的请求以及请求响应体是一个对象的请求，Spring Cloud Feign接口调用如何绑定参数？注意：在各参数绑定时，@RequestParam,@RequestHeader等可以指定参数名称的注解，它的value不能少。在SpringMVC程序中，这些注解会根据参数名作为默认值，但是在Feign中绑定参数必须通过value属性来指明具体的参数名，不然抛异常，value不能少。 Feign与Ribbon，Hystrix的关系Feign的客户端负载均衡是通过SpringCloud Ribbon实现的，所以可以直接通过配置Ribbon客户端的方式来自定义各个服务客户端的参数。Feign引入了服务保护与容错的工具Hystix,默认情况下，Feign将所有Feign客户端方法都封装到Hystrix命令中进行服务保护。 Ribbon全局配置，Ribbon指定服务配置，Ribbon重要的配置参数，Ribbon重试机制Ribbon全局配置： 12ribbon.ConnectTimeout = 500ribbon.ReadTimeout = 5000 Ribbon指定服务配置： 1在SpringCloud Feign中针对各个服务客户端进行个性化配置的方式与在SpringCloud Ribbon的配置方式是一样的，都采用&lt;Client&gt;.ribbon.key=value的格式进行设置。服务名就是@FeignClients指定的name或value。 Ribbon重要的配置参数： 12345678910#建立连接的超时时间HELLO-SERVICE.ribbon.ConnectTimeout = 500#连接超时时间HELLO-SERVICE.ribbon.ReadTimeout = 2000#所有操作都重试 HELLO-SERVICE.ribbon.OkToRetryOnAllOperations = true#重试发生，更换节点数最大值HELLO-SERVICE.ribbon.MaxAutoRetriesNextServer = 2#单个节点重试最大值HELLO-SERVICE.ribbon.MaxAutoRetries = 1 重试机制： 1234567891011121314SpringCloud Feign默认实现了重试机制，重试时间计算方式。Feign重试机制默认关闭，以免和ribbon冲突。ribbon: ReadTimeout: 3000 ConnectTimeout: 3000 MaxAutoRetries: 1 #同一台实例最大重试次数,不包括首次调用 MaxAutoRetriesNextServer: 1 #重试负载均衡其他的实例最大重试次数,不包括首次调用 OkToRetryOnAllOperations: false #是否所有操作都重试 根据上面的参数计算重试的次数：MaxAutoRetries+MaxAutoRetriesNextServer+(MaxAutoRetries *MaxAutoRetriesNextServer) 即重试3次则一共产生4次调用。如果在重试期间，时间超过了hystrix的超时时间，便会立即执行熔断，fallback。所以要根据上面配置的参数计算hystrix的超时时间，使得在重试期间不能达到hystrix的超时时间，不然重试机制就会没有意义hystrix超时时间的计算： (1 + MaxAutoRetries + MaxAutoRetriesNextServer) * ReadTimeout 即按照以上的配置 hystrix的超时时间应该配置为 （1+1+1）*3=9秒当ribbon超时后且hystrix没有超时，便会采取重试机制。当OkToRetryOnAllOperations设置为false时，只会对get请求进行重试。如果设置为true，便会对所有的请求进行重试，如果是put或post等写操作，如果服务器接口没做幂等性，会产生不好的结果，所以OkToRetryOnAllOperations慎用。如果不配置ribbon的重试次数，默认会重试一次注意： 默认情况下,GET方式请求无论是连接异常还是读取异常,都会进行重试 非GET方式请求,只有连接异常时,才会进行重试 Ribbon超时与Hystrix超时区别让Hystrix超时时间大于Ribbon超时时间,否则Hystrix命令直接熔断，重试机制就没意义了 服务降级，服务容错，断路器三者关系服务降级是服务容错的重要功能，服务降级的实现只需为Feign客户端定义接口编写一个具体的接口实现类，每个重写方法的实现逻辑都可以用来定义相应的服务降级逻辑。每一个服务接口的断路器实际就是实现类中重写函数的实现。 Zuul网关服务主要解决的问题从运维人员角度看： 客户端请求通过F5,Nginx等设施的路由和负载均衡分配后，被转发到不同的服务实例上。当有实例增减或是IP地址变动时，运维人员需要手工维护路由规则和服务实例。当系统复杂时，就需要一套机制有效降低维护路由规则与服务实例列表的难度。 从开发人员角度看： 为了保证对外服务的安全性，所有对外的微服务接口往往都会有一定的权限校验机制。为了防止客户端在发起请求时被篡改等安全方面的考虑，需要签名校验，对校验的修改，扩展，优化，就需要解决各个前置校验的冗余问题。 API网关是整个微服务架构系统的入口，所有的外部客户端访问都需要经过他来进行调度和过滤。它处理要实现请求路由，负载均衡，校验过滤等功能之外，还需要更多的能力，比如与服务治理框架结合，请求转发时的熔断机制，服务的聚合等一系列高级功能。 Zuul如何维护路由规则和服务实例列表，如何解决签名校验，登录校验在微服务架构中的冗余问题服务实例列表的维护：Zuul与服务治理框架整合，将自身注册为服务治理下的应用，从服务中心获取所有其他微服务的实例信息。 路由规则的维护：Zuul默认会通过以服务名的作为contextPath的方式来创建路由映射。 校验逻辑在本质上与微服务应用自身的业务没有多大关系，完全可以把它们独立成一个单独的服务存在，只是这个微服务不是给各个微服务调用，而是在API网关服务上进行统一调用来对微服务接口做前置过滤，以实现对微服务接口的拦截和过滤。Zuul提供了一套过滤器机制，通过创建各种过滤器，然后指定哪些规则的请求需要执行校验逻辑，只有通过的校验才会被路由到具体的微服务接口，不然返回错误信息。 Zuul的请求过滤功能(安全校验，权限控制)通过继承ZuulFilter抽象类并重写了下面4个方法，来实现自定义的过滤器 filterType:过滤器类型，决定过滤器在请求的哪个生命周期中执行，具体如下： pre:可以在请求被路由之前调用 routing:在路由请求时被调用 post:在routing和error过滤器之后被调用 error:处理请求时发生错误时被调用 filterOrder:过滤器执行顺序，当请求存在多个过滤器时，需要根据该方法返回值依次执行，数值越小优先级越高 shouldFilter:判断该过滤器是否需要被执行。实际中我们利用这个函数来制定过滤器的有效范围 run:过滤器具体的执行逻辑 1234567891011121314public Object run() throws ZuulException &#123; RequestContext ctx = RequestContext.getCurrentContext(); HttpServletRequest request = ctx.getRequest(); logger.info("send &#123;&#125; request to &#123;&#125;" + request.getMethod(), request.getRequestURL().toString()); Object accessToken = request.getParameter("accessToken"); if (accessToken == null) &#123; logger.warn("access token is empty!"); ctx.setSendZuulResponse(false); //不对请求进行路由 ctx.setResponseStatusCode(401); //设置错误码 return null; //请求返回值为null &#125; logger.info("access token ok"); return null; //实际返回值路由成功后的返回值 &#125; Zuul两个核心功能1.作为系统的统一入口，屏蔽了系统内部各个微服务的细节 2.可以与服务治理框架结合，实现自动化的服务实例以及负载均衡的路由转发功能 3.可以实现接口的权限校验与微服务业务逻辑的解耦。 4.通过网关中的过滤器，在各个生命周期中去校验请求的内容，将原本在对外服务层做的校验前移，保证了微服务的无状态性，同时降低了微服务的测试难度，让服务本身更专注业务逻辑的处理。 Zuul完成路由工作原理Zuul与服务治理框架整合，在其帮助下，API网关服务本身就已经维护了系统中所有serviceId与实例地址的映射关系。当有外部请求到达API网关的时候，根据请求的url路径找到最佳匹配的path规则，API网关就可以知道要将该请求路由到哪个具体的serviceId上去。由于在API网关中已经知道serviceId对应服务实例的地址清单，那么只需要通过Ribbon的负载均衡策略，直接在这些实例清单中选择一个具体的实例进行转发就能完成路由工作。 Zuul对Cookie和头信息的处理默认情况下，SpringCloud Zuul在请求路由时，会过滤掉HTTP请求头信息的一些敏感信息，防止他们被传递到下游的外部服务器。默认的敏感头信息通过zuul.sensitiveHeaders参数定义，包括Cookie，Set-Cookie，Authorization三个属性。Cookie在Zuul网关中默认是不会传递的，引发问题：Web应用无法实现登录和鉴权。 全局设置： zuul.sensitive-headers= #是所有经过zuul的敏感头信息有效 指定路由设置： zuul.routes..sensitive-headers= #对指定路由开启自定义敏感头 zuul.routes..custom-sensitive-headers=true #将指定路由的敏感头设置为空 Zuul异常处理SpringBoot全局异常处理 实现SendErrorFilter过滤器的处理异常响应结果 Zuul动态加载微服务架构中，由于API网关服务担负着外部访问统一入口的重任，与其他应用不同，任务关闭应用和重启应用的操作都会使系统对外服务停止，对于高可用的系统来说绝对不能被允许。所以网关服务必须具备动态更新内部逻辑的能力，比如动态修改路由规则，动态添加/删除过滤器等。 动态路由：对于路由规则的控制几乎都可以在配置文件中完成，所以很自然的将它与SpringCloud Config动态刷新机制联系到一起。只需将API网关服务的配置文件通过Config连接的Git仓库存储和管理，就能轻松实现动态刷新路由规则的功能。 /routes 接口来获取当前网关上的路由规则 /refresh 接口发送POST请求来刷新配置信息 动态过滤器； Config:分布式配置中心Config介绍为分布式系统中的微服务应用提供集中化的外部配置支持，分为服务端与客户端。其中服务端也称为分布式配置中心，用来连接配置仓库并未客户端提供获取配置信心，加密/解密信息等访问接口；而客户端则是微服务架构中的各个微服务应用或基础设施，通过指定的配置中心来管理应用资源与业务相关的配置内容，并在启动的时候从配置中心获取和加载配置信息。 客户端应用从配置管理中获取配置信息的执行流程1.客户端应用启动时，根据bootstrap.properties中配置的应用名{application},环境名{profile},分支名{label}，向Config Server请求获取配置信息 2.Config Server根据自己维护的Git仓库信息和客户端传递过来的配置定位信息去查找配置信息 3.通过git clone命令将找到的配置信息下载到Config Server的文件系统中 4.Config Server创建Spring的ApplicationContext实例，并从Git本地仓库中加载配置文件，最后将这些配置内容读取出来返回给客户端应用 5.客户端应用在获得外部配置文件后加载到客户端的ApplicationContext实例中，该配置内容的优先级大于客户端jar包内部的配置内容，所以jar包中重复的内容将不再被加载 Git配置仓库，SVN配置仓库，本地仓库，高可用配置高可用配置两种方式：传统模式(不需要注册中心),服务模式(将Config Server作为一个普通的微服务应用，微服务应用通过配置中心的服务名获取配置信息) 健康监测属性覆盖Config Server可以使用属性覆盖Git配置的属性信息，可以为SpringCloud应用配置一些共同属性或默认属性 安全保护，加密解密，高可用配置加密解密：对称加密与非对称加密 配置文件中数据库的用户名与密码等敏感信息不应该对外暴露，就可以使用加密方式，对密码用户名进行加密。当微服务客户端加载配置时，配置中心会自动为带有{cipher}前缀的值进行解密。 高可用配置-服务端： 12345678910111213 server: port: 13001spring: application: name: config-server cloud: config: server: git: uri: https://gitee.com/SunYanGang/cloud-config-server.git search-paths: /SunYanGang/cloud-config-server password: mayun_6688 username: 18811748164 高可用配置-客户端： 123456789101112131415161718192021222324252627 eureka: client: fetch-registry: true register-with-eureka: true service-url: defaultZone: http://eurekaserver01:8080/eureka,http://eurekaserver02:8081/eureka,http://eurekaserver03:8082/eurekaserver: port: 12001spring: application: name: api-gateway #指定应用名称 cloud: config: profile: dev #指定配置文件的环境 label: master #指定git discovery: enabled: true #开启通过服务来访问Config Server的功能 service-id: config-server #指定服务中心注册的服务名management: security: enabled: falseeureka: client: service-url: defaultZone: http://eurekaserver01:8080/eureka,http://eurekaserver02:8081/eureka,http://eurekaserver03:8082/eureka fetch-registry: true register-with-eureka: true 失败快速响应与重试SpringCloud Config的客户端会预先加载很多其他信息，然后再开始连接Config Server进行属性的注入。连接Config Server发费较长的时间，所以我们希望快速知道当前应用是否能顺利地从Config Server获取到配置信息。还有重试机制，避免一些间歇性问题引起的失败导致客户端应用无法启动的情况。 获取远程配置application:应用名 profile:应用环境 label:分支名 通过客户端配置方式加载的内容如下： spring.application.name:对应配置文件中的{application}内容 spring.application.name:对应配置文件中的{application}内容 spring.cloud.config.label:对应分支内容，如不配置，默认为master 动态刷新配置配置中心的客户端pom.xml文件新增 spring-boot-starter-actuator监控模块，其中包含了/refresh端点的实现，该端点将用于实现客户端应用配置信息的重新获取与刷新。 POST请求向客户端发送 http://配置中心客户端：端口/refresh consul,eureka,ribbon,hystrix,feign,zuul,config,bus(rabbitmq)理解与问题Eureka理论：1.服务治理：为了解决微服务架构中的服务实例维护问题(传统的一个服务调用另一个服务，需要静态配置另一个服务的接口路径，一旦IP，端口等变化，需要手工修改)，产生了大量的服务治理框架和产品。这些框架和产品的实现都是围绕着服务注册和服务发现机制来完成对微服务应用实例的自动化管理。 服务注册：服务治理框架通常会构建一个注册中心，每个注册单元向注册中心登记自己提供的服务，将主机与端口号，版本号，协议等一些附加信息告知注册中心，注册中心按照服务名组织服务清单。当服务都注册到注册中心并启动后，注册中心还需要以心跳的方式检测清单中的服务是否可用，若不可用则从服务清单中剔除，达到排除故障服务的效果。 服务发现：由于在服务治理框架下运作，服务之间的调用不再通过具体的实例地址来实现，而是通过向服务名发起请求调用实现。所以服务调用方在调用其他服务的时候，并不知道具体的服务实例位置。因此调用方需要向注册中心咨询服务，并获取所有的服务实例清单，以实现对具体的服务实例的访问。调用其他服务的时候，可能对应多个服务实例，服务调用方从这多个实例中以某种轮询策略取出一个位置来进行服务调用，就是客户端负载均衡。实际框架为了性能等因素，不会采用每次想服务注册中心获取服务的方式，并且不同的应用场景在缓存和服务剔除等机制上也会有一些不同的实现策略。 2.Eureka: Eureka服务端，也称为服务注册中心。支持高可用配置，提供良好的服务实例可用性，可以应对多种不同的故障场景。如果Eureka以集群模式部署，当集群中有分片出现故障时，那么Eureka就转入自我保护模式。它允许分片故障期间继续提供服务的发现与注册，当故障分片恢复运行时，集群中的分片会把他们的状态再次同步回来。 Eureka客户端，主要处理服务注册和发现的。(重点：服务续约，刷新服务状态)客户端服务通过注解和参数配置的方式，嵌入在客户端应用程序的代码中，在应用程序运行时，Eureka客户端向注册中心注册自身提供的服务并周期性地发送心跳来更新它的服务租约。同时，它也能从服务端查询当前注册的服务信息并把它们缓存到本地并周期性地刷新服务状态。 3.服务治理基础架构的三个核心要素： 服务注册中心：提供服务注册和发现的功能 服务提供者：提供服务的应用，自身注册到注册中心，以便其他服务发现和调用 服务消费者：从注册中心获取服务，从而使消费者可以知道去何处调用所需要的服务 4.服务提供者，服务消费者，服务注册中心各自重要的通信行为 注意服务续约，服务剔除，自我保护，服务获取，服务同步，服务续约和服务剔除有关系，服务剔除和服务注册中心自我保护有关系获取服务缓存到本地有个更新时间间隔。 4.1 服务提供者： 服务注册：服务提供者在启动服务时发送Rest请求将自己注册到服务注册中心，同时带上自身的一些元数据信息。注册中心接到Rest请求后，将元数据信息存储到双层Map中，其中第一层的key是服务名，第二层的key是具体服务的实例名。 服务同步：指”高可用的注册中心”集群中多个注册中心之间的同步，当服务提供者发送注册请求到一个注册中心时，该注册中心会将请求转发给集群中相连的其他注册中心，从而实现注册中心之间服务的同步。 服务续约：服务提供者会维护一个心跳用来持续告诉注册中心自己还活着，防止服务注册中心将该服务实例从服务列表中剔除。有两个比较重要的属性：续租任务的调用时间间隔，默认30s（客户端定义）;服务失效的时间间隔，默认90s（服务端定义） 4.2 服务消费者： 获取服务：启动服务消费者的时候，它会发送一个REST请求给服务注册中心，来获取上面注册的服务清单。为了性能考虑，服务注册注册中心会维护一个只读的服务清单来返回给客户端，同时客户端缓存清单会每隔30s更新一次。 服务调用：服务消费者在获取到服务清单后，通过服务名可以获取具体提供服务的实例名和该实例的元数据信息。在Ribbon中会采用默认的轮询方式进行调用，从而实现客户端的负载均衡。 服务下线：关闭和重启服务实例，包括正常关闭和非正常关闭。 4.3 服务注册中心： 失效剔除：服务注册中心启动时创建定时任务，每隔一段时间（默认60s）将清单中超时(默认90s)没有续约的服务剔除出去。对于服务正常下线，会告诉注册中心，而服务由于内存溢出，网络故障等原因使得服务不能正常工作，注册中心并不会收到“服务下线”的请求。 自我保护：服务注册中心在运行期间统计心跳失败比例，若是网络延迟不稳定导致，则会将当前实例注册信息保护起来，不让服务实例过期而被服务剔除。 但会引发问题：消费者拿到已不存在发服务实例报错，因此客户端必须要有容错机制，比如请求重试，断路器等机制。 4.4 Eureka Client负责下面的任务： 向Eureka Server注册服务实例 向Eureka Server服务租约 当服务关闭时向Eureka Server取消租约 查询Eureka Server中的服务实例列表]]></content>
      <categories>
        <category>基础面试题</category>
      </categories>
      <tags>
        <tag>基础面试题</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis 面试题]]></title>
    <url>%2F2019%2F05%2F12%2F%E9%9D%A2%E8%AF%95%E6%80%BB%E7%BB%93%2FRedis%20%E9%9D%A2%E8%AF%95%E9%A2%98%2F</url>
    <content type="text"><![CDATA[1、什么是Redis?答：Redis全称为：Remote Dictionary Server（远程数据服务），是一个基于内存的高性能key-value数据库。 2、Redis的数据类型？答：Redis支持五种数据类型：string（字符串），hash（哈希），list（列表），set（集合）及zset(sorted set：有序集合)。 我们实际项目中比较常用的是string，hash如果你是Redis中高级用户，还需要加上下面几种数据结构HyperLogLog、Geo、Pub/Sub。 如果你说还玩过Redis Module，像BloomFilter，RedisSearch，Redis-ML，面试官得眼睛就开始发亮了。 3、使用Redis有哪些好处？(1) 速度快，因为数据存在内存中，类似于HashMap，HashMap的优势就是查找和操作的时间复杂度都是O(1) (2) 支持丰富数据类型，支持string，list，set，Zset，hash等 (3) 支持事务，操作都是原子性，所谓的原子性就是对数据的更改要么全部执行，要么全部不执行 (4) 丰富的特性：可用于缓存，消息，按key设置过期时间，过期后将会自动删除 4、Redis相比Memcached有哪些优势？(1) Memcached所有的值均是简单的字符串，redis作为其替代者，支持更为丰富的数据类型 (2) Redis的速度比Memcached快很多 (3) Redis可以持久化其数据 5、Memcache与Redis的区别都有哪些？(1)、存储方式 Memecache把数据全部存在内存之中，断电后会挂掉，数据不能超过内存大小。 Redis有部份存在硬盘上，这样能保证数据的持久性。 (2)、数据支持类型 Memcache对数据类型支持相对简单。 Redis有复杂的数据类型。 (3)、使用底层模型不同 它们之间底层实现方式 以及与客户端之间通信的应用协议不一样。 Redis直接自己构建了VM 机制 ，因为一般的系统调用系统函数的话，会浪费一定的时间去移动和请求。 6、Redis是单进程单线程的？答：Redis是单进程单线程的，redis利用队列技术将并发访问变为串行访问，消除了传统数据库串行控制的开销。 7、一个字符串类型的值能存储最大容量是多少？答：512M 8、Redis的持久化机制是什么？各自的优缺点？Redis提供两种持久化机制RDB和AOF机制: 1）RDB(Redis DataBase)持久化方式： 是指用数据集快照的方式(半持久化模式)记录redis数据库的所有键值对,在某个时间点将数据写入一个临时文件，持久化结束后，用这个临时文件替换上次持久化的文件，达到数据恢复。 优点： 1.只有一个文件dump.rdb，方便持久化。 2.容灾性好，一个文件可以保存到安全的磁盘。 3.性能最大化，fork子进程来完成写操作，让主进程继续处理命令，所以是IO最大化。(使用单独子进程来进行持久化，主进程不会进行任何IO操作，保证了redis的高性能) 4.相对于数据集大时，比AOF的启动效率更高。 缺点： 1.数据安全性低。(RDB是间隔一段时间进行持久化，如果持久化之间redis发生故障，会发生数据丢失。所以这种方式更适合数据要求不严谨的时候) 2）AOF(Append-only file)持久化方式： 是指所有的命令行记录以redis命令请求协议的格式(完全持久化存储)保存为aof文件。 优点： 1.数据安全，aof持久化可以配置appendfsync属性，有always，每进行一次命令操作就记录到aof文件中一次。 2.通过append模式写文件，即使中途服务器宕机，可以通过redis-check-aof工具解决数据一致性问题。 3.AOF机制的rewrite模式。(AOF文件没被rewrite之前（文件过大时会对命令进行合并重写），可以删除其中的某些命令（比如误操作的flushall）) 缺点： 1.AOF文件比RDB文件大，且恢复速度慢。 2.数据集大的时候，比rdb启动效率低。 9、Redis常见性能问题和解决方案：(1) Master最好不要写内存快照，如果Master写内存快照，save命令调度rdbSave函数，会阻塞主线程的工作，当快照比较大时对性能影响是非常大的，会间断性暂停服务。 (2) 如果数据比较重要，某个Slave开启AOF备份数据，策略设置为每秒同步一次 (3) 为了主从复制的速度和连接的稳定性，Master和Slave最好在同一个局域网内 (4) 尽量避免在压力很大的主库上增加从库 (5) 主从复制不要用图状结构，用单向链表结构更为稳定，即：Master &lt;- Slave1 &lt;- Slave2 &lt;- Slave3…这样的结构方便解决单点故障问题，实现Slave对Master的替换。如果Master挂了，可以立刻启用Slave1做Master，其他不变。 10、redis过期键的删除策略？(1)、定时删除:在设置键的过期时间的同时，创建一个定时器(timer). 让定时器在键的过期时间来临时，立即执行对键的删除操作。 (2)、惰性删除:放任键过期不管，但是每次从键空间中获取键时，都检查取得的键是否过期，如果过期的话，就删除该键;如果没有过期，就返回该键。 (3)、定期删除:每隔一段时间程序就对数据库进行一次检查，删除里面的过期键。至于要删除多少过期键，以及要检查多少个数据库，则由算法决定。 11、Redis的回收策略（淘汰策略）?volatile-lru：从已设置过期时间的数据集（server.db[i].expires）中挑选最近最少使用的数据淘汰 volatile-ttl：从已设置过期时间的数据集（server.db[i].expires）中挑选将要过期的数据淘汰 volatile-random：从已设置过期时间的数据集（server.db[i].expires）中任意选择数据淘汰 allkeys-lru：从数据集（server.db[i].dict）中挑选最近最少使用的数据淘汰 allkeys-random：从数据集（server.db[i].dict）中任意选择数据淘汰 no-enviction（驱逐）：禁止驱逐数据 注意这里的6种机制，volatile和allkeys规定了是对已设置过期时间的数据集淘汰数据还是从全部数据集淘汰数据，后面的lru、ttl以及random是三种不同的淘汰策略，再加上一种no-enviction永不回收的策略。 使用策略规则： 1、如果数据呈现幂律分布，也就是一部分数据访问频率高，一部分数据访问频率低，则使用allkeys-lru 2、如果数据呈现平等分布，也就是所有的数据访问频率都相同，则使用allkeys-random 12、为什么edis需要把所有数据放到内存中？答：Redis为了达到最快的读写速度将数据都读到内存中，并通过异步的方式将数据写入磁盘。所以redis具有快速和数据持久化的特征。如果不将数据放在内存中，磁盘I/O速度为严重影响redis的性能。在内存越来越便宜的今天，redis将会越来越受欢迎。如果设置了最大使用的内存，则数据已有记录数达到内存限值后不能继续插入新值。 13、Redis的同步机制了解么？答：Redis可以使用主从同步，从从同步。第一次同步时，主节点做一次bgsave，并同时将后续修改操作记录到内存buffer，待完成后将rdb文件全量同步到复制节点，复制节点接受完成后将rdb镜像加载到内存。加载完成后，再通知主节点将期间修改的操作记录同步到复制节点进行重放就完成了同步过程。 14、Pipeline有什么好处，为什么要用pipeline？答：可以将多次IO往返的时间缩减为一次，前提是pipeline执行的指令之间没有因果相关性。使用redis-benchmark进行压测的时候可以发现影响redis的QPS峰值的一个重要因素是pipeline批次指令的数目。 15、是否使用过Redis集群，集群的原理是什么？(1)、Redis Sentinal着眼于高可用，在master宕机时会自动将slave提升为master，继续提供服务。 (2)、Redis Cluster着眼于扩展性，在单个redis内存不足时，使用Cluster进行分片存储。 16、Redis集群方案什么情况下会导致整个集群不可用？答：有A，B，C三个节点的集群,在没有复制模型的情况下,如果节点B失败了，那么整个集群就会以为缺少5501-11000这个范围的槽而不可用。 17、Redis支持的Java客户端都有哪些？官方推荐用哪个？答：Redisson、Jedis、lettuce等等，官方推荐使用Redisson。 18、Jedis与Redisson对比有什么优缺点？答：Jedis是Redis的Java实现的客户端，其API提供了比较全面的Redis命令的支持；Redisson实现了分布式和可扩展的Java数据结构，和Jedis相比，功能较为简单，不支持字符串操作，不支持排序、事务、管道、分区等Redis特性。Redisson的宗旨是促进使用者对Redis的关注分离，从而让使用者能够将精力更集中地放在处理业务逻辑上。 19、Redis如何设置密码及验证密码？设置密码：config set requirepass 123456 授权密码：auth 123456 20、说说Redis哈希槽的概念？答：Redis集群没有使用一致性hash,而是引入了哈希槽的概念，Redis集群有16384个哈希槽，每个key通过CRC16校验后对16384取模来决定放置哪个槽，集群的每个节点负责一部分hash槽。 21、Redis集群的主从复制模型是怎样的？答：为了使在部分节点失败或者大部分节点无法通信的情况下集群仍然可用，所以集群使用了主从复制模型,每个节点都会有N-1个复制品. 22、Redis集群会有写操作丢失吗？为什么？答：Redis并不能保证数据的强一致性，这意味这在实际中集群在特定的条件下可能会丢失写操作。 23、Redis集群之间是如何复制的？答：异步复制 24、Redis集群最大节点个数是多少？答：16384个。 25、Redis集群如何选择数据库？答：Redis集群目前无法做数据库选择，默认在0数据库。 26、怎么测试Redis的连通性？答：使用ping命令。 27、怎么理解Redis事务？答： 1）事务是一个单独的隔离操作：事务中的所有命令都会序列化、按顺序地执行。事务在执行的过程中，不会被其他客户端发送来的命令请求所打断。 2）事务是一个原子操作：事务中的命令要么全部被执行，要么全部都不执行。 28、Redis事务相关的命令有哪几个？答：MULTI、EXEC、DISCARD、WATCH 29、Redis key的过期时间和永久有效分别怎么设置？答：EXPIRE和PERSIST命令。 30、Redis如何做内存优化？答：尽可能使用散列表（hashes），散列表（是说散列表里面存储的数少）使用的内存非常小，所以你应该尽可能的将你的数据模型抽象到一个散列表里面。比如你的web系统中有一个用户对象，不要为这个用户的名称，姓氏，邮箱，密码设置单独的key,而是应该把这个用户的所有信息存储到一张散列表里面. 31、Redis回收进程如何工作的？答：一个客户端运行了新的命令，添加了新的数据。Redi检查内存使用情况，如果大于maxmemory的限制, 则根据设定好的策略进行回收。一个新的命令被执行，等等。所以我们不断地穿越内存限制的边界，通过不断达到边界然后不断地回收回到边界以下。如果一个命令的结果导致大量内存被使用（例如很大的集合的交集保存到一个新的键），不用多久内存限制就会被这个内存使用量超越。 32、都有哪些办法可以降低Redis的内存使用情况呢？答：如果你使用的是32位的Redis实例，可以好好利用Hash,list,sorted set,set等集合类型数据，因为通常情况下很多小的Key-Value可以用更紧凑的方式存放到一起。 33、Redis的内存用完了会发生什么？答：如果达到设置的上限，Redis的写命令会返回错误信息（但是读命令还可以正常返回。）或者你可以将Redis当缓存来使用配置淘汰机制，当Redis达到内存上限时会冲刷掉旧的内容。 34、一个Redis实例最多能存放多少的keys？List、Set、Sorted Set他们最多能存放多少元素？答：理论上Redis可以处理多达232的keys，并且在实际中进行了测试，每个实例至少存放了2亿5千万的keys。我们正在测试一些较大的值。任何list、set、和sorted set都可以放232个元素。换句话说，Redis的存储极限是系统中的可用内存值。 35、MySQL里有2000w数据，redis中只存20w的数据，如何保证redis中的数据都是热点数据？答：Redis内存数据集大小上升到一定大小的时候，就会施行数据淘汰策略。 相关知识：Redis提供6种数据淘汰策略： voltile-lru：从已设置过期时间的数据集（server.db[i].expires）中挑选最近最少使用的数据淘汰 volatile-ttl：从已设置过期时间的数据集（server.db[i].expires）中挑选将要过期的数据淘汰 volatile-random：从已设置过期时间的数据集（server.db[i].expires）中任意选择数据淘汰 allkeys-lru：从数据集（server.db[i].dict）中挑选最近最少使用的数据淘汰 allkeys-random：从数据集（server.db[i].dict）中任意选择数据淘汰 no-enviction（驱逐）：禁止驱逐数据 36、Redis最适合的场景？（1）、会话缓存（Session Cache） 最常用的一种使用Redis的情景是会话缓存（session cache）。用Redis缓存会话比其他存储（如Memcached）的优势在于：Redis提供持久化。当维护一个不是严格要求一致性的缓存时，如果用户的购物车信息全部丢失，大部分人都会不高兴的，现在，他们还会这样吗？ 幸运的是，随着 Redis 这些年的改进，很容易找到怎么恰当的使用Redis来缓存会话的文档。甚至广为人知的商业平台Magento也提供Redis的插件。 （2）、全页缓存（FPC） 除基本的会话token之外，Redis还提供很简便的FPC平台。回到一致性问题，即使重启了Redis实例，因为有磁盘的持久化，用户也不会看到页面加载速度的下降，这是一个极大改进，类似PHP本地FPC。 再次以Magento为例，Magento提供一个插件来使用Redis作为全页缓存后端。 此外，对WordPress的用户来说，Pantheon有一个非常好的插件 wp-redis，这个插件能帮助你以最快速度加载你曾浏览过的页面。 （3）、队列 Reids在内存存储引擎领域的一大优点是提供 list 和 set 操作，这使得Redis能作为一个很好的消息队列平台来使用。Redis作为队列使用的操作，就类似于本地程序语言（如Python）对 list 的 push/pop 操作。 如果你快速的在Google中搜索“Redis queues”，你马上就能找到大量的开源项目，这些项目的目的就是利用Redis创建非常好的后端工具，以满足各种队列需求。例如，Celery有一个后台就是使用Redis作为broker，你可以从这里去查看。 （4），排行榜/计数器 Redis在内存中对数字进行递增或递减的操作实现的非常好。集合（Set）和有序集合（Sorted Set）也使得我们在执行这些操作的时候变的非常简单，Redis只是正好提供了这两种数据结构。所以，我们要从排序集合中获取到排名最靠前的10个用户–我们称之为“user_scores”，我们只需要像下面一样执行即可： 当然，这是假定你是根据你用户的分数做递增的排序。如果你想返回用户及用户的分数，你需要这样执行： ZRANGE user_scores 0 10 WITHSCORES Agora Games就是一个很好的例子，用Ruby实现的，它的排行榜就是使用Redis来存储数据的，你可以在这里看到。 （5）、发布/订阅 最后（但肯定不是最不重要的）是Redis的发布/订阅功能。发布/订阅的使用场景确实非常多。我已看见人们在社交网络连接中使用，还可作为基于发布/订阅的脚本触发器，甚至用Redis的发布/订阅功能来建立聊天系统！ 37、假如Redis里面有1亿个key，其中有10w个key是以某个固定的已知的前缀开头的，如果将它们全部找出来？答：使用keys指令可以扫出指定模式的key列表。 对方接着追问：如果这个redis正在给线上的业务提供服务，那使用keys指令会有什么问题？ 这个时候你要回答redis关键的一个特性：redis的单线程的。keys指令会导致线程阻塞一段时间，线上服务会停顿，直到指令执行完毕，服务才能恢复。这个时候可以使用scan指令，scan指令可以无阻塞的提取出指定模式的key列表，但是会有一定的重复概率，在客户端做一次去重就可以了，但是整体所花费的时间会比直接用keys指令长。 38、如果有大量的key需要设置同一时间过期，一般需要注意什么？答：如果大量的key过期时间设置的过于集中，到过期的那个时间点，redis可能会出现短暂的卡顿现象。一般需要在时间上加一个随机值，使得过期时间分散一些。 39、使用过Redis做异步队列么，你是怎么用的？答：一般使用list结构作为队列，rpush生产消息，lpop消费消息。当lpop没有消息的时候，要适当sleep一会再重试。 如果对方追问可不可以不用sleep呢？ list还有个指令叫blpop，在没有消息的时候，它会阻塞住直到消息到来。如果对方追问能不能生产一次消费多次呢？使用pub/sub主题订阅者模式，可以实现1:N的消息队列。 如果对方追问pub/sub有什么缺点？ 在消费者下线的情况下，生产的消息会丢失，得使用专业的消息队列如RabbitMQ等。 如果对方追问redis如何实现延时队列？ 我估计现在你很想把面试官一棒打死如果你手上有一根棒球棍的话，怎么问的这么详细。但是你很克制，然后神态自若的回答道：使用sortedset，拿时间戳作为score，消息内容作为key调用zadd来生产消息，消费者用zrangebyscore指令获取N秒之前的数据轮询进行处理。到这里，面试官暗地里已经对你竖起了大拇指。但是他不知道的是此刻你却竖起了中指，在椅子背后。 40、使用过Redis分布式锁么，它是什么回事？先拿setnx来争抢锁，抢到之后，再用expire给锁加一个过期时间防止锁忘记了释放。 这时候对方会告诉你说你回答得不错，然后接着问如果在setnx之后执行expire之前进程意外crash或者要重启维护了，那会怎么样？ 这时候你要给予惊讶的反馈：唉，是喔，这个锁就永远得不到释放了。紧接着你需要抓一抓自己得脑袋，故作思考片刻，好像接下来的结果是你主动思考出来的，然后回答：我记得set指令有非常复杂的参数，这个应该是可以同时把setnx和expire合成一条指令来用的！对方这时会显露笑容，心里开始默念：摁，这小子还不错。 41、memcached 和 redis 使用场景及优缺点对比简述 memcached 和 redis 都很类似：都是内存型数据库，数据保存在内存中，通过tcp直接存取，优势是速度快，并发高，缺点是数据类型有限，查询功能不强，一般用作缓存。 那么题主说 memcached 的事情 redis 都可以做，那么为什么 memcached 还有人用？那是因为它们两者并不是完全可以相互替代的，它们也有各自的长短优缺点 Memcached的优点： Memcached可以利用多核优势，单实例吞吐量极高，可以达到几十万QPS（取决于key、value的字节大小以及服务器硬件性能，日常环境中QPS高峰大约在4-6w左右）。 适用于最大程度扛量，有效为服务器减压。 支持直接配置为session handle。 配置维护的坑比较少。 Memcached的局限性： 数据结构很简单单一，只支持简单的key/value数据结构，不像Redis可以支持丰富的数据类型。 无法进行持久化，数据不能备份，只能用于缓存使用，且重启后数据全部丢失。 无法进行数据同步，不能将MC中的数据迁移到其他MC实例中。 Memcached内存分配采用Slab Allocation机制管理内存，value大小分布差异较大时会造成内存利用率降低，并引发低利用率时依然出现踢出等问题。需要用户注重value设计。 memcached服务端原生不支持水平扩展，必须在客户端编写缓存分布策略来实现分布式缓存，并且由于无法进行数据同步，因此生产环境中出现单机故障时可能会影响部分业务运行。 Redis的优点： 支持多种数据结构，比如 string（字符串）、 list(双向链表)、dict(hash表)、set(集合）、zset(排序set)、hyperloglog（基数估算）等等。 支持持久化操作，可以进行aof及rdb数据持久化到磁盘，从而进行数据备份或数据恢复等操作，较好的防止数据丢失的手段。 支持通过Replication进行数据复制，通过master-slave机制，可以实时进行数据的同步复制，支持多级复制和增量复制，master-slave机制是Redis进行HA的重要手段。 单线程请求，所有命令串行执行，并发情况下不需要考虑数据一致性问题。 支持pub/sub消息订阅机制，可以用来进行消息订阅与通知。 支持简单的事务需求，但业界使用场景很少，并不成熟。 Redis的局限性： Redis只能使用单线程，性能受限于CPU性能，故单实例CPU最高才可能达到5-6wQPS每秒（取决于数据结构，数据大小以及服务器硬件性能，日常环境中QPS高峰大约在1-2w左右）。 支持简单的事务需求，但业界使用场景很少，并不成熟，既是优点也是缺点。 Redis在string类型上会消耗较多内存，可以使用dict（hash表）压缩存储以降低内存耗用。 总结 在我看来，Redis在很多方面具备数据库的特征，或者说就是一个数据库系统，而Memcached只是简单的K/V缓存。 而且到底是用 redis 还是 memcached 这个还是看需求，因为单纯是做缓存的话，memcached已经足够应付绝大部分的需求，redis 的出现只是提供了一个更加好的选择，但是不代表redis就能完全替代 memcached ，还是那句话，看你的需求是怎么样的。 按照技术的新旧来讲，redis 比 memcached 还更加新，但是成熟醒来说，memcached 应该更加好，再说现在潮流也有开始转投 mongodb了，因为redis 的数据库特征，mongodb更胜一筹。 很多公司的缓存策略中使用memcached的还是占大多数的，再者是redis，最后才是mongodb，发现没有，最新的技术在公司团队的应用还是需要时间的，旧的技术策略还是很多公司团队在用，因为技术成型而且稳定性要好，这也是memcached比redis要被提及的更多的原因。]]></content>
      <categories>
        <category>基础面试题</category>
      </categories>
      <tags>
        <tag>基础面试题</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JVM 面试题]]></title>
    <url>%2F2019%2F05%2F12%2F%E9%9D%A2%E8%AF%95%E6%80%BB%E7%BB%93%2FJVM%20%E9%9D%A2%E8%AF%95%E9%A2%98%2F</url>
    <content type="text"><![CDATA[JVM 内存结构 主要由 Java 堆，JVM栈，方法区，本地方法栈和程序计数器组成。 方法区：主要存储类结构信息、常量、静态变量、即时编译器编译后的代码等数据。 Java堆：主要存放对象实例，给对象分配内存。 JVM栈：主要描述 Java 方法执行的内存模型，用于存储局部变量表、操作栈、动态链接、方法出口等信息。 本地方法栈：为虚拟机使用到的Native方法服务。 程序计数器：当前线程所执行的字节码的行号指示器。 类加载过程类的加载全过程：==加载，验证，准备，解析和初始==化这五个阶段。 加载：==通过一个类的全限定名来获取其定义的二进制字节流，并将这个字节流所代表的静态存储结构转化为方法区的运行时数据结构，在Java堆中生成一个代表这个类的 java.lang.Class 对象，作为对方法区中这些数据的访问入口==。 可以通过一些渠道加载 class 文件 从本地系统中直接加载 通过网络下载.class文件 从zip，jar等归档文件中加载.class文件 从专有数据库中提取.class文件 将Java源文件动态编译为.class文件 验证：==目的是为了确保Class文件的字节流中包含的信息符合当前虚拟机的要求，并且不会危害虚拟机自身的安全==。 验证阶段大致会完成4个阶段的检验动作： 文件格式验证：验证字节流是否符合Class文件格式的规范；例如：是否以0xCAFEBABE开头、主次版本号是否在当前虚拟机的处理范围之内、常量池中的常量是否有不被支持的类型。 元数据验证：对字节码描述的信息进行语义分析（注意：对比javac编译阶段的语义分析），以保证其描述的信息符合Java语言规范的要求；例如：这个类是否有父类，除了java.lang.Object之外。 字节码验证：通过数据流和控制流分析，确定程序语义是合法的、符合逻辑的。 符号引用验证：确保解析动作能正确执行 准备：==为类变量分配内存并设置类的初始值的阶段== 准备阶段是正式为类变量分配内存并设置类变量初始值的阶段，这些内存都将在方法区中分配。对于该阶段有以下几点需要注意： 这时候进行内存分配的仅包括类变量（static），而不包括实例变量，实例变量会在对象实例化时随着对象一块分配在Java堆中。 这里所设置的初始值通常情况下是数据类型默认的零值（如0、0L、null、false等），而不是被在Java代码中被显式地赋予的值。 假设一个类变量的定义为：public static int value = 3；(如果变量用 final 修饰，则是初始化为3，并将 3 存放常量池中) 那么变量value在准备阶段过后的初始值为0，而不是3，因为这时候尚未开始执行任何Java方法，而把value赋值为3的public static指令是在程序编译后，存放于类构造器（）方法之中的，所以把value赋值为3的动作将在初始化阶段才会执行。 解析：==解析阶段是虚拟机将常量池内的符号引用替换为直接引用的过程== 解析动作主要针对类或接口、字段、类方法、接口方法、方法类型、方法句柄和调用点限定符7类符号引用进行。符号引用就是一组符号来描述目标，可以是任何字面量。 直接引用就是直接指向目标的指针、相对偏移量或一个间接定位到目标的句柄。 初始化：==为类的静态变量赋予正确的初始值，JVM负责对类进行初始化，主要对类变量进行初始化== GC回收机制在什么时候 首先需要知道，GC又分为minor GC 和 Full Gc(也称为Major GC)。Java 堆内存分为新生代和老年代，新生代中又分为1个Eden区域 和两个 Survivor区域。 那么对于 Minor GC 的触发条件：大多数情况下，直接在 Eden 区中进行分配。如果 Eden区域没有足够的空间，那么就会发起一次 Minor GC；对于 Full GC（Major GC）的触发条件：也是如果老年代没有足够空间的话，那么就会进行一次 Full GC。 Ps：能说明minor gc/full gc的触发条件、OOM的触发条件，降低GC的调优的策略。 eden满了minor gc，升到老年代的对象大于老年代剩余空间full gc，或者小于时被HandlePromotionFailure参数强制full gc；gc与非gc时间耗时超过了GCTimeRatio的限制引发OOM，调优诸如通过NewRatio控制新生代老年代比例，通过MaxTenuringThreshold控制进入老年前生存次数等…… 对什么东西 判断对象是否存活一般有两种方式： 引用计数：每个对象有一个引用计数属性，新增一个引用时计数加1，引用释放时计数减1，计数为0时可以回收。此方法简单，无法解决对象相互循环引用的问题。 可达性分析（Reachability Analysis）：从GC Roots开始向下搜索，搜索所走过的路径称为引用链。当一个对象到GC Roots没有任何引用链相连时，则证明此对象是不可用的。不可达对象。 从GC Roots搜索不到，而且经过第一次标记、清理后，仍然没有复活的对象。 做什么事情 主要做了清理对象，整理内存的工作。Java堆分为新生代和老年代，采用了不同的回收方式。例如新生代采用了复制算法，老年代采用了标记整理法。在新生代中，分为一个Eden 区域和两个Survivor区域，真正使用的是一个Eden区域和一个Survivor区域，GC的时候，会把存活的对象放入到另外一个Survivor区域中，然后再把这个Eden区域和Survivor区域清除。那么对于老年代，采用的是标记整理法，首先标记出存活的对象，然后再移动到一端。这样也有利于减少内存碎片。 双亲委派模型双亲委派模型的工作流程是：如果一个类加载器收到了类加载的请求，它首先不会自己去尝试加载这个类，而是把请求委托给父加载器去完成，依次向上，因此，所有的类加载请求最终都应该被传递到顶层的启动类加载器中，只有当父加载器在它的搜索范围中没有找到所需的类时，即无法完成该加载，子加载器才会尝试自己去加载该类。 双亲委派模型意义： 当不同类加载器加载同一份 class 文件时，JVM 会存在两个独立的对象。（系统类防止内存中出现多份同样的字节码） 保证Java程序安全稳定运行 JVM 调优及参数 参数 -Xms：初始堆大小 -Xmx ：最大堆大小 此值可以设置与-Xmx相同，以避免每次垃圾回收完成后JVM重新分配内存 -Xmn ：年轻代大小 整个堆大小=年轻代大小 + 年老代大小 + 持久代大小。持久代一般固定大小为64m，所以增大年轻代后，将会减小年老代大小。此值对系统性能影响较大，Sun官方推荐配置为整个堆的3/8。 -XX:NewSize：设置年轻代大小 -XX:MaxNewSize：年轻代最大值 -XX:NewRatio 年老代与年轻代的比值 -XX:SurvivorRatio：设置年轻代中Eden区与Survivor区的大小比值 -XX:MaxTenuringThreshold：设置垃圾最大年龄。如果设置为0的话，则年轻代对象不经过Survivor区，直接进入年老代。对于年老代比较多的应用，可以提高效率。如果将此值设置为一个较大值，则年轻代对象会在Survivor区进行多次复制，这样可以增加对象在年轻代的存活时间，增加在年轻代即被回收的概论。 -XX:PermSize：设置持久带 -XX:MaxPermSize：设置持久代最大值 调优 JVM调优主要是针对内存管理方面的调优，包括控制各个代的大小，GC策略。由于GC开始垃圾回收时会挂起应用线程，严重影响了性能，调优的目是为了尽量降低GC所导致的应用线程暂停时间、 减少Full GC次数。 关键参数：-Xms -Xmx 、-Xmn 、-XX:SurvivorRatio、-XX:MaxTenuringThreshold、-XX:PermSize、-XX:MaxPermSize （1）-Xms、 -Xmx 通常设置为相同的值，避免运行时要不断扩展JVM内存，这个值决定了JVM heap所能使用的最大内存。 （2）-Xmn 决定了新生代空间的大小，新生代Eden、S0、S1三个区域的比率可以通过-XX:SurvivorRatio来控制(假如值为 4 表示：Eden:S0:S1 = 4:3:3 ) （3）-XX:MaxTenuringThreshold 控制对象在经过多少次minor GC之后进入老年代，此参数只有在Serial 串行GC时有效。 （4）-XX:PermSize、-XX:MaxPermSize 用来控制方法区的大小，通常设置为相同的值。 java对象创建过程 jvm遇到一条新建对象的指令时首先去检查这个指令的参数是否能在常量池中定义到一个类的符号引用。然后加载这个类（类加载过程在后边讲） 为对象分配内存。一种办法“指针碰撞”、一种办法“空闲列表”，最终常用的办法“本地线程缓冲分配(TLAB)” 将除对象头外的对象内存空间初始化为0 对对象头进行必要设置 java对象结构java对象由三个部分组成：对象头、实例数据、对齐填充。 对象头由两部分组成，第一部分存储对象自身的运行时数据：哈希码、GC分代年龄、锁标识状态、线程持有的锁、偏向线程ID（一般占32/64 bit）。第二部分是指针类型，指向对象的类元数据类型（即对象代表哪个类）。如果是数组对象，则对象头中还有一部分用来记录数组长度。 实例数据用来存储对象真正的有效信息（包括父类继承下来的和自己定义的） 对齐填充：jvm要求对象起始地址必须是8字节的整数倍（8字节对齐） java对象的定位方式句柄池、直接指针。 如何判断对象可以被回收 引用计数（有bug 不能解决循环引用的问题） 可达性分析: 可选做GC Roots的对象包括以下几种： 虚拟机栈（栈帧中局部变量表）中引用的数据 方法区中静态属性引用的对象（static） 方法区中常量引用的对象（final） 本地方法栈（native方法）中引用的对象 引用的分类强引用：GC时不会被回收 软引用：描述有用但不是必须的对象，在发生内存溢出异常之前被回收 弱引用：描述有用但不是必须的对象，在下一次GC时被回收 虚引用（幽灵引用/幻影引用）:无法通过虚引用获得对象，用PhantomReference实现虚引用，虚引用用来在GC时返回一个通知。 判断一个对象应该被回收 该对象没有与GC Roots相连 该对象没有重写finalize()方法或finalize()已经被执行过则直接回收（第一次标记）否则将对象加入到F-Queue队列中（优先级很低的队列）在这里finalize()方法被执行，之后进行第二次标记，如果对象仍然应该被GC则GC，否则移除队列。（在finalize方法中，对象很可能和其他 GC Roots中的某一个对象建立了关联，finalize方法只会被调用一次，且不推荐使用finalize方法） 回收方法区方法区回收价值很低，主要回收废弃的常量和无用的类。 如何判断无用的类： 该类所有实例都被回收（java堆中没有该类的对象） 加载该类的ClassLoader已经被回收 该类对应的java.lang.Class对象没有在任何地方被引用，无法在任何地方利用反射访问该类 垃圾收集算法 标记-清除 先标记再清除、效率不高、产生碎片。 复制算法 内存划分为大小相等的两块，每次只使用一块，当这一块用完了就将对象中存活的对象复制到另一块上边。 效率较高、有内存被空闲不能完全利用。 商业虚拟机目前使用这种算法回收新生代。用8:1:1来分配eden:From Survivor: To Survivo，即可以使用90%的对象，survivor空间不够时需要老年代分配担保 标记-整理 标记之后将所有存活的对象向一端移动，不产生碎片 OopMap、安全点、安全区域、抢先式中断、主动中断OopMap：一组特定的数据结构，记录数据引用（用来判断哪些对象是存活的） 安全点：在特定位置生成OopMap，这些位置成为安全点，只有在安全点才能暂停进行GC 安全区域：一段代码中引用关系不会变化，这一段代码称为安全区域（扩展的安全点） 抢先式中断:GC是所有线程都中断、把不在安全点上的线程重新启动跑到安全点上。 主动式中断：多线程去轮询一个标志位，当发现标志位为真时则自动挂起（标志位为真的位置应该与安全点位置重合） 垃圾收集器Serial:新生代复制算法（单线程）STW，老年代标记整理。STW ParNew:新生代复制算法（多线程）STW，老年代标记整理。STW Parrallel Scavenge：新生代收集器，专注于吞吐量吞吐量 = 运行用户代码时间/(运行用户代码时间 P+ GC时间) CMS（Concurrent Mark Sweep）收集器：以获取最短回收时间为目标的收集器。分为四个步骤 初始标记：STW 很快，用来标记GC Roots能直接关联到的对象 并发标记：并行 重新标记：并行，修正并发标记中产生变动的对象记录 并发清理CMS对CPU资源敏感、无法处理浮动垃圾、用标记清除实现有内存碎片 G1(Garbage-First)整体看来采用标记整理，局部看来(Region之间)采用复制算法。G1将整个java堆分成许多Region，跟踪各个Region里垃圾的回收价值大小，优先回收价值大的Region.步骤：初始标记、并发标记、最终标记、筛选回收。除了最后一步都和CMS差不多 自动化内存分配是怎么分配的对象主要分配在新生代的Eden区（新生代）上，如果启用了TLAB(本地线程分配缓冲)就分配在TLAB上，大对象（比如很长的数组）直接进入老年代，长期存活的对象进入老年代（参考对象头中的年代值）动态对象年龄判定（如果在survivor空间中相同年龄所有对象的大小总和大于等于survivor空间的一半，则年龄大于或等于该年龄的对象直接进入老年代） 空间分配担保在Minor GC之前，jvm会检查老年代最大可用连续空间是否大于新生代所有对象的空间。如果成立，则Minor GC是安全的。否则 jvm设置不允许担保失败则立刻Full GC jvm设置允许担保失败则检查老年代最大可用连续空间是否大于历次晋升到老年代对象的平均大小，如果大于则进行一次冒险的Minor GC 否则Full GC PS: Full GC 其实是为了让老年代腾出更多空间 Class文件结构 Class文件是一组以8位字节（1byte = 8bit）为基础单位的二进制流，中间没有任何分隔符，遇1个字节以上的数据，采用高地址对应低字节来分割存储。 class文件从前到后依次是魔数、版本号、常量池计数、常量池、访问标识、类索引、父类索引、接口计数、接口表、字段计数、字段表、方法计数、方法表、属性计数、属性表。 常量池从1开始计数，存储两样东西：字面量（Literal）和符号引用。字面量可以理解为常量（文本字符串、声明为final的变量）符号引用则包括以下几种：类和接口的全限定名、字段的名称和描述（字段可以理解为在class内，方法外声明的那些变量）、方法的名称和描述。 class中字符串、方法名的长度（占用byte的个数）用一个2字节的int来表示，即方法名和字符串最大长度为 2^16 * 1字节 = 64KB 也就是说，超过这个长度的方法名或者字符创（”xxxxx”）将无法编译 访问标识占两个字节，用来标识是否 public abstract interface final 注解 枚举 等等 类索引、父类索引确定类的全限定名和父类的全限定名 字段… 这段以后再总结吧 类加载时机、什么时候类会加载类在内存中的整个生命周期包括加载链接： 验证、准备、解析初始化使用卸载除解析外所有步骤依次执行，解析可能会发生在初始化之后（多态，动态绑定） 什么时候会加载？ 遇到***指令的时候，即new对象了，调用类静态字段（fina定义的除外且有初值的除外 ）、静态方法 反射调用 初始化一个类时需要先初始化其父类 jvm启动时加载的包含main()的那个类 jdk1.7中反射调用方法。。。没看懂。。。 类加载过程 通过一个类的全限定名获取定义这个类的二进制字节流包括：.class 、jar、 网络、 甚至实时生成(动态代理 java.lang.reflect.Proxy)、其他文件（jsp）… 将这个字节流所代表的类的静态数据结构存储在方法区中（转换成了动态结构） 在内存中生成一个代表该类的java.lang.Class对象，作为方法区中所有访问该类的数据访问入口 类验证过程 文件格式验证 Class文件是否符合规范，当前虚拟机版本是否能处理 元数据验证 进行语义分析：是否有父类、是否实现了所继承的抽象类中所有方法… 字节码验证 通过数据流和控制流分析，语义是合法合理的，进一步的语义分析。（一个int的值没有被当成lang来写入） 符号引用验证 发生在将符号引用转换为直接应用的时候（解析阶段）用来检查权限，以及是否能成功解析 类的准备阶段正式为类变量分配内存并设置变量初始值（0和null）,只包括的类变量(static修饰)，不包括实例变量 类的解析解析阶段是将常量池内的符号引用替换为直接引用的过程 解析的动作针对于：类或接口、字段、类方法、接口方法、方法类型、方法句柄、调用点限定符 符号引用：用一组符号（字符串）来描述所引用的目标，符号可以使任何形式的字面量，符号引用与jvm的内存布局无关，此时类可以没有加载到内存中 直接引用：可以使直接指向目标的指针、相对偏移量或一个能间接定位到目标的句柄，直接引用与内存布局相关，此时类一定已经加载到内存中了 类的初始化真正开始执行类中定义的java程序代码，编译器收集类中所有变量的赋值语句和静态语句块合并成&lt;clinit方法（执行该方法前一定要执行父类的方法，所以所有类中最先执行的&lt;clinit方法一定是java.lang.Objer）并执行 类加载器、双亲委派模型对于任意一个类都需要由加载它的类加载器和这个类本身一同确立其在jvm中的唯一性（即比较两个类是否相等先要比较这两个类是否由同一个ClassLoader加载） jvm的角度来看有两种不同的类加载器： 启动类加载器，用C++实现，是虚拟机的一部分，负责启动&lt;java_home&gt;\lib目录中，或者用-Xbootclasspath参数指定的类 所有其他类的加载器（可以细分为扩展类加载器、应用程序类加载器），由java实现，独立于虚拟机，继承于java.lang.ClassLoader 扩展类加载器用来加载&lt;java_home&gt;\lib\ext目录中的 双亲委派模型：当一个类加载器收到了类加载的请求，它首先不会自己去加载这个类，二十把这个请求委派给父类加载器去完成，只有当父加载器反馈无法加载这个请求（如：它的搜索范围中没有找到所需要的类），子类才会去加载（好处就是java类随着加载器一起构成了一个优先级关系，比如我同样定义了一个java.lang.Object类最终加载出来的还是系统自带的那个。。。也有可能编译通过，拒绝加载） 启动类加载器 |_扩展类加载器 ​ |_应用程序类加载器 ​ |_自定义类加载器 破坏双亲委派模型：基础类需要回调用户的代码（即要先加载用户类 比如 JNDI服务）采用 线程上下文类加载器（父类请求子类的加载器去加载这个类） 运行时栈帧结构栈帧是用来支持虚拟机进行方法调用和方法执行的数据结构，jvm运行时数据区中虚拟机栈的栈元素。栈帧中存储了：局部变量表、操作数栈、动态链接、返回地址。每个方法从调用到执行完成对应着一个栈帧在虚拟机栈中从入栈到出栈的过程。只有栈顶的当前栈帧是有效的 局部变量表 是一组变量值存储空间。用来存储方法参数和方法内部定义的局部变量。在.class中已经由code属性的max_locals项决定了局部变量表的最大容量。 局部变量表容量以变量槽（Slot）为最小单位（32/64bit） 局部变量表中第0为存储的是：方法所属对象的实例引用，即 this~~ slot可以重用，即局部变量表的空间可能会比实际所有方法中变量需要占用的空间要小，有时候把不使用的对象手动赋null，可能会有助于GC 类变量（成员变量）（class内 方法外）是默认有初值的（在类准备阶段被赋0和nul了），而方法内的变量，即局部变量是没有初值的，不能直接调用 long和double占两个slot，可以用volatile关键字修饰，防止在多线程环境下的字撕裂。 操作数栈 操作数栈中每一个元素可以是任意的java类型 操作数栈中的元素数据类型必须与字节码执行指令序列严格匹配（eg:当前指令时iadd~两个int相加的指令，那么操作数栈顶必须是两个int类型！） 方法返回地址方法返回地址八成存的是上一个方法PC计数器的下一条指令。方法执行后有两种方式退出方法。 正常退出 异常退出（不会给上层调用者提供返回值） 正常退出的情况下，需要把当前栈帧出栈、恢复上层方法的局部变量表和操作数栈、把返回值（如果有）压入栈顶。调整PC计数器让它指向方法调用指令的后一条指令 动态链接（多态/动态绑定 DynamicLinking） 语法层面简单的说，多态：有继承、有重写、父类引用指向子类对象。 深层次的讲： 每个栈帧都包含一个指向运行时常量池中改栈帧所述方法的引用（即一个符号引用，表明这个栈帧代表哪个类的哪个方法？）。在一种情况下：类加载阶段或第一次使用的时候转化为直接引用，这种转化称为静态解析。另一种情况：在方法的每一次运行时都进行解析转换为直接引用，这就是动态链接 方法调用，不等于方法执行！方法调用的目的是确定需要调用方法的哪个版本（即调用哪个类的哪个方法） 调用目标在程序代码写好后，编译期编译时就能确定的方法，就可以用静态解析，即“编译期可知，运行期不变”。包括两种方法：其一，静态方法（static 与类的类型直接关联）.其二，私有方法（private 不能被继承并重写，因此只有一个版本） 补充：实例构造器、父类方法也可以静态解析，这四种方法统称为非虚方法（外加fina修饰的方法），所有其他的方法称为虚方法。 分派：解析调用一定是静态过程（在运行期之前就已经确定），而分派可能是静态可能是动态的过程，即分为静态单分派、静态多分派、动态单分派、动态多分派。具体是多还是单是有分派所依据的宗量数决定； eg 123456789101112131415161718192021class Human&#123; &#125;class Man extends Human&#123; &#125;class Woman extends Human&#123; &#125;public static void say(Human guy)&#123;System.out.printlun(“Human”);&#125;public static void say(Man guy)&#123;System.out.printlun(“Man”);&#125;public static void say(Woman guy)&#123;System.out.printlun(“Woman”);&#125;public static void main(String[] args)&#123; Human man= new Man(); Human woman = new Woman(); say(man); say(woman);&#125;//结果//Human//Human eg 123456789101112131415161718192021222324252627282930class Human&#123; public void say()&#123; System.out.println("Human say"); &#125;&#125;class Man extends Human&#123; public void say()&#123; System.out.println("Man say"); &#125;&#125;class Woman extends Human&#123; public void say()&#123; System.out.println("Woman say"); &#125;&#125;public static void main(String[] args)&#123; Human m1 = new Human(); Human m2 = new Man(); Human m3 = new Woman(); m1.say(); m2.say(); m3.say(); m3 = new Man(); m3.say();&#125;//结果//Human say//Man say//Woman say//Man say java中重写是动态分派，当运行期调用某个对象的方法时需经历以下几个过程： 找到操作数栈顶的第一个元素（即当前对象）的实际类型，记为C 如果在类型C中找到了与常量中描述符和简单名称都相符的方法（方法名一样、参数类型个数一样）则进行权限访问检查，能通过检查则调用这个方法，否则返回java.lang.IllegalAccessError. 否则按照继承关系依次对C的父类进行第二步的操作。 最终如果没有找到合适方法，抛异常~ 单分派与多分派。 宗量：方法的接受者与方法的参数统称为总量 A.method( XX,YY,ZZ) (A XX YY ZZ 称为method的宗量 ) 单分派：根据一个宗量对目标方法进行选择（选择哪个类、选择这个类中的哪个方法） 多分派：根据超过一个宗量对目标方法进行选择 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263eg:public class JavaDispath&#123; public static void main(String[] args)&#123; Father father = new Father(); Father son = new Son(); father.choice(new QQ()); son.choice(new _360()); //son.choice(new _360(),1); System.out.println("~~~~~~~~~~~"); QQ qiruiQQ = new QiruiQQ(); son.choice(qiruiQQ); &#125;&#125;class QQ&#123; static void say()&#123; System.out.println("I'm QQ"); &#125;&#125;class QiruiQQ extends QQ&#123; static void say()&#123; System.out.println("I'm QiruiQQ"); &#125;&#125;class TengxunQQ extends QQ&#123; static void say()&#123; System.out.println("I'm TengxunQQ"); &#125;&#125;class _360&#123;&#125;class Man&#123; void choice(_360 arg, int a)&#123; System.out.println("Man chooce 360"); &#125;&#125;class Father extends Man&#123; void choice(QQ arg)&#123; QQ.say(); System.out.println("Father chooce QQ"); &#125; void choice(_360 arg)&#123; System.out.println("Father chooce 360"); &#125;&#125;class Son extends Father&#123; void choice(QQ arg)&#123; QQ.say(); System.out.println("Son chooce QQ"); &#125; void choice(_360 arg)&#123; System.out.println("Son chooce 360"); &#125; void choice(QiruiQQ arg)&#123; QQ.say(); System.out.println("Son chooce QQ"); &#125;&#125;//结果//Father chooce 360//I'm QQ//Son chooce QQ//~~~~~~~~~~~//I'm QQ//Son chooce QQ 上边的程序运行过程时这样的： 分割线之前 在编译期（静态分派的过程~），系统根据father和son的静态类型即Fahter和后边的方法名还有参数，决定调用 Father.choice(360) 和Father.choice(360) 因此静态分派是多分派（超过了一个宗量） 此时是方法调用的过程 在运行期（动态分派过程~），系统根据father和son的动态类型决定调用 Father.choice(360) 和Son.choice(QQ)，此时是动态分派，根据实际类型去找到应该执行哪个类的哪个方法，此时根据son的动态类型Son来选择了执行Son.choice(QQ)。而不论里边的QQ传进来的到底是奇瑞QQ、还是腾讯QQ，jvm都不会去理睬，只是把它当做一个普通QQ来对待。因此动态分派只取决于方法的接受者的实际类型，与参数无关。因此java语言的动态分派是单分派（一个宗量决定）。 分割线之后 在分割线之后，我们尝试着在son.choice中传入了一个QiruiQQ 但是jvm还是只把它当成一个普通QQ来看待，验证了我们上边的说法。 总的来说，java是静态多分派，动态单分派。 多态是怎么实现的 出于效率的考虑，我们上文中所说的那种一层层向上搜索的算法在实际中不会用到。。。 实际中为每个类在方法区的位置中搞了一个虚方法表（也有接口方法表），方法表中罗列着这个类所继承的所有方法的全限定名以及他们的引用指向，比如上文Father类的方法表中clone()方法指向java.lang.Objer.clone()的入口地址，而choice(QQ)方法，就指向自身的choice(QQ)方法。]]></content>
      <categories>
        <category>基础面试题</category>
      </categories>
      <tags>
        <tag>基础面试题</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[程序员如何高效率拿 offer]]></title>
    <url>%2F2019%2F05%2F12%2F%E9%9D%A2%E8%AF%95%E6%80%BB%E7%BB%93%2F%E9%9D%A2%E8%AF%95%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[面试题如何准备 你的简历内容可能会决定面试官要问的问题，所以准备面试题的时候，别忘了回头去看你的简历写了哪些内容，准备与该内容相关的面试题。也许你简历上写的技术（在项目中设计到的技术），你不是很熟悉，你就是用该技术最基础的东西实现最简单的功能。这也没关系，你只要认真去看你接触的这个东西一些相关的面试内容即可。因为面试官问来问去也离不开你背过的哪些关键字或词。 以下的面试题可以供大家参考： https://swenfang.github.io/2019/04/22/Spring面试题/ https://swenfang.github.io/2019/05/12/面试总结/JVM 面试题/ https://swenfang.github.io/2019/05/12/面试总结/线程面试题/ https://swenfang.github.io/2019/05/12/面试总结/Redis 面试题/ https://swenfang.github.io/2019/05/12/面试总结/数据库面试题/ https://swenfang.github.io/2019/05/12/面试总结/日志框架面试题/ https://swenfang.github.io/2019/05/12/面试总结/Mybaties 面试题/ https://swenfang.github.io/2019/05/12/面试总结/全文检索面试题/ https://swenfang.github.io/2019/05/12/面试总结/Jdk1.7与 jdk1.8的区别和最新的特征/ 面试过程如何应对在被提问的过程，很多人应该会像我一样会关心一个问题。就是，面试官问我的问题我不会或者不敢确定答案的正确性怎么办？其实，技术上的问题如果你是真的不会你可以向面试官委婉的表达，即“不好意思，您说的这个问题我暂时没去了解过，回答不了您” 或者 “您说的这个问题我目前还没有遇到过，暂时不知道怎么结局，请问贵公司这边是怎样来处理这种的问题的呢？”（反问面试官的次数最多不要超过两次，这是我个人的原则）或则“不好意思，您说的这个点我暂时没有去了解。”以这种比较委婉的方式去缓和你在面试过程中的不足是比较合适的。 在言行举止方面，在面试刚开始的时候紧张是很正常，你能做的就是保持微笑，认真的去回答面试官提出的每个问题，在这个过程中，你也可以同他进行相互的交流，也就是我上面提到的访问，如果面试官问的问题你没听明白，你可以说“不好意思我不太明白您说的问题，麻烦您再给我描述一下。”如果面试官提到的问题你只知道一点点（知道是什么或知道怎么用），那么你在回答完之后，你可以说“我只知道这些”，这样说的目的让面试官知道，你只知道这些，再深入的东西我可能就回答不了。当你掌握了这些面试沟通技巧的时候，你会发现整场面试下来，自己的心情会比较轻松、愉快一些，这样你挂在脸上的微笑也会更加自然。 面试技巧投递简历 技术性招聘一般都是通过 Boss 直聘、拉钩，如果工作 7、8 年以上，并且想找架构或管理方面可通过 猎聘找猎头推荐，想要内推的话可以找下 内推网 里面一般都是大厂。 投递简历一般选择在工作日投，尽量不要在周末投，一般 HR 查看简历都是前面几份筛选，如果周末投 HR 会收到很多简历也不会细心取看。 投递后一般都不会有很快反应，一般有些公司预期招聘就会在各平台发布招聘信息，后面等一个多星期或是更久才会查看收到的简历，所以一般投递后都是一个星期内才有反应。 简历最好不要准备一份，如果想要投递自己向往的公司的话，需要针对该公司所写的招聘信息对应去修改自己的简历，这样面试通知你面试的机会更大些。 注意：在还是投递简历的时候最好不要先投递自己向往的公司，先投递几家公司练练手，熟悉一下面试的流程与常问的问题，后面投递自己向往的公司获取 Office 机会比较大点。 时间选择如果收到面试邀请，如果是早上面试最好跟 HR 预定10点过去，下午3点。这个时间点一方面自己有复习的时间，另一方面是对方公司面试官能确认已经上班。 我的个人总结总体的感觉是，外包公司实在太多，所以我找工作平台的聊天中，若有公司招聘人员约我去面试的时候，我都会先问一句“请问贵公司是项目外包吗？”。一般是外包公司，我个人是选择不去面试。当然，如果你是应届毕业生或者你是很久都没有参与过面试很缺乏面试经验，那么我个人觉得你不应该放弃或丢失任何一次面试机会。因为你面试的场数越多，首先对面试题有越多的回忆和更进一步的巩固，其次应对面试官的提问你的心态会更加的淡定和自然，回答问题也会更加顺畅,有了前面的面试经验你的答案也会更加完整，最后你收到 offer 的机会就会越多。 最后想跟大家说一件非常重要的事：千里执行，始于足下。九层之台，起于累土。不管你现在身处什么样的工作环境，都要时刻记得抽出时间去学习去积累（学习你在工作中没有用到的技术，积累每天一点一点的阅读内容）。]]></content>
      <categories>
        <category>基础面试题</category>
      </categories>
      <tags>
        <tag>基础面试题</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[关于GC overhead limit exceeded]]></title>
    <url>%2F2019%2F05%2F03%2FJava%20JVM%2F%E5%85%B3%E4%BA%8EGCOverheadLimitExceeded%2F</url>
    <content type="text"><![CDATA[关于GC overhead limit exceeded 我遇到这样的问题，本地部署时抛出异常java.lang.OutOfMemoryError：GC overhead limit exceeded导致服务起不来，查看日志发现加载了太多资源到内存，本地的性能也不好，gc时间消耗的较多。解决这种问题两种方法是，增加参数，-XX:-UseGCOverheadLimit，关闭这个特性，同时增加heap大小，-Xmx1024m。坑填了，but why？ OOM大家都知道，就是JVM内存溢出了，那GC overhead limit exceed呢？ GC overhead limt exceed检查是Hotspot VM 1.6定义的一个策略，通过统计GC时间来预测是否要OOM了，提前抛出异常，防止OOM发生。Sun 官方对此的定义是：“并行/并发回收器在GC回收时间过长时会抛出OutOfMemroyError。过长的定义是，超过98%的时间用来做GC并且回收了不到2%的堆内存。用来避免内存过小造成应用不能正常工作。“ 听起来没啥用…预测OOM有啥用？起初开来这玩意只能用来Catch住释放内存资源，避免应用挂掉。后来发现一般情况下这个策略不能拯救你的应用，但是可以在应用挂掉之前做最后的挣扎，比如数据保存或者保存现场（Heap Dump）。 而且有些时候这个策略还会带来问题，比如加载某个大的内存数据时频繁OOM。 假如你也生产环境中遇到了这个问题，在不知道原因时不要简单的猜测和规避。可以通过-verbose:gc -XX:+PrintGCDetails看下到底什么原因造成了异常。通常原因都是因为old区占用过多导致频繁Full GC，最终导致GC overhead limit exceed。如果gc log不够可以借助于JProfile等工具查看内存的占用，old区是否有内存泄露。分析内存泄露还有一个方法-XX:+HeapDumpOnOutOfMemoryError，这样OOM时会自动做Heap Dump，可以拿MAT来排查了。还要留意young区，如果有过多短暂对象分配，可能也会抛这个异常。 日志的信息不难理解，就是每次gc时打条日志，记录GC的类型，前后大小和时间。举个例子。 33.125: [GC [DefNew: 16000K-&gt;16000K(16192K), 0.0000574 secs][Tenured: 2973K-&gt;2704K(16384K), 0.1012650 secs] 18973K-&gt;2704K(32576K), 0.1015066 secs] 100.667:[Full GC [Tenured: 0K-&gt;210K(10240K), 0.0149142 secs] 4603K-&gt;210K(19456K), [Perm : 2999K-&gt;2999K(21248K)], 0.0150007 secs] GC和Full GC代表gc的停顿类型，Full GC代表stop-the-world。箭头两边是gc前后的区空间大小，分别是young区、tenured区和perm区，括号里是该区的总大小。冒号前面是gc发生的时间，单位是秒，从jvm启动开始计算。DefNew代表Serial收集器，为Default New Generation的缩写，类似的还有PSYoungGen，代表Parallel Scavenge收集器。这样可以通过分析日志找到导致GC overhead limit exceeded的原因，通过调节相应的参数解决问题。 文中涉及到的名词解释， Eden Space：堆内存池，大多数对象在这里分配内存空间。 Survivor Space：堆内存池，存储在Eden Space的gc中存活下来的对象。 Tenured Generation：堆内存池，存储Survivor Space中存活过几次gc的对象。 Permanent Generation：非堆空间，存储的是class和method对象。 Code Cache：非堆空间，JVM用来存储编译和存储native code。 最后附上GC overhead limit exceed HotSpot的实现： 123456789101112131415161718192021222324252627282930313233343536373839404142bool print_gc_overhead_limit_would_be_exceeded = false;if (is_full_gc) &#123; if (gc_cost() &gt; gc_cost_limit &amp;&amp; free_in_old_gen &lt; (size_t) mem_free_old_limit &amp;&amp; free_in_eden &lt; (size_t) mem_free_eden_limit) &#123; inc_gc_overhead_limit_count(); if (UseGCOverheadLimit) &#123; if (gc_overhead_limit_count() &gt;= AdaptiveSizePolicyGCTimeLimitThreshold)&#123; // All conditions have been met for throwing an out-of-memory set_gc_overhead_limit_exceeded(true); // Avoid consecutive OOM due to the gc time limit by resetting // the counter. reset_gc_overhead_limit_count(); &#125; else &#123; // The required consecutive collections which exceed the // GC time limit may or may not have been reached. We // are approaching that condition and so as not to // throw an out-of-memory before all SoftRef's have been // cleared, set _should_clear_all_soft_refs in CollectorPolicy. // The clearing will be done on the next GC. bool near_limit = gc_overhead_limit_near(); if (near_limit) &#123; collector_policy-&gt;set_should_clear_all_soft_refs(true); if (PrintGCDetails &amp;&amp; Verbose) &#123; gclog_or_tty-&gt;print_cr(" Nearing GC overhead limit, " "will be clearing all SoftReference"); &#125; &#125; &#125; &#125; // Set this even when the overhead limit will not // cause an out-of-memory. Diagnostic message indicating // that the overhead limit is being exceeded is sometimes // printed. print_gc_overhead_limit_would_be_exceeded = true; &#125; else &#123; // Did not exceed overhead limits reset_gc_overhead_limit_count(); &#125;&#125;]]></content>
      <categories>
        <category>Java JVM</category>
      </categories>
      <tags>
        <tag>Java JVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring 面试题]]></title>
    <url>%2F2019%2F04%2F22%2FSpring%E9%9D%A2%E8%AF%95%E9%A2%98%2F</url>
    <content type="text"><![CDATA[Spring IOC、AOP的理解、实现的原理，以及优点 Spring的IoC容器是Spring的核心，Spring AOP是spring框架的重要组成部分 IOC 我的理解 正常的情况下，比如有一个类，在类里面有方法（不是静态的方法），调用类里面的方法，创建类的对象，使用对象调用方法，创建类对象的过程，需要new出来对象 通过控制反转，把对象的创建不是通过new方式实现，而是交给Spring配置创建类对象 IOC的意思是控件反转也就是由容器控制程序之间的关系，这也是spring的优点所在，把控件权交给了外部容器，之前的写法，由程序代码直接操控，而现在控制权由应用代码中转到了外部容器，控制权的转移是所谓反转。换句话说之前用new的方式获取对象，现在由spring给你至于怎么给你就是di了。 Spring IOC实现原理 创建xml配置文件，配置要创建的对象类 通过反射创建实例 获取需要注入的接口实现类并将其赋值给该接口 优点 解耦合，开发更方便组织分工 高层不依赖于底层（依赖倒置） 是应用更容易测试 因为把对象生成放在了XML里定义，所以当我们需要换一个实现子类将会变成很简单（一般这样的对象都是现实于某种接口的），只要修改XML就可以了，这样我们甚至可以实现对象的热插拨 AOP(面向切面编程,主要的作用是不需要修改源代码的基础扩展功能) 我的理解 AOP（Aspect Oriented Programming ）称为面向切面编程，扩展功能不是修改源代码实现，在程序开发中主要用来解决一些系统层面上的问题，比如日志，事务，权限等待，Struts2的拦截器设计就是基于AOP的思想，是个比较经典的例子。 面向切面编程（aop）是对面向对象编程（oop）的补充 面向切面编程提供声明式事务管理 AOP就是典型的代理模式的体现 Spring AOP实现原理 动态代理（利用反射和动态编译将代理模式变成动态的） JDK的动态代理 JDK内置的Proxy动态代理可以在运行时动态生成字节码，而没必要针对每个类编写代理类 JDKProxy返回动态代理类，是目标类所实现接口的另一个实现版本，它实现了对目标类的代理（如同UserDAOProxy与UserDAOImp的关系） cglib动态代理 CGLibProxy返回的动态代理类，则是目标代理类的一个子类（代理类扩展了UserDaoImpl类） cglib继承被代理的类，重写方法，织入通知，动态生成字节码并运行 优点 各个步骤之间的良好隔离性 源代码无关性 松耦合 易扩展 代码复用 项目中Spring AOP用在什么地方，为什么这么用，切点，织入，通知用自己的话描述一下 Joinpoint（连接点）（重要） 类里面可以被增强的方法，这些方法称为连接点 Pointcut（切入点）（重要） 所谓切入点是指我们要对哪些Joinpoint进行拦截的定义 Advice（通知/增强）（重要） 所谓通知是指拦截到Joinpoint之后所要做的事情就是通知.通知分为前置通知，后置通知，异常通知，最终通知，环绕通知（切面要完成的功能） Aspect（切面） 是切入点和通知（引介）的结合 Introduction（引介） 引介是一种特殊的通知在不修改类代码的前提下， Introduction可以在运行期为类动态地添加一些方法或Field. Target（目标对象） 代理的目标对象（要增强的类） Weaving（织入） 是把增强应用到目标的过程，把advice 应用到 target的过程 Proxy（代理） 一个类被AOP织入增强后，就产生一个结果代理类 AOP（Aspect Oriented Programming ）称为面向切面编程，扩展功能不是修改源代码实现，在程序开发中主要用来解决一些系统层面上的问题，比如日志，事务，权限等待，Struts2的拦截器设计就是基于AOP的思想，是个比较经典的例子。 AOP动态代理2种实现原理，他们的区别是什么？ 动态代理与cglib实现的区别 JDK动态代理只能对实现了接口的类生成代理，而不能针对类 cglib是针对类实现代理，主要是对指定的类生成一个子类，覆盖其中的方法因为是继承，所以该类或方法最好不要声明成final JDK代理是不需要以来第三方的库，只要JDK环境就可以进行代理 cglib必须依赖于cglib的类库，但是它需要类来实现任何接口代理的是指定的类生成一个子类，覆盖其中的方法，是一种继承 Spring事物的七种事物传播属性行为及五种隔离级别 Spring七个事物传播属性： PROPAGATION_REQUIRED – 支持当前事务，如果当前没有事务，就新建一个事务。这是最常见的选择。 PROPAGATION_SUPPORTS – 支持当前事务，如果当前没有事务，就以非事务方式执行。 PROPAGATION_MANDATORY – 支持当前事务，如果当前没有事务，就抛出异常。 PROPAGATION_REQUIRES_NEW – 新建事务，如果当前存在事务，把当前事务挂起。 PROPAGATION_NOT_SUPPORTED – 以非事务方式执行操作，如果当前存在事务，就把当前事务挂起。 PROPAGATION_NEVER – 以非事务方式执行，如果当前存在事务，则抛出异常。 PROPAGATION_NESTED – 如果当前存在事务，则在嵌套事务内执行。如果当前没有事务，则进行与PROPAGATION_REQUIRED类似的操作。 五个隔离级别： ISOLATION_DEFAULT 这是一个PlatfromTransactionManager默认的隔离级别，使用数据库默认的事务隔离级别.另外四个与JDBC的隔离级别相对应； ISOLATION_READ_UNCOMMITTED 这是事务最低的隔离级别，它充许别外一个事务可以看到这个事务未提交的数据。这种隔离级别会产生脏读，不可重复读和幻像读。(读未提交) ISOLATION_READ_COMMITTED 保证一个事务修改的数据提交后才能被另外一个事务读取。另外一个事务不能读取该事务未提交的数据。这种事务隔离级别可以避免脏读出现，但是可能会出现不可重复读和幻像读。(读已提交) ISOLATION_REPEATABLE_READ 这种事务隔离级别可以防止脏读，不可重复读。但是可能出现幻像读。它除了保证 一个事务不能读取另一个事务未提交的数据外，还保证了避免下面的情况产生 (不可重复读)。 ISOLATION_SERIALIZABLE 这是花费最高代价但是最可靠的事务隔离级别。事务被处理为顺序执行。除了防止脏读，不可重复读外，还避免了幻像读。(串行化) Spring 工作流程描述 用户向服务器发送请求，请求被Spring 前端控制Servelt DispatcherServlet捕获； DispatcherServlet对请求URL进行解析，得到请求资源标识符（URI）。然后根据该URI，调用HandlerMapping获得该Handler配置的所有相关的对象（包括Handler对象以及Handler对象对应的拦截器），最后以HandlerExecutionChain对象的形式返回； DispatcherServlet 根据获得的Handler，选择一个合适的HandlerAdapter。（附注：如果成功获得HandlerAdapter后，此时将开始执行拦截器的preHandler(…)方法） 提取Request中的模型数据，填充Handler入参，开始执行Handler（Controller)。 在填充Handler的入参过程中，根据你的配置，Spring将帮你做一些额外的工作：ttpMessageConveter： 将请求消息（如Json、xml等数据）转换成一个对象，将对象转换为指定的响应信息；数据转换：对请求消息进行数据转换。如String转换成Integer、Double等；数据根式化：对请求消息进行数据格式化。 如将字符串转换成格式化数字或格式化日期等；数据验证： 验证数据的有效性（长度、格式等），验证结果存储到BindingResult或Error中； Handler执行完成后，向DispatcherServlet 返回一个ModelAndView对象； 根据返回的ModelAndView，选择一个适合的ViewResolver（必须是已经注册到Spring容器中的ViewResolver)返回给DispatcherServlet ； ViewResolver 结合Model和View，来渲染视图 将渲染结果返回给客户端。 为什么Spring只使用一个Servlet(DispatcherServlet)来处理所有请求？ Spring工作流程描述 为什么Spring只使用一个Servlet(DispatcherServlet)来处理所有请求？提供一个集中的请求处理机制，所有的请求都将由一个单一的处理程序处理。该处理程序可以做认证/授权/记录日志，或者跟踪请求，然后把请求传给相应的处理程序。以下是这种设计模式的实体。 Spring为什么要结合使用HandlerMapping以及HandlerAdapter来处理Handler? 符合面向对象中的单一职责原则，代码架构清晰，便于维护，最重要的是代码可复用性高。如HandlerAdapter可能会被用于处理多种Handler。 Spring有哪些优点 1.轻量级：Spring在大小和透明性方面绝对属于轻量级的，基础版本的Spring框架大约只有2MB。2.控制反转(IOC)：Spring使用控制反转技术实现了松耦合。依赖被注入到对象，而不是创建或寻找依赖对象。3.面向切面编程(AOP)： Spring支持面向切面编程，同时把应用的业务逻辑与系统的服务分离开来。4.容器：Spring包含并管理应用程序对象的配置及生命周期。5.MVC框架：Spring的web框架是一个设计优良的web MVC框架，很好的取代了一些web框架。6.事务管理：Spring对下至本地业务上至全局业务(JAT)提供了统一的事务管理接口。7.异常处理：Spring提供一个方便的API将特定技术的异常(由JDBC, Hibernate, 或JDO抛出)转化为一致的、Unchecked异常。 什么是控制反转(IoC)？什么是依赖注入？ IoC，是 Inversion of Control 的缩写，即控制反转。 上层模块不应该依赖于下层模块，它们共同依赖于一个抽象 抽象不能依赖于具体实现，具体实现依赖于抽象。 注：又称为依赖倒置原则。这是设计模式六大原则之一。 DI，是 Dependency Injection 的缩写，即依赖注入。 依赖注入是 IoC 的最常见形式。 容器全权负责的组件的装配，它会把符合依赖关系的对象通过 JavaBean 属性或者构造函数传递给需要的对象。 依赖注入三种形式：1.构造器注入；2.setter 方法注入；3.接口注入 Spring 中的 IoC BeanFactory 是 Spring IoC 容器的具体实现，用来包装和管理前面提到的各种 bean。BeanFactory 接口是 Spring IoC 容器的核心接口。IOC:把对象的创建、初始化、销毁交给 spring 来管理，而不是由开发者控制，实现控制反转。 什么是Spring的依赖注入 依赖注入，是IOC的一个方面，是个通常的概念，它有多种解释。这概念是说你不用创建对象，而只需要描述它如何被创建。你不在代码里直接组装你的组件和服务，但是要在配置文件里描述哪些组件需要哪些服务，之后一个容器（IOC容器）负责把他们组装起来。 有哪些不同类型的IOC（依赖注入）方式 构造器依赖注入：构造器依赖注入通过容器触发一个类的构造器来实现的，该类有一系列参数，每个参数代表一个对其他类的依赖。 Setter方法注入：Setter方法注入是容器通过调用无参构造器或无参static工厂 方法实例化bean之后，调用该bean的setter方法，即实现了基于setter的依赖注入。 两种依赖方式都可以使用，构造器注入和Setter方法注入。最好的解决方案是用构造器参数实现强制依赖，setter方法实现可选依赖。 IOC容器是什么其优点 Spring IOC负责创建对象、管理对象(通过依赖注入)、整合对象、配置对象以及管理这些对象的生命周期。优点:IOC或依赖注入减少了应用程序的代码量。它使得应用程序的测试很简单，因为在单元测试中不再需要单例或JNDI查找机制。简单的实现以及较少的干扰机制使得松耦合得以实现。IOC容器支持勤性单例及延迟加载服务。 BeanFactory和ApplicationContext有什么区别 BeanFactory 包含了种 bean 的定义，以便在接收到客户端请求时将对应的 bean 实例化。BeanFactory 还能在实例化对象的时生成协作类之间的关系。BeanFactory 还包含了 bean 生命周期的控制，调用客户端的初始化方法（initialization methods）和销毁方法（destruction methods）。 ApplicationContext 扩展了 BeanFactory：1.提供了支持国际化的文本消息2.统一的资源文件读取方式3.已在监听器中注册的 bean 的事件 三种较常见的 ApplicationContext 实现： FileSystemXmlApplicationContext ：此容器从一个XML文件中加载beans的定义，XML Bean 配置文件的全路径名必须提供给它的构造函数。 ClassPathXmlApplicationContext：此容器也从一个XML文件中加载beans的定义，这里，你需要正确设置classpath因为这个容器将在classpath里找bean配置。 WebXmlApplicationContext：此容器加载一个XML文件，此文件定义了一个WEB应用的所有bean。 如果使用ApplicationContext，如果配置的bean是singleton，那么不管你有没有或想不想用它，它都会被实例化。好处是可以预先加载，坏处是浪费内存。BeanFactory，当使用BeanFactory实例化对象时，配置的bean不会马上被实例化，而是等到你使用该bean的时候（getBean）才会被实例化。好处是节约内存，坏处是速度比较慢。多用于移动设备的开发。 Spring有几种配置方式？ 将Spring配置到应用开发中有以下三种方式： 基于XML的配置 基于注解的配置 基于Java的配置 请解释Spring Bean的生命周期？对于普通的Java对象，当new的时候创建对象，当它没有任何引用的时候被垃圾回收机制回收。而由Spring IoC容器托管的对象，它们的生命周期完全由容器控制。Spring中每个Bean的生命周期如下： 实例化Bean：对于BeanFactory容器，当客户向容器请求一个尚未初始化的bean时，或初始化bean的时候需要注入另一个尚未初始化的依赖时，容器就会调用createBean进行实例化。对于ApplicationContext容器，当容器启动结束后，便实例化所有的bean。 容器通过获取BeanDefinition对象中的信息进行实例化。并且这一步仅仅是简单的实例化，并未进行依赖注入。 实例化对象被包装在BeanWrapper对象中，BeanWrapper提供了设置对象属性的接口，从而避免了使用反射机制设置属性。 设置对象属性（依赖注入）：实例化后的对象被封装在BeanWrapper对象中，并且此时对象仍然是一个原生的状态，并没有进行依赖注入。 紧接着，Spring根据BeanDefinition中的信息进行依赖注入。并且通过BeanWrapper提供的设置属性的接口完成依赖注入。 注入Aware接口：紧接着，Spring会检测该对象是否实现了xxxAware接口，并将相关的xxxAware实例注入给bean。 BeanPostProcessor：当经过上述几个步骤后，bean对象已经被正确构造，但如果你想要对象被使用前再进行一些自定义的处理，就可以通过BeanPostProcessor接口实现。该接口提供了两个函数： postProcessBeforeInitialzation( Object bean, String beanName ) 当前正在初始化的bean对象会被传递进来，我们就可以对这个bean作任何处理。 这个函数会先于InitialzationBean执行，因此称为前置处理。 所有Aware接口的注入就是在这一步完成的。 postProcessAfterInitialzation( Object bean, String beanName ) 当前正在初始化的bean对象会被传递进来，我们就可以对这个bean作任何处理。 这个函数会在InitialzationBean完成后执行，因此称为后置处理。 InitializingBean与init-method：当BeanPostProcessor的前置处理完成后就会进入本阶段。 InitializingBean接口只有一个函数：afterPropertiesSet()这一阶段也可以在bean正式构造完成前增加我们自定义的逻辑，但它与前置处理不同，由于该函数并不会把当前bean对象传进来，因此在这一步没办法处理对象本身，只能增加一些额外的逻辑。若要使用它，我们需要让bean实现该接口，并把要增加的逻辑写在该函数中。然后Spring会在前置处理完成后检测当前bean是否实现了该接口，并执行afterPropertiesSet函数。当然，Spring为了降低对客户代码的侵入性，给bean的配置提供了init-method属性，该属性指定了在这一阶段需要执行的函数名。Spring便会在初始化阶段执行我们设置的函数。init-method本质上仍然使用了InitializingBean接口。 DisposableBean和destroy-method：和init-method一样，通过给destroy-method指定函数，就可以在bean销毁前执行指定的逻辑。 解释Spring支持的几种bean的作用域 Spring框架支持以下五种bean的作用域： singleton : bean在每个Spring ioc 容器中只有一个实例。 prototype：一个bean的定义可以有多个实例。 request：每次http请求都会创建一个bean，该作用域仅在基于web的Spring ApplicationContext情形下有效。 session：在一个HTTP Session中，一个bean定义对应一个实例。该作用域仅在基于web的Spring ApplicationContext情形下有效。 global-session：在一个全局的HTTP Session中，一个bean定义对应一个实例。该作用域仅在基于web的Spring ApplicationContext情形下有效。 缺省的Spring bean 的作用域是Singleton. Spring框架中的单例Beans是线程安全的么？ Spring框架中的单例bean不是线程安全的。Spring框架并没有对单例bean进行任何多线程的封装处理。关于单例bean的线程安全和并发问题需要开发者自行去搞定。最浅显的解决办法就是将多态bean的作用域由“singleton”变更为“prototype”。 请举例说明如何在Spring中注入一个Java Collection？ Spring提供以下几种集合的配置元素： : 该标签用来装配可重复的list值。 : 该标签用来装配没有重复的set值。 : 该标签可用来注入键和值可以为任何类型的键值对。 : 该标签支持注入键和值都是字符串类型的键值对。 什么是bean装配 装配，或bean 装配是指在Spring 容器中把bean组装到一起，前提是容器需要知道bean的依赖关系，如何通过依赖注入来把它们装配到一起。 什么是bean的自动装配 Spring 容器能够自动装配相互合作的bean，这意味着容器不需要和配置，能通过Bean工厂自动处理bean之间的协作。 解释不同方式的自动装配 有五种自动装配的方式，可以用来指导Spring容器用自动装配方式来进行依赖注入。 no：默认的方式是不进行自动装配，通过显式设置ref 属性来进行装配。 byName：通过参数名 自动装配，Spring容器在配置文件中发现bean的autowire属性被设置成byname，之后容器试图匹配、装配和该bean的属性具有相同名字的bean。 byType:：通过参数类型自动装配，Spring容器在配置文件中发现bean的autowire属性被设置成byType，之后容器试图匹配、装配和该bean的属性具有相同类型的bean。如果有多个bean符合条件，则抛出错误。 constructor：这个方式类似于byType， 但是要提供给构造器参数，如果没有确定的带参数的构造器参数类型，将会抛出异常。 autodetect：首先尝试使用constructor来自动装配，如果无法工作，则使用byType方式。 自动装配有哪些局限性 自动装配的局限性是： 重写： 你仍需用 和 配置来定义依赖，意味着总要重写自动装配。 基本数据类型：你不能自动装配简单的属性，如基本数据类型，String字符串，和类。 模糊特性：自动装配不如显式装配精确，如果有可能，建议使用显式装配。 什么是基于Java的Spring注解配置? 给一些注解的例子 基于Java的配置，允许你在少量的Java注解的帮助下，进行你的大部分Spring配置而非通过XML文件。以@Configuration 注解为例，它用来标记类可以当做一个bean的定义，被Spring IOC容器使用。另一个例子是@Bean注解，它表示此方法将要返回一个对象，作为一个bean注册进Spring应用上下文。 什么是基于注解的容器配置 相对于XML文件，注解型的配置依赖于通过字节码元数据装配组件，而非尖括号的声明。开发者通过在相应的类，方法或属性上使用注解的方式，直接组件类中进行配置，而不是使用xml表述bean的装配关系。 @Required 注解 这个注解表明bean的属性必须在配置的时候设置，通过一个bean定义的显式的属性值或通过自动装配，若@Required注解的bean属性未被设置，容器将抛出BeanInitializationException。 @Autowired 注解 @Autowired 注解提供了更细粒度的控制，包括在何处以及如何完成自动装配。它的用法和@Required一样，修饰setter方法、构造器、属性或者具有任意名称和/或多个参数的PN方法。 @Qualifier 注解 当有多个相同类型的bean却只有一个需要自动装配时，将@Qualifier 注解和@Autowire 注解结合使用以消除这种混淆，指定需要装配的确切的bean。 构造方法注入和设值注入有什么区别？ 请注意以下明显的区别： 在设值注入方法支持大部分的依赖注入，如果我们仅需要注入int、string和long型的变量，我们不要用设值的方法注入。对于基本类型，如果我们没有注入的话，可以为基本类型设置默认值。在构造方法注入不支持大部分的依赖注入，因为在调用构造方法中必须传入正确的构造参数，否则的话为报错。 设值注入不会重写构造方法的值。如果我们对同一个变量同时使用了构造方法注入又使用了设置方法注入的话，那么构造方法将不能覆盖由设值方法注入的值。很明显，因为构造方法尽在对象被创建时调用。 在使用设值注入时有可能还不能保证某种依赖是否已经被注入，也就是说这时对象的依赖关系有可能是不完整的。而在另一种情况下，构造器注入则不允许生成依赖关系不完整的对象。 在设值注入时如果对象A和对象B互相依赖，在创建对象A时Spring会抛出sObjectCurrentlyInCreationException异常，因为在B对象被创建之前A对象是不能被创建的，反之亦然。所以Spring用设值注入的方法解决了循环依赖的问题，因对象的设值方法是在对象被创建之前被调用的。 Spring框架中有哪些不同类型的事件？ Spring 提供了以下5中标准的事件： 上下文更新事件（ContextRefreshedEvent）：该事件会在ApplicationContext被初始化或者更新时发布。也可以在调用ConfigurableApplicationContext 接口中的refresh()方法时被触发。 上下文开始事件（ContextStartedEvent）：当容器调用ConfigurableApplicationContext的Start()方法开始/重新开始容器时触发该事件。 上下文停止事件（ContextStoppedEvent）：当容器调用ConfigurableApplicationContext的Stop()方法停止容器时触发该事件。 上下文关闭事件（ContextClosedEvent）：当ApplicationContext被关闭时触发该事件。容器被关闭时，其管理的所有单例Bean都被销毁。 请求处理事件（RequestHandledEvent）：在Web应用中，当一个http请求（request）结束触发该事件。 BeanFactory 和 ApplicationContext 有什么区别 BeanFactory 可以理解为含有bean集合的工厂类。BeanFactory 包含了种bean的定义，以便在接收到客户端请求时将对应的bean实例化。BeanFactory还能在实例化对象的时生成协作类之间的关系。此举将bean自身与bean客户端的配置中解放出来。BeanFactory还包含了bean生命周期的控制，调用客户端的初始化方法（initializationmethods）和销毁方法（destruction methods）。从表面上看，applicationcontext如同beanfactory一样具有bean定义、bean关联关系的设置，根据请求分发bean的功能。但applicationcontext在此基础上还提供了其他的功能。提供了支持国际化的文本消息统一的资源文件读取方式已在监听器中注册的bean的事件 Spring Bean 的生命周期 Spring Bean的生命周期简单易懂。在一个bean实例被初始化时，需要执行一系列的初始化操作以达到可用的状态。同样的，当一个bean不在被调用时需要进行相关的析构操作，并从bean容器中移除。Spring bean factory 负责管理在spring容器中被创建的bean的生命周期。Bean的生命周期由两组回调（call back）方法组成。初始化之后调用的回调方法。销毁之前调用的回调方法。Spring框架提供了以下四种方式来管理bean的生命周期事件： InitializingBean和DisposableBean回调接口针对特殊行为的其他Aware接口Bean配置文件中的Custom init()方法和destroy()方法@PostConstruct和@PreDestroy注解方式 Spring IOC 如何实现 Spring中的 org.springframework.beans包和org.springframework.context包构成了Spring框架IoC容器的基础。BeanFactory 接口提供了一个先进的配置机制，使得任何类型的对象的配置成为可能。ApplicationContex接口对BeanFactory（是一个子接口）进行了扩展，在BeanFactory的基础上添加了其他功能，比如与Spring的AOP更容易集成，也提供了处理messageresource的机制（用于国际化）、事件传播以及应用层的特别配置，比如针对Web应用的WebApplicationContext。org.springframework.beans.factory.BeanFactory是SpringIoC容器的具体实现，用来包装和管理前面提到的各种bean。BeanFactory接口是Spring IoC 容器的核心接口。 说说 Spring AOP 面向切面编程，在我们的应用中，经常需要做一些事情，但是这些事情与核心业务无关，比如，要记录所有update方法的执行时间时间，操作人等等信息，记录到日志，通过spring的AOP技术，就可以在不修改update的代码的情况下完成该需求。 Spring AOP 实现原理 Spring AOP中的动态代理主要有两种方式，JDK动态代理和CGLIB动态代理。JDK动态代理通过反射来接收被代理的类，并且要求被代理的类必须实现一个接口。JDK动态代理的核心是InvocationHandler接口和Proxy类。 如果目标类没有实现接口，那么Spring AOP会选择使用CGLIB来动态代理目标类。CGLIB（Code Generation Library），是一个代码生成的类库，可以在运行时动态的生成某个类的子类，注意，CGLIB是通过继承的方式做的动态代理，因此如果某个类被标记为final，那么它是无法使用CGLIB做动态代理的。 动态代理（cglib 与 JDK） JDK 动态代理类和委托类需要都实现同一个接口。也就是说只有实现了某个接口的类可以使用Java动态代理机制。但是，事实上使用中并不是遇到的所有类都会给你实现一个接口。因此，对于没有实现接口的类，就不能使用该机制。而CGLIB则可以实现对类的动态代理。 有几种不同类型的自动代理 BeanNameAutoProxyCreator DefaultAdvisorAutoProxyCreator Metadata autoproxying AOP与OOP的区别 OOP面向对象编程，针对业务处理过程的实体及其属性和行为进行抽象封装，以获得更加清晰高效的逻辑单元划分。而AOP则是针对业务处理过程中的切面进行提取，它所面对的是处理过程的某个步骤或阶段，以获得逻辑过程的中各部分之间低耦合的隔离效果。这两种设计思想在目标上有着本质的差异。 举例：对于“雇员”这样一个业务实体进行封装，自然是OOP的任务，我们可以建立一个“Employee”类，并将“雇员”相关的属性和行为封装其中。而用AOP 设计思想对“雇员”进行封装则无从谈起。同样，对于“权限检查”这一动作片段进行划分，则是AOP的目标领域。OOP面向名次领域，AOP面向动词领域。总之AOP可以通过预编译方式和运行期动态代理实现在不修改源码的情况下，给程序动态同意添加功能的一项技术。 Aspect 切面 AOP核心就是切面，它将多个类的通用行为封装成可重用的模块，该模块含有一组API提供横切功能。比如，一个日志模块可以被称作日志的AOP切面。根据需求的不同，一个应用程序可以有若干切面。在Spring AOP中，切面通过带有@Aspect注解的类实现。 Spring 事务实现方式 1、编码方式所谓编程式事务指的是通过编码方式实现事务，即类似于JDBC编程实现事务管理。2、声明式事务管理方式声明式事务管理又有两种实现方式：基于xml配置文件的方式；另一个实在业务方法上进行@Transaction注解，将事务规则应用到业务逻辑中 Spring 事务底层原理 a、划分处理单元——IOC由于spring解决的问题是对单个数据库进行局部事务处理的，具体的实现首相用spring中的IOC划分了事务处理单元。并且将对事务的各种配置放到了ioc容器中（设置事务管理器，设置事务的传播特性及隔离机制）。b、AOP拦截需要进行事务处理的类Spring事务处理模块是通过AOP功能来实现声明式事务处理的，具体操作（比如事务实行的配置和读取，事务对象的抽象），用TransactionProxyFactoryBean接口来使用AOP功能，生成proxy代理对象，通过TransactionInterceptor完成对代理方法的拦截，将事务处理的功能编织到拦截的方法中。读取ioc容器事务配置属性，转化为spring事务处理需要的内部数据结构（TransactionAttributeSourceAdvisor），转化为TransactionAttribute表示的数据对象。c、对事物处理实现（事务的生成、提交、回滚、挂起）spring委托给具体的事务处理器实现。实现了一个抽象和适配。适配的具体事务处理器：DataSource数据源支持、hibernate数据源事务处理支持、JDO数据源事务处理支持，JPA、JTA数据源事务处理支持。这些支持都是通过设计PlatformTransactionManager、AbstractPlatforTransaction一系列事务处理的支持。 为常用数据源支持提供了一系列的TransactionManager。d、结合PlatformTransactionManager实现了TransactionInterception接口，让其与TransactionProxyFactoryBean结合起来，形成一个Spring声明式事务处理的设计体系。 spring的事务有几种它的隔离级别和传播行为 声明式事务和编程式事务 隔离级别： DEFAULT(default)使用数据库默认的隔离级别 READ_UNCOMMITTED(read_uncommitted)会出现脏读，不可重复读和幻影读问题 READ_COMMITTED(read_committed)会出现重复读和幻影读 REPEATABLE_READ(repeatable_read)会出现幻影读 SERIALIZABLE(serialzable)最安全，但是代价最大，性能影响极其严重 传播行为： REQUIRED(required)存在事务就融入该事务，不存在就创建事务 SUPPORTS(supports)存在事务就融入事务，不存在则不创建事务 MANDATORY(mandatory)存在事务则融入该事务，不存在，抛异常 REQUIRES_NEW(requirse_new)总是创建新事务 NOT_SUPPORTED(not_supported)存在事务则挂起，一直执行非事务操作 NEVER(never)总是执行非事务，如果当前存在事务则抛异常 NESTED(nested)嵌入式事务 如何自定义注解实现功能 创建自定义注解和创建一个接口相似，但是注解的interface关键字需要以@符号开头。注解方法不能带有参数；注解方法返回值类型限定为：基本类型、String、Enums、Annotation或者是这些类型的数组；注解方法可以有默认值；注解本身能够包含元注解，元注解被用来注解其它注解。 Spring MVC 运行流程 1.spring mvc将所有的请求都提交给DispatcherServlet,它会委托应用系统的其他模块负责对请求 进行真正的处理工作。2.DispatcherServlet查询一个或多个HandlerMapping,找到处理请求的Controller.3.DispatcherServlet请请求提交到目标Controller4.Controller进行业务逻辑处理后，会返回一个ModelAndView5.Dispathcher查询一个或多个ViewResolver视图解析器,找到ModelAndView对象指定的视图对象6.视图对象负责渲染返回给客户端。 Spring MVC 启动流程 在 web.xml 文件中给SpringMVC的Servlet配置了load-on-startup,所以程序启动的时候会初始化 Spring MVC，在 HttpServletBean 中将配置的 contextConfigLocation属性设置到 Servlet 中，然后在FrameworkServlet 中创建了 WebApplicationContext,DispatcherServlet根据contextConfigLocation 配置的 classpath 下的 xml 文件初始化了Spring MVC 总的组件。 Spring 的单例实现原理 Spring 对 Bean 实例的创建是采用单例注册表的方式进行实现的，而这个注册表的缓存是 ConcurrentHashMap 对象。 Spring 框架中用到了哪些设计模式 代理模式—在AOP和remoting中被用的比较多。单例模式—在spring配置文件中定义的bean默认为单例模式。模板方法—用来解决代码重复的问题。比如. RestTemplate, JmsTemplate, JpaTemplate。前端控制器—Spring提供了DispatcherServlet来对请求进行分发。视图帮助(View Helper)—Spring提供了一系列的JSP标签，高效宏来辅助将分散的代码整合在视图里。依赖注入—贯穿于BeanFactory / ApplicationContext接口的核心理念。工厂模式—BeanFactory用来创建对象的实例。 动态代理与cglib实现的区别 • JDK动态代理只能对实现了接口的类生成代理，而不能针对类.• CGLIB是针对类实现代理，主要是对指定的类生成一个子类，覆盖其中的方法因为是继承，所以该类或方法最好不要声明成final。• JDK代理是不需要以来第三方的库，只要JDK环境就可以进行代理• CGLib 必须依赖于CGLib的类库，但是它需要类来实现任何接口代理的是指定的类生成一个子类，覆盖其中的方法，是一种继承. Spring MVC的工作原理Spring MVC的工作原理 • 1 客户端的所有请求都交给前端控制器DispatcherServlet来处理，它会负责调用系统的其他模块来真正处理用户的请求。• 2 DispatcherServlet收到请求后，将根据请求的信息(包括URL、HTTP协议方法、请求头、请求参数、Cookie等)以及HandlerMapping的配置找到处理该请求的Handler(任何一个对象都可以作为请求的Handler)。• 3在这个地方Spring会通过HandlerAdapter对该处理进行封装。• 4 HandlerAdapter是一个适配器，它用统一的接口对各种Handler中的方法进行调用。• 5 Handler完成对用户请求的处理后，会返回一个ModelAndView对象给DispatcherServlet,ModelAndView顾名思义，包含了数据模型以及相应的视图的信息。• 6 ModelAndView的视图是逻辑视图，DispatcherServlet还要借助ViewResolver完成从逻辑视图到真实视图对象的解析工作。• 7 当得到真正的视图对象后，DispatcherServlet会利用视图对象对模型数据进行渲染。• 8 客户端得到响应，可能是一个普通的HTML页面，也可以是XML或JSON字符串，还可以是一张图片或者一个PDF文件。 你分析过SpringMVC的源码吗？1. MVC使用 在研究源码之前，先来回顾以下springmvc 是如何配置的，这将能使我们更容易理解源码。 1.1 web.xml1234567891011121314151617&lt;servlet&gt; &lt;servlet-name&gt;mvc-dispatcher&lt;/servlet-name&gt; &lt;servlet-class&gt;org.springframework.web.servlet.DispatcherServlet&lt;/servlet-class&gt; &lt;!-- 配置springMVC需要加载的配置文件 spring-dao.xml,spring-service.xml,spring-web.xml Mybatis - &gt; spring -&gt; springmvc --&gt; &lt;init-param&gt; &lt;param-name&gt;contextConfigLocation&lt;/param-name&gt; &lt;param-value&gt;classpath:spring/spring-*.xml&lt;/param-value&gt; &lt;/init-param&gt;&lt;/servlet&gt;&lt;servlet-mapping&gt; &lt;servlet-name&gt;mvc-dispatcher&lt;/servlet-name&gt; &lt;!-- 默认匹配所有的请求 --&gt; &lt;url-pattern&gt;/&lt;/url-pattern&gt;&lt;/servlet-mapping&gt; 值的注意的是contextConfigLocation和DispatcherServlet(用此类来拦截请求)的引用和配置。 1.2 spring-web.xml1234567891011121314151617181920212223&lt;!-- 配置SpringMVC --&gt;&lt;!-- 1.开启SpringMVC注解模式 --&gt;&lt;!-- 简化配置： (1)自动注册DefaultAnootationHandlerMapping,AnotationMethodHandlerAdapter (2)提供一些列：数据绑定，数字和日期的format @NumberFormat, @DateTimeFormat, xml,json默认读写支持 --&gt;&lt;mvc:annotation-driven /&gt;&lt;!-- 2.静态资源默认servlet配置 (1)加入对静态资源的处理：js,gif,png (2)允许使用"/"做整体映射 --&gt; &lt;mvc:default-servlet-handler/&gt; &lt;!-- 3.配置jsp 显示ViewResolver --&gt; &lt;bean class="org.springframework.web.servlet.view.InternalResourceViewResolver"&gt; &lt;property name="viewClass" value="org.springframework.web.servlet.view.JstlView" /&gt; &lt;property name="prefix" value="/WEB-INF/jsp/" /&gt; &lt;property name="suffix" value=".jsp" /&gt; &lt;/bean&gt; &lt;!-- 4.扫描web相关的bean --&gt; &lt;context:component-scan base-package="com.xxx.fantj.web" /&gt; 值的注意的是InternalResourceViewResolver，它会在ModelAndView返回的试图名前面加上prefix前缀，在后面加载suffix指定后缀。 SpringMvc主支源码分析 上图流程总体来说可分为三大块： Map的建立(并放入WebApplicationContext) HttpRequest请求中Url的请求拦截处理(DispatchServlet处理) 反射调用Controller中对应的处理方法，并返回视图 本文将围绕这三块进行分析。 1. Map的建立 在容器初始化时会建立所有 url 和 Controller 的对应关系,保存到 Map中，那是如何保存的呢。 ApplicationObjectSupport #setApplicationContext方法123456// 初始化ApplicationContext@Overridepublic void initApplicationContext() throws ApplicationContextException &#123; super.initApplicationContext(); detectHandlers();&#125; AbstractDetectingUrlHandlerMapping #detectHandlers()方法：12345678910111213141516171819202122232425262728/** * 建立当前ApplicationContext 中的 所有Controller 和url 的对应关系 */protected void detectHandlers() throws BeansException &#123; if (logger.isDebugEnabled()) &#123; logger.debug("Looking for URL mappings in application context: " + getApplicationContext()); &#125; // 获取容器中的beanNames String[] beanNames = (this.detectHandlersInAncestorContexts ? BeanFactoryUtils.beanNamesForTypeIncludingAncestors(getApplicationContext(), Object.class) : getApplicationContext().getBeanNamesForType(Object.class)); // 遍历 beanNames 并找到对应的 url // Take any bean name that we can determine URLs for. for (String beanName : beanNames) &#123; // 获取bean上的url(class上的url + method 上的 url) String[] urls = determineUrlsForHandler(beanName); if (!ObjectUtils.isEmpty(urls)) &#123; // URL paths found: Let's consider it a handler. // 保存url 和 beanName 的对应关系 registerHandler(urls, beanName); &#125; else &#123; if (logger.isDebugEnabled()) &#123; logger.debug("Rejected bean name '" + beanName + "': no URL paths identified"); &#125; &#125; &#125;&#125; determineUrlsForHandler()方法： 该方法在不同的子类有不同的实现，我这里分析的是DefaultAnnotationHandlerMapping类的实现，该类主要负责处理@RequestMapping注解形式的声明。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647/** * 获取@RequestMaping注解中的url */@Overrideprotected String[] determineUrlsForHandler(String beanName) &#123; ApplicationContext context = getApplicationContext(); Class&lt;?&gt; handlerType = context.getType(beanName); // 获取beanName 上的requestMapping RequestMapping mapping = context.findAnnotationOnBean(beanName, RequestMapping.class); if (mapping != null) &#123; // 类上面有@RequestMapping 注解 this.cachedMappings.put(handlerType, mapping); Set&lt;String&gt; urls = new LinkedHashSet&lt;String&gt;(); // mapping.value()就是获取@RequestMapping注解的value值 String[] typeLevelPatterns = mapping.value(); if (typeLevelPatterns.length &gt; 0) &#123; // 获取Controller 方法上的@RequestMapping String[] methodLevelPatterns = determineUrlsForHandlerMethods(handlerType); for (String typeLevelPattern : typeLevelPatterns) &#123; if (!typeLevelPattern.startsWith("/")) &#123; typeLevelPattern = "/" + typeLevelPattern; &#125; for (String methodLevelPattern : methodLevelPatterns) &#123; // controller的映射url+方法映射的url String combinedPattern = getPathMatcher().combine(typeLevelPattern, methodLevelPattern); // 保存到set集合中 addUrlsForPath(urls, combinedPattern); &#125; addUrlsForPath(urls, typeLevelPattern); &#125; // 以数组形式返回controller上的所有url return StringUtils.toStringArray(urls); &#125; else &#123; // controller上的@RequestMapping映射url为空串,直接找方法的映射url return determineUrlsForHandlerMethods(handlerType); &#125; &#125; // controller上没@RequestMapping注解 else if (AnnotationUtils.findAnnotation(handlerType, Controller.class) != null) &#123; // 获取controller中方法上的映射url return determineUrlsForHandlerMethods(handlerType); &#125; else &#123; return null; &#125;&#125; 更深的细节代码就比较简单了，有兴趣的可以继续深入。 到这里，Controller和Url的映射就装配完成，下来就分析请求的处理过程。 2. url的请求处理 我们在xml中配置了DispatcherServlet为调度器，所以我们就来看它的代码，可以从名字上看出它是个Servlet,那么它的核心方法就是doService() DispatcherServlet #doService():123456789101112131415161718192021222324252627282930313233343536373839404142/** * 将DispatcherServlet特定的请求属性和委托 公开给&#123;@link #doDispatch&#125;以进行实际调度。 */@Overrideprotected void doService(HttpServletRequest request, HttpServletResponse response) throws Exception &#123; if (logger.isDebugEnabled()) &#123; String requestUri = new UrlPathHelper().getRequestUri(request); logger.debug("DispatcherServlet with name '" + getServletName() + "' processing " + request.getMethod() + " request for [" + requestUri + "]"); &#125; //在包含request的情况下保留请求属性的快照， //能够在include之后恢复原始属性。 Map&lt;String, Object&gt; attributesSnapshot = null; if (WebUtils.isIncludeRequest(request)) &#123; logger.debug("Taking snapshot of request attributes before include"); attributesSnapshot = new HashMap&lt;String, Object&gt;(); Enumeration attrNames = request.getAttributeNames(); while (attrNames.hasMoreElements()) &#123; String attrName = (String) attrNames.nextElement(); if (this.cleanupAfterInclude || attrName.startsWith("org.springframework.web.servlet")) &#123; attributesSnapshot.put(attrName, request.getAttribute(attrName)); &#125; &#125; &#125; // 使得request对象能供 handler处理和view处理 使用 request.setAttribute(WEB_APPLICATION_CONTEXT_ATTRIBUTE, getWebApplicationContext()); request.setAttribute(LOCALE_RESOLVER_ATTRIBUTE, this.localeResolver); request.setAttribute(THEME_RESOLVER_ATTRIBUTE, this.themeResolver); request.setAttribute(THEME_SOURCE_ATTRIBUTE, getThemeSource()); try &#123; doDispatch(request, response); &#125; finally &#123; // 如果不为空，则还原原始属性快照。 if (attributesSnapshot != null) &#123; restoreAttributesAfterInclude(request, attributesSnapshot); &#125; &#125;&#125; 可以看到，它将请求拿到后，主要是给request设置了一些对象，以便于后续工作的处理(Handler处理和view处理)。比如WebApplicationContext，它里面就包含了我们在第一步完成的controller与url映射的信息。 DispatchServlet # doDispatch()123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103/** * 控制请求转发 */protected void doDispatch(HttpServletRequest request, HttpServletResponse response) throws Exception &#123; HttpServletRequest processedRequest = request; HandlerExecutionChain mappedHandler = null; int interceptorIndex = -1; try &#123; ModelAndView mv; boolean errorView = false; try &#123; // 1. 检查是否是上传文件 processedRequest = checkMultipart(request); // 2. 获取handler处理器，返回的mappedHandler封装了handlers和interceptors mappedHandler = getHandler(processedRequest, false); if (mappedHandler == null || mappedHandler.getHandler() == null) &#123; // 返回404 noHandlerFound(processedRequest, response); return; &#125; // 获取HandlerInterceptor的预处理方法 HandlerInterceptor[] interceptors = mappedHandler.getInterceptors(); if (interceptors != null) &#123; for (int i = 0; i &lt; interceptors.length; i++) &#123; HandlerInterceptor interceptor = interceptors[i]; if (!interceptor.preHandle(processedRequest, response, mappedHandler.getHandler())) &#123; triggerAfterCompletion(mappedHandler, interceptorIndex, processedRequest, response, null); return; &#125; interceptorIndex = i; &#125; &#125; // 3. 获取handler适配器 Adapter HandlerAdapter ha = getHandlerAdapter(mappedHandler.getHandler()); // 4. 实际的处理器处理并返回 ModelAndView 对象 mv = ha.handle(processedRequest, response, mappedHandler.getHandler()); // Do we need view name translation? if (mv != null &amp;&amp; !mv.hasView()) &#123; mv.setViewName(getDefaultViewName(request)); &#125; // HandlerInterceptor 后处理 if (interceptors != null) &#123; for (int i = interceptors.length - 1; i &gt;= 0; i--) &#123; HandlerInterceptor interceptor = interceptors[i]; // 结束视图对象处理 interceptor.postHandle(processedRequest, response, mappedHandler.getHandler(), mv); &#125; &#125; &#125; catch (ModelAndViewDefiningException ex) &#123; logger.debug("ModelAndViewDefiningException encountered", ex); mv = ex.getModelAndView(); &#125; catch (Exception ex) &#123; Object handler = (mappedHandler != null ? mappedHandler.getHandler() : null); mv = processHandlerException(processedRequest, response, handler, ex); errorView = (mv != null); &#125; if (mv != null &amp;&amp; !mv.wasCleared()) &#123; render(mv, processedRequest, response); if (errorView) &#123; WebUtils.clearErrorRequestAttributes(request); &#125; &#125; else &#123; if (logger.isDebugEnabled()) &#123; logger.debug("Null ModelAndView returned to DispatcherServlet with name '" + getServletName() + "': assuming HandlerAdapter completed request handling"); &#125; &#125; // 请求成功响应之后的方法 triggerAfterCompletion(mappedHandler, interceptorIndex, processedRequest, response, null); &#125; catch (Exception ex) &#123; // Trigger after-completion for thrown exception. triggerAfterCompletion(mappedHandler, interceptorIndex, processedRequest, response, ex); throw ex; &#125; catch (Error err) &#123; ServletException ex = new NestedServletException("Handler processing failed", err); // Trigger after-completion for thrown exception. triggerAfterCompletion(mappedHandler, interceptorIndex, processedRequest, response, ex); throw ex; &#125; finally &#123; // Clean up any resources used by a multipart request. if (processedRequest != request) &#123; cleanupMultipart(processedRequest); &#125; &#125;&#125; 该方法主要是 通过request对象获取到HandlerExecutionChain，HandlerExecutionChain对象里面包含了拦截器interceptor和处理器handler。如果获取到的对象是空，则交给noHandlerFound返回404页面。 拦截器预处理，如果执行成功则进行3 获取handler适配器 Adapter 实际的处理器处理并返回 ModelAndView 对象 下面是该方法中的一些核心细节： DispatchServlet #doDispatch # noHandlerFound核心源码： 1response.sendError(HttpServletResponse.SC_NOT_FOUND); DispatchServlet #doDispatch #getHandler方法事实上调用的是AbstractHandlerMapping #getHandler方法,我贴出一个核心的代码： 12345678// 拿到处理对象Object handler = getHandlerInternal(request);...String handlerName = (String) handler;handler = getApplicationContext().getBean(handlerName);...// 返回HandlerExecutionChain对象return getHandlerExecutionChain(handler, request); 可以看到，它先从request里获取handler对象，这就证明了之前DispatchServlet #doService为什么要吧WebApplicationContext放入request请求对象中。 最终返回一个HandlerExecutionChain对象. 3. 反射调用处理请求的方法，返回结果视图 在上面的源码中，实际的处理器处理并返回 ModelAndView 对象调用的是mv = ha.handle(processedRequest, response, mappedHandler.getHandler());这个方法。该方法由AnnotationMethodHandlerAdapter #handle() #invokeHandlerMethod()方法实现. AnnotationMethodHandlerAdapter #handle() #invokeHandlerMethod()12345678910111213141516171819202122/** * 获取处理请求的方法,执行并返回结果视图 */protected ModelAndView invokeHandlerMethod(HttpServletRequest request, HttpServletResponse response, Object handler) throws Exception &#123; // 1.获取方法解析器 ServletHandlerMethodResolver methodResolver = getMethodResolver(handler); // 2.解析request中的url,获取处理request的方法 Method handlerMethod = methodResolver.resolveHandlerMethod(request); // 3. 方法调用器 ServletHandlerMethodInvoker methodInvoker = new ServletHandlerMethodInvoker(methodResolver); ServletWebRequest webRequest = new ServletWebRequest(request, response); ExtendedModelMap implicitModel = new BindingAwareModelMap(); // 4.执行方法（获取方法的参数） Object result = methodInvoker.invokeHandlerMethod(handlerMethod, handler, webRequest, implicitModel); // 5. 封装成mv视图 ModelAndView mav = methodInvoker.getModelAndView(handlerMethod, handler.getClass(), result, implicitModel, webRequest); methodInvoker.updateModelAttributes(handler, (mav != null ? mav.getModel() : null), implicitModel, webRequest); return mav;&#125; 这个方法有两个重要的地方，分别是resolveHandlerMethod和invokeHandlerMethod。 resolveHandlerMethod 方法methodResolver.resolveHandlerMethod(request):获取controller类和方法上的@requestMapping value,与request的url进行匹配,找到处理request的controller中的方法.最终拼接的具体实现是org.springframework.util.AntPathMatcher#combine方法。 invokeHandlerMethod方法 从名字就能看出来它是基于反射，那它做了什么呢。 解析该方法上的参数,并调用该方法。 123456//上面全都是为解析方法上的参数做准备...// 解析该方法上的参数Object[] args = resolveHandlerArguments(handlerMethodToInvoke, handler, webRequest, implicitModel);// 真正执行解析调用的方法return doInvokeMethod(handlerMethodToInvoke, handler, args); invokeHandlerMethod方法#resolveHandlerArguments方法 代码有点长，我就简介下它做了什么事情吧。 如果这个方法的参数用的是注解，则解析注解拿到参数名，然后拿到request中的参数名，两者一致则进行赋值(详细代码在HandlerMethodInvoker#resolveRequestParam)，然后将封装好的对象放到args[]的数组中并返回。 如果这个方法的参数用的不是注解，则需要asm框架(底层是读取字节码)来帮助获取到参数名，然后拿到request中的参数名，两者一致则进行赋值，然后将封装好的对象放到args[]的数组中并返回。 invokeHandlerMethod方法#doInvokeMethod方法123456789101112private Object doInvokeMethod(Method method, Object target, Object[] args) throws Exception &#123; // 将一个方法设置为可调用，主要针对private方法 ReflectionUtils.makeAccessible(method); try &#123; // 反射调用 return method.invoke(target, args); &#125; catch (InvocationTargetException ex) &#123; ReflectionUtils.rethrowException(ex.getTargetException()); &#125; throw new IllegalStateException("Should never get here");&#125; 到这里,就可以对request请求中url对应的controller的某个对应方法进行调用了。 总结： 看完后脑子一定很乱，有时间的话还是需要自己动手调试一下。本文只是串一下整体思路，所以功能性的源码没有全部分析。 其实理解这些才是最重要的。 用户发送请求至前端控制器DispatcherServlet DispatcherServlet收到请求调用HandlerMapping处理器映射器。 处理器映射器根据请求url找到具体的处理器，生成处理器对象及处理器拦截器(如果有则生成)一并返回给DispatcherServlet。 DispatcherServlet通过HandlerAdapter处理器适配器调用处理器 HandlerAdapter执行处理器(handler，也叫后端控制器)。 Controller执行完成返回ModelAndView HandlerAdapter将handler执行结果ModelAndView返回给DispatcherServlet DispatcherServlet将ModelAndView传给ViewReslover视图解析器 ViewReslover解析后返回具体View对象 DispatcherServlet对View进行渲染视图（即将模型数据填充至视图中）。 DispatcherServlet响应用户]]></content>
      <categories>
        <category>基础面试题</category>
      </categories>
      <tags>
        <tag>基础面试题</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[第 17 章 Spring MVC]]></title>
    <url>%2F2019%2F04%2F21%2FSpring%2F%E7%AC%AC17%E7%AB%A0SpringMVC%2F</url>
    <content type="text"><![CDATA[第 17 章 Spring MVCSpring MVC 的框架设计 其中带有阿拉伯数字的说明，是 MVC 框架运行的流程。处理请求先到达控制器（Controller），控制器的作用是进行请求分发，这样它会根据请求的内容去访问模型层（Model）；现今互联网系统中，数据主要从数据库和 NoSQL 中来，而且对于数据库而言往往还存在事务的机制，为了适应这种变化，设计者会将模型层再分成两层，即服务层（Service）和（DAO）;当控制器获取到由模型层返回的数据后，就将数据渲染到视图中，这样就能够展现给用户了。 Spring MVC 流程 它是 Spring MVC 运行的全流程，其中图中的阿拉伯数字是其执行流程。在启用 Spring MVC 时，它开始初始化一些重要组件，如 DispactherServlet、HandlerAdapter的实现类 RequestMappingHandlerAdapter 等组件对象。关于这些组件的初始化，可以看 spring-webmvc-xxx.jar 的属性文件 DispatcherServlet.properties，它定义的对象都是在 Spring MVC 开始时就初始化，并且存放在 Spring IOC 容器中。 DispatcherServlet.properties 源码如下： 12345678910111213141516171819202122232425262728# 国际化解析器org.springframework.web.servlet.LocaleResolver=org.springframework.web.servlet.i18n.AcceptHeaderLocaleResolver# 主题解析器org.springframework.web.servlet.ThemeResolver=org.springframework.web.servlet.theme.FixedThemeResolver# HandlerMapping 实例org.springframework.web.servlet.HandlerMapping=org.springframework.web.servlet.handler.BeanNameUrlHandlerMapping,\ org.springframework.web.servlet.mvc.annotation.DefaultAnnotationHandlerMapping # 处理器适配器 org.springframework.web.servlet.HandlerAdapter=org.springframework.web.servlet.mvc.HttpRequestHandlerAdapter,\ org.springframework.web.servlet.mvc.SimpleControllerHandlerAdapter,\ org.springframework.web.servlet.mvc.annotation.AnnotationMethodHandlerAdapter# 处理器异常解析器org.springframework.web.servlet.HandlerExceptionResolver=org.springframework.web.servlet.mvc.annotation.AnnotationMethodHandlerExceptionResolver,\ org.springframework.web.servlet.mvc.annotation.ResponseStatusExceptionResolver,\ org.springframework.web.servlet.mvc.support.DefaultHandlerExceptionResolver# 策略视图名称转换器，当你没有返回视图逻辑名称的时候，通过它可以生成默认的视图名称org.springframework.web.servlet.RequestToViewNameTranslator=org.springframework.web.servlet.view.DefaultRequestToViewNameTranslator# 视图解析器org.springframework.web.servlet.ViewResolver=org.springframework.web.servlet.view.InternalResourceViewResolver# FlashMap 管理器。org.springframework.web.servlet.FlashMapManager=org.springframework.web.servlet.support.SessionFlashMapManager 首先这些组件会在 Spring MVC 得到初始化。 其次是开发控制器（Controller），代码如下： 123456789101112131415161718192021@Controller@RequestMapping("/user")public class UserController &#123; //注入用户服务类 @Autowired private UserService userService = null; // 展示用户详情 @RequestMapping("details") public ModelAndV工ewdetails(Long工d)&#123; // 访问模型层得到数据 User user= userService.getUser(id); // 模型和视图 ModelAndView mv = new ModelAndView(); // 定义模型视图 mv.setViewName("user/details"); // 加入数据模型 mv.addObject("user",user); // 返回模型和视图 return mv; &#125;&#125; 这里的注解 @Controller 表明这是一个控制器，然后 @RequestMapping 代表请求路径和控制器（或其他方法）的映射关系，它会在Web 服务器启动 Spring MVC 时，被扫描到 HandlerMapping 的机制中存储，之后在用户发起请求被 DispatcherServlet 拦截后，通过 URI 和其他的条件，通过 HandlerMapper 机制就能找到对应的控制器（或其方法）进行响应。只是通过 HandlerMapping 返回的是一个 HandlerExecutionChain 对象，这个对象的源码如下。HandlerExecutionChain 对象包含一个处理器（handler）。这里的处理器是对控制器（controller）的包装，因为我们的控制器方法就可以读入 HTTP 和上下文的相关参数，然后再传递给控制器方法。而在控制器执行完成返回后，处理器又可以通过配置信息对控制器的返回结果进行处理。 处理器包含了控制器方法的逻辑，此外还有处理器的拦截器（interceptor），这样就能够通过拦截器进一步地增强处理器的功能。 12345678910111213public class HandlerExecutionChain &#123; // 日志 private static final Log logger = LogFactory.getLog(HandlerExecutionChain.class); // 处理器 private final Object handler; // 拦截器数组 private HandlerInterceptor[] interceptors; // 拦截器列表 private List&lt;HandlerInterceptor&gt; interceptorList; // 拦截器当前下标 private int interceptorIndex; ...&#125; 得到了处理器（handler），还需要去运行，但是我们有普通HTTP请求，也有按 BeanName 的请求，甚至是 WebSocket ，所以还需要一个适配器去运行 HandlerExecutionChain 对象包含的处理器，就是 HandlerAdapter 接口定义的实现类，HttpRequestHandlerAdapter。]]></content>
      <categories>
        <category>Spring</category>
      </categories>
      <tags>
        <tag>Spring</tag>
      </tags>
  </entry>
  <entry>
    <title></title>
    <url>%2F2019%2F04%2F19%2FSolr%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[CentOS安装Tomcat]]></title>
    <url>%2F2019%2F04%2F19%2FCentOS%2FCentOS%E5%AE%89%E8%A3%85Tomcat%2F</url>
    <content type="text"><![CDATA[CentOS安装Tomcat一.tomcat的简介这是Apache Tomcat Servlet / JSP容器的文档包的顶级入口点 。的Apache Tomcat 8.0版实现了Servlet 3.1和JavaServer Pages 2.3 规范从 Java社区进程，并包含许多额外的功能，使开发和部署Web应用程序和Web服务的有用平台 二.tomcat的安装1.tomcat下载官网地址：http://tomcat.apache.org/ 123[admin@node21 software]$ wget http://mirrors.shu.edu.cn/apache/tomcat/tomcat-8/v8.0.53/bin/apache-tomcat-8.0.53.tar.gz[admin@node21 software]$ ll-rw-rw-r-- 1 admin admin 9455895 Jun 30 00:39 apache-tomcat-8.0.53.tar.gz 2.tomcat安装查看是否安装 JDK 1）解压缩安装包 1[admin@node21 software]$ tar zxvf apache-tomcat-8.0.53.tar.gz 2）移动安装包到/usr/local/tomcat目录下，也可以不移动设置tomcat环境变量 1[admin@node21 software]$ sudo mv apache-tomcat-8.0.53 /usr/local/tomcat8 3.启动tomcat 123[admin@node21 bin]$ pwd/usr/local/tomcat8/bin[admin@node21 bin]$ ./startup.sh 4.测试内部是否启动成功1curl &quot;http://47.106.180.186:8089/&quot; 5.WebUI访问tomcat默认端口8080，访问地址：http://47.106.180.186:8089/，默认页面如下 6.停止tomcat1[admin@node21 webapps]$ /usr/local/tomcat8/bin/shutdown.sh 三.Tomcat服务部署web应用第一种方式：利用Tomcat自动部署 ​ 利用Tomcat自动部署方式是最简单的、最常用的方式。若一个web应用结构为D:\workspace\WebApp\AppName\WEB-INF*，只要将一个Web应用的WebContent级的AppName直接扔进%Tomcat_Home%\webapps文件夹下，系统会把该web应用直接部署到Tomcat中。所以这里不再赘述 第二种方式：手动部署修改%Tomcat_Home%\conf\server.xml文件来部署web应用 打开%Tomcat_Home%\conf\server.xml文件并在其中标签里增加以下元素： 1&lt;Context docBase=&quot;D:\workspace\WebApp\AppName&quot; path=&quot;/XXX&quot; debug=&quot;0&quot; reloadable=&quot;false&quot; /&gt; 然后启动Tomcat即可。 注意： ​ （1）以上代码中的workDir表示将该Web应用部署后置于的工作目录（Web应用中JSP编译成的Servlet都可在其中找到）。如果自定义web部署文件XXX.xml中未指明workdir，则web应用将默认部署在 1%Tomcat_Home%\work\Catalina\localhost 路径下新建的以XXX命名的文件夹下。（Web应用中JSP编译成的Servlet都可在其中找到） ​ （2）Context path即指定web应用的虚拟路径名。docBase指定要部署的Web应用的源路径。 四.解决中文乱码及测试访问页1.测试修改访问页面 12345&lt;html&gt; &lt;body&gt; &lt;h1&gt;Hello,世界!&lt;/h1&gt; &lt;/body&gt;&lt;/html&gt; 再次启动tomcat，输入：http://47.106.180.186:8089/hello/index.html，出现下图，发现有中文乱码现象。 2.解决中文乱码乱码原因：tomcat8之前，URL中参数的默认解码是ISO-8859-1，而tomcat8的默认解码为utf-8。ISO-8859-1并未包括中文字符，中文字符不能被正确解析了。]]></content>
      <categories>
        <category>CentOS</category>
      </categories>
      <tags>
        <tag>CentOS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CentOS安装JAVA环境（JDK 1.8）]]></title>
    <url>%2F2019%2F04%2F19%2FCentOS%2FCentOS%E5%AE%89%E8%A3%85JAVA%E7%8E%AF%E5%A2%83%2F</url>
    <content type="text"><![CDATA[CentOS安装JAVA环境（JDK 1.8）通过yum安装12如果服务器没有wget服务，使用yum -y install wget安装yum install java-1.8.0-openjdk* -y 执行这一条命令就可以直接安装，并且无需配置就能使用。 下载tar包安装http://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html 我选择linux x64版本： 下载 下载以后通过命令检查安装包大小是否符合 1ls -lht 安装（1）创建安装目录 1mkdir /usr/local/java/ （2）解压至安装目录 1tar -zxvf jdk-8u171-linux-x64.tar.gz -C /usr/local/java/ 设置环境变量打开文件 1vim /etc/profile 在末尾添加 1234export JAVA_HOME=/usr/local/java/jdk1.8.0_171export JRE_HOME=$&#123;JAVA_HOME&#125;/jreexport CLASSPATH=.:$&#123;JAVA_HOME&#125;/lib:$&#123;JRE_HOME&#125;/libexport PATH=$&#123;JAVA_HOME&#125;/bin:$PATH 使环境变量生效 1source /etc/profile 添加软链接 1ln -s /usr/local/java/jdk1.8.0_171/bin/java /usr/bin/java 检查 1java -version]]></content>
      <categories>
        <category>CentOS</category>
      </categories>
      <tags>
        <tag>CentOS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CentOS搭建Solr7单机服务]]></title>
    <url>%2F2019%2F04%2F19%2FCentOS%2FCentOS%E6%90%AD%E5%BB%BASolr7%20%E5%8D%95%E6%9C%BA%E6%9C%8D%E5%8A%A1%2F</url>
    <content type="text"><![CDATA[CentOS搭建Solr7单机服务一.Solr安装环境1.官方参考文档Solr教程参考指南：http://lucene.apache.org/solr/guide/7_4/solr-tutorial.html 2.Solr运行环境系统要求：Java 8+ 这里我们把solr服务部署到Tomacat服务器中，Tomcat安装过程参考：https://swenfang.github.io/2019/04/19/CentOS/CentOS安装Tomcat/ 在solr5以前solr的启动都有tomcat作为容器，但是从solr5以后solr内部集成jetty服务器，可以通过bin目录中脚本直接启动。就是从solr5以后跟solr4最大的区别是被发布成一个独立的应用。 3.Solr下载 下载地址：http://archive.apache.org/dist/lucene/solr/ 123[admin@node21 software]$ wget http://archive.apache.org/dist/lucene/solr/7.4.0/solr-7.4.0.tgz[admin@node21 software]$ ll-rw-rw-r-- 1 admin admin 167346886 Jun 19 02:51 solr-7.4.0.tgz 二.Solr单机安装1. 解压安装包123[admin@node21 software]$ tar zxvf solr-7.4.0.tgz [admin@node21 software]$ ls solr-7.4.0bin CHANGES.txt contrib dist docs example licenses LICENSE.txt LUCENE_CHANGES.txt NOTICE.txt README.txt server 2.部署solr到tomcat下注意，这里因为我用的是solr7.4最新版，所以跟solr4版本要拷贝*.war文件，然后再启动tomcat解压的操作是不一样的 ， 1）复制并重命名solr目录里的server/solr-webapp/webapp文件夹到/usr/local/tomcat8/webapps/solr 1[admin@node21 software]$ sudo cp -r solr-7.4.0/server/solr-webapp/webapp /usr/local/tomcat8/webapps/solr 2）拷贝solr-7.4.0\server\lib\ext 下的jar包以及lib目录下gmetric4j-1.0.7.jar和metrics开头的jar包拷贝到 tomcat8\webapps\solr 项目的WEB-INF\lib下 123[admin@node21 software]$ sudo cp solr-7.4.0/server/lib/ext/* /usr/local/tomcat8/webapps/solr/WEB-INF/lib/[admin@node21 software]$ sudo cp solr-7.4.0/server/lib/gmetric4j-1.0.7.jar /usr/local/tomcat8/webapps/solr/WEB-INF/lib/[admin@node21 software]$ sudo cp solr-7.4.0/server/lib/metrics-* /usr/local/tomcat8/webapps/solr/WEB-INF/lib/ 3）创建一个索引库solrhome 拷贝solr-7.4.0\server 下的solr文件夹到其它非中文目录下，重命名为solrhome，我是建立到了/usr/local/tomcat8/solrhome下 1[admin@node21 software]$ sudo cp -r solr-7.4.0/server/solr /usr/local/tomcat8/solrhome 4）关联solr及索引库solrhome，需要修改tomcat里solr工程的web.xml文件 1[admin@node21 software]$ sudo vi /usr/local/tomcat8/webapps/solr/WEB-INF/web.xml 找到如下代码，打开注释，修改自己的solrhome的路径/put/your/solr/home/here，我的是 /usr/local/tomcat8/solrhome 路径。 1234567&lt;!-- &lt;env-entry&gt; &lt;env-entry-name&gt;solr/home&lt;/env-entry-name&gt; &lt;env-entry-value&gt;/put/your/solr/home/here&lt;/env-entry-value&gt; &lt;env-entry-type&gt;java.lang.String&lt;/env-entry-type&gt; &lt;/env-entry&gt; --&gt; 如下图： 然后到最下方，将这一段注释掉，不然会报403错误，完成后保存退出（solr4部署不用注释这个） 5）拷贝solr7.4.0\server\resources下的 log4j2.xml 到tomcat8/webapps/solr/WEB-INF\classes，如果WEB-INF下没有classes文件那么就创建一个classes文件夹 12[admin@node21 tomcat8]$ sudo mkdir -p /usr/local/tomcat8/webapps/solr/WEB-INF/classes/[admin@node21 tomcat8]$ sudo cp -r /opt/software/solr-7.4.0/server/resources/log4j2.xml /usr/local/tomcat8/webapps/solr/WEB-INF/classes/ 6）修改tomcat的bin目录下catalina.bat脚本，增加solr.log.dir系统变量，指定solr日志记录存放地址。 12[root@node21 solr]# vi /usr/local/tomcat8/bin/catalina.sh JAVA_OPTS=&quot;$JAVA_OPTS -Dsolr.log.dir=/usr/local/tomcat8/solrhome/logs&quot; 3.启动服务启动tomcat，访问需要完整路径，我的是 http://47.106.180.186:8089/solr/index.html]]></content>
      <categories>
        <category>CentOS</category>
      </categories>
      <tags>
        <tag>CentOS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java 内存区域与内存溢出异常]]></title>
    <url>%2F2019%2F04%2F09%2FJava%20JVM%2FJava%20%E7%A8%8B%E5%BA%8F%E7%9A%84%E8%BF%90%E8%A1%8C%E6%9C%BA%E5%88%B6%2F</url>
    <content type="text"><![CDATA[Java 内存区域与内存溢出异常对于 Java 程序员来说，把内存控制权力给了 Java 虚拟机，一旦出现了内存泄漏和溢出方面的问题，如果不了解虚拟机是怎样使用内存的，那么排查错误将会成为一项异常艰难的工作。 运行时数据区域java 虚拟机在执行 Java 程序的过程中会把它所管理的内存划分为若干个不同的数据区域。这些区域都有各自的用途，以及创建和销毁时间，有的区域随着虚拟机进程的启动而存在，有些区域则依赖用户线程的启动和结束而建立和销毁。java 虚拟机所管理的内存将会包括以下几个运行时数据区域： 程序计数器程序计数器是一块较小的内存，它可以看成当前线程所执行的字节码的行号指示器。在虚拟机的概念模型里，字节码解释器工作时就是通过改变这个计数器的值来选取下一条需要执行的字节码指令，分支、循环、跳转、异常处理、线程恢复等基础功能都需要依赖这个计数器来完成。]]></content>
      <categories>
        <category>Java JVM</category>
      </categories>
      <tags>
        <tag>Java JVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[并发编程之ThreadLocal、Volatile、synchronized、Atomic关键字]]></title>
    <url>%2F2019%2F04%2F08%2Fjava%20%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB%2F%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E4%B9%8BThreadLocal%E3%80%81Volatile%E3%80%81synchronized%E3%80%81Atomic%E5%85%B3%E9%94%AE%E5%AD%97%E6%89%AB%E7%9B%B2%2F</url>
    <content type="text"><![CDATA[并发编程之ThreadLocal、Volatile、synchronized、Atomic关键字前言对于ThreadLocal、Volatile、synchronized、Atomic这四个关键字，我想一提及到大家肯定都想到的是解决在多线程并发环境下资源的共享问题，但是要细说每一个的特点、区别、应用场景、内部实现等，却可能模糊不清，说不出个所以然来，所以，本文就对这几个关键字做一些作用、特点、实现上的讲解。 Atomic作用对于原子操作类，Java的concurrent并发包中主要为我们提供了这么几个常用的：AtomicInteger、AtomicLong、AtomicBoolean、AtomicReference&lt;T&gt;。对于原子操作类，最大的特点是在多线程并发操作同一个资源的情况下，使用Lock-Free算法来替代锁，这样开销小、速度快，对于原子操作类是采用原子操作指令实现的，从而可以保证操作的原子性。什么是原子性？比如一个操作i++；实际上这是三个原子操作，先把i的值读取、然后修改(+1)、最后写入给i。所以使用Atomic原子类操作数，比如：i++；那么它会在这步操作都完成情况下才允许其它线程再对它进行操作，而这个实现则是通过Lock-Free+原子操作指令来确定的如：AtomicInteger类中： 12345678public final int incrementAndGet() &#123; for (;;) &#123; int current = get(); int next = current + 1; if (compareAndSet(current, next)) return next; &#125;&#125; 而关于Lock-Free算法，则是一种新的策略替代锁来保证资源在并发时的完整性的，Lock-Free的实现有三步： 1、循环（for(;;)、while）2、CAS（CompareAndSet）3、回退（return、break） 用法比如在多个线程操作一个count变量的情况下，则可以把count定义为AtomicInteger，如下： 1234567891011public class Counter &#123; private AtomicInteger count = new AtomicInteger(); public int getCount() &#123; return count.get(); &#125; public void increment() &#123; count.incrementAndGet(); &#125;&#125; 在每个线程中通过increment()来对count进行计数增加的操作，或者其它一些操作。这样每个线程访问到的将是安全、完整的count。 内部实现采用Lock-Free算法替代锁+原子操作指令实现并发情况下资源的安全、完整、一致性 Volatile作用Volatile可以看做是一个轻量级的synchronized，它可以在多线程并发的情况下保证变量的“可见性”，什么是可见性？就是在一个线程的工作内存中修改了该变量的值，该变量的值立即能回显到主内存中，从而保证所有的线程看到这个变量的值是一致的。所以在处理同步问题上它大显作用，而且它的开销比synchronized小、使用成本更低。 举个栗子：在写单例模式中，除了用静态内部类外，还有一种写法也非常受欢迎，就是Volatile+DCL： 1234567891011121314151617public class Singleton &#123; private static volatile Singleton instance; private Singleton() &#123; &#125; public static Singleton getInstance() &#123; if (instance == null) &#123; synchronized (Singleton.class) &#123; if (instance == null) &#123; instance = new Singleton(); &#125; &#125; &#125; return instance; &#125;&#125; 这样单例不管在哪个线程中创建的，所有线程都是共享这个单例的。 虽说这个Volatile关键字可以解决多线程环境下的同步问题，不过这也是相对的，因为它不具有操作的原子性，也就是它不适合在对该变量的写操作依赖于变量本身自己。举个最简单的栗子：在进行计数操作时count++，实际是count=count+1;，count最终的值依赖于它本身的值。所以使用volatile修饰的变量在进行这么一系列的操作的时候，就有并发的问题 举个栗子：因为它不具有操作的原子性，有可能1号线程在即将进行写操作时count值为4；而2号线程就恰好获取了写操作之前的值4，所以1号线程在完成它的写操作后count值就为5了，而在2号线程中count的值还为4，即使2号线程已经完成了写操作count还是为5，而我们期望的是count最终为6，所以这样就有并发的问题。而如果count换成这样：count=num+1；假设num是同步的，那么这样count就没有并发的问题的，只要最终的值不依赖自己本身。 用法因为volatile不具有操作的原子性，所以如果用volatile修饰的变量在进行依赖于它自身的操作时，就有并发问题，如：count，像下面这样写在并发环境中是达不到任何效果的： 12345678910public class Counter &#123; private volatile int count; public int getCount()&#123; return count; &#125; public void increment()&#123; count++; &#125;&#125; 而要想count能在并发环境中保持数据的一致性，则可以在increment()中加synchronized同步锁修饰，改进后的为： 12345678910public class Counter &#123; private volatile int count; public int getCount()&#123; return count; &#125; public synchronized void increment()&#123; count++; &#125;&#125; 内部实现汇编指令实现可以看这篇详细了解：Volatile实现原理 synchronized作用synchronized叫做同步锁，是Lock的一个简化版本，由于是简化版本，那么性能肯定是不如Lock的，不过它操作起来方便，只需要在一个方法或把需要同步的代码块包装在它内部，那么这段代码就是同步的了，所有线程对这块区域的代码访问必须先持有锁才能进入，否则则拦截在外面等待正在持有锁的线程处理完毕再获取锁进入，正因为它基于这种阻塞的策略，所以它的性能不太好，但是由于操作上的优势，只需要简单的声明一下即可，而且被它声明的代码块也是具有操作的原子性。 用法123456789public synchronized void increment()&#123; count++;&#125;public void increment()&#123; synchronized (Counte.class)&#123; count++; &#125;&#125; 内部实现重入锁ReentrantLock+一个Condition，所以说是Lock的简化版本，因为一个Lock往往可以对应多个Condition ThreadLocal作用关于ThreadLocal，这个类的出现并不是用来解决在多线程并发环境下资源的共享问题的，它和其它三个关键字不一样，其它三个关键字都是从线程外来保证变量的一致性，这样使得多个线程访问的变量具有一致性，可以更好的体现出资源的共享。 而ThreadLocal的设计，并不是解决资源共享的问题，而是用来提供线程内的局部变量，这样每个线程都自己管理自己的局部变量，别的线程操作的数据不会对我产生影响，互不影响，所以不存在解决资源共享这么一说，如果是解决资源共享，那么其它线程操作的结果必然我需要获取到，而ThreadLocal则是自己管理自己的，相当于封装在Thread内部了，供线程自己管理。 用法一般使用ThreadLocal，官方建议我们定义为private static ，至于为什么要定义成静态的，这和内存泄露有关，后面再讲。 它有三个暴露的方法，set、get、remove。 12345678910111213141516171819202122232425public class ThreadLocalDemo &#123; private static ThreadLocal&lt;String&gt; threadLocal = new ThreadLocal&lt;String&gt;()&#123; @Override protected String initialValue() &#123; return "hello"; &#125; &#125;; static class MyRunnable implements Runnable&#123; private int num; public MyRunnable(int num)&#123; this.num = num; &#125; @Override public void run() &#123; threadLocal.set(String.valueOf(num)); System.out.println("threadLocalValue:"+threadLocal.get()); &#125; &#125; public static void main(String[] args)&#123; new Thread(new MyRunnable(1)); new Thread(new MyRunnable(2)); new Thread(new MyRunnable(3)); &#125;&#125; 运行结果如下，这些ThreadLocal变量属于线程内部管理的，互不影响： threadLocalValue:2threadLocalValue:3threadLocalValue:4 对于get方法，在ThreadLocal没有set值得情况下，默认返回null，所有如果要有一个初始值我们可以重写initialValue()方法，在没有set值得情况下调用get则返回初始值。 值得注意的一点：ThreadLocal在线程使用完毕后，我们应该手动调用remove方法，移除它内部的值，这样可以防止内存泄露，当然还有设为static。 内部实现ThreadLocal内部有一个静态类ThreadLocalMap，使用到ThreadLocal的线程会与ThreadLocalMap绑定，维护着这个Map对象，而这个ThreadLocalMap的作用是映射当前ThreadLocal对应的值，它key为当前ThreadLocal的弱引用：WeakReference 内存泄露问题对于ThreadLocal，一直涉及到内存的泄露问题，即当该线程不需要再操作某个ThreadLocal内的值时，应该手动的remove掉，为什么呢？我们来看看ThreadLocal与Thread的联系图：此图来自网络： 其中虚线表示弱引用，从该图可以看出，一个Thread维持着一个ThreadLocalMap对象，而该Map对象的key又由提供该value的ThreadLocal对象弱引用提供，所以这就有这种情况： 如果ThreadLocal不设为static的，由于Thread的生命周期不可预知，这就导致了当系统gc时将会回收它，而ThreadLocal对象被回收了，此时它对应key必定为null，这就导致了该key对应得value拿不出来了，而value之前被Thread所引用，所以就存在key为null、value存在强引用导致这个Entry回收不了，从而导致内存泄露。 所以避免内存泄露的方法，是对于ThreadLocal要设为static静态的，除了这个，还必须在线程不使用它的值是手动remove掉该ThreadLocal的值，这样Entry就能够在系统gc的时候正常回收，而关于ThreadLocalMap的回收，会在当前Thread销毁之后进行回收。 总结 关于Volatile关键字具有可见性，但不具有操作的原子性，而synchronized比volatile对资源的消耗稍微大点，但可以保证变量操作的原子性，保证变量的一致性，最佳实践则是二者结合一起使用。 1、对于synchronized的出现，是解决多线程资源共享的问题，同步机制采用了“以时间换空间”的方式：访问串行化，对象共享化。同步机制是提供一份变量，让所有线程都可以访问。 2、对于Atomic的出现，是通过原子操作指令+Lock-Free完成，从而实现非阻塞式的并发问题。 3、对于Volatile，为多线程资源共享问题解决了部分需求，在非依赖自身的操作的情况下，对变量的改变将对任何线程可见。 4、对于ThreadLocal的出现，并不是解决多线程资源共享的问题，而是用来提供线程内的局部变量，省去参数传递这个不必要的麻烦，ThreadLocal采用了“以空间换时间”的方式：访问并行化，对象独享化。ThreadLocal是为每一个线程都提供了一份独有的变量，各个线程互不影响。]]></content>
      <categories>
        <category>Java 源码解读</category>
      </categories>
      <tags>
        <tag>Java 源码解读</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[第16章任务调度和异步执行器]]></title>
    <url>%2F2019%2F04%2F04%2FSpring%2F%E7%AC%AC16%E7%AB%A0%E4%BB%BB%E5%8A%A1%E8%B0%83%E5%BA%A6%E5%92%8C%E5%BC%82%E6%AD%A5%E6%89%A7%E8%A1%8C%E5%99%A8%2F</url>
    <content type="text"><![CDATA[第16章任务调度和异步执行器 任务调度概述Quartz快速进阶Quartz 基础结构 [ ] Job：开发者通过实现该接口来定义需要执行的的任务。Job 运行时的信息都保存在 JobDataMap 实例中。 [ ] JobDetail：描述 Job 的实现类及其他相关的静态信息，如 Job 名称、描述、关联监听器等信息。 [ ] Trigger：是一个类，描述触发 Job 执行的时间触发规则。 主要有 SimpleTrigger 和 CronTrigger 这两个子类。当仅需要触发一次或者以固定间隔周期性执行时，SimpleTrigger 是最合适的选择；而 CronTrigger 则可以通过表达式定义出各种复杂的调度方案，如每天早晨 9:00 执行，每周一，周三，周五下午 5:00 执行等。 [ ] Calendar：是一些日历特定时间点的集合 [ ] Scheduler：代表一个 Quartz 的独立运行容器。 [ ] ThreadPool：Scheduler 使用一个线程池作为任务运行的基础设施，任务通过共享线程池的线程来提高运行效率。 如下图，描述了 Scheduler 的内部组件结构。SchedulerContext 提供了 Scheduler 全局可见的上下文信息，每个任务都对应一个 JobDataMap ，虚线框中的 JobDataMap 表示有状态的任务。 一个 Scheduler 可以拥有多个 Trigger 和多个 JobDetail ，它们可以分到不同的组中。在注册 Trigger 和 JobDetail 时，如果不显示指定所属的组，那么 Scheduler 将放到默认的组中，默认的组名为 Scheduler.DEFAULT_GROUP。组名和名称组成了对象的全名，同一类型对象（Job 或 Trigger）的全名不能相同。 Scheduler 本身就是一个容器，它维护者 Quartz 的各种组件并实施调度的规则。Scheduler 还拥有一个线程池，线程池为任务提供执行线程。这比执行任务时简单的创建一个新的线程要拥有更高的效率，同时通过共享机制可以较少资源的占用。基于线程池组件的支持，对于繁忙度高、压力大的任务调度，Quartz 可以提供良好的伸缩性。 使用Simple TriggerSImpleTrigger 拥有多个重载的构造函数，用于在不同场合下构造对应的实例。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849public class SimpleTrigger extends Trigger &#123; ... /*指定 Trigger 所属组和名称*/ public SimpleTrigger(String name, String group) &#123; this(name, group, new Date(), (Date)null, 0, 0L); &#125; /*指定 Trigger 所属组、名称和触发时间*/ public SimpleTrigger(String name, Date startTime) &#123; this(name, (String)null, startTime); &#125; /*指定 Trigger 所属组、名称、开始时间、结束时间、重复次数、时间间隔*/ public SimpleTrigger(String name, String group, Date startTime, Date endTime, int repeatCount, long repeatInterval) &#123; super(name, group); this.startTime = null; this.endTime = null; this.nextFireTime = null; this.previousFireTime = null; this.repeatCount = 0; this.repeatInterval = 0L; this.timesTriggered = 0; this.complete = false; this.setStartTime(startTime); this.setEndTime(endTime); this.setRepeatCount(repeatCount); this.setRepeatInterval(repeatInterval); &#125; /*这是最复杂的一个构造函数，在指定触发参数的同时，通过 jobGroup 和 jobName ,使该 Trigger 和 Scheduler 中的某个任务关联起来*/ public SimpleTrigger(String name, String group, String jobName, String jobGroup, Date startTime, Date endTime, int repeatCount, long repeatInterval) &#123; super(name, group, jobName, jobGroup); this.startTime = null; this.endTime = null; this.nextFireTime = null; this.previousFireTime = null; this.repeatCount = 0; this.repeatInterval = 0L; this.timesTriggered = 0; this.complete = false; this.setStartTime(startTime); this.setEndTime(endTime); this.setRepeatCount(repeatCount); this.setRepeatInterval(repeatInterval); &#125; ...&#125; 通过实现 org.quartz.Job 接口，可以是 Java 类变成可调度的任务，如下内容： 123456public class SimpleJob implements Job &#123; @Override public void execute(JobExecutionContext jobCtx) throws JobExecutionException &#123; System.out.println(jobCtx.getTrigger().getName()+"triggered. time is:"+new Date()); &#125;&#125; 通过 SimpleTrigger 对 SimpleJob 进行调度 123456789101112131415161718192021222324public class SimpleTriggerRunner &#123; public static void main(String args[])&#123; try &#123; /*创建一个 JobDetail 实例，指定 SimpleJob*/ JobDetail jobDetail = new JobDetail("job1_1","jdroup1", SimpleJob.class); /*通过 SimpleTrigger 定义调度规则：马上启动，每2秒运行一次，共运行100次*/ SimpleTrigger simpleTrigger = new SimpleTrigger("Trigger1_1","tgroup1"); simpleTrigger.setStartTime(new Date()); simpleTrigger.setRepeatInterval(2000); simpleTrigger.setRepeatCount(100); /*通过 SchedulerFactory 获取一个调度器实例*/ SchedulerFactory schedulerFactory = new StdSchedulerFactory(); Scheduler scheduler = schedulerFactory.getScheduler(); /*注册并进行调度*/ scheduler.scheduleJob(jobDetail,simpleTrigger); scheduler.start(); /*调度启动*/ &#125; catch (SchedulerException e) &#123; e.printStackTrace(); &#125; &#125;&#125; 还可以通过 SimpleTrigger 的 setStartTime(Date startTime) 和 setEndTime(Date endTime) 的方法指定运行的时间范围。当运行次数和时间范围产生冲突时，超过时间范围的任务不被执行。如可以通过 simpleTrigger.setStartTime(new Date(System.CurrentTimeMillis()+60000L))方法指定 60 秒后开始运行。 执行结果： 使用 CronTriggerCronTrigger 能够提供比 SimpleTrigger 更有具体实际意义的调度方案，调度规则基于 Cron 表达式。CronTrigger 支持日历相关的周期时间间隔（比如每月第一个周一执行），而不是简单的周期时间间隔。 Cron 表达式Quartz 使用类似于 Linux 下的 Cron 表达式定义时间规则。Cron 表达式由 6 或 7 个空格的时间字段组成，如下表： Cron 表达式时间字段： 位置 时间域名 允许值 允许的特殊字符 1 秒 0-59 ,-*/ 2 分钟 0-59 ,-*/ 3 小时 0-23 ,-*/ 4 日期 1-31 ,-*?/L W C 5 月份 1-12 ,-*/ 6 星期 1-7 ,-*?/L C # 7 年（可选） 空值1970-2099 ,-*/ Cron 表达式的时间字段除允许设置数值外，该可以使用一些特殊的字符，提供列表、范围、通配符等功能，如下： 星号（*）：可用在所有字段中，表示对应时间域的某一时刻。 【例如：*在分钟字段时，表示“每分钟”。】 问好（?）：该字符只在日期和星期字段中使用，它通常指定为“毫无意义的值”，相当于占位符 减号（-）：表示一个范围。【例如：在小时字段中使用”10-12”，则表示从10点到12点，即10，11，12】 逗号（,）：表示一个列表值。【例如：在星期字段中使用”MON,WED,FRI”，则表示星期一、星期三和星期】 斜杠（/）：x/y 表达一个等长序列，x 为起始值，y为增量步长值。如在分钟字段中使用 0/15 ,则表示为 0,15,30 和 45 秒；而 5/15 在分钟字段中表示 5,20,35,50 。用户可以使用 */y，它等同于 0/y。 L：在日期和星期字段中使用，在日期中表示这个月的最后一天，在星期中使用表示这星期的周六。但是，如果L出现在星期字段里，而且前面有一个数字N，则表示“这个月的最后N天”。【例如：6L 表示该约的最后一个星期五】 W：该字符只出现在日期字段里，是对前导日期的修饰，表示该日期最近的工作日。W字符串只能指定单一日期，而不能指定日期范围。 LW组合：在日期字段中使用，当月的最后一个工作日。 井号（#）：该字符只能在星期字段中使用，表示当月的某个工作日。【例如：6#3表示当月的第3个星期五（6表示星期五，#3 表示当前的第三个），而 4#5 表示当月的第五个星期三。假如当月没有第五个星期三，则忽略不触发。】 C：只在日期和星期字段中使用，代表”Calendar”的意思。它是指计划所关联的日期，如果日期没有被关联到，则相当于日历中的所有日期。【例如：5C 在日期字段中相当于5日以后的那一天，1C在星期字段中相当于星期日后的第一天】 Cron 表达式对特殊字符的大小写不敏感，对代表星期的缩写也不敏感。 Cron 表示式示例： 表示式 说明 “0 0 12 ? “ 每天 12:00 运行 “0 15 10 ? “ 每天 10:15 运行 “0 15 10 ?” 每天 10:15 运行 “0 15 10 ? *” 每天 10:15 运行 “0 15 10 ? 2008” 2008 年的每天 10:15 运行 “0 14 * ?” 每天14点到15点每分钟运行一次。开始于14:00 ，结束于14:59 “0 0/15 14 ?” 每天14点到15点每5分钟运行一次，开始于 14:00 ，结束于14:59 “0 0/5 14,18 ?” 每天14点到15点每5分钟运行一次，此外每天 18点到19点每5分钟也运行一次 “0 0-15 14 ?” 每天14:00 到 14:05，每分钟运行一次 “0 10,44 14 ? 3 WED” 3月每周三的 14:10 到 14:44 ,每分钟运行一次 “0 15 10 ? * MON-FRI” 每周一、二、三、四、五的 10:15 运行 “0 15 10 15 * ?” 每月15 日的 10:15 运行 “0 15 10 L * ?” 每月最后一天星期五的 10:15 运行 “0 15 10 ? * 6L” 每月最后一个星期五的 10:15 运行 “0 15 10 ? 6L 2014-2016” 2014年、2015年、2016年每月最后一个星期五的 10:15 运行 “0 15 10 ? * 6#3” 每月第三个星期五的 10:15 运行 CronTrigger 实例12345678910111213141516171819202122232425public class CronTriggerRunner &#123; public static void main(String args[])&#123; try &#123; JobDetail jobDetail = new JobDetail("job_2","jGroup1", SimpleJob.class); /*创建 CronTrigger,指定组及名称*/ CronTrigger cronTrigger = new CronTrigger("trigger1_2","tgroup1"); /*定义 Cron 表达式*/ CronExpression cexp = new CronExpression("0/5 * * * * ?"); /*设置 Cron 表达式*/ cronTrigger.setCronExpression(cexp); /*②*/ SchedulerFactory schedulerFactory = new StdSchedulerFactory(); Scheduler scheduler = schedulerFactory.getScheduler(); scheduler.scheduleJob(jobDetail,cronTrigger); scheduler.start(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125;&#125; 运行 CronTriggerRunner，每5秒将触发 SimpleJob 运行一次。在默认情况下，Cron 表达式对应当前的时区，可以通过 CronTriggerRunner 的 setTimeZone(TimeZone timeZone) 方法显示指定时区。也可以指定开始时间和结束时间。 注意：在代码 ② 处需要通过 Thread.currentThread.sleep() 方法让主线程睡眠一段时间，使调度器可以继续执行任务调度的工作；否则在调度器启动后，因为主线程立即退出，寄生于主线程的调度器也将关闭，调度器的任务都将相应的销毁，这将导致看不到实际的运行效果。在单元测试的时候，使主线程休眠一段时间以便让任务线程不被提前终止是经常使用的测试方法。对于测试某些长周期执行的调度任务，开发者可以简单地调整操作系统时间进行模拟。 运行结果： 使用 Calender123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354import com.mrsw.adx.admin.service.impl.SimpleJob;import org.quartz.*;import org.quartz.impl.StdSchedulerFactory;import org.quartz.impl.calendar.AnnualCalendar;import java.util.ArrayList;import java.util.Calendar;import java.util.Date;import java.util.GregorianCalendar;public class CalendarExample &#123; public static void main(String[] args)&#123; try &#123; SchedulerFactory sf = new StdSchedulerFactory(); Scheduler scheduler = sf.getScheduler(); /*法定节日是以每年为周期的，所以使用 AnnualCalendar*/ AnnualCalendar holidays = new AnnualCalendar(); /*五一劳动节*/ Calendar laborDay = new GregorianCalendar(); laborDay.add(Calendar.MONTH,5); laborDay.add(Calendar.DATE,1); /*国庆节*/ Calendar nationalDay = new GregorianCalendar(); nationalDay.add(Calendar.MONTH,10); nationalDay.add(Calendar.DATE,1); /*排除这两个特殊日期*/ ArrayList&lt;Calendar&gt; calendars = new ArrayList&lt;Calendar&gt;(); calendars.add(laborDay); calendars.add(nationalDay); holidays.setDaysExcluded(calendars);/*①*/ /*向 Scheduler 注册日历*/ scheduler.addCalendar("holidays",holidays,false,false); /*4月1日上午10点*/ Date runDate = TriggerUtils.getDateOf(0,0,10,1,4); JobDetail job = new JobDetail("job1","group1", SimpleJob.class); SimpleTrigger trigger = new SimpleTrigger("trigger1","group1",runDate, null,SimpleTrigger.REPEAT_INDEFINITELY, 60L * 60L * 1000L); trigger.setCalendarName("holidays"); /*让 Trigger 应用指定的日历规则*/ scheduler.scheduleJob(job,trigger); scheduler.start(); // 在实际应用中主线程不能停止，否则 Scheduler 得不到执行，此处省略 &#125; catch (SchedulerException e) &#123; e.printStackTrace(); &#125; &#125;&#125; 注意：在向 Scheduler 注册日历的时候，addCalendar(String calName,Calendar calendar,boolean replace,boolean updateTrigger)。如果 updateTrigger 为 true,则 Scheduler 中引用 Calendar 的 Trigger 将得到更新，如①所示。 任务调度信息存储默认情况下，Quartz 将任务调度的运行信息（调度现场信息包括运行次数、调度规则和JobDataMap 中的数据等。）保存在内存中。这种方法提供了最佳的性能，因为在内存中数据访问速度最快；不足之处在于缺乏数据的持久性，当程序中途停止或系统崩溃时，所有运行的信息都会丢失。 持久化任务调度信息，可以通过调整 Quartz 的属性文件，将这些数据保存到数据库。 通过配置文件调整任务调度信息的保存策略在 Quartz JAR 文件的 org.quartz 包下就包含了一个 quartz.properties 属性配置文件，并提供了默认配置。如果需要调整默认配置，则可以直接在类路径下建立一个新的 quartz.properties 属性文件，它将被 Quartz加载并覆盖默认的配置。 Quartz 的属性文件配置主要包括以下三方面的信息： 集群信息 调度器线程池 任务调度现场数据的保存 注意：如果任务数目很大，则可以通过增大线程池获得更好的性能。 可以通过 以下设置将任务调度现场数据保存到数据库。 12345678910111213# 要将任务调度保存到数据库，必须使用 JobStoreTX 代替原来的 RAMJobStoreorg.quartz.jobStore.class = org.quartz.impl.jdbcjobstore.JobStoreTX# 数据库表前缀org.quartz.jobStore.tablePrefix = QRTZ_# 数据源名称org.quartz.jobStore.dataSource = qzDS# 定义数据源的具体属性org.quartz.dataSource.qzDS.driver = com.mysql.jdbc.Driverorg.quartz.dataSource.qzDS.URL = jdbc:mysql://localhost:3306/sampledborg.quartz.dataSource.qzDS.user = stamenorg.quartz.dataSource.qzDS.password = abdorg.quartz.dataSource.qzDS.maxConnections = 10 注意：必须事先在相应的数据库中创建 Quartz 的数据表（8张），在 Quartz 的完整发布的 dosc/dbTables 目录下拥有对应的不同的数据库脚本。 查询数据库的运行信息在Spring中使用QuartzSpring 为创建 Quartz 的 Scheduler 、Trigger 和 JobDetail 提供了便利的 FactoryBean 类，以便能够在 Spring 容器中享受注入的好处。 Spring 为 Quartz 提供了两个方面的支持： （1）为 Quartz 的重要组件提供更具 Bean 风格的扩展类 （2）提供创建 Scheduler 的 BeanFactory 类，方便在 Spring 环境下创建对应的组件对象，并结合 Spring 容器生命周期执行启动和停止的动作。 创建 JobDetailSpring 通过扩展 JobDetail 提供了一个更具 Bean 风格的 JobDetailFactoryBean。还提供了一个 MethodInvokingJobDetailBean，通过这个 FactoryBean 可以将 Spring 容器中 Bean 的方法包装成 Quartz 任务，这样开发者就不必为 Job 创建对应的类。 JobDetailFactoryBean扩展于 Quartz 的 JobDetail 。使用该 Bean 声明 JobDetail 时，bean 的名字即任务的名字，如果没有指定所属组，就使用默认组。除了 JobDetail 的属性外，还定义了以下属性： jobCalss：类型为 Class，实现 Job 接口的任务类 beanName：默认为 Bean 的 id 名，通过该属性显示指定 Bean 名称，它对应任务的名称。 jobDataAsMap：类型为 Map ，为任务所对应的 JobDataMap 提供值。 applicationContextJobDataKey：可以将 Spring ApplicationContext 的引用保存到 JobDataMap 中，以便在 Job 的代码中访问 ApplicationContext。需要指定一个健用于在 jobDataAsMap 中保存 ApplicationContext。 jobListenerName：类型为 String[] ，指定注册在 Scheduler 中的 JobDataMap 名称，以便让这些监听器对本任务的事件进行监听。 在下面的配置片段中使用 JobDetailBean 在 Spring 中配置一个 JobDetail 123456789&lt;bean name="jobDetail" class="org.springframework.scheduling.quartz.JobDetailBean" p:jobClass="com.smart.quartz.MyJob" p:applicationContextJobDataKey="applicationContext"&gt; &lt;property&gt; &lt;map&gt; &lt;entry key=“size” value="10"/&gt; &lt;/map&gt; &lt;/property&gt;&lt;/bean&gt; 说明：JobDetailFactoryBean 封装了 MyJob 任务，并为 Job 对应的 JobDataMap 设置了一个健为 size 的数据。此外，通过指定 applicationContextJobDataKey ，让 Job 的 JobDataMap 持有 Spring ApplicationContext 的引用。 这样，MyJob 在运行时就可以通过 JobDataMap 访问到 size 和 ApplicationContext。 1234567891011121314public class MyJob implements Job &#123; @Override public void execute(JobExecutionContext jctx) throws JobExecutionException &#123; /*获取 JobDetail 关联的 JobDataMap*/ Map dataMap = jctx.getJobDetail().getJobDataMap(); String size = (String) dataMap.get("size"); ApplicationContext ctx = (ApplicationContext)dataMap.get("applicationContext"); System.out.println("size："+size); /*① 对 JobDataMap 所做的更改是否被持久化取决于任务的类型*/ dataMap.put("size",size+"0"); &#125;&#125; 在代码 ① 处对 JobDataMap 进行修改。如果 MyJob 实现了 Job 接口，则这种更改对于下一次执行是不可见的；如果 MyJob 实现了 StatefulJob 接口，则这种更改对于下一次执行是可见的。 12345678910111213public class MyJob implements StatefulJob &#123; public void execute(JobExecutionContext jctx) throws JobExecutionException &#123;// Map dataMap = jctx.getJobDetail().getJobDataMap(); Map dataMap = jctx.getTrigger().getJobDataMap(); String size =(String)dataMap.get("size"); ApplicationContext ctx = (ApplicationContext)dataMap.get("applicationContext"); System.out.println("size:"+size); dataMap.put("size",size+"0"); String count =(String)dataMap.get("count"); System.out.println("count:"+count); &#125;&#125; MethodInvokingJobDetailFactryBean12345&lt;!-- 通过封装服务类方法实现 --&gt;&lt;bean id="jobDetail_1" class="org.springframework.scheduling.quartz.MethodInvokingJobDetailFactoryBean"p:targetObject-ref="myService" p:targetMethod="doJob"p:concurrent="false"/&gt;&lt;bean id="myService" class="com.smart.service.MyService" /&gt; jobDetail_1 将 MyService#doJob() 封装成一个任务，同时通过 concurrent 属性指定任务的类型。默认情况下为无状态的任务。如果希望封装为有状态的任务，仅需将 concurrent 属性设置为 false 就可以了。Sping 通过名为 concurrent 的属性指定任务类型，能够更直接的描述任务执行的方式（有状态的任务不能并发执行，无状态的任务可以并发执行），对于不熟悉 Quartz 内部机制的用户来说，比起 stateful ，concurrent 更简明达意。 123456public class MyService &#123; /*被封装成任务的目标方法*/ public void doJob()&#123; System.out.println("in MyService.dojob()."); &#125;&#125; 注意：通过 MethodInvokingJobDetailFactoryBean 产生的 JobDetail 不能被序列化，所以不能持久化到数据库。若希望使用持久化任务，则只能创建正规的 Quartz 的 Job 实现类 。 创建 TriggerSimpleTriggerFactoryBean在默认情况下，通过 SimpleTriggerFactoryBean 配置的 Trigger 名称即为 Bean 的名称，属于默认组。SimpleTriggerFactoryBean 在 SimpleTrigger 的基础上新增了以下属性。 jobDetail：对应的 JobDetail。 beanName：默认为 Bean 的 id 名，通过该属性显示指定 Bean 名称，它对应 Trigger 的名称。 jobDataAsMap：以 Map 类型为 Trigger 关联的 JobDataMap 提供值。 startDelay：延迟多少时间开始触发，单位为毫秒，默认值为0 triggerListenerNames：类型为 String[]，指定注册在 Scheduler 中的 TriggerListener 名称，以便让这些监听器对本触发器的时间进行监听。 123456789&lt;bean id="simpleTrigger" class="org.springframework.scheduling.quartz.SimpleTriggerBean" p:jobDetail-ref="jobDetail" p:startDelay="1000" p:repeatInterval="2000" p:repeatCount="100"&gt; &lt;property name="jobDataAsMap"&gt;&lt;!--①--&gt; &lt;map&gt; &lt;entry key="count" value="10" /&gt; &lt;/map&gt; &lt;/property&gt;&lt;/bean&gt; 代码①处配置的 Map 数据将填充到 Trigger 的 JobDataMap 中，执行任务时必须通过以下方式获取配置的值： 12345678public class MyJob implements StatefulJob &#123; public void execute(JobExecutionContext jctx) throws JobExecutionException &#123; Map dataMap = jctx.getTrigger().getJobDataMap(); /*对 JobDataMap 的更改不会被持久化，不影响下次的执行*/ String count =(String)dataMap.get("count"); System.out.println("count:"+count); &#125; &#125; CronTriggerFactoryBean扩展于 CronTrigger ,触发器的名称即为 Bean 的名称，保存在默认组中。在 CronTrigger 的基础上，新增的属性和 SimpleTriggerFactoryBean 大致相同，配置的方法也和 SimpleTriggerFactoryBean 相同。 1234&lt;bean id="checkImagesTrigger" class="org.springframework.scheduling.quartz.CronTriggerBean" p:jobDetail-ref="jobDetail" p:cronExpression="0/5 * * * * ?"/&gt; 创建SchedulerQuartz 的 SchedulerFactory 是标准的工厂类，不太合适在 Spring 环境下使用。此外，为了保证 Scheduler 能够感知 Spring 容器的生命周期，在 Spring 容器启动后，Scheduler 自动开始工作，而在 Spring 容器关闭之前，自动关闭 Scheduler 。Spring 提供了 SchedulerFactoryBean，这个 FactoryBean 大致拥有以下功能。 以更具 Bean 风格的方式为 FactoryBean 提供配置信息。 让 Scheduler 和 Spring 容器的生命周期建立关联，相生相息 通过属性配置的方式代替 Quartz 自身的配置文件 12345678910111213141516&lt;bean id="scheduler" class="org.springframework.scheduling.quartz.SchedulerFactory"&gt; &lt;!--注册多个 trigger--&gt; &lt;property name="triggers"&gt; &lt;list&gt; &lt;ref bean="simpleTrigger"/&gt; &lt;/list&gt; &lt;/property&gt; &lt;!--以Map类型设置 SchedulerContext 数据--&gt; &lt;property name="schedulerContextAsMap"&gt; &lt;map&gt; &lt;entry key="timeout" value="30"/&gt; &lt;/map&gt; &lt;/property&gt; &lt;!--显示指定 quartz 的配置文件地址--&gt; &lt;properties name="configLocation" value="classpath:com/smart/quartz/quartz.properties"/&gt;&lt;/bean&gt; SchedulerFactoryBean 还有以下常见的属性： calendars：类型为 Map，通过该属性向 Scheduler 注册 Calendar jobDetails：类型为 JobDetail[]，通过该属性向 Scheduler 注册 JobDetail autoStartup：SchedulerFactoryBean 在初始化后是否马上启动 Scheduler，默认为 true。若设置为 false，则需要手动启动 Scheduler startupDelay：在 SchedulerFactoryBean 在初始化完成后，延迟多少秒后启动 Scheduler，默认为0。除非拥有需要立即执行的任务，一般情况下，可以通过 startupDelay 属性让 Scheduler 延迟一小段时间后启动，以便让 Spring 能够更快初始化容器中剩余的 Bean SchedulerFactoryBean 的一个重要功能是允许用户将 Quartz 配置文件的信息转移到 Spring 配置文件中。SchedulerFactoryBean 通过以下属性代替框架的自身配置文件： dataSource：当需要持久化任务调度数据时，在 Quartz 中配置数据源，也可以直接在 Spring 中通过 dataSource 指定一个 Spring 管理的数据源。如果指定了该属性，即使 quartz.properties 中已经定义了数据源，也会被 dataSource 覆盖 transactionManager：可以通过该属性设置一个 Spring 事务管理器 nonTransactionalDataSource：在全局事务的情况下，如果不希望 Scheduler 执行的相关数据操作参与到全局事务中，则可以通过该属性指定数据源。在 Spring 本地事务的情况下，使用 dataSource 属性就足够了 quartzProperties：类型为 properties ，允许用户在 Spring 中定义 Quartz 的属性，其值将覆盖 quartz.properties 配置文件中的设置。 12345678910111213&lt;bean id="scheduler" class="org.springframework.scheduling.quartz.SchedulerFactoryBean"&gt; &lt;property name="quartzProperties"&gt; &lt;props&gt; &lt;!--属性值1--&gt; &lt;prop key="org.quartz.threadPool.class"&gt; org.quartz.simpl.SimpleThreadPool &lt;/prop&gt; &lt;!--属性值2--&gt; &lt;prop key="org.quartz.threadPool.threadCount"&gt;10&lt;/prop&gt; &lt;/props&gt; &lt;/property&gt;&lt;/bean&gt; 在Spring中使用JDK TimerTimer 和 TimerTaskTimerTask 代表一个需要多次执行的任务，它实现了 Runnable 接口，可以在 run() 方法定义任务逻辑。而 Timer 负责制定调度规则并调度 TimerTask。 TimerTask相当于 Quartz 的 Job ，代表一个被调度的任务。二者区别在于，每次执行任务时，Quartz 都创建一个 Job 实例，而 JDK Timer 则使用相同的 TimerTask 实例。 实现了 Runnable 接口，是一个抽象类，只有以下3个方法： abstract void run()：子类覆盖这个方法并定义任务执行逻辑，每次执行任务时，run() 方法就被调用一次。 boolean cancel()：取消任务。 long scheduledExecutionTime()：返回词此任务的计划执行时间。该方法一般在固定频率执行时使用才会有意义。 TimerTimer 的构造函数在创建 Timer 对象的同时将启动一个 Time 背景线程。构造函数如下： Timer()：创建一个Timer，背景线程是一个非守护线程 Timer(boolean isDaemon)：当 isDaemon 为 true，背景线程为守护线程，守护线程将在应用程序主线程停止后自动退出。 Timer(String name)：与 Timer() 类似，只是通过 name 指定守护线程名称。 通过以下方法执行任务： schedule(TimerTask task,Date time)：在特定的时间点执行一次任务。 schedule(TimerTask task,long delay)：延迟指定时间后执行一次任务，delay的单位为毫秒 通过以下方按固定时间间隔执行任务： schedule(TimerTask task,Date firstTime,long period)：从指定时间开始周期性地执行任务，period 为毫秒，后一次执行将在前一次执行完成后才开始计时。如任务被安排每 2 秒执行一次，假设第一次任务在 0 秒时间点开始执行并花费了 1.5 秒，则第二次将在第 3.5 秒时执行。 schedule(TimerTask task,long delay,long period)：在延迟指定时间后，周期性地执行任务 通过以下方法按照固定频率执行任务： scheduleAtFixedRate(TimerTask task,Date firstTime,long period)：在指定时间点后，以指定频率执行任务。 Java Timer 实例Spring 对 Java Timer 的支持Spring 对 Java 5.0 Executor 的支持了解 Java 5.0 的 ExecutorSpring 对 Executor 所提供的抽象实际应用中的调度对于那些运行规则固定的静态任务（如每隔30分钟更新缓存），可通过 Spring 配置文件定义调度规则并在 Spring 容器中启动运行调度。若任务的执行时间非常重要，不允许发生时间漂移，那么 Quartz 是最好的选择。 如何产生任务在业务流程中产生如果任务的执行时间点离业务的操作时间点不是很长，则可以使用。例如：电力传输管理系统的功能，将一条传输线路在某段时间内停止供电。用户在执行线路停电安排的业务时，立即向 Scheduler 中注册两个任务：某段时间执行断电和执行恢复供电的两个任务。 扫描线程产生有严格的执行时间点并减小数据库的影响，需要一个用于产生最近执行任务的扫描任务定期查询数据库，并为那些在一小段时间后就要执行的潜在任务进行动态安排。 说明：T0 对应一个定时的任务，它负责周期性地扫描业务表，查找在后续的扫描周期时间范围内要执行的任务，并创建这些任务。这中方式带来的好处如下： 降低对数据库的影响 缩短调度器中任务列队的长度（由于不是将所有潜在任务提前一段很长时间就进行安排，而仅是对一个扫描周期内的任务进行安排，所以调度器中任务列表的长度可以得到有效的控制） 保证任务在精确的时间点执行 任务调度对应程序集群的影响对于有集群要求的 Web 应用来说，如果应用系统本身有任务调度的功能，就必须在系统设计初期仔细分析任务调度功能是否适合集群。按任务执行结果影响的范围，可以将任务分为如下两类： 全局任务：指定那些执行结果会影响到应用系统全局的任务。例如：每天凌晨生成业务报表、定期调用短信接口发送短信、定期清理系统过期数据，它们的执行结果都会给系统带来“全局可见”的结果。所以，在传统的集群系统中，全局任务最好在一个独立部署的服务节点执行，否则可能会因重复多次执行而引发系统逻辑的错误。 本地任务：指执行结果的影响范围仅限于本地，不会造成全局影响的任务。例如：定期刷新本地缓存、定期清除本地节点临时文件，它们的执行结果只对本地服务节点有影响，需要在每个本地服务节点部署任务。 Quartz 可支持集群部署，其原理很简单，即让多个调度节点虎威热备，在同一时刻只有一个节点是激活的，任务只有在激活的节点中执行，其他节点都是“休眠”状态；当激活的调度节点崩溃时，则唤醒某一个“休眠”的调度节点，以接管任务调度的工作。 Quartz 可通过两种方式实现集群：1.通过一个中间数据库，使集群节点相互感知，以实现故障切换；2.通过 Terracotta。 任务调度云 Web应用程序中调度器的启动和关闭问题我们知道，静态变量是 ClassLoad 级别的，如果 Web 应用程序停止，那么这些静态变量也会从 JVM 中清除。但线程是 JVM 级别的，如果用户在 Web 应用中启动了一个线程，那么线程的生命周期并不会和 Web 应用程序保持同步。也就是说，即使停止了 Web 应用，这个线程依旧是活动的。 问题： 如果手工使用 JDK Timer （Quartz 的 Scheduler），在 Web 容器启动非守护线程的 Timer ，当 Web 容器关闭时，除非用户手动关闭这个 Timer ,否则 Timer 中的任务还会继续。 解决方法： Spring 为 JDK Timer 和 Quartz Scheduler 所提供的 TimerFactoryBean 和 SchedulerFactoryBean 能够与 Spring 容器的生命周期关联，在 Spring 容器启动时启动调度器，而在 Spring 容器关闭时停止调度器。所以在 Spring 中通过配置两个 FactoryBean 配置调度器，再从 Spring IOC 中获取调度器的引用进行任务调度，这样就不会出现这种 Web 容器关闭而任务依然执行的问题。 小结Quartz 提供了极为丰富的任务调度功能，不但可以制定周期性执行的任务调度方案，还可以让用户按照日历相关的方式进行任务调度。 Quartz 框架的重要组件包括 Job、JobDetail、Trigger、Scheduler 及辅助性的 JobDataMap 和 SchedulerContext。 Quartz 拥有一个线程池，通过线程池为任务提供执行线程，可以通过配置配置文件对线程池进行参数定制。 Quartz 还有一个重要功能，将任务调度信息持久化到数据库中，以便系统重启时能够恢复已经安排的任务。 Quartz 还拥有完善的事件体系，允许用户注册各种事件的监听器。 Spring 为 Quartz 的 JobDetail 和 Trigger 提供了更具 Bean 风格的支持类，使得用户能够方便地在 Spring 中通过配置定制这些组件的实例。 Spring 的 SchedulerFactoryBean 让用户可以脱离 Quartz 自身的体系，而以更具 Spring 风格的方式定义 Scheduler。Scheduler 生命周期和 Spring 容器生命周期绑定。 JDK Timer 可以满足一些简单的任务调度需求，好处就是用户不必引用 JDK 之外的第三方类库；只能支持小型的任务且任务很快就能完成。 JDK Timer 只能做到近似时间安排。]]></content>
      <categories>
        <category>Spring</category>
      </categories>
      <tags>
        <tag>Spring</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[第3章数据结构]]></title>
    <url>%2F2019%2F03%2F30%2FSoftTestTechnique%2F%E7%AC%AC3%E7%AB%A0%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%2F</url>
    <content type="text"><![CDATA[第3章数据结构 线性结构线性表栈和队列串数组、矩阵和广义表数组矩阵广义表树树与二叉树的定义二叉树的性质与存储结构二叉树的遍历线索二叉树最优二叉树树和森林图图的定义与储存图的遍历生成树与最小生成树拓扑排序和关键路径最短路径查找查找的基本概念静态查找表的查找方法动态查找表哈希表排序 十种常见排序算法可以分为两大类： 比较类排序：通过比较来决定元素间的相对次序，由于其时间复杂度不能突破O(nlogn)，因此也称为非线性时间比较类排序。 非比较类排序：不通过比较来决定元素间的相对次序，它可以突破基于比较排序的时间下界，以线性时间运行，因此也称为线性时间非比较类排序。 排序的基本概念假设含 $n$ 个记录文件内容为{$R_{1}, R_{2},\ldots ,R_{n}$}，相应的关键字{$k_{1}, k_{2},\ldots ,k_{n}$}。经过排序确定一种排列{$R_{j_{1}},R_{j_{2}},\ldots ,R_{j_{n}}$}，使得它们的关键字满足以下递增（或递减）关系：$k_{j_1}\leq k_{j_{2}}\leq \ldots \leq k_{j_{n}}$（或$k_{j_{1}}\geq k_{j_{2}}\geq \ldots k_{jn}$）。 若在待排序中的一个序列中，$R_{i}$和$R_{j}$的关键字相同，即 $k_{i}$=$k_{j}$，且在排序前 $R_{i}$先于$R_{j}$，那么在排序后，如果$R_{i}$和$R_{j}$的相对次序保持不变，$R_{i}$仍领先于$R_{j}$，则此类排序算法为稳定的。若在排序后的序列中有可能出现$R_{j}$领先于$R_{i}$的情形，则此类排序为不稳定的。 内部排序：指待排序记录全部存放在内存中进行排序的过程。 外部排序：指待排序记录的数量很大，以至于内存不能容纳全部记录，在排序过程中尚需对外存进行访问的排序过程。 简单排序直接插入排序又称简单插入排序，是一种简单的排序算法。 具体做法：在插入第$ i $个记录时，$R_{1}$、$R_{2}$、….、$R_{i-1}$ 已经安排好序，这时将 $R_{i}$的关键字 $k_{i}$依次与 $K_{i-1}$、$K_{i-2}$等进行比较，从而找到应该插入的位置并将$R_{i}$插入，插入位置及其后的记录向后移动。 实现程序 1234567891011121314public int[] insertionSort(int[] arr)&#123; int len = arr.length; int preIndex,current; for(int i = 1;i&lt;len;i++)&#123; preIndex = i - 1; current = arr[i]; while(preIndex&gt;=0 &amp;&amp; current&lt;arr[preIndex])&#123; arr[preIndex+1] = arr[preIndex]; preIndex--; &#125; arr[preIndex+1] = current; &#125; return arr;&#125; 直接排序算法在最好的情况下（待排序列已按关键码有序），每趟只需操作 1 次比较且不需要移动元素，因此$n$个元素排序时的总比较次数为$ n-1$ 次，总移动次数为0。在最坏情况下（元素已经逆序排序），进行第 $i$ 趟排序时，待插入的记录需要同前面的 $i$ 个记录进行比较，因此，总比较次数为 $\dfrac {n\left( n-1\right) }{2}$在排序过程中，第 $i$ 趟排序时移动记录的次数为 $i+1$ （包括移进、移除tmp）,总移动次数为 $\dfrac {\left( n+3\right) \left( n+2\right) }{2}$ 注意：是一种稳定的排序算法;时间复杂度$O\left( n^{2}\right)$;在排序过程中仅需要一个元素的辅助空间用于交换，空间复杂度为$O\left( 1\right)$ 冒泡排序一种简单的排序算法。它重复地走访过要排序的数列，一次比较两个元素，如果它们的顺序错误就把它们交换过来。走访数列的工作是重复地进行直到没有再需要交换，也就是说该数列已经排序完成。这个算法的名字由来是因为越小的元素会经由交换慢慢“浮”到数列的顶端。 具体的做法： 实现程序： 12345678910111213public int[] bubbleSort(int[] arr)&#123; int n = arr.length; for(int i = 0;i &lt; n - 1;i++)&#123; for(int j = 0 ; j &lt; n - 1 - i;j++)&#123; if(arr[j]&gt;arr[j+1])&#123; int temp = arr[j+1]; arr[j+1] = arr[j]; arr[j] = temp; &#125; &#125; &#125; return arr;&#125; 冒泡排序在最好情况下（待排序列已按关键码有序），只需要做一趟，元素的比较次数为 $n-1$ 且不需要交换元素。在最坏情况下（元素已经逆序排序），在进行第 $j$ 趟排序时，最大的 $j-1$ 个元素已经排好序，其余的 $n-(j-1)$ 个元素需要进行 $n-j$ 次比较和 $n-j$ 次交换。因此总比较次数为：$\dfrac {n\left( n-1\right) }{2}$ ，总的交换次数为：$\dfrac {n\left( n-1\right) }{2}$ 注意：稳定的排序方法；时间复杂度$O\left( n^{2}\right)$;在排序过程中仅需要一个元素的辅助空间用于交换，空间复杂度为$O\left( 1\right)$ 简单选择排序是一种简单直观的排序算法。工作原理：首先在末尾排序序列中找到最小（大）元素，存放到排序序列的其实位置，然后，再从剩下元素中继续寻找最小（最大）元素，然后放到已排序序列的末尾。以此类推，直到所有元素均排序完毕。 具体做法： 程序实现： 12345678910111213141516public int[] selectionSort(int[] arr)&#123; int n = arr.length; int minIndex,temp; for(int i = 0;i &lt; n;i++)&#123; minIndex = i; for(int j = i+1;j&lt;n;j++)&#123; if(arr[j]&lt;arr[minIndex])&#123; minIndex = j; &#125; &#125; temp = arr[i]; arr[i] = arr[minIndex]; arr[minIndex] = temp; &#125; return arr;&#125; 简单选择排序算法在最好情况下（待排序列按关键码有序），不需要移动元素，因此 n 个元素排序时的总移动次数为 0 次。在坏情况下（元素已经逆序排序），前$\dfrac {n}{2}$趟中，每趟排序移动记录的次数都为3次（两个数组元素加交换值），其后不再移动元素，共进行 n-1 趟排序，总移动次数为$3\left( n-1\right) /2$。无论哪种情况，元素总比较次数为$\dfrac {n\left( n-1\right) }{2}$。 注意：不稳定的排序方法；时间复杂度为：$O\left( n^{2}\right)$；在排序过程中仅需要一个元素的辅助空间用于数组元素的交换，空间复杂度为$O\left( 1\right)$ 希尔排序是第一个突破$O\left( n^{2}\right)$的排序算法，是简单插入排序的改进版。与插入排序的不同之处在于，它会优先比较距离较远的元素，通过逐步减少间距，最终以1为间距或者进行一次常规的插入排序。希尔排序又叫缩小增量排序。 具体做法：先取一个小于 n 的整数 d1 作为第一个增量，把文件的全部记录分成 d1 个组，即将所有距离为 d1 倍数序号的记录放在同一个组中，在和组内进行直接插入排序；然后取第二个增量 d2（d2&lt;d1），重复上述分组和排序工作，依次类推，直到所有的增量 d = 1，即所有的记录放在同一组进行直接插入排序为止。 程序实现： 123456789101112131415public int[] shellSort(int[] arr)&#123; int n = arr.length; for(int gap = (int)Math.floor(n/2);gap &gt; 0;gap = (int)Math.floor(gap/2))&#123; for(int i = gap;i &lt; n;i++)&#123; int j = i; int current = arr[i]; while(j - gap &gt;= 0 &amp;&amp; current &lt; arr[j - gap])&#123; arr[j] = arr[j - gap]; j = j - gap; &#125; arr[j] = current; &#125; &#125; return arr;&#125; 注意：是一种不稳定的排序方法；时间复杂度为：$O\left( n^{1.3}\right)$；空间复杂度数为：$O\left( 1\right)$ 快速排序基本思想：通过一趟排序将待排序的记录划分为独立的两部分，称为前半区和后半区，其中，前半区中记录的关键字均不大于后半区记录的关键字，然后再分别对这两部分记录继续进行快速排序，从而使整个序列有序。 一趟快速排的过程称为一次划分，具体的做法是：设两个位置指示变量 i 和 j，它们的初始值分别指向序列的第一个记录和最后一个记录。设枢轴记录（通常是第一个记录）的关键字为 pivot,则首先从 j 所指定的位置向前搜索，找到第一个关键字小于 pivot 的记录时向前移到 i 的位置，然后从 i 所指位置向后搜索，找到第一个关键字大于 pivot 的记录时将改记录向后移到 j 所指位置，重复该过程直至 i 与 j 相等位置。 具体做法： 请看 https://swenfang.github.io/2019/03/30/SoftTestTechnique/快速排序原理和实现/#more 这篇文章。 程序实现： 123456789101112131415161718192021222324252627282930313233343536public class QuickSort&#123; public static void main(String[] args) &#123; int[] arr = &#123;6,1,2,7,9,3,4,5,10,8&#125;; quickSort(arr,0,arr.length-1); System.out.println(Arrays.toString(arr)); &#125; // 划分 private static int partition(int arr[],int low,int high)&#123; // 用 arr[low] 作为枢纽轴元素 pivot 进行划分 // 使得 arr[low,...,i-1] 均不大于 pivot，data[i+1,...,high]均小于 pivot int i,j; int pivot; pivot = arr[low]; i = low;j = high; while(i &lt; j)&#123; /*从数组的两端交替地向中间扫描*/ while(i &lt; j &amp;&amp; arr[j] &gt;= pivot)j--; arr[i] = arr[j];/*比枢轴元素小者往前移动*/ while(i &lt; j &amp;&amp; arr[i] &lt;= pivot)i++; arr[j] = arr[i];/*比枢轴元素大者往后移动*/ &#125; arr[i] = pivot; return i; &#125; // 对整型数组进行非递减排序 private static int[] quickSort(int arr[],int low,int high)&#123; if(low&lt;high)&#123; int loc = partition(arr,low,high);/*进行划分*/ quickSort(arr,low,loc-1);/*对前半区进行快速排序*/ quickSort(arr,loc+1,high);/*对后半区进行快速排序*/ &#125; return arr; &#125; &#125; 注意：快速排序算法的时间复杂度为 $O\left( n\log _{2}n\right)$，在所有算法复杂度为此数量级的排序方法中，快速排序被认为是平均性能最好的一种。但是，若初始记录序列按关键字有序或基本有序时，即每次划分都是将序列分为某一半序列的长度为0的情况，此时快速排序的性能退化为时间复杂度是$O\left( n^{2}\right)$。快速排序是一种不稳定的排序算法。 堆排序归并排序归并排序是建立在归并操作上的一种有效的排序算法。该算法是采用分治法（Divide and Conquer）的一个非常典型的应用。将已有序的子序列合并，得到完全有序的序列；即先使每个子序有序，再使子序列段间有序。若将两个有序表合并成一个有序表，称为2-路归并。 具体做法： 程序实现： 递归法（Top-down） 申请空间，使其大小为两个已经排序序列之和，该空间用来存放合并后的序列 设定两个指针，最初位置分别为两个已经排序序列的起始位置 比较两个指针所指向的元素，选择相对小的元素放入到合并空间，并移动指针到下一位置 重复步骤3直到某一指针到达序列尾 将另一个序列剩下的所有元素直接复制到合并序列尾 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748public class MergeSort&#123; public static void main(String[] args)&#123; int[] arr = &#123;6,1,2,7,9,3,4,5,10,8&#125;; sort(arr,0,arr.length-1); System.out.println(Arrays.toString(arr)); &#125; // 使用递归 public static void Sort(int[] arr,int leftStart,int rightEnd)&#123; // 满足基本条件 if(leftStart&gt;=rightEnd)&#123; return; &#125; // 算出左边数组结束位置 int leftEnd = (leftStart+rightEnd) &gt;&gt; 1; // 算出右边数组开始位置 int rightStart = leftEnd+1; /*递归地对arr[leftStart...leftEnd]进行归并排序*/ Sort(arr,leftStart,leftEnd); /*递归地对arr[rightStart...rightEnd]进行归并排序*/ Sort(arr,rightStart,rightEnd); /*对一维数组arr[leftStart...rightEnd]中的元素进行两路归并牌排序*/ Merge(arr,leftStart,leftEnd,rightStart,rightEnd); &#125; // 两两归并 public static Merge(int[] arr,int leftStart,int leftEnd,int rightStart,int rightEnd)&#123; // 定义一个临时数组 int[] tempArr = new int[arr.length]; int tempIndex = leftStart; int resultIndex = leftStart; // 进行比较 while(leftStart &lt;= leftEnd &amp;&amp; rightStart &lt;= rightEnd)&#123; tempArr[tempIndex++] = arr[leftStart] &lt;= arr[rightStart] ? arr[leftStart++]:arr[rightStart++]; &#125; // 把最后一个比较的元素存放到临时素组中 while(leftStart&lt;=leftEnd)&#123; tempArr[tempIndex++] = arr[leftStart++]; &#125; while(rightStart &lt;= rightEnd)&#123; tempArr[tempIndex++] = arr[rightStart++]; &#125; // 把临时数组元素顺序复制到原数组 while(resultIndex &lt;= rightEnd)&#123; arr[resultIndex] = tempArr[resultIndex++] &#125; &#125;&#125; 迭代法（Bottom-up） 假设序列共有 n 个元素 将序列每相邻两个数字进行归并操作，形成 floor(n/2)个序列，排序后每个序列包含两/一个元素 若此时序列数不是1个则将上述序列再次归并，形成 floor(n/4)，每个序列包含四/三个元素 重复步骤2，直到所有元素排序完毕，即序列数为1 12 适用：多链表排序。 基数排序内部排序方法小结外部排序]]></content>
      <categories>
        <category>软件技术</category>
      </categories>
      <tags>
        <tag>软件技术</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[第2章程序设计语言基础知识]]></title>
    <url>%2F2019%2F03%2F30%2FSoftTestTechnique%2F%E7%AC%AC2%E7%AB%A0%E7%A8%8B%E5%BA%8F%E8%AE%BE%E8%AE%A1%E8%AF%AD%E8%A8%80%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%2F</url>
    <content type="text"><![CDATA[第2章程序设计语言基础知识程序设计语言概述程序设计语言的基本概念低级语言和高级语言机器语言和汇编语言为高级语言。在此基础上，人们开发了功能更强、抽象级别更高的语言以支持程序设计，于是产生了面向各类应用程序的程序设计语言，称为高级语言。ge:Java、C、C++、PHP、Python等。 编译程序和解释程序语言之间的翻译方式有：汇编、解释和编译。 解释程序也称为解释器，它或者直接解释执行源程序，或者将源程序翻译成某种中间代码后再加以执行。 编译程序（编译器）则是将源程序翻译成目标语言程序，然后再计算机上运行目标程序。 根本区别:编译方式下，编译器则将源程序翻译成独立保存的目标程序。在解释程序下，翻译源程序时不生成独立的目标程序。 程序设计语言的定义程序设计语言的定义都涉及语法、语义和语用。 语法是指由程序设计语言的基本符号组成程序中的各个语法成分（包含程序）的一组规则，其中由基本字符构成的符号（单词）书写规则称为词法规则，由符号构成语法成分的规则称为语法规则。程序设计语言的语法可用形式语言进行描述。 语义是程序设计语言中按语法规则构成的各个语法成分的含义，可分为静态语义和动态语义。静态语义指编译时可以确定的语法成分的含义，而运行时才能确定的含义是动态语义。一个程序的执行效果说明了该程序的语义，它取决于构成程序的各个组成部分和语义。 语用表示了构成语言的各个记号和使用者的关系，涉及符号的来源、使用和影响。 语言的实现则有语境问题。语境是指理解和实现程序设计语言的环境，包括编译环境和运行环境。 程序设计语言分类程序设计语言发展概述 语言名称 概述 Fortran 第一个被广泛用来进行科学和工程计算的高级语言。 ALGOL 为软件自动及软件可靠性的发展墓定了基础 PASCAL 是一种过程式、结构化程序设计语言 C 语言 20世纪70年代初发展起来的一种通用程序设计语言 C++ 基于C语言发展起来的，比C多了封装和抽象，增加了类机制是C++成为面向对象程序设计语言 C# 由 Microsoft 公司开发的一种面向对象语言，较于C++它在许多方面进行了限制和增强 Objective-C 根据C衍生出来的语言，与 C# 类似，它仅支持单一父类继承，不支持多重继承 Java 产生于20世纪70年代，保留了 C++ 的基本语法、类和继承等概念，与 C++ 相比，其语法和语义更合理 Ruby 约1993年设计的一种解释性、面向对象、动态类型的脚本语言。 PHP 是一种在服务器端执行的、嵌入 HTML 文档的脚本语言，语言风格类似于 C 语言，由网站编程人员广泛运用。它可以快速的执行动态网页，其语法混了C、Java、Perl以及 PHP 自创的语法。由于在服务器端执行，PHP 能充分利用服务器的性能。PHP 支持几乎所有流行的数据库以及操作系统。 Python 是一种面向对象的解释型程序设计语言，可用于编写独立程序、快速脚本和复杂应用的原型。Python 也是一种脚本语言，它支持多操作系统的底层访问，也可以将 Python 源程序翻译成字节码在 Python 虚拟机上运行。虽然 Python 的内核很小，但它提供了丰富的基本构建块，还可以用 C、C++ 和 Java 等进行扩展，因此可以用它开发任何类型的程序。 Java Script 是一种脚本语言，被广泛用于 Web 应用开发。通常，将 Java Script 脚本嵌入到 HTML 中来实现自身的功能 Delphi 是一种可视化开发工具，主要特性是基于窗体和面向对象的方法、高速的编译器、强大的数据库支持、与 Windows 编程紧密结合以及成熟的组件技术。 Visual Basse.NET y用 .NET 语言开发的程序源代码被编译为中间代码 MSIL 然后通过 .NET Framework 的通用语言运行时（CLR）来执行。 程序设计语言分类根据设计程序的方法将程序语言大致分为命令式和结构化程序设计语言、面向对象的程序设计语言、函数式程序设计语言和逻辑型程序程序设计语言等。 1、命令式和结构化程序设计语言 通常所以称的结构化程序语言属于命令式语言类，其结构特性主要反映在以下几个方面： 用自顶向下逐步精化的方法编程 按模块组织的方法编程 程序只包含顺序、判定（分支）及循环构造 C、PASCAL 等都是典型的结构化程序设计语言。 2、面向对象的程序设计语言 C++、Java 和 Smalltalk 是面向对象程序设计语言的代表，它们都必须支持新的程序设计技术，如数据隐式、数据抽象、用户定义类型、继承和多态 3、函数式程序设计语言 优点是对表达式中出现的任何函数都可以用其他函数来代替，只要这些函数调用产生相同的值。 函数式语言的代表 LISP 在许多方面与其他语言不同，最为显著的是，其程序和数据的形式是等价的，这样的数据结构就可以作为程序执行，程序也可以作为数据修改。常见的函数式语言有 Hashell、Scala、Scheme、APL 等。 4、逻辑型程序设计语言 是一类以形式逻辑为基础的语言。 程序设计语言的基本成分程序设计语言的数据成分1）常量和变量 按照程序运行时数据的值能否改变，将数据分为常量和变量。程序中的数据对象可以具有左值和（或）右值，左值指存储单元（或地址、容量）。变量具有左值和右值，在程序运行过程中其右值可以改变；常量只有右值，在程序运行过程中其右值不能改变。 2）全局量和局部量 数据按在程序代码中的作用范围（作用域）可以分为全局量和局部量。一般情况下，全局变量的作用域为整个文件或程序，系统为全局变量分配的存储空间在程序运行的过程中是不改变的，局部变量的作用域为定义它的函数或语句块，为局部变量分配的存储单元是动态改变的。 3）数据类型 按照数据组织形式的不同可将数据分为基本类型、用户定义类型、构造类型及其他类型。C（C++）的数据类型如下： 基本数据类型：整型（int）、字符型（char）、实型（float、double）和布尔类型（bool） 特殊类型：空类型（void） 用户定义类型：枚举类型（enum） 构造类型：数组、结构、联合 指针类型：type* 抽象数据类型：类类型 程序设计语言的运算成分程序设计语言的控制部分顺序结构 选择结构选择结构提供了在两种或多种分支中选择其中一个的逻辑。基本的选择结构是指定一个条件P，然后根据条件的成立与否决定控制流计算A和B，从两个分支中选择一个执行（如图a）。选择结构中的计算A或B还包含顺序、选择和重复结构。程序设计语言中还通常提供简化了的选择结构，也就没有计算 B 的分支结构，（如图b）。 循环结构主要有两种形式：while 型 和 do-while 型循环结构。 C（C++）语言提供的控制语句 复合语句。是一系列用”{“和”}”括起来的声明和语句，主要作用是将多条语句组成一个可执行单元。复合语句是一个整体，要么全部执行，要么一句也不执行。 if 语句和swith语句 循环语句 程序设计语言的传输成分指明语言允许的数据传输方式，如赋值处理、数据的输入和输出等。 函数函数是程序块的主要成分，它是一段具有独立功能的程序。 语言处理程序基础语言处理程序是一类系统软件的总称，主要作用是将高级语言或汇编语言编写的程序翻译成某种机器语言程序，使程序可在计算机上运行。语言处理程序主要分为汇编程序、编译程序和解释程序3中基本类型。 汇编程序基本原理汇编语言是为特定的计算机设计的面向机器的符号化的程序设计语言。 汇编语言源程序由若干语句组成，其中可以有三类语句：指令语句、伪指令语句和宏指令语句。 指令语句又称机器指令语句，其汇编后能产生相应的机器代码，这些代码能被 CPU 直接识别并执行相应的操作。基本的指令有 ADD、SUB 和 AND 等，书写指令语句时必须遵循指令的格式要求。 指令语句可分为传送指令、算术运算指令、逻辑运算指令、移位指令、转移指令和处理机控制指令等类型。 伪指令语句指汇编程序在汇编源代码时完成某些工作，例如为变量分配存储单元地址，给某个符号赋一个值等。 伪指令语句和指令语句的区别是：伪指令语句经汇编后不产生机器代码，而指令语句经汇编后要产生相应的机器代码。伪指令语句所指示的操作是在源程序被编译时完成的，而指令语句的操作必须在程序运行时完成。 宏指令语句在汇编语言中，还允许用户将多次重复使用的程序段定义为宏。每个宏都有相应的宏名。在程序的任意位置，若需要使用这段程序，只要使用该宏名就使用了这段程序。因此，宏指令语句就是宏的引用。 汇编程序功能是将用汇编语言编写的源程序翻译成机器指令程序。 汇编程序一般需要两次扫描源程序才能完成翻译过程。 第一次扫描的主要工作是：定义符号的值并创建一个符号表ST，记录汇编时所遇到的符号的值。还有一个固定的表MOT1，记录每条机器指令的记忆码和指令的长度。为了计算各汇编语句标号的地址，需要设立一个位置计数器或单元地址计数器LC，初始值一般为0。 汇编程序第一次扫描的过程： 单元计数器初始值为0 打开源程序文件 从源程序中读入第一条语句 while（若当前语句不是END语句）{ if（语句有标号）将标号和单元计数器LC的当前值填入符号表ST； if（语句有可执行的汇编指令语句）查找 MOT1 表获取当前指令的长度K，并令 LC=LC+K； if（指令是伪指令）查找 POT1 表并调用相应的子程序； if（指令的操作码是非法记忆码）调用出错处理子程序。 从源程序中读取下一条语句； } 关闭源程序文件 汇编程序第二次扫描的任务是产生目标程序。 编译程序基本原理编译过程概述 1）词法分析对源程序从前到后（从左到右）逐字符的扫描，从中识别出一个个”单词”符号。（单词：关键字、标识符、常数、运算符和分割符） 2）语法分析确定整个输入串是否构成一个语法上的正确程序 3）语义分析分析各词法结构的含义。 4）中间代码生成根据语义分析的输出生成中间代码。 5）代码优化因为中间代码的生成是机械的、按固定模式进行的，所以在时间和空间上有较大的浪费。要生成高效的代码就得进行优化。优化一般建立在对程序的控制流和数据流分析的基础之上，与具体的机器无关。可在中间代码生成或目标代码生成阶段进行。 6）目标代码生成把中间代码变换成特定机器上的绝对指令代码、可重定位的指令代码或汇编指令代码。此阶段与机器密切相关。 7）符号表管理符号表的作用是记录源程序中各个符号的必要信息。符号表的建立可以在词法分析、语法分析、语义分析阶段 8）出错处理在逻辑上分为前端和后端。前端（包括从词法分析到中间代码生成），后端（中间代码优化和目标代码优化） 以中间代码为分水岭，把编译器分为与机器有关的部分和与机器无关的部分。 文法和语言的形式描述1）字母表、字符串、字符串集合与运算 2）文法和语言的形式描述 定义：描述语言语法结构的规则 分类：0型、1型、2型、3型（差别在于对产生式要施加不同的限制）；0型又称短语文法，递归可枚举的、1型又称上下文有关文法、2型又称上下文无关文法、3型文法等价正规式 3）句子和语言 推导与直接推导 直接规约和规约 句型和句子 语言 4）文法的等价 文法 G1 和 G2 产生的语言相同 语法分析词法规则可用3型文法（正规文法）或正规表达式描述，它产生的集合是语言基本字符集 $\Sigma​$ （字符表）上的字符串的一个子集，称为正规集。 1）正规表达式和正规集 $\varepsilon​$是一个正规式，它表示集合 $L\left( \varepsilon \right) =​${$\varepsilon​$} 若 $a​$ 是 $\Sigma​$ 上的字符，则 $a​$ 是一个正规式，它所表示的正规集合为 {$a​$ } 若正规表达式 r 和 s 分别表示 $L\left( r\right)​$ 和 $L\left( s\right)​$，则 r|s 式正规式，表示集合 $L\left( r\right)​$ U $L\left( s\right)​$ r*s 是正规式，表示集合$L\left( r\right)​$ $L\left( s\right)​$ $r^{\ast }​$是正规式，表示集合$\left( L\left( r\right) \right) ^{\ast }​$ $\left( r\right)$是正规式，表示集合$L\left( r\right)$ 仅通过有限地使用上述3个步骤定义的表达式才是$\Sigma$ 上的正规式，其中，运算符“|” “.” 和 “” 分别称为 “或”“连接”和“闭包”。在正规式的书写中，连接运算符 “.” 可以省略。运算优先级（高到低）为 . | 设 $\Sigma$ ={a,b} ，列出 $\Sigma$ 上的一些正规式和相应的正规集： 2）有限自动机 是一种识别装置的抽象概念，它能准确的识别正规集。分为确定的有限自动机（DFA）和不确定的有限自动机（NFA）。 确定的有限自动机（DFA）。由五个元组（S， $\Sigma$，$f$，$S_{0}$，Z）其中： S 是一个有限集，其每个元素称为一个状态 $\Sigma​$是一个有穷字母表，其每个元素称为一个输入符 $f​$ 是 $S\times \Sigma \rightarrow 2^{s}​$ 上的单值部分映像。$f\left( A,a\right) =Q​$表示当前状态为A、输入为 $a​$ 时，将转换到下一个状态 Q,称Q为A的一个后继状态 $S_{0}\in S​$，是唯一的一个开始状态 Z 是非空的终止状态集合，$Z\subseteq S​$ 注意：DFA 中的每个状态对应转换图中的一个结点，每个转换函数对应一条有向弧，若转换函数为$f\left( A,a\right) =Q​$，则该有向弧从结点A出发，进入到Q，字符 $a​$ 是弧上的标记 不确定的有限自动机（NFA）。也是由五个元组，它与确定有限自动机的区别如下： $f​$ 是 $S\times \Sigma \rightarrow 2^{s}​$ 上的映像。对于 S 中的一个给定状态及输入符合，返回一个状态的集合。即当前状态的后继状态不一定是唯一的。 有向弧上的标记可以是 $\varepsilon$ 3）NFA 到 DFA 的转换 定义转换过程中需要的计算： 1、若 $I​$ 是 NFA M 的状态集合的一个子集。定义$\varepsilon​$_CLOSURE($I​$) 如下： 状态集$I$ 的 $\varepsilon$_CLOSURE($I$) 是一个状态集 状态集 $I$的所有状态属于 $\varepsilon$_CLOSURE($I$) 若 $S\in I​$，那么从 S 出发经过任意条 $\varepsilon​$ 弧达到的状态 S’ 都属于 $\varepsilon​$_CLOSURE($I​$) 状态集合 $\varepsilon​$CLOSURE($I​$) 称为 $I​$的 $\varepsilon​$闭包。 由上可知，$I​$ 的 $\varepsilon​$_闭包 就是从状态集$I​$ 的状态出发，经 $\varepsilon​$ 所能到达的全体。假定 $I​$ 是 NFA M 的状态集的一个子集，$a​$ 是 $\Sigma ​$ 中的一个字符，定义： $I_{a}$ = $\varepsilon$_CLOSURE(J) J 是那些可以从 $I$中的某一状态结点出发经过一条 $a$弧而到达的状态结点的全体。 2、NFA 转换为 DNF 。 4）DFA 的最小化 正规式与有限自动机的转换1）有限自动机转换为正规式 2）正规式转换为有限自动机 词法分析器的构造步骤： （1）用正规式描述语言中的单词构成规则 （2）为每个正规式构造一个 NFA ，它识别正规式所表示的正规集 （3）将构造出的 NFA 转换称等价的 DFA （4）对 DFA 进行最小化处理，使其最简 （5）从 DFA 构造词法分析器 语义分析任务是分析单词串是否构成表达式、语句和程序等基本语言结构，同时检查和处理程序中的语法错误。 程序语言的绝大多数语法规则采用上下无关文法进行扫描。 根据产生的语法树的方向，可分为自底向上和自顶向下两类。 1）上下文无关文法 规范推导；短语、直接短语和句柄； 2）自顶向下语法分析方法 基本思想：对于给定的输入串 $\omega​$ ，从文法的开始符号 S 出发进行最左推导，直到得到一个合法的句子或者发现一个非法结构。在推导的过程中试图用一切可能的方法，自上而下、从左到右地输入串 $\omega​$ 建立语法树。整个过程是一个反复试探的过程。 消除文法的左递归 提起公共左因子 LL(1) 文法 递归下降分析法 预测分析法 3）自底向上语法分析方法 又称移进-归约分析法。基本思想是：对输入序号 $\omega​$ 自左向右进行扫描，并输入符号逐个移进栈中，边移进边分析，一旦栈顶符号串形成某个句型的归约串，就用某个产生式的左部非终结符来替代，这一步称为归约。重复这一过程，直到栈中只剩下文法的开始符号且输入串也被扫描完为止。 中间代码生成代码优化目标代码生成符号表管理出错处理解释程序基本原理]]></content>
      <categories>
        <category>软件技术</category>
      </categories>
      <tags>
        <tag>软件技术</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[快速排序原理和实现]]></title>
    <url>%2F2019%2F03%2F30%2FSoftTestTechnique%2F%E5%BF%AB%E9%80%9F%E6%8E%92%E5%BA%8F%E5%8E%9F%E7%90%86%E5%92%8C%E5%AE%9E%E7%8E%B0%2F</url>
    <content type="text"><![CDATA[快速排序原理和实现原理高快省的排序算法，既不浪费空间也可以快一点的排序算法。 假设我们现在对“6 1 2 7 9 3 4 5 10 8”这个10个数进行排序。首先在这个序列中随便找一个数作为基准数。为了方便，就让第一个数6作为基准数吧。接下来，需要将这个序列中所有比基准数大的数放在6的右边，比基准数小的数放在6的左边，类似下面这种排列：3 1 2 5 4 6 9 7 10 8 在初始状态下，数字6在序列的第1位。我们的目标是将6挪到序列中间的某个位置，假设这个位置是k。现在就需要寻找这个k，并且以第k位为分界点，左边的数都小于等于6，右边的数都大于等于6。 方法其实很简单：分别从初始序列“6 1 2 7 9 3 4 5 10 8”两端开始“探测”。先从右往左找一个小于6的数，再从左往右找一个大于6的数，然后交换他们。这里可以用两个变量i和j，分别指向序列最左边和最右边。我们为这两个变量起个好听的名字“哨兵i”和“哨兵j”。刚开始的时候让哨兵i指向序列的最左边（即i=1），指向数字6。让哨兵j指向序列的最右边（即=10），指向数字。 首先哨兵j开始出动。因为此处设置的基准数是最左边的数，所以需要让哨兵j先出动，这一点非常重要（请自己想一想为什么）。哨兵j一步一步地向左挪动（即j–），直到找到一个小于6的数停下来。接下来哨兵i再一步一步向右挪动（即i++），直到找到一个数大于6的数停下来。最后哨兵j停在了数字5面前，哨兵i停在了数字7面前。 现在交换哨兵i和哨兵j所指向的元素的值。交换之后的序列如下：6 1 2 5 9 3 4 7 10 8 到此，第一次交换结束。接下来开始哨兵j继续向左挪动（再友情提醒，每次必须是哨兵j先出发）。他发现了4（比基准数6要小，满足要求）之后停了下来。哨兵i也继续向右挪动的，他发现了9（比基准数6要大，满足要求）之后停了下来。此时再次进行交换，交换之后的序列如下：6 1 2 5 4 3 9 7 10 8 第二次交换结束，“探测”继续。哨兵j继续向左挪动，他发现了3（比基准数6要小，满足要求）之后又停了下来。哨兵i继续向右移动，糟啦！此时哨兵i和哨兵j相遇了，哨兵i和哨兵j都走到3面前。说明此时“探测”结束。我们将基准数6和3进行交换。交换之后的序列如下：3 1 2 5 4 6 9 7 10 8 到此第一轮“探测”真正结束。此时以基准数6为分界点，6左边的数都小于等于6，6右边的数都大于等于6。回顾一下刚才的过程，其实哨兵j的使命就是要找小于基准数的数，而哨兵i的使命就是要找大于基准数的数，直到i和j碰头为止。OK，解释完毕。现在基准数6已经归位，它正好处在序列的第6位。此时我们已经将原来的序列，以6为分界点拆分成了两个序列，左边的序列是“3 1 2 5 4”，右边的序列是“9 7 10 8”。接下来还需要分别处理这两个序列。因为6左边和右边的序列目前都还是很混乱的。不过不要紧，我们已经掌握了方法，接下来只要模拟刚才的方法分别处理6左边和右边的序列即可。现在先来处理6左边的序列现吧。 左边的序列是“3 1 2 5 4”。请将这个序列以3为基准数进行调整，使得3左边的数都小于等于3，3右边的数都大于等于3。好了开始动笔吧 如果你模拟的没有错，调整完毕之后的序列的顺序应该是： 2 1 3 5 4 OK，现在3已经归位。接下来需要处理3左边的序列“2 1”和右边的序列“5 4”。对序列“2 1”以2为基准数进行调整，处理完毕之后的序列为“1 2”，到此2已经归位。序列“1”只有一个数，也不需要进行任何处理。至此我们对序列“2 1”已全部处理完毕，得到序列是“1 2”。序列“5 4”的处理也仿照此方法，最后得到的序列如下： 1 2 3 4 5 6 9 7 10 8 对于序列“9 7 10 8”也模拟刚才的过程，直到不可拆分出新的子序列为止。最终将会得到这样的序列，如下 1 2 3 4 5 6 7 8 9 10 到此，排序完全结束。细心的同学可能已经发现，快速排序的每一轮处理其实就是将这一轮的基准数归位，直到所有的数都归位为止，排序就结束了。下面上个霸气的图来描述下整个算法的处理过程。 这是为什么呢？快速排序之所比较快，因为相比冒泡排序，每次交换是跳跃式的。每次排序的时候设置一个基准点，将小于等于基准点的数全部放到基准点的左边，将大于等于基准点的数全部放到基准点的右边。这样在每次交换的时候就不会像冒泡排序一样每次只能在相邻的数之间进行交换，交换的距离就大的多了。因此总的比较和交换次数就少了，速度自然就提高了。当然在最坏的情况下，仍可能是相邻的两个数进行了交换。因此快速排序的最差时间复杂度和冒泡排序是一样的都是O(N2)，它的平均时间复杂度为O(NlogN)。其实快速排序是基于一种叫做“二分”的思想。我们后面还会遇到“二分”思想，到时候再聊。 实现123456789101112131415161718192021222324252627282930313233343536public class QuickSort &#123; public static void main(String[] args) &#123; int[] arr = &#123;6,1,2,7,9,3,4,5,10,8&#125;; quickSort(arr,0,arr.length-1); System.out.println(Arrays.toString(arr)); &#125; // 划分 private static int partition(int arr[],int low,int high)&#123; // 用 arr[low] 作为枢纽轴元素 pivot 进行划分 // 使得 arr[low,...,i-1] 均不大于 pivot，data[i+1,...,high]均小于 pivot int i,j; int pivot; pivot = arr[low]; i = low;j = high; while(i &lt; j)&#123; /*从数组的两端交替地向中间扫描*/ while(i &lt; j &amp;&amp; arr[j] &gt;= pivot)j--; arr[i] = arr[j];/*比枢轴元素小者往前移动*/ while(i &lt; j &amp;&amp; arr[i] &lt;= pivot)i++; arr[j] = arr[i];/*比枢轴元素大者往后移动*/ &#125; arr[i] = pivot; return i; &#125; // 对整型数组进行非递减排序 private static int[] quickSort(int arr[],int low,int high)&#123; if(low&lt;high)&#123; int loc = partition(arr,low,high);/*进行划分*/ quickSort(arr,low,loc-1);/*对前半区进行快速排序*/ quickSort(arr,loc+1,high);/*对后半区进行快速排序*/ &#125; return arr; &#125;&#125;]]></content>
      <categories>
        <category>软件技术</category>
      </categories>
      <tags>
        <tag>软件技术</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[第1章计算机网络概述]]></title>
    <url>%2F2019%2F03%2F30%2FSoftTestTechnique%2F%E7%AC%AC1%E7%AB%A0%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E6%A6%82%E8%BF%B0%2F</url>
    <content type="text"><![CDATA[第1章计算机网络概述计算机系统基础知识 计算机系统硬件基本组成计算机系统是由硬件和软件组成的。基本硬件系统由 运算器 控制器 存储器 输入设备 输出设备。 1、中央处理单元 运算器、控制器 等部件被集成在一起统称为中央处理单元（CPU）（硬件系统的核心）。CPU 用于数据加工处理，完成各种算术、逻辑运算及控制能力。 2、存储器 存储器 是计算机系统中的记忆设备，分为内部存储器和外部存储器 。内部存储器速度高、容量小。一般用于临时存放程序，数据及中间结果。；外部存储器容量大、速度慢。可以长期保存程序和数据。 3、外部设备 输入和输出设备合称为 外部设备 中央处理单元CPU 是计算机系统的核心，负责获取程序指令、对指令进行译码并加以执行。 功能 程序控制（重要功能）：通过执行指令来控制程序的顺序。 操作控制：生产指令操作信号–&gt;把操作信号送往对应的部件–&gt;控制相应的部件按指令的功能要求进行操作。 时间控制：指令执行过程中操作信号的出现时间、持续时间以及出现的时间顺序都需要进行严格控制。 数据处理（根本任务）：对数据进行算术运算及逻辑运算等方式对数据进行加工处理。 组成运算器、控制器、寄存器和内部总线等部件组成。 运算器由算术逻辑单元、累加寄存器、数据缓冲寄存器、状态条件寄存器组成。是数据加工处理部件。 注意：运算器所进行的全部操作都是由控制器发出的控制信号来指挥的，so 它是执行部件。 1、算术逻辑单元（ALU）：负责数据处理，实现对数据的算术运算和逻辑运算。 2、累加寄存器（AC）: 为 ALU 提供一个工作区。例如：在执行一个减法运算前，先将被减数取出暂存在 AC 中，再从内存存储器中取出减数，然后同 AC 的内容相减，将所得的结果送回 AC 中。 注意：运算器中至少要有一个累加器寄存器。 3、数据缓冲寄存器（DR）：作为 CPU 和内存、外部设备之间数据传送的中转站。作为 CPU 和内存、外围设备之间在操作速度上的缓冲。 注意：在单累加器结构的运算器中，数据缓冲寄存器还可兼作为操作数寄存器。 4、状态条件寄存器（PSW）：状态条件寄存器保存由算术指令和逻辑指令运行或测试或测试的结果建立的各种条件码内容，主要分为状态标志和控制标志。 功能1、执行所有的算术运算 2、执行所有的逻辑运算并进行逻辑测试 控制器用于控制整个 CPU 的工作，它决定了计算机运行过程的自动化。控制器一般包括指令控制逻辑、时序控制逻辑、总线控制逻辑和中断控制逻辑等几个部分。 指令控制逻辑过程：取指令–&gt;指令译码–&gt;按指令操作执行–&gt;形成下一条指令地址。 1、指令寄存器（IR）：当 CPU 执行一条指令时，先把它从内存储器取到缓冲寄存器中，再送入 IR 暂存，指令译码器根据 IR 的内容产生各种微操作指令，控制其他的组成部件工作，完成所有的功能。 2、程序计数器（PC）：具有寄存信息和计数功能，即又称指令计数器。程序的执行一般分为两种，一种是顺序执行，一种是转移执行。由于大多数指令都是按顺序来执行的，所以修改的过程通常只是简单地对 PC 加 1。当遇到转移指令时，后继指令的地址根据当前指令的地址加上一个向前或向后的位移的位移得到，或者根据转移指令给出直接转移的地址得到。 3、地址寄存器（AR）： 保存当前 CPU 所访问的内存单元的地址。 4、指令译码器（ID）：指令包含操作码和地址码。指令译码器是对指令中的操作码字段进行分析解析，识别该指令规定的操作，向操作控制器发出具体的控制信号，控制各部件工作，完成所需的功能。 时序控制逻辑–&gt;为每条指令按时间顺序提供有应有的控制信号。 总线逻辑–&gt;为多个功能部件服务的信息通路的控制电路。 中断控制逻辑–&gt;用于控制各种中断请求，并根据优先级的高低对中断请求进行排队，逐个交给 CPU 处理。 寄存器组分为专用寄存器和通用寄存器。运算器和控制器中的寄存器是专用寄存器，其作用是固定的。 多核 CPU核心又称为内核，是 CPU 最重要的组成部分。CPU 所有的计算、接受/存储命令、处理数据都由核心执行。各种 CPU 核心都是具有固定的逻辑结构，一级缓存、二级缓存、执行单元、指令级单元和总线接口等逻辑单元都会有合理的布局。 多核即在一个单芯片上面集成两个甚至更多个处理器内核，其中每一个内核都有自己的逻辑单元、控制单元、中断处理器、运算单元，一级缓存、二级 Cache 共享独有，其中部件的完整性和单核处理器内核相比完全一样。 多核 CPU 最大的优点是可以满足用户同时进行多任务处理的要求。 要发挥 CPU 的多核性能，就需要操作系统能够及时、合理得给各个核分配任务和资源（如：缓存、总线、内存等），也需要应用软件在运行时可以把并行的线程同时交付给多个核心分别处理。 数据表示各种数值在计算机中表示的形式称为机器数。 机器数分为：无符号数和 带符号数 无符号数表示正数，在机器数中没有符号位。 带符号数机器数的最高位是表示正、负的符号位，其余位则表示数值。为了便于运算，带符号的机器数采用原码、反码、补码等不同的编码方法，机器数的这些编码方法称为码制。 原码、反码、补码和移码原码表示法n为机器字长 最高位是符号位，0表示正数，1表示负数，其余的 n-1 位表示数值的绝对值。 例子：若机器字长为8位 反码表示法n为机器字长 正数的反码和原码相同，负数的反码则是其绝对值按位求反。 例子：若机器字长为8位 补码表示法n为机器字长 正数的补码与其原码和反码相同，负数的补码等于其反码的末尾加1。 例子：若机器字长为8位 移码表示法只有将补码的符号位取反。 定点数和浮点数定点数小数点的位置固定不变的数。小数点的位置通常有两种约定方式：定点正数、定点小数 浮点数小数点的位置不确定 一个二进制 N 可以表示为更一般的形式 $N=2^{E}\times F$ ，其中 E 称为阶码，F称为尾数。用阶码和尾数表示的数称为浮点数，表示方法称为浮点表示法。 在浮点表示法中，阶码为带符号的纯正数，尾码为带符号的纯小数。 浮点数的表示格式： 浮点数所能表示的数值范围有由阶码决定，精度由于尾数决定 规范化就是将位数的绝对值限定在区间[0.5,1]。 当尾数用补码表示时，要注意： 若为数 $$M\geq 0​$$ ，规格化的尾数形式为 M=0.1XXX…X ,X 可为0，也可为1，即将尾数限定在区间[0.5,1]。 若为数 $$M &lt;0$$，………..即将尾数限定在区间[-1,-0.5]。 当浮点数的阶码用 R 位的移码表示，尾数用 M 位的补码表示，浮点数表示的数值范围： 最大：$+\left( 1-2^{-m+1}\right) \times 2^\left( 2^{r-1}-1\right)$ 最小：$ -1 \times 2^\left( 2^{r-1}-1\right)$ 工业标准 IEEE754标准的表示形式为： $\left( -1\right) ^{S}2^{E}\left( b_{0}b_{1}b_{3}\ldots b_{p-1}\right)$ 其中，$\left( -1\right) ^{S}​$ 浮点数的数符，当 S 为 0 时表示正数，S 为 1 时表示负数；E 为指数（阶码），用移码表示； $\left( b_{0}b_{1}b_{3}\ldots b_{p-1}\right)$为尾数，长度为 p 位，用原码表示。 根据 IEEE754 标准，被编码的值分为 3 种不同的情况：规范化的值、非规范化的值和特殊值。 【1】、规范化的值：当阶码部分的二进制值不全为0，也不全为1时。 例子：利用 IEEE754 标准将数 176.0625 表示为单精度浮点数。 1、十进制数–&gt;转成二进制 （转换过程访问：https://www.cnblogs.com/xkfz007/articles/2590472.html） $\left( 176.065\right) _{10}=\left( 10110000.001\right) _{2}$ 2、规格化处理 $10110000.001$ = $1\langle \rangle 01100000001\times 2^{7}$ 将 $$b_{0}​$$ 去掉并扩展为单浮点数所规定的 23 位尾数。 01100000010000000000000 3、求阶码 $1\langle \rangle 01100000001\times 2^{7}$$的指数为 7 ，单精度浮点数规定指数的偏移量为 127。 所有 E = 7 + 127 = 134 ,求得移码为 10000110。 0 10000110 01100000010000000000000 【2】、非规范化的值 阶码的二进制全为 0 。指数的真值为1–偏移量（对于单精度浮点数为-126，双精度浮点数为-1022），尾数的值就是二进制形式对应的小数，不包含隐含的1。 用途：①表示数值0；②表示哪些非常接近于0的数。 【3】、特殊值 阶码的二进制全为1。 尾数部分全为0：无穷大；符号为为0：$+\infty​$，1：$-\infty​$; 尾数部分不全为0：NaN; 浮点数的运算 校验码使用校验码的方法来检测传送的数据是否出错。 码距指一个编码系统中任意两个合法编码之间至少有多少个二进制位不同。例如：4 位 8421 码的码距为1 ，在传输过程中，该代码的一位或多位发生错误，都将变成另一个合法的编码，因此这样编码无检错能力。 奇偶校验码在编码中增加一位校验位来使编码中 1 的个数为奇数（奇校验）或者为偶数（偶校验），从而时码距变为2。奇校验，可以检测代码中奇数位出错的编码，但不能发现偶数位出错的情况，即当合法编码中的奇数位发生了变化，即当编码中的0变成了1或1变成了0，则该编码中1 的个数的奇偶性就发生了变化。 常用的奇偶检验码有：水平奇偶检验码、垂直奇偶校验码、水平垂直奇偶校验 海明码构成方法是在数据位之间的特定位置上插入 k 个校验位，通过扩大码距来实现检错和纠错。 设数据位是 n 位， 校验位是 k 位，则 n 与 k 必须满足以下关系： $2^{k}-1\geq n+k$ 计算机体系结构计算机体系结构的发展概述计算机体系结构、计算机组织和计算机实现的关系： 体系结构：计算机的概念性结构和功能属性。 组织：计算机组成原理（计算机体系结构的逻辑实现，包括计算机内的数据流和控制流的组成以及逻辑设计等）。 实现：计算机组织的物理实现。 分类1、（宏观）按处理机的数量分：单处理系统、并行处理与多处理系统、分布式处理系统 2、（微观）按并行程度分：Flynn 分类法、冯泽云分类法、Handler 分类法、Kuck分类法 指令流：机器执行的指令序列； 数据流：指令调用的数据序列； 并行度：计算机系统在单位时间内能够处理的最大二进制位数。 Flynn ：按指令流和数据流的多少分类（单/多指令：单/多数据流，4种） 冯泽云：按并行度分类（字串/并行：位串/并行，4种） Handler：提出一个基于硬件并行程度计算并行度的方法。 Kuck：按指令流和执行流及其多重性分（单/多指令：单/多执行流，4钟） 指令系统指令集体系结构：一个处理器的指令和指令的字节级编码；不同的处理器族支持不同的指令集体系结构。 指令集体系结构的分类从体系结构的观点分1、操作数在 CPU 中的存储方式 2、显式操作数的数量 3、操作数的位置 4、指令的操作 5、操作数的类型与大小 按暂存机制分堆栈（Stack）、累加器（Accumulator）、寄存器组 CISC 和 RISCCISC（复杂 指令集）和 RISC（精简指令集）是指令集发展的两种途径。 CISC：用更为复杂的新指令取代原先由软件子程序完成的功能，实现软件的硬件化，导致机器的指令系统越来月庞大、复杂。 RISC：通过减少指令总数和简化指令功能降低硬件设计的复杂度，使用指令能够单周期执行，并通过优化编译提高指令的执行速度。采用硬布线控制逻辑优化编译程序。 优化： 提高目标程序的实现效率：对动态和静态使用频率进行优化，既可以减少程序所需的存储空间，又可以提高程序的执行速度。 面向高级程序语言：缩小高级语言与机器语言之间的语义差距。 面向操作系统：缩小操作系统与体系结构之间的语义差异。 指令的流水处理指令控制方式①顺序执行：指各条机器指令之间顺序的执行。优点是控制简单；缺点是速度慢，各部件的利用率低。 ②重叠方式：指在解释第K条指令的操作完成之间就可以开始第K+1条指令。优点是速度提高，控制简单；缺点是出现冲突、转移和相关等问题。 ③流水方式：把并行性和并发性嵌入到计算机系统的一种形式，把重复的顺序处理过程分解为若干个子过程，每个子过程能在独立的模块上有效地并发工作。 流水线的种类①级别：部件级、处理级、系统级 ②功能：单功能、多功能 ③连接：静态、动态 ④是否有反馈回路：线性流水、非线性流水 ⑤流动顺序：同步流水、异步流水 ⑥数据表示：标准、向量 流水的相关处理1、流水线时同时处理多条指令，会出现相关。相关带来的影响是局部性的，所以称为局部性相关。解决方式：推后法和通路法。推后法：推后相关单元的读，直到写入完成。通路法：设置相关专用通路直接使用运算结果，以加快速度。 2、转移指令与后面的指令之间存在关联，使之不能同时解释。执行转移指令时，可能会改动指令缓冲器中预取到的指令内容，造成流水线吞吐率和效率下降，称全局性相关。解决：猜测转移分支、加快和提前形成条形码、加快断循环程序的处理。 3、RISC 采用的流水技术： ①超流水线：在所有的功能单元都采用流水，并有更高的时钟频率和更深的流水深度。 ②超标量： ③超长指令字： 吞吐率和流水建立的时间单位时间内流水线处理机流出的结果数（对指令，单位时间内执行的指令数）。 当流水线的子过程所用的时间不一样，吞吐率 $p=1/\max \left{ \Delta t_{1},\Delta t_{2},\ldots ,\Delta t_{m}\right}$ 若m个子过程所用的时间一样，均为 $\Delta t_{0}$ ,则建立时间 $T_{0}=m\Delta t_{0}$ 阵列、并行、多处理机并行性：同一时刻发生；并发性：同一时间间隔内连续发生。 1、阵列处理机：将重复处理的多个单元（PU）按一定方式连成阵列。是一中单指令多数据流计算机，通过资源重复实现并行。 2、并行处理机：SIMD（共享存储器和分布式存取器形式） 和 MIMD 是典型的并行计算机。 3、多处理机：由多台处理及组成的。各自独立，共享一个主存器和所有的外部设备。多指令多数据流计算机。 4、其他计算机：集群计算机是一种并行或分布式处理系统。主要用来解决大型计算问题。这种系统可以提供一种价格合理的且可获得所需性能和快速而可靠的服务的解决方案。 存储系统存储器的层次结构例子：CPU 内部的通用寄存器组、CPU 内的 Cache(高速缓存)、CPU 外的 Cache、主板上的主存储器等。 注意：Cache 和主存之间的交互功能全部由硬件实现，主存和辅存由硬件和软件结合起来实现。 存储器的分类1、按位置： 内存（主存） 外存（辅存） 主机内和主机板上 磁盘、磁带和光盘 容量小速度快（相对于外存） 存放当前运行的程序和数据，向CPU提供信息 存放不参与运行的信息，需要是才调入 2、按构成材料： 磁存储器、半导体存储器、光存储器 3、按工作方式： 读/写存储器、只读存储器 4、按访问方式： 按地址访问存储器、按内容访问存储器 5、按寻址方式： 随机存储器、顺序存储器、直接存储器 相连存储器是一种按内容访问的存储器。按关键字顺序存储和读取。适合于信息的检索和更新。相连存储器可用在高速缓存存储器中，在虚拟存储器中用来作为段表、页表和快表存储器，用在数据库和知识库中。 高速缓存用来存放当前最活跃和程序和数据。特点：位于 CPU 与主存之间；容量 几千~~几兆字节；速度比主存快 5~10 倍，由快速半导体存储器构成；其内容是主存局部域和副本，对程序员来说是透明的。 组成高速缓存（Cache）、主存（Main Memory）与 CPU。关系如下： 现代 CPU 的缓存分为多层，关系如下： 地址映像方法CPU 工作时，送出去的是主存单元地址，而应从 Cache 存储器中读/写信息。需要将主存地址转化成 Cache 存储器地址，这种地址转换称为地址映像。 ①直接映像：主存块与Cache 的对应关系是固定。 ②全相联映像：主存的任一块可以调入Cache 存储器的任何一个块的空间。 ③组相联映像：将 Cache 中的块再分成组。 映射名称 优点 缺点 直接映像 地址变换很简单 灵活性差 全相联映像 主存的块调入Cache 的位置不限制，灵活 无法从主存块号中直接获得 Cache 的块号，变换复杂，速度比较慢 组相联映像 前两种方式的折中 替换算法目的：使 Cache 获得尽可能高的命中率。 ①随机替换：随机产生一个要替换的块号，将该块替换出去。 ②先进先出：将最先进入Cache的块替换出去。 ③近期最少使用： ④优化替换：必须先执行一次程序，统计 Cache 的替换情况。在第二次执行该程序便可以使用做有效的方式来替换。 性能分析设 $H_{c}$ 是Cache 的命中率， $t_{c}$ 是Cache 的存取时间， $t_{m}$ 是主存的访问时间，则 Cache 存储器的等效加权平均访问时间 $t_{a}$ 为：$t_{a}=H_{c}t_{c}+\left( 1-H_{c}\right) t_{m}=t_{c}+\left( 1-H_{c}\right) \left( t_{m}-t_{c}\right)$ 假设 Cache 访问和主存的访问是同时启动的， $t_{c}$ 是Cache 的存取时间，$\left( t_{m}-t_{c}\right)$ 为失效访问时间。如果在 Cache 不命中时才启动主内存，则 ：$t_{a}=t_{c}+\left( 1-H_{c}\right) t_{m}$ 总结：降低 Cache 的失效率是提高 Cache 性能的一项重要措施。主要方法是：选择恰当的块容量、提高 Cache 的容量和提高 Cache 的相联度。 Cache 容量越大（增加 Cache 的成本和Cache 的命中时间），命中率越高，随着 Cache 容量的增加，失效率接近0%。 多级 Cache多级，一般分为：一级（L1 Cache），二级（L2 Cache），三级（L3 Cache）等。CPU 访问首先查找 一级，若命不中就访问 二级，直到所有级别的 Cache 都命不中才访问主存。 虚拟存储器是一种对主存的抽象。使用虚拟地址（Virtual Address ，由 CPU 生成）的概念来访问主存，使用专门的 MMU 将虚拟地址转为物理地址后访问主存。它实际上是一种逻辑存储器，实质是对物理存储设备进行逻辑化的处理。 访问主存中的数据：CPU 给出存储单元地址–&gt;主存的读写控制部件定位对应的存储单元–&gt;进行读（写）操作完成访问。 外存储器存放暂时不用的程序和数据，以文件的形式存储。CPU 不能直接访问外存中的程序和数据，只有将其以文件为单位调入主存才可访问。 磁表面存储器由盘片（控制数据写入和读出）、驱动器（寻找目标磁道位置）、控制器（控制驱动器的读/写操作）和接口（主机和磁盘存储器之间的连接逻辑）组成。 注意：控制器接收主机发来的命令，转化成控制命令，实现主机和驱动器之间数据格式的转换及数据传送，以控制驱动器的读/写操作。 硬盘是常见的外部存储器。 硬盘的寻址信息由硬盘驱动号、圆柱面号、磁头号（记录面）、数据块号（或扇区号）、交换量组成。 磁盘容量的两中指标： ①非格式化容量（总位数）：面数 X（磁道数/面）X 内圆周长 X 最大位密度 ②格式化容量（各扇区中数据区容量的总和）：面数 X（磁道数/面）X（扇区数/道）X（字节数/扇区） 光盘存储器采用聚焦激光束在盘式介质上非接触地记录高密度信息的新型存储装置。由光学、电学和机器部件组成。 特点是：记录密度高、存储容量大、采用非接触式读/写信息，信息可长期保存、数据传送率可超过200Mb/s、制造成本低、对机械结构的精度要求不高、存取时间较长。 分为：只读型光盘、只写型光盘、可擦除型光盘 固态硬盘具有传统机械硬盘不具备的读写快速、质量轻、能耗低以及体积小等特定。但其价格昂贵、容量较低、一旦硬件损坏，数据较难恢复。 存储介质分为两种：采用闪存（FLASH 芯片）、采用DRAM 作为介质。 磁盘阵列由多台磁盘存储器组成的一个快速、大容量、高可靠的外存子系统。常见的磁盘阵列称为廉价冗余磁盘阵列（RAID）。 级别：从 RAID-0 到 RAID-6，各种类型的 RAID 可以组合起来，构成复合型的 RAID。 存储域网络（SAN）把一个或多个服务器与多个存储设备连接起来，每个存储设备可以是 RAID 、磁带备份系统、磁带库和 CD-ROM 库等。 解决了①服务器对存储容量的要求；②多个服务器之间共享文件系统和辅助存储空间；③实现分布式存储系统的集中管理，降低管理成本，提高管理效率； SAN 的结构：存储域网络是连接服务器与存储设备的网络，它将多个分布在不同地点的 RAID 组织成一个逻辑存储设备，供多个服务器共同访问。 输入/输出技术微型计算机中最常用的内存与接口的编址方法常见的两种：内存与接口独立编制和内存与接口地址统一编址 说明 优点 缺点 内存地址和接口地址是完全独立的两个地址空间 在编程序或读程序很易使用和辨认 接口的指令太少，功能太弱。 内存单元和接口共用地址空间 指令上不再区分内存指令和接口指令，增强了对接口的操作功能 内存地址不连续，需要参数定义表辨别 直接程序控制指外设数据的输入/输出过程是在 CPU 执行程序的控制下完成的。 分为：无条件传送(可随时向 CPU 接收或发送数据)、有条件传送（需要CPU查询外设状态；降低 CPU 的效率、无法实时的响应外部事件） 中断方式程序控制 I/O 的方法。 缺点：CPU 需要定期查询 I/O 的状态，来确认传输是否完成，降低了系统的性能。 与程序控制方式相比，中断方式因为CPU无需等待而提高了效率。 处理方法 名称 说明 多中断信号线 中断请求信号线向 CPU 提出中断请求 中断软件查询 轮询每个中断源确定发出请求中断者 菊花链（硬件查询） 中断确认信号在 I/O 模块间传递，直达发出请求的模块，该模块把它的ID送往数据线由 CPU 读取 总线仲裁 在发中断前需要获取总线控制权，总线可决定谁可以发出中断请求 中断向量表 中断向量表保存各个中断源的中断服务程序入口地址。 优先级控制给最紧迫的中断源分配高的优先级，给相对不紧迫的中断源分配低一些的优先级。 优先级控制可以解决以下情况： 1、当多个优先级不同的中断请求提出时，CPU 优先响应优先级最高的中断源。 2、执行中断嵌套。 直接存储器存取方式直接存储器简称 DMA。指在内存与I/O设备传送一个数据块的过程，不需要 CPU 的干涉（只需要在过程开启和结束）。在 DMA 传送过程中，整个系统都交给了 DMA 控制器（DMAC），由它控制系统总线完成数据传送。在 DMA 传送数据期间，CPU 不能使用总线。 输入输出处理机简称 IOP。IOP 也被成为通道，它分担了 CPU 的一部分功能，实现对外围设备的统一管理，完成外围设备与主存之间的数据传送，提高 CPU 的工作效率，但是它需要增加更多的硬件作为代价。 几步发展产品：外围处理机（PPU）、专用处理机（PPU） 总线结构总线简称 Bus。指计算机设备和设备之间传输信息的公共数据通道。它的特征是由总线上的所有设备共享，可将计算机系统内的多种设备连接到总线上。 总线的分类数据总线、地址总线、控制总线，不同型号的CPU芯片，它们也不同。 1、数据总线（DB）：双向、CPU通过DB从内存或输入输出设备读入数据也可以传送数据、宽度决定了CPU和计算机其他设备之间每次交换数据的位数。 2、地址总线（AB）：用于传送 CPU 发出的地址信息、单向、宽度决定了 CPU 的最大寻址能力。 3、控制总线（CB）：用来传送信号（例如：控制信号、时序信号、状态信号）、作为整体是双向的。 总结：总线的性能直接影响到整体系统的性能；任何系统研制和外围模块的开发都必须依从所采用的总线规范。 常见总线 名称 说明 ISA 总线（AT标准） 工业标准；支持16位I/O;传速16Mb/s EISE总线 基于ISA起来的；32位；传输 33Mb/s PCI总线 使用较广的内总线，采用并发传输方式；32/64位；PCI上的设备是即插即用；在传输时会进奇偶校验 PCI Express 总线 简称 PCI-E;采用点对点串行连接，无需向总线请求宽带，提高传输频率；PCI-E支持热拔插；双向传输，可运行全双工和双单连接工模式，连接的每个装置都可使用到做大宽度。 前端总线 简称 FSB;将CPU 连接到北桥芯片。系统需要主板和CPU都支持某个总线，才能工作；数据传输能力决定对计算机整体性能影响大； RS-232C 串行外总线；至少需要三条；传输距离远；具有较好的抗干扰性 SCSI总线 并行外总线；广用于连接硬磁盘、光盘、扫描仪；传输距离20m（差分传送） SATA 串行；主要用在主板和大量存储（硬盘、光盘）之间传输；能对传输指令进行校验并自动校正；接口优点：结构简单、支持热拔插 USB 串行；由四条信号线组成（两条数据传送，两条传送电源）；优点：支持热插拔、即插即用 IEEE-1394 最高速串行外总线；支持热拔插，可提供电源；支持同步和异步数据传输；由6条信号线组成（两条传送数据两条传送控制信号；两条传送电源）；传输可达3.2Gb/s IEEE-488 总线 并行总线接口标准；总线连接方式；仪器设备（微计算机、数字电压表、数码显示器等）不需中介单元直接并行、字节串行双向异步方式传输信号；最多可连接15台设备； 安全性、可靠性与系统性能评测基础知识计算机安全概述计算机安全包括：安全管理、通信与网络安全、密码学、安全体系及模型、容错与容灾。 计算机的安全等级4组7个等级 组 安全级别 定义 1 A1 可验证安全设计。提供B3级保护，给出形式化隐秘通道分析，非形式化代码一致性验证 2 B3 安全域。TCB必须满足访问监控的要求，提供系统恢复过程 B2 结构化安全保护。建立安全策略模型，对所有的主体和客体实施自主访问和强制访问 B1 标记安全保护。对数据进行标记，对标记的主体和客体实施强制存取控制 3 C2 受控访问控制。存取控制以用户为单位 C1 c初级的自主安全保护，实现用户和数据的分离，进行自主存取控制，以用户组为单位 4 D 最低级、保护措施很小、没有安全功能 安全威胁分两类：故意（黑客渗透）和 偶然（信息法网错误的地址） 典型的安全威胁 授权侵犯、拒绝服务、窃听、信息泄露、截获/修改、假冒、否认、非法使用、人员疏忽、完整性破坏、媒体清理、物理入侵、资源耗尽 影响数据安全因素1、内部因素：数据加密；安全规划 ；安全存储等 2、外部因素：设置权限；设置身份、密码、口令等多种认证；设置防火墙； 加密认证技术和认证技术加密技术密钥加密技术的密码体制分为对称密钥体制和非对称密钥体制。 数据加密的技术分为两类，即对称加密（私人密钥加密）和非对称加密（公开密钥加密）。 对称机密技术文件加密和解密使用相同的密钥。 常用的对称加密算法： （1）、数据加密标准算法简称DES：运算速度块、密钥产生容易；适合在当前大多数计算机上用软件方法实现；适合于在专用芯片上实现。 （2）、三重DES（3DES或TDEA）：在 DES 的基础上采用三重DES，密钥长度加倍；发送方式，K1加密–&gt;k2解密–&gt;K1加密；接收方式，K1解密–&gt;k2加密–&gt;K1解密 （3）、RC-5：是在RCF2040中定义的 （4）、国际数据加密算法（IDEA）：密钥为 128位置；是一种据库块加密算法 （5）、高级加密标准（AES）算法：基于排列（对数据进行重新安排）和置换（将一个数据单元替换为另一个）运算。AES 是一个迭代的、对称密钥分组的密码，可使用 128、192和256位密钥。 非对称加密技术因为加密和解密使用的是两个不同的密钥。非对称加密算法需要两个密钥：公开密钥和私有密钥。若使用公开密钥进行加密则私有密钥才能解密；反之； 优缺点：保密性好；加密和解密花费的事件长、速度慢，不适合对文件加密，适合少量数据加密。 RSA算法是一种公钥加密算法；安全性是基于大素数分解的困难性能。 密钥管理包括密钥生产、密钥备份和恢复、密钥更新等 密钥生产：密钥对的产生是证书申请中的一步，产生的私密由用户保留，公密由和其他信息交给CA中心进行签证，从而产生证书。普通/测试证书由浏览器或固定的终端应用产生。重要（商家证书、服务器证书）的证书，一般由专用应用或CA中心直接产生。 密钥备份和恢复：在 PKI（公开密钥体系） 系统中，即使密钥丢失，使用 PKI 的企业和组织必须仍能得到确认，受密钥保护的重要信息也必须能够恢复。 密钥更新：对没和由CA颁发的证书都会有有效期，密钥对生命周期的长短由签发证书的CA中心来确定，各证书的有效期都不同。 多密钥的管理：Kerberos 建立了一个安全的、可信任的密钥中心（KDC），每个用户只要知道一个和KDC进行会话的密钥就可以了。 认证技术主要解决网络通信过程中通信双方的身份认可。涉及加密（对称加密、不对称加密、两种混合）和密钥交换。认证一般有账户名/口令认证、使用摘要算法认证和基于PKI认证。 认证机构：数字证书的申请及签发机关（CA）； PKI 的主要目的通过自动管理密钥和证书为用户建立一个安全的网络运行环境，是用户可以在多种环境下方便的使用加密和数字签名技术，保证网上数据的机密性、完整性和有效性。 KPI标准化主要有两方面：1、RSA 公司的公钥加密标准；2、一组具有互操作性的公钥基础设施协议 Hash 函数与信息摘要信息摘要：简要地描述一份较长的信息或文件。用于创建数字签名（是唯一的）。 例子： MD5（MD表示信息摘要），是被广泛应用的 Hash 函数。特点：压缩性（长度固定）、容易计算、抗修改性、强抗碰撞。 数字签名数字签名过程：信息发送者对信息生成信息摘要–&gt;信息发送者使用私钥签名信息摘要–&gt;把信息本身和已签名的信息摘要一起发送出去–&gt;（使用同一个散列函数）接收者对信息本身生成新的信息摘要，并使用公钥对信息摘要进行验证 总结：数字签名是一对多的关系；可以验证数字签名的正确性； 数字加密过程： 1、信息发送者需要生成一个对称密钥，用该密钥加密要发送的报文。 2、信息发送者用接收者的公钥加密对称密钥。 3、数字信封：步骤1和步骤2结合在一起的结果。 4、接收者使用私钥解密被解密的对称密钥，再用对称密钥解密被加密的密文，得到原文。 总结：数字加密是多对一的关系；任何知道公钥的人都可向接收方发送加密信息；采用对称加密算法和非对称加密算法，保证发送信息的保密性。 SSL 协议SSL 是安全套接层，主要用于提高应用程序之间数据的安全系数。是一个保证计算机通信安全的协议。 提供以下3方面的服务： 1、用户和服务器的合法性认证。是它们确信数据被送到正确的客户端和服务器上。验证用户的合法性，安全套层协议要求在握交换数据时，进行数字认证。 2、加密数据易隐藏被发送的数据。在客户端与服务器进行数据交换之前，交换SSL初始握手信息，在 SSL 握手信息中采用了各种加密技术对其加密，保证其机密性和数据完整性，并用数字证书进行鉴别，防止破译。 3、保护数据的完整性。安全套接层协议采用 Hash 函数和机密共享的方法来提供信息的完整性服务，建立客户端与服务器之间的安全通道。 主要经过的阶段： 接通（服务响应）–&gt;密码交换–&gt;会谈密码–&gt;检验–&gt;结束 发送时，信息用对称密钥加密，对称密钥用非对称算法加密，再把两个包捆绑在一起送过去。 数字事件戳技术是数字签名技术的一种变种应用。 数字时间戳服务（DTS）是网上电子商务安全服务项目之一，提供电子文件的日期和时间信息的安全保护。是一个经加密后形成的凭证文档。 包括如下3步骤： 文件的摘要–&gt;DTS收到文件的日期和时间–&gt;DTS的数字签名 计算机可靠性计算机可靠性概述其中，$R\left( t\right)$ 是正常运行的概率；$$\lambda$$ 指单位时间内失效的元件数与元件总数的比例，当它为常量时，可靠性与失效性的关系为：$R\left( t\right) =e^{-\lambda t}​$ 失效率特征如下图 平均无故障时间：$MTBF=\dfrac {1}{\lambda }$ 计算机的维修效率（MTRF）。计算机的可用性是指计算机的使用效率。它以系统在执行任务的任意时刻能正常工作的概率A表示，即：$A=\dfrac {ETBF}{MTBF+MTRT}$ 计算机的 RAS 是指用可靠性 R 、可用性 A 和可维修性S这3个指衡量一个计算机系统。 计算机可靠性模型常见的系统可靠性数学模型有一下3种： 串联系统且仅当所欲的子系统都能正常工作时。系统才正常工作。 可靠性公式：（$R_{1}$ $R_{2}$ 表示子系统） $R=R_{1}R_{2}\ldots R_{N}​$ 失效公式：（$$\lambda _{1}​$$ $$\lambda _{2}​$$ 表示子系统的失效率） $\lambda =\lambda _{1}+\lambda _{2}+\ldots +\lambda _{N}$ 并联系统只要有一个子系统正常工作，系统就能正常运行 可靠性公式：$R=1-\left( 1-R_{1}\right) \left( 1-R_{2}\right) \ldots \left( 1-R_{N}\right)$ 失效率：$\mu =\dfrac {1}{\dfrac {1}{\lambda }\sum ^{N}_{j=1}\dfrac {1}{j}}$ N 模冗余系统由N个（N=2n+1）相同的子系统和一个表决器组成。只要有 n+1 个或 n+1 个以上的子系统能正常工作，系统就正常工作。 公式： $R=\sum ^{N}{i=n+1}\begin{pmatrix} j \ N \end{pmatrix}\times R^{i}{0}\left( 1-R_{0}\right) ^{N-i}$ $\begin{pmatrix} j \ N \end{pmatrix}$ 表示从 N 个元素中取 i 个元素的组合数。 提高可靠性：提高元器件数量；发展容错技术。 计算机系统的性能评价性能测评的常用方法（1）时钟频率。主频越高，速度越快 （2）指令执行速度。加法指令的运算速度衡量计算机的速度。 （3）等效指令法。统计各类指令在程序中所占的比例，并进行折算。 （4）数据处理频率法（PDR）。采用计算 PDR 值得方法来衡量机器性能，PDR 值越大，机器性能越好。注意，PDR 值只要对 CPU 和主存储器的速度进行度量，但不适合衡量机器的整体速度，因为它没有涉及到Cache、多功能部件等技术对性能的影响。 （5）核心程序法。把应用程序中用得最频率的那部分核心程序作为评价计算机性能的标准程序，在不同的计算机上运行，测得其执行时间。机器软/硬件结构的特点能在核心程序中的到反映，但是核心程序各部分之间的联系较小。 基准测试程序目前测试性能较好的方法。有多种多样的基准程序。 （1）整数测试程序。测试编译器及CPU处理整数指令和控制功能的有效性。 （2）浮点测试程序。机器的浮点性能对系统的应用有很大的影响。 （3）SPEC基准程序。 （4）TPC基准程序。]]></content>
      <categories>
        <category>软件技术</category>
      </categories>
      <tags>
        <tag>软件技术</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[废掉一个人最隐蔽的方式是让他忙到没时间成长]]></title>
    <url>%2F2019%2F03%2F29%2FEssay%2F%E5%BA%9F%E6%8E%89%E4%B8%80%E4%B8%AA%E4%BA%BA%E6%9C%80%E9%9A%90%E8%94%BD%E7%9A%84%E6%96%B9%E5%BC%8F%E6%98%AF%E8%AE%A9%E4%BB%96%E5%BF%99%E5%88%B0%E6%B2%A1%E6%97%B6%E9%97%B4%E6%88%90%E9%95%BF%2F</url>
    <content type="text"><![CDATA[废掉一个人最隐蔽的方式是让他忙到没时间成长大家可能都看过一个观点：废掉一个人最快的方式，就是让他闲着。我认同么？非常认同。但我今天想讲另一个角度，废掉一个人最隐蔽的方式，是让他忙到没时间成长。而且我认为这个点更重要。因为大部分人都不闲，大家都在努力的过活，北上广深等大城市的年轻人，尤其是互联网从业者，不仅不闲，还每天忙得要死，甚至周末都不休息。这样一群人，我根本不怕他们太闲，我更怕他们太忙，尤其是怕他们忙到没时间成长。 废掉一个人最隐蔽的方式是让他忙到没时间成长我的团队成员都是从我的社群里招的，所以大家价值观一致、做事理念一致、目标一致，我们彼此深度信任。因此即便我不怎么管他们，他们也超级努力，一个表现就是：下班后他们主动加班，周末他们主动给自己工作加量，更快的推进工作，总之特别忙，工作时间超长。很多老板喜欢看到员工这样，但我总是担心，担心他们忙到没时间成长。我经常告诉他们要注意休息，周末也不要一直干活。我有个课程助理，前几天我跟他说了这么一段话： 你一定要控制好自己的工作节奏，不用推进得太猛，每天早点下班，周末也不用这么拼。 你空出来的时间，除了休息好，就是用来自我成长。 比如，你的工作需要写东西，那么你必须保证每天拿出固定的时间学习、大量阅读、听课，甚至这要变成一个强制性学习任务，跟你的工作任务同样重要，甚至你上班时间做也没关系。 我写作三年多，为什么越写越好，永不枯竭，永远有新东西可写，永远能提出新观点。 因为每天晚上12点到凌晨3点左右，几乎是我雷打不动学习时间，不管当天多累，这个习惯我都没中断。 你必须有时间成长，而不是无休止的工作。不信你问问自己：过去一个月的忙碌中，过去一年的忙碌中，你有多少时间用来成长？你成长了多少？如果你每天忙到连认真读几篇好文章的时间都没有，忙到连听一个小时课的时间都没有，忙到想在周末精进一下某项能力、某项技能的时间都没有，你必须要正视这个问题了。很多人沉浸在每天的忙碌工作中，早上八点出门，晚上十点多回家，累得洗漱完倒头就睡，明天又这样重复一天，后天又这样重复一天……日复一日。这是废掉一个人最隐蔽的方式，因为你会觉得明明自己每一天都特别充实，每天都干了很多活。你每天都干了很多活，是没错，但你过去半年、一年进步了多少？你自己心里没数么。为什么说这是废掉呢？因为学习是一种习惯，成长是一种习惯，精进也是一种习惯。你若是一年两年里都忙到没时间成长，你最终也会习惯了每天不成长的状态，毕竟不成长本身也是人最舒服的状态。因此，时间长了，你就丧失了成长的能力。而大部分老板不关心这个，为什么？比如一个新媒体工作的老板招一个小编，如果这个小编成长很快，可以做更有价值的事情，这对老板来说是意外惊喜。如果这个小编不成长，那完全没关系，老板招你来就是让你做小编的工作的，你一年没进步也没关系，你就日复一日做好你的本职工作就行。因此，你成长，是老板可遇不可求的，是不可控的；而在你能胜任的工作上给你不断加任务、加工作量，是老板可控制的、可明显有更大收益的做法。所以，你需要自己保留成长时间。 一个人在职场里持续上升必须要有持续的增量成长有人说：工作的过程，不就是很好的成长过程么？我认同么？非常认同。但同时，有很大的局限性。 为什么呢？ 每份工作必然包含大比例的重复性、同质化工作。 比如我做新媒体讲师时，前期准备课程时，我每天进步很大，我每天在打磨新的东西，等这个课程做出来，我出去讲一次自己进步很大，再去讲第二次还有很大进步….但这毕竟是一个重复的、同质化的工作，我每次出去就是讲那几百页 PPT。当我疲于奔波在一个个公开课和一家家企业，没有时间学新东西时，我的进步就变得很慢了。事实上，任何一份工作都必然包含大比例的重复性、同质化工作： 比如： 一个商务每天都要跟一堆人重复说同样的话术； 一个客服每天要解答成百个同质化问题； 一个小编每天都要用标准模版给一篇稿子排版； 一个银行职员每天都是那个流程服务客户； 一个的士司机每天那样拉客； 一个电商文案每天用同质化的模版写同质化的句子； 一个设计师可能做了一年没太有啥区别的课程海报或商品海报…… 这个社会分工越细，这个趋势越重，因为这样大家加起来的效率最高，但个人的成长不是如此。 重复单一的学习，对成长的边际贡献越来越低。 我有个做销售的朋友，每天特别忙，做了两年了。 我问：“你做了两年了，有没有自己一套成熟的销售知识体系了？” 她回：“啥体系？我就每天那套话术给客户讲呗。” 我说：“最近有个讲怎么做销售的课挺火的，老师也有多年的实战经验，你没去听一下、研究一下么？” 她回：“朋友圈里看见过，但我哪有时间学啊。” 我问：“关于营销、品牌的书和文章，你看得多么？” 她回：“我光谈客户做销售都累死了，哪有时间再学别的……” 如果一个人只是从工作过程中学习，那么这种单一的学习方式，必然导致它对成长的边际贡献越来越低。每天一定要让自己有时间成长，其实是让你拓展除工作外的其它成长方式，你学习的方式多元化，对成长的边际贡献就越高，你的成长就越快。 我跟那个做销售的朋友说： 你做了两年销售了，如果继续这样只是天天谈客户天天谈客户，第三年你可能还是这样。 你要抽时间多听一些课程，更新完善你的知识体系，并在之后的销售过程中去运用实践。 你要多学一点品牌的知识、营销的知识，这样对销售工作帮助很大，你要多一点商业知识商业思维，这样你越来越专业，谈客户更有说服力等等。 一个人在职场里持续上升，必须要有持续的增量成长。如果你每天、每周、每个月都忙到没时间成长，那么你每一天的工作，都是在消耗存量。你的存量是有限的，一定是越消耗越少。一个日复一日消耗存量的员工，必然是一个不能提供惊喜的员工。所谓普通人，就是不能提供惊喜的人，有增量才有惊喜，才能持续上升。 一份好工作的标准里必须有一条你有时间成长人人都说要找份好工作，那什么是好工作？标准有很多，但可以简单归纳为三点：有钱、有闲、有成长。 有钱 工作不主要为了钱，但钱很少，也从一个侧面反映这不是什么能创造大价值的工作。 有闲 不能工作量大到每天加班到很晚，周末至少能好好休息一天。 有成长 工作本身价值高、锻炼人；公司属于行业里专业性很强的，可以学到东西；老板、领导愿意培养你，等等。 如果一份工作，能同时满足这三点，那一定是很好的工作；如果一份工作，一点都满足不了，那你还留在那耗什么？ 其中第二条有闲非常重要。之前有个同学就跟我吐槽，他说老板经常说的一句话是工作是最好的修行，然后鼓励大家多加班，说那些不怎么加班的员工没有上进心之类的。同学说了很重要一点：我下班就走了，可能有两种状态，一种状态可能是我下班回家去打游戏了、追剧了、刷抖音了，另一种状态是我可能每天下班后都回家学习去了，读书、听课、提升工作技能等。我很同意这种说法，并不是每天加班到凌晨才离开办公室的员工才有上进心，你根本不知道一个人下班后去干嘛了。 有可能半年后，那个下了班就走的人，表现的比那个天天加班的更好。如果一份工作，钱没有特别多，工作本身也不能给你很大成长，但是你有很多自己的时间，这种工作在很多时候也是好的。因为你可以在自己能力不够强的时候，先领着这份并非高薪但能满足基本生活的工资，在每天快速完成工作后，疯狂的自我成长。 直到有一天，你有实力找到一份工资更高、价值更大的工作。如果一份工作收入可观，工作价值也可以，但每天忙得要死，没有时间让你持续有增量成长，你就要小心了。因为你虽然现在月薪1万，但没有成长的话你明年、后年可能还是月薪1万，更没机会升职。一份工作，增长性是最重要的，不要让自己混成一颗钉在一个位置上的零件。如果一份工作本身价值不够，钱又不多，还每天忙得要死，根本没时间成长，你就很危险了。你很可能会长期在最基层工作很难跳脱，甚至久而久之，你习惯了那状态，丧失了成长的能力。一个人不怕现状不好，怕的是习惯了，心死了，这是废掉一个人最隐蔽的方式。 成长这件事，一定是贯穿整个职业生涯的，甚至是一辈子的事。任何时候，都不能让自己忙到连成长的时间都没有，你现在再强，如果你的成长是停滞的，你也很快被别人淘汰。因为别人在成长。 要有增量思维，持续有增量成长，而不是一直消耗存量。]]></content>
      <categories>
        <category>美文</category>
      </categories>
      <tags>
        <tag>美文</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Future 任务机制和 FutureTask]]></title>
    <url>%2F2019%2F03%2F23%2FFuture%20%E4%BB%BB%E5%8A%A1%E6%9C%BA%E5%88%B6%E5%92%8C%20FutureTask%2F</url>
    <content type="text"><![CDATA[Future 任务机制和 FutureTask前言今天在完成功能的时候，使用到 Future 在这里记录一下，自己所了解的到知识，希望可以帮到需要的朋友。 Future 类Future 类就是对于具体的 Runnable 或者 Callable 任务的执行结果进行取消、查询是否已经完成、获取结果。必要时可以通过 get 方法获取执行结果，该方法会阻塞直到任务返回结果。Future 位于 java.util.concurren 包下，它也是一个接口，如下： 123456789101112131415161718192021222324public interface Future&lt;V&gt; &#123; /* * 用来取消任务,如果取消任务成功，则返回 true，失败则返回 false 。参数 mayInterrypIfRunning * 表示是否允许取消正在执行却没有执行完毕的任务，如果设置 true ，则表示可以取消正在执行中的任务 。 * 如果任务已经完成，则无论 mayInterruptIfRunning 为 ture 还是 false ，都返回 false，即如果 * 取消已经完成的任务会返回 false ；如果任务正在执行，若 mayInterrupIfRunning 设置为 true 则 * 返回 true ，设置为 false 则返回 false；如果任务还没有执行，都返回false。 */ boolean cancel(boolean mayInterruptIfRunning); /* * 表示任务是否已经完成，若任务完成则返回 true */ boolean isDone(); /* * 用来获取执行结果，这个方法会产生阻塞，会一直等到任务执行完毕才返回。 */ V get() throws InterruptedException,ExecutionException; /* * 用来获取执行结果，如果在指定时间内，还没有获取到返回结果，就直接返回 null */ V get(long timeout,TimeUnit unit) throws InterruptedException,ExecutionException,TimeoutException; &#125; 也就是说 Future 提供了三种功能： 判断任务是否完成。 能够中断任务。 能够获取任务执行结果。 FutureTask 类因为 Future 只是一个接口，所以无法直接用来创建对象使用的，因此就有了 FutureTask 。 FutureTask 目前是 Future 接口的一个唯一实现类： FutureTask 类 123public class FutureTask&lt;V&gt; implements RunnableFuture&lt;V&gt; &#123; ...&#125; RunableFuture 类 123public interface RunnableFuture&lt;V&gt; extends Runnable, Future&lt;V&gt; &#123; void run();&#125; 可以看出 RunnableFuture 继承了 Runnable 和 Future 接口，而 FutureTask 实现了 RunnableFuture 接口。所以 FutureTask 既可以作为 Runnable 被线程执行，又可以作为 Future 得到 Callable 的返回值。 FutureTask 提供了2个构造器： 123456789101112// 创建一个 FutureTask ，一旦运行就执行给定的 Callablepublic FutureTask(Callable&lt;V&gt; callable) &#123; if (callable == null) throw new NullPointerException(); this.callable = callable; this.state = NEW; // ensure visibility of callable&#125;// 创建一个 FutureTask ，一旦运行就执行给定的 Runnable ，并安排成功时 get 返回给定的结果。public FutureTask(Runnable runnable, V result) &#123; this.callable = Executors.callable(runnable, result); this.state = NEW; // ensure visibility of callable&#125; 使用场景在实际工作中，可能需要统计各种类型的报表呈现结果，可能一个大的报表需要依赖很多很小的模块的运算结果，一个线程做可能比较慢，就可拆分成 N 多个小线程，然后将结果合并起来作为大的报表呈现结果。Fork/Join 就是基于 Future 实现的]]></content>
      <categories>
        <category>多线程</category>
      </categories>
      <tags>
        <tag>多线程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[lucene搜索之索引的查询原理和查询工具类（支持分页）示例]]></title>
    <url>%2F2019%2F03%2F16%2FLucene%2F8%E3%80%81lucene%E6%90%9C%E7%B4%A2%E4%B9%8B%E7%B4%A2%E5%BC%95%E7%9A%84%E6%9F%A5%E8%AF%A2%E5%8E%9F%E7%90%86%E5%92%8C%E6%9F%A5%E8%AF%A2%E5%B7%A5%E5%85%B7%E7%B1%BB%EF%BC%88%E6%94%AF%E6%8C%81%E5%88%86%E9%A1%B5%EF%BC%89%E7%A4%BA%E4%BE%8B%2F</url>
    <content type="text"><![CDATA[lucene（8）—lucene搜索之索引的查询原理和查询工具类（支持分页）示例IndexSearcher常用方法IndexSearcher提供了几个常用的方法： IndexSearcher.doc(int docID) 获取索引文件中的第n个索引存储的相关字段，返回为Document类型，可以据此读取document中的Field.STORE.YES的字段； IndexSearcher.doc(int docID, StoredFieldVisitor fieldVisitor) 获取StoredFieldVisitor指定的字段的document，StoredFieldVisitor定义如下 1StoredFieldVisitor visitor = new DocumentStoredFieldVisitor(String... fields); IndexSearcher.doc(int docID, Set fieldsToLoad) 此方法同上边的IndexSearcher.doc(int docID, StoredFieldVisitor fieldVisitor) ，其实现如下图 IndexSearcher.count(Query query) 统计符合query条件的document个数 IndexSearcher.searchAfter(final ScoreDoc after, Query query, int numHits) 此方法会返回符合query查询条件的且在after之后的numHits条记录； 其实现原理为： 先读取当前索引文件的最大数据条数limit，然后判断after是否为空和after对应的document的下标是否超出limit的限制，如果超出的话抛出非法的参数异常； 设置读取的条数为numHits和limit中最小的（因为有超出最大条数的可能，避免超出限制而造成的异常） 接下来创建一个CollectorManager类型的对象，该对象定义了要返回的TopDocs的个数，上一页的document的结尾（after）,并且对查询结果进行分析合并 最后调用search(query,manager)来查询结果 IndexSearcher.search(Query query, int n) 查询符合query条件的前n个记录 IndexSearcher.search(Query query, Collector results) 查询符合collector的记录，collector定义了分页等信息 IndexSearcher.search(Query query, int n,Sort sort, boolean doDocScores, boolean doMaxScore) 实现任意排序的查询，同时控制是否计算hit score和max score是否被计算在内，查询前n条符合query条件的document; IndexSearcher.search(Query query, CollectorManager&lt;C, T&gt; collectorManager) 利用给定的collectorManager获取符合query条件的结果，其执行流程如下： 先判断是否有ExecutorService执行查询的任务，如果没有executor，IndexSearcher会在单个任务下进行查询操作； 如果IndexSearcher有executor，则会由每个线程控制一部分索引的读取，而且查询的过程中采用的是future机制，此种方式是边读边往结果集里边追加数据，这种异 步的处理机制也提升了效率，其执行过程如下： 编码实践我中午的时候写了一个SearchUtil的工具类，里边添加了多目录查询和分页查询的功能，经测试可用，工具类和测试的代码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156package com.lucene.search.util;import java.io.File;import java.io.IOException;import java.nio.file.Paths;import java.util.Set;import java.util.concurrent.ExecutorService;import org.apache.lucene.document.Document;import org.apache.lucene.index.DirectoryReader;import org.apache.lucene.index.IndexReader;import org.apache.lucene.index.MultiReader;import org.apache.lucene.search.BooleanQuery;import org.apache.lucene.search.IndexSearcher;import org.apache.lucene.search.Query;import org.apache.lucene.search.ScoreDoc;import org.apache.lucene.search.TopDocs;import org.apache.lucene.search.BooleanClause.Occur;import org.apache.lucene.store.FSDirectory; /**lucene索引查询工具类 * @author lenovo */public class SearchUtil &#123; /**获取IndexSearcher对象 * @param indexPath * @param service * @return * @throws IOException */ public static IndexSearcher getIndexSearcherByParentPath(String parentPath,ExecutorService service) throws IOException&#123; MultiReader reader = null; //设置 try &#123; File[] files = new File(parentPath).listFiles(); IndexReader[] readers = new IndexReader[files.length]; for (int i = 0 ; i &lt; files.length ; i ++) &#123; readers[i] = DirectoryReader.open(FSDirectory.open(Paths.get(files[i].getPath(), new String[0]))); &#125; reader = new MultiReader(readers); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; return new IndexSearcher(reader,service); &#125; /**根据索引路径获取IndexReader * @param indexPath * @return * @throws IOException */ public static DirectoryReader getIndexReader(String indexPath) throws IOException&#123; return DirectoryReader.open(FSDirectory.open(Paths.get(indexPath, new String[0]))); &#125; /**根据索引路径获取IndexSearcher * @param indexPath * @param service * @return * @throws IOException */ public static IndexSearcher getIndexSearcherByIndexPath(String indexPath,ExecutorService service) throws IOException&#123; IndexReader reader = getIndexReader(indexPath); return new IndexSearcher(reader,service); &#125; /**如果索引目录会有变更用此方法获取新的IndexSearcher这种方式会占用较少的资源 * @param oldSearcher * @param service * @return * @throws IOException */ public static IndexSearcher getIndexSearcherOpenIfChanged(IndexSearcher oldSearcher,ExecutorService service) throws IOException&#123; DirectoryReader reader = (DirectoryReader) oldSearcher.getIndexReader(); DirectoryReader newReader = DirectoryReader.openIfChanged(reader); return new IndexSearcher(newReader, service); &#125; /**多条件查询类似于sql in * @param querys * @return */ public static Query getMultiQueryLikeSqlIn(Query ... querys)&#123; BooleanQuery query = new BooleanQuery(); for (Query subQuery : querys) &#123; query.add(subQuery,Occur.SHOULD); &#125; return query; &#125; /**多条件查询类似于sql and * @param querys * @return */ public static Query getMultiQueryLikeSqlAnd(Query ... querys)&#123; BooleanQuery query = new BooleanQuery(); for (Query subQuery : querys) &#123; query.add(subQuery,Occur.MUST); &#125; return query; &#125; /**根据IndexSearcher和docID获取默认的document * @param searcher * @param docID * @return * @throws IOException */ public static Document getDefaultFullDocument(IndexSearcher searcher,int docID) throws IOException&#123; return searcher.doc(docID); &#125; /**根据IndexSearcher和docID * @param searcher * @param docID * @param listField * @return * @throws IOException */ public static Document getDocumentByListField(IndexSearcher searcher,int docID,Set&lt;String&gt; listField) throws IOException&#123; return searcher.doc(docID, listField); &#125; /**分页查询 * @param page 当前页数 * @param perPage 每页显示条数 * @param searcher searcher查询器 * @param query 查询条件 * @return * @throws IOException */ public static TopDocs getScoreDocsByPerPage(int page,int perPage,IndexSearcher searcher,Query query) throws IOException&#123; TopDocs result = null; if(query == null)&#123; System.out.println(" Query is null return null "); return null; &#125; ScoreDoc before = null; if(page != 1)&#123; TopDocs docsBefore = searcher.search(query, (page-1)*perPage); ScoreDoc[] scoreDocs = docsBefore.scoreDocs; if(scoreDocs.length &gt; 0)&#123; before = scoreDocs[scoreDocs.length - 1]; &#125; &#125; result = searcher.searchAfter(before, query, perPage); return result; &#125; public static TopDocs getScoreDocs(IndexSearcher searcher,Query query) throws IOException&#123; TopDocs docs = searcher.search(query, getMaxDocId(searcher)); return docs; &#125; /**统计document的数量,此方法等同于matchAllDocsQuery查询 * @param searcher * @return */ public static int getMaxDocId(IndexSearcher searcher)&#123; return searcher.getIndexReader().maxDoc(); &#125; &#125; 相关测试代码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051package com.lucene.index.test;import java.io.IOException;import java.util.HashSet;import java.util.Set;import java.util.concurrent.ExecutorService;import java.util.concurrent.Executors;import org.apache.lucene.document.Document;import org.apache.lucene.index.Term;import org.apache.lucene.search.IndexSearcher;import org.apache.lucene.search.Query;import org.apache.lucene.search.ScoreDoc;import org.apache.lucene.search.TermQuery;import org.apache.lucene.search.TopDocs;import com.lucene.search.util.SearchUtil; public class TestSearch &#123; public static void main(String[] args) &#123; ExecutorService service = Executors.newCachedThreadPool(); try &#123; IndexSearcher searcher = SearchUtil.getIndexSearcherByParentPath("index",service); System.out.println(SearchUtil.getMaxDocId(searcher)); Term term = new Term("content", "lucene"); Query query = new TermQuery(term); TopDocs docs = SearchUtil.getScoreDocsByPerPage(2, 20, searcher, query); ScoreDoc[] scoreDocs = docs.scoreDocs; System.out.println("所有的数据总数为："+docs.totalHits); System.out.println("本页查询到的总数为："+scoreDocs.length); for (ScoreDoc scoreDoc : scoreDocs) &#123; Document doc = SearchUtil.getDefaultFullDocument(searcher, scoreDoc.doc); //System.out.println(doc); &#125; System.out.println("\n\n"); TopDocs docsAll = SearchUtil.getScoreDocs(searcher, query); Set&lt;String&gt; fieldSet = new HashSet&lt;String&gt;(); fieldSet.add("path"); fieldSet.add("modified"); for (int i = 0 ; i &lt; 20 ; i ++) &#123; Document doc = SearchUtil.getDocumentByListField(searcher, docsAll.scoreDocs[i].doc,fieldSet); System.out.println(doc); &#125; &#125; catch (IOException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125;finally&#123; service.shutdownNow(); &#125; &#125; &#125; 代码下载代码下载请点击http://download.csdn.net/detail/wuyinggui10000/8703165，运行时请先运行IndexTest类进行索引的创建~！]]></content>
      <categories>
        <category>lucene</category>
      </categories>
      <tags>
        <tag>lucene</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[lucene搜索之拼写检查和相似度查询提示（spellcheck）]]></title>
    <url>%2F2019%2F03%2F16%2FLucene%2F9%E3%80%81lucene%E6%90%9C%E7%B4%A2%E4%B9%8B%E6%8B%BC%E5%86%99%E6%A3%80%E6%9F%A5%E5%92%8C%E7%9B%B8%E4%BC%BC%E5%BA%A6%E6%9F%A5%E8%AF%A2%E6%8F%90%E7%A4%BA%EF%BC%88spellcheck%EF%BC%89%2F</url>
    <content type="text"><![CDATA[lucene（9）—lucene搜索之拼写检查和相似度查询提示（spellcheck）suggest应用场景用户的输入行为是不确定的，而我们在写程序的时候总是想让用户按照指定的内容或指定格式的内容进行搜索，这里就要进行人工干预用户输入的搜索条件了；我们在用百度谷歌等搜索引擎的时候经常会看到按键放下的时候直接会提示用户是否想搜索某些相关的内容，恰好lucene在开发的时候想到了这一点，lucene提供的suggest包正是用来解决上述问题的。 suggest包联想词相关介绍suggest包提供了lucene的自动补全或者拼写检查的支持； 拼写检查相关的类在org.apache.lucene.search.spell包下； 联想相关的在org.apache.lucene.search.suggest包下； 基于联想词分词相关的类在org.apache.lucene.search.suggest.analyzing包下； 拼写检查原理Lucene的拼写检查由org.apache.lucene.search.spell.SpellChecker类提供支持；SpellChecker设置了默认精度0.5，如果我们需要细粒度的支持可以通过调用setAccuracy(float accuracy)来设定；spellChecker会将外部来源的词进行索引；这些来源包括： DocumentDictionary查询document中的field对应的值； FileDictionary基于一个文本文件的Directionary,每行一项，词组之间以&quot;\t&quot; TAB分隔符进行，每项中不能含有两个以上的分隔符； HighFrequencyDictionary从原有的索引文件中读取某个term的值，并按照出现次数检查； LuceneDictionary也是从原有索引文件中读取某个term的值，但是不检查出现次数； PlainTextDictionary从文本中读取内容，按行读取，没有分隔符； 其索引的原理如下： 对索引过程加syschronized同步；检查Spellchecker是否已经关闭，如果关闭，抛出异常，提示内容为：Spellchecker has been closed；对外部来源的索引进行遍历，统计被遍历的词的长度，如果长度小于三，忽略该词，反之构建document对象并索引到本地文件，创建索引的时候会对每个单词进行详细拆分（对应addGram方法），其执行过程如下所示 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667/**- Indexes the data from the given &#123;@link Dictionary&#125;.- @param dict Dictionary to index- @param config &#123;@link IndexWriterConfig&#125; to use- @param fullMerge whether or not the spellcheck index should be fully merged- @throws AlreadyClosedException if the Spellchecker is already closed- @throws IOException If there is a low-level I/O error. */public final void indexDictionary(Dictionary dict, IndexWriterConfig config, boolean fullMerge) throws IOException &#123;synchronized (modifyCurrentIndexLock) &#123; ensureOpen(); final Directory dir = this.spellIndex; final IndexWriter writer = new IndexWriter(dir, config); IndexSearcher indexSearcher = obtainSearcher(); final List&lt;TermsEnum&gt; termsEnums = new ArrayList&lt;&gt;(); final IndexReader reader = searcher.getIndexReader(); if (reader.maxDoc() &gt; 0) &#123; for (final LeafReaderContext ctx : reader.leaves()) &#123; Terms terms = ctx.reader().terms(F_WORD); if (terms != null) termsEnums.add(terms.iterator(null)); &#125; &#125; boolean isEmpty = termsEnums.isEmpty(); try &#123; BytesRefIterator iter = dict.getEntryIterator(); BytesRef currentTerm; terms: while ((currentTerm = iter.next()) != null) &#123; String word = currentTerm.utf8ToString(); int len = word.length(); if (len &lt; 3) &#123; continue; // too short we bail but "too long" is fine... &#125; if (!isEmpty) &#123; for (TermsEnum te : termsEnums) &#123; if (te.seekExact(currentTerm)) &#123; continue terms; &#125; &#125; &#125; // ok index the word Document doc = createDocument(word, getMin(len), getMax(len)); writer.addDocument(doc); &#125; &#125; finally &#123; releaseSearcher(indexSearcher); &#125; if (fullMerge) &#123; writer.forceMerge(1); &#125; // close writer writer.close(); // TODO: this isn't that great, maybe in the future SpellChecker should take // IWC in its ctor / keep its writer open? // also re-open the spell index to see our own changes when the next suggestion // is fetched: swapSearcher(dir);&#125;&#125; 对词语进行遍历拆分的方法为addGram,其实现为： 查看代码可知，联想词的索引不仅关注每个词的起始位置，也关注其倒数的位置； 联想词查询的时候，先判断grams里边是否包含有待查询的词拆分后的内容，如果有放到结果SuggestWordQueue中，最终结果为遍历SuggestWordQueue得来的String[],其代码实现如下：​ 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051public String[] suggestSimilar(String word, int numSug, IndexReader ir,String field, SuggestMode suggestMode, float accuracy) throws IOException &#123; // obtainSearcher calls ensureOpen final IndexSearcher indexSearcher = obtainSearcher(); try &#123; if (ir == null || field == null) &#123; suggestMode = SuggestMode.SUGGEST_ALWAYS; &#125; if (suggestMode == SuggestMode.SUGGEST_ALWAYS) &#123; ir = null; field = null; &#125;final int lengthWord = word.length(); final int freq = (ir != null &amp;&amp; field != null) ? ir.docFreq(new Term(field, word)) : 0; final int goalFreq = suggestMode==SuggestMode.SUGGEST_MORE_POPULAR ? freq : 0; // if the word exists in the real index and we don't care for word frequency, return the word itself if (suggestMode==SuggestMode.SUGGEST_WHEN_NOT_IN_INDEX &amp;&amp; freq &gt; 0) &#123; return new String[] &#123; word &#125;; &#125; BooleanQuery query = new BooleanQuery(); String[] grams; String key; for (int ng = getMin(lengthWord); ng &lt;= getMax(lengthWord); ng++) &#123; key = "gram" + ng; // form key grams = formGrams(word, ng); // form word into ngrams (allow dups too) if (grams.length == 0) &#123; continue; // hmm &#125; if (bStart &gt; 0) &#123; // should we boost prefixes? add(query, "start" + ng, grams[0], bStart); // matches start of word &#125; if (bEnd &gt; 0) &#123; // should we boost suffixes add(query, "end" + ng, grams[grams.length - 1], bEnd); // matches end of word &#125; for (int i = 0; i &lt; grams.length; i++) &#123; add(query, key, grams[i]); &#125; &#125; int maxHits = 10 * numSug; // System.out.println("Q: " + query); ScoreDoc[] hits = indexSearcher.search(query, maxHits).scoreDocs; // System.out.println("HITS: " + hits.length()); SuggestWordQueue sugQueue = new SuggestWordQueue(numSug, comparator); 1234567891011121314151617181920212223242526272829303132333435363738394041424344 // go thru more than 'maxr' matches in case the distance filter triggers int stop = Math.min(hits.length, maxHits); SuggestWord sugWord = new SuggestWord(); for (int i = 0; i &lt; stop; i++) &#123; sugWord.string = indexSearcher.doc(hits[i].doc).get(F_WORD); // get orig word // don't suggest a word for itself, that would be silly if (sugWord.string.equals(word)) &#123; continue; &#125; // edit distance sugWord.score = sd.getDistance(word,sugWord.string); if (sugWord.score &lt; accuracy) &#123; continue; &#125; if (ir != null &amp;&amp; field != null) &#123; // use the user index sugWord.freq = ir.docFreq(new Term(field, sugWord.string)); // freq in the index // don't suggest a word that is not present in the field if ((suggestMode==SuggestMode.SUGGEST_MORE_POPULAR &amp;&amp; goalFreq &gt; sugWord.freq) || sugWord.freq &lt; 1) &#123; continue; &#125; &#125; sugQueue.insertWithOverflow(sugWord); if (sugQueue.size() == numSug) &#123; // if queue full, maintain the minScore score accuracy = sugQueue.top().score; &#125; sugWord = new SuggestWord(); &#125; // convert to array string String[] list = new String[sugQueue.size()]; for (int i = sugQueue.size() - 1; i &gt;= 0; i--) &#123; list[i] = sugQueue.pop().string; &#125; return list;&#125; finally &#123; releaseSearcher(indexSearcher);&#125;&#125; 编程实践以下是我根据FileDirectory相关描述编写的一个测试程序 123456789101112131415161718192021222324252627282930313233343536373839404142434445package com.lucene.search; import java.io.File;import java.io.FileInputStream;import java.io.IOException;import java.nio.file.Paths; import org.apache.lucene.analysis.Analyzer;import org.apache.lucene.index.IndexWriterConfig;import org.apache.lucene.index.IndexWriterConfig.OpenMode;import org.apache.lucene.search.spell.SpellChecker;import org.apache.lucene.search.suggest.FileDictionary;import org.apache.lucene.store.Directory;import org.apache.lucene.store.FSDirectory;import org.wltea.analyzer.lucene.IKAnalyzer; public class SuggestUtil &#123; public static void main(String[] args) &#123; Directory spellIndexDirectory; try &#123; spellIndexDirectory = FSDirectory.open(Paths.get("suggest", new String[0])); SpellChecker spellchecker = new SpellChecker(spellIndexDirectory ); Analyzer analyzer = new IKAnalyzer(true); IndexWriterConfig config = new IndexWriterConfig(analyzer); config.setOpenMode(OpenMode.CREATE_OR_APPEND); spellchecker.setAccuracy(0f); //HighFrequencyDictionary dire = new HighFrequencyDictionary(reader, field, thresh) spellchecker.indexDictionary(new FileDictionary(new FileInputStream(new File("D:\\hadoop\\lucene_suggest\\src\\suggest.txt"))),config,false); String[] similars = spellchecker.suggestSimilar("中国", 10); for (String similar : similars) &#123; System.out.println(similar); &#125; spellIndexDirectory.close(); spellchecker.close(); &#125; catch (IOException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125; &#125;&#125; 其中，我用的suggest.txt内容为： 123456中国人民 100奔驰3 101奔驰中国 102奔驰S级 103奔驰A级 104奔驰C级 105 测试结果为： 12中国人民奔驰中国]]></content>
      <categories>
        <category>lucene</category>
      </categories>
      <tags>
        <tag>lucene</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[lucene搜索之IndexSearcher构建过程]]></title>
    <url>%2F2019%2F03%2F16%2FLucene%2F7%E3%80%81lucene%E6%90%9C%E7%B4%A2%E4%B9%8BIndexSearcher%E6%9E%84%E5%BB%BA%E8%BF%87%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[lucene（7）—lucene搜索之IndexSearcher构建过程IndexSearcher搜索引擎的构建分为索引内容和查询索引两个大方面，这里要介绍的是lucene索引查询器即IndexSearcher的构建过程； 首先了解下IndexSearcher： IndexSearcher提供了对单个IndexReader的查询实现； 我们对索引的查询，可以通过调用search(Query,n)或者search(Query,Filter,n)方法； 在索引内容变动不大的情况下，我们可以对索引的搜索采用单个IndexSearcher共享的方式来提升性能； 如果索引有变动，我们就需要使用DirectoryReader.openIfChanged(DirectoryReader)来获取新的reader，然后创建新的IndexSearcher对象； 为了使查询延迟率低，我们最好使用近实时搜索的方法（此时我们的DirectoryReader的构建就要采用DirectoryReader.open(IndexWriter, boolean)） IndexSearcher实例是完全线程安全的,这意味着多个线程可以并发调用任何方法。如果需要外部同步,无需添加IndexSearcher的同步； IndexSearcher的创建过程 根据索引文件路径创建FSDirectory的实例，返回的FSDirectory实例跟系统或运行环境有关，对于Linux, MacOSX, Solaris, and Windows 64-bit JREs返回的是一个MMapDirectory实例，对于其他非windows JREs环境返回的是NIOFSDirectory，而对于其他Windows的JRE环境返回的是SimpleFSDirectory，其执行效率依次降低 接着DirectoryReader根据获取到的FSDirectory实例读取索引文件并得到DirectoryReader对象；DirectoryReader的open方法返回实例的原理：读取索引目录中的Segments文件内容，倒序遍历SegmentInfos并填充到SegmentReader（IndexReader的一种实现）数组，并构建StandardDirectoryReader的实例 有了IndexReader，IndexSearcher对象实例化就手到拈来了，new IndexSearcher(DirectoryReader)就可以得到其实例；如果我们想提高IndexSearcher的执行效率可以new IndexSearcher(DirecotoryReader,ExcuterService)来创建IndexSearcher对象，这样做的好处为对每块segment采用了分工查询，但是要注意IndexSearcher并不维护ExcuterService的生命周期，我们还需要自行调用ExcuterService的close/awaitTermination 相关实践以下是根据IndexSearcher相关的构建过程及其特性编写的一个搜索的工具类 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223package com.lucene.search;import java.io.File;import java.io.IOException;import java.nio.file.Paths;import java.util.concurrent.ExecutorService;import org.apache.lucene.analysis.Analyzer;import org.apache.lucene.analysis.standard.StandardAnalyzer;import org.apache.lucene.document.Document;import org.apache.lucene.index.DirectoryReader;import org.apache.lucene.index.IndexReader;import org.apache.lucene.index.MultiReader;import org.apache.lucene.index.Term;import org.apache.lucene.search.BooleanClause.Occur;import org.apache.lucene.search.BooleanQuery;import org.apache.lucene.search.IndexSearcher;import org.apache.lucene.search.MatchAllDocsQuery;import org.apache.lucene.search.NumericRangeQuery;import org.apache.lucene.search.Query;import org.apache.lucene.search.ScoreDoc;import org.apache.lucene.search.Sort;import org.apache.lucene.search.SortField;import org.apache.lucene.search.SortField.Type;import org.apache.lucene.search.TermQuery;import org.apache.lucene.search.TopDocs;import org.apache.lucene.search.TopFieldCollector;import org.apache.lucene.store.FSDirectory;import com.lucene.index.IndexUtil;public class SearchUtil &#123; public static final Analyzer analyzer = new StandardAnalyzer(); /**获取IndexSearcher对象（适合单索引目录查询使用） * @param indexPath 索引目录 * @return * @throws IOException * @throws InterruptedException */ public static IndexSearcher getIndexSearcher(String indexPath,ExecutorService service,boolean realtime) throws IOException, InterruptedException&#123; DirectoryReader reader = DirectoryReader.open(IndexUtil.getIndexWriter(indexPath, true), realtime); IndexSearcher searcher = new IndexSearcher(reader,service); if(service != null)&#123; service.shutdown(); &#125; return searcher; &#125; /**多目录多线程查询 * @param parentPath 父级索引目录 * @param service 多线程查询 * @return * @throws IOException * @throws InterruptedException */ public static IndexSearcher getMultiSearcher(String parentPath,ExecutorService service,boolean realtime) throws IOException, InterruptedException&#123; MultiReader multiReader; File file = new File(parentPath); File[] files = file.listFiles(); IndexReader[] readers = new IndexReader[files.length]; if(!realtime)&#123; for (int i = 0 ; i &lt; files.length ; i ++) &#123; readers[i] = DirectoryReader.open(FSDirectory.open(Paths.get(files[i].getPath(), new String[0]))); &#125; &#125;else&#123; for (int i = 0 ; i &lt; files.length ; i ++) &#123; readers[i] = DirectoryReader.open(IndexUtil.getIndexWriter(files[i].getPath(), true), true); &#125; &#125; multiReader = new MultiReader(readers); IndexSearcher searcher = new IndexSearcher(multiReader,service); if(service != null)&#123; service.shutdown(); &#125; return searcher; &#125; /**从指定配置项中查询 * @return * @param analyzer 分词器 * @param field 字段 * @param fieldType 字段类型 * @param queryStr 查询条件 * @param range 是否区间查询 * @return */ public static Query getQuery(String field,String fieldType,String queryStr,boolean range)&#123; Query q = null; if(queryStr != null &amp;&amp; !"".equals(queryStr))&#123; if(range)&#123; String[] strs = queryStr.split("\\|"); if("int".equals(fieldType))&#123; int min = new Integer(strs[0]); int max = new Integer(strs[1]); q = NumericRangeQuery.newIntRange(field, min, max, true, true); &#125;else if("double".equals(fieldType))&#123; Double min = new Double(strs[0]); Double max = new Double(strs[1]); q = NumericRangeQuery.newDoubleRange(field, min, max, true, true); &#125;else if("float".equals(fieldType))&#123; Float min = new Float(strs[0]); Float max = new Float(strs[1]); q = NumericRangeQuery.newFloatRange(field, min, max, true, true); &#125;else if("long".equals(fieldType))&#123; Long min = new Long(strs[0]); Long max = new Long(strs[1]); q = NumericRangeQuery.newLongRange(field, min, max, true, true); &#125; &#125;else&#123; if("int".equals(fieldType))&#123; q = NumericRangeQuery.newIntRange(field, new Integer(queryStr), new Integer(queryStr), true, true); &#125;else if("double".equals(fieldType))&#123; q = NumericRangeQuery.newDoubleRange(field, new Double(queryStr), new Double(queryStr), true, true); &#125;else if("float".equals(fieldType))&#123; q = NumericRangeQuery.newFloatRange(field, new Float(queryStr), new Float(queryStr), true, true); &#125;else&#123; Term term = new Term(field, queryStr); q = new TermQuery(term); &#125; &#125; &#125;else&#123; q= new MatchAllDocsQuery(); &#125; System.out.println(q); return q; &#125; /**多条件查询类似于sql in * @param querys * @return */ public static Query getMultiQueryLikeSqlIn(Query ... querys)&#123; BooleanQuery query = new BooleanQuery(); for (Query subQuery : querys) &#123; query.add(subQuery,Occur.SHOULD); &#125; return query; &#125; /**多条件查询类似于sql and * @param querys * @return */ public static Query getMultiQueryLikeSqlAnd(Query ... querys)&#123; BooleanQuery query = new BooleanQuery(); for (Query subQuery : querys) &#123; query.add(subQuery,Occur.MUST); &#125; return query; &#125; /**对多个条件进行排序构建排序条件 * @param fields * @param type * @param reverses * @return */ public static Sort getSortInfo(String[] fields,Type[] types,boolean[] reverses)&#123; SortField[] sortFields = null; int fieldLength = fields.length; int typeLength = types.length; int reverLength = reverses.length; if(!(fieldLength == typeLength) || !(fieldLength == reverLength))&#123; return null; &#125;else&#123; sortFields = new SortField[fields.length]; for (int i = 0; i &lt; fields.length; i++) &#123; sortFields[i] = new SortField(fields[i], types[i], reverses[i]); &#125; &#125; return new Sort(sortFields); &#125; /**根据查询器、查询条件、每页数、排序条件进行查询 * @param query 查询条件 * @param first 起始值 * @param max 最大值 * @param sort 排序条件 * @return */ public static TopDocs getScoreDocsByPerPageAndSortField(IndexSearcher searcher,Query query, int first,int max, Sort sort)&#123; try &#123; if(query == null)&#123; System.out.println(" Query is null return null "); return null; &#125; TopFieldCollector collector = null; if(sort != null)&#123; collector = TopFieldCollector.create(sort, first+max, false, false, false); &#125;else&#123; sort = new Sort(new SortField[]&#123;new SortField("modified", SortField.Type.LONG)&#125;); collector = TopFieldCollector.create(sort, first+max, false, false, false); &#125; searcher.search(query, collector); return collector.topDocs(first, max); &#125; catch (IOException e) &#123; // TODO Auto-generated catch block &#125; return null; &#125; /**获取上次索引的id,增量更新使用 * @return */ public static Integer getLastIndexBeanID(IndexReader multiReader)&#123; Query query = new MatchAllDocsQuery(); IndexSearcher searcher = null; searcher = new IndexSearcher(multiReader); SortField sortField = new SortField("id", SortField.Type.INT,true); Sort sort = new Sort(new SortField[]&#123;sortField&#125;); TopDocs docs = getScoreDocsByPerPageAndSortField(searcher,query, 0, 1, sort); ScoreDoc[] scoreDocs = docs.scoreDocs; int total = scoreDocs.length; if(total &gt; 0)&#123; ScoreDoc scoreDoc = scoreDocs[0]; Document doc = null; try &#123; doc = searcher.doc(scoreDoc.doc); &#125; catch (IOException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125; return new Integer(doc.get("id")); &#125; return 0; &#125;&#125; 相关代码下载http://download.csdn.net/detail/wuyinggui10000/8697451]]></content>
      <categories>
        <category>lucene</category>
      </categories>
      <tags>
        <tag>lucene</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[lucene索引优化之多线程创建索引]]></title>
    <url>%2F2019%2F03%2F16%2FLucene%2F6%E3%80%81lucene%E7%B4%A2%E5%BC%95%E4%BC%98%E5%8C%96%E4%B9%8B%E5%A4%9A%E7%BA%BF%E7%A8%8B%E5%88%9B%E5%BB%BA%E7%B4%A2%E5%BC%95%2F</url>
    <content type="text"><![CDATA[lucene（6）—lucene索引优化之多线程创建索引前面了解到lucene在索引创建的时候一个IndexWriter获取到一个读写锁，这样势在lucene创建大数据量的索引的时候，执行效率低下的问题； 查看前面文档 lucene（5）—lucene的索引构建原理 可以看出，lucene索引的建立，跟以下几点关联很大； 磁盘空间大小，这个直接影响索引的建立，甚至会造成索引写入提示完成，但是没有同步的问题； 索引合并策略的选择，这个类似于sql里边的批量操作，批量操作的数量过多直接影响执行效率，对于lucene来讲，索引合并前是将document放在内存中，因此选择合适的合并策略也可以提升索引的效率； 唯一索引对应的term的选择，lucene索引的创建过程中是先从索引中删除包含相同term的document然后重新添加document到索引中，这里如果term对应的document过多，会占用磁盘IO，同时造成IndexWriter的写锁占用时间延长，相应的执行效率低下； 综上所述，索引优化要保证磁盘空间，同时在term选择上可以以ID等标识来确保唯一性，这样第一条和第三条的风险就规避了； 本文旨在对合并策略和采用多线程创建的方式提高索引的效率； 多线程创建索引，我这边还设计了多目录索引创建，这样避免了同一目录数据量过大索引块合并和索引块重新申请； 废话不多说，这里附上代码，代码示例是读取lucene官网下载并解压的文件夹并给文件信息索引起来 首先定义FileBean来存储文件信息 12345678910111213141516171819202122232425262728package com.lucene.bean; public class FileBean &#123; //路径 private String path; //修改时间 private Long modified; //内容 private String content; public String getPath() &#123; return path; &#125; public void setPath(String path) &#123; this.path = path; &#125; public Long getModified() &#123; return modified; &#125; public void setModified(Long modified) &#123; this.modified = modified; &#125; public String getContent() &#123; return content; &#125; public void setContent(String content) &#123; this.content = content; &#125;&#125; 接下来是一个工具类，用以将文件夹的信息遍历读取并转换成FileBean的集合 12345678910111213141516171819202122232425262728293031323334353637383940package com.lucene.index.util; import java.io.File;import java.io.IOException;import java.nio.file.Files;import java.nio.file.Paths;import java.util.LinkedList;import java.util.List; import com.lucene.bean.FileBean; public class FileUtil &#123; /**读取文件信息和下属文件夹 * @param folder * @return * @throws IOException */ public static List&lt;FileBean&gt; getFolderFiles(String folder) throws IOException &#123; List&lt;FileBean&gt; fileBeans = new LinkedList&lt;FileBean&gt;(); File file = new File(folder); if(file.isDirectory())&#123; File[] files = file.listFiles(); if(files != null)&#123; for (File file2 : files) &#123; fileBeans.addAll(getFolderFiles(file2.getAbsolutePath())); &#125; &#125; &#125;else&#123; FileBean bean = new FileBean(); bean.setPath(file.getAbsolutePath()); bean.setModified(file.lastModified()); bean.setContent(new String(Files.readAllBytes(Paths.get(folder)))); fileBeans.add(bean); &#125; return fileBeans; &#125; &#125; 定义一个公共的用于处理索引的类 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129package com.lucene.index; import java.io.File;import java.io.IOException;import java.text.ParseException;import java.util.List;import java.util.concurrent.CountDownLatch; import org.apache.lucene.index.IndexWriter; public abstract class BaseIndex&lt;T&gt; implements Runnable&#123; /** * 父级索引路径 */ private String parentIndexPath; /** * 索引编写器 */ private IndexWriter writer; private int subIndex; /** * 主线程 */ private final CountDownLatch countDownLatch1; /** *工作线程 */ private final CountDownLatch countDownLatch2; /** * 对象列表 */ private List&lt;T&gt; list; public BaseIndex(IndexWriter writer,CountDownLatch countDownLatch1, CountDownLatch countDownLatch2, List&lt;T&gt; list)&#123; super(); this.writer = writer; this.countDownLatch1 = countDownLatch1; this.countDownLatch2 = countDownLatch2; this.list = list; &#125; public BaseIndex(String parentIndexPath, int subIndex, CountDownLatch countDownLatch1, CountDownLatch countDownLatch2, List&lt;T&gt; list) &#123; super(); this.parentIndexPath = parentIndexPath; this.subIndex = subIndex; try &#123; //多目录索引创建 File file = new File(parentIndexPath+"/index"+subIndex); if(!file.exists())&#123; file.mkdir(); &#125; this.writer = IndexUtil.getIndexWriter(parentIndexPath+"/index"+subIndex, true); &#125; catch (IOException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125;; this.subIndex = subIndex; this.countDownLatch1 = countDownLatch1; this.countDownLatch2 = countDownLatch2; this.list = list; &#125; public BaseIndex(String path,CountDownLatch countDownLatch1, CountDownLatch countDownLatch2, List&lt;T&gt; list) &#123; super(); try &#123; //单目录索引创建 File file = new File(path); if(!file.exists())&#123; file.mkdir(); &#125; this.writer = IndexUtil.getIndexWriter(path,true); &#125; catch (IOException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125;; this.countDownLatch1 = countDownLatch1; this.countDownLatch2 = countDownLatch2; this.list = list; &#125; /**创建索引 * @param writer * @param carSource * @param create * @throws IOException * @throws ParseException */ public abstract void indexDoc(IndexWriter writer,T t) throws Exception; /**批量索引创建 * @param writer * @param t * @throws Exception */ public void indexDocs(IndexWriter writer,List&lt;T&gt; t) throws Exception&#123; for (T t2 : t) &#123; indexDoc(writer,t2); &#125; &#125; @Override public void run() &#123; try &#123; countDownLatch1.await(); System.out.println(writer); indexDocs(writer,list); &#125; catch (InterruptedException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125; catch (Exception e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125;finally&#123; countDownLatch2.countDown(); try &#123; writer.commit(); writer.close(); &#125; catch (IOException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125; &#125; &#125;&#125; FileBeanIndex类用于处理FileBean的索引创建 123456789101112131415161718192021222324252627282930313233343536373839404142package com.lucene.index; import java.util.List;import java.util.concurrent.CountDownLatch; import org.apache.lucene.document.Document;import org.apache.lucene.document.Field;import org.apache.lucene.document.LongField;import org.apache.lucene.document.StringField;import org.apache.lucene.document.TextField;import org.apache.lucene.index.IndexWriter;import org.apache.lucene.index.IndexWriterConfig;import org.apache.lucene.index.Term; import com.lucene.bean.FileBean; public class FileBeanIndex extends BaseIndex&lt;FileBean&gt;&#123; public FileBeanIndex(IndexWriter writer, CountDownLatch countDownLatch1, CountDownLatch countDownLatch2, List&lt;FileBean&gt; list) &#123; super(writer, countDownLatch1, countDownLatch2, list); &#125; public FileBeanIndex(String parentIndexPath, int subIndex, CountDownLatch countDownLatch1, CountDownLatch countDownLatch2, List&lt;FileBean&gt; list) &#123; super(parentIndexPath, subIndex, countDownLatch1, countDownLatch2, list); &#125; @Override public void indexDoc(IndexWriter writer, FileBean t) throws Exception &#123; Document doc = new Document(); System.out.println(t.getPath()); doc.add(new StringField("path", t.getPath(), Field.Store.YES)); doc.add(new LongField("modified", t.getModified(), Field.Store.YES)); doc.add(new TextField("content", t.getContent(), Field.Store.YES)); if (writer.getConfig().getOpenMode() == IndexWriterConfig.OpenMode.CREATE)&#123; writer.addDocument(doc); &#125;else&#123; writer.updateDocument(new Term("path", t.getPath()), doc); &#125; &#125; &#125; IndexUtil工具类里边设置索引合并的策略 12345678910111213141516171819202122232425262728293031323334353637383940414243package com.lucene.index; import java.io.IOException;import java.nio.file.Paths; import org.apache.lucene.analysis.Analyzer;import org.apache.lucene.analysis.standard.StandardAnalyzer;import org.apache.lucene.index.IndexWriter;import org.apache.lucene.index.IndexWriterConfig;import org.apache.lucene.index.LogByteSizeMergePolicy;import org.apache.lucene.index.LogMergePolicy;import org.apache.lucene.store.Directory;import org.apache.lucene.store.FSDirectory; public class IndexUtil &#123; /**创建索引写入器 * @param indexPath * @param create * @return * @throws IOException */ public static IndexWriter getIndexWriter(String indexPath,boolean create) throws IOException&#123; Directory dir = FSDirectory.open(Paths.get(indexPath, new String[0])); Analyzer analyzer = new StandardAnalyzer(); IndexWriterConfig iwc = new IndexWriterConfig(analyzer); LogMergePolicy mergePolicy = new LogByteSizeMergePolicy(); //设置segment添加文档(Document)时的合并频率 //值较小,建立索引的速度就较慢 //值较大,建立索引的速度就较快,&gt;10适合批量建立索引 mergePolicy.setMergeFactor(50); //设置segment最大合并文档(Document)数 //值较小有利于追加索引的速度 //值较大,适合批量建立索引和更快的搜索 mergePolicy.setMaxMergeDocs(5000); if (create)&#123; iwc.setOpenMode(IndexWriterConfig.OpenMode.CREATE); &#125;else &#123; iwc.setOpenMode(IndexWriterConfig.OpenMode.CREATE_OR_APPEND); &#125; IndexWriter writer = new IndexWriter(dir, iwc); return writer; &#125;&#125; TestIndex类执行测试程序 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950package com.lucene.index.test; import java.util.List;import java.util.concurrent.CountDownLatch;import java.util.concurrent.ExecutorService;import java.util.concurrent.Executors; import org.apache.lucene.index.IndexWriter; import com.lucene.bean.FileBean;import com.lucene.index.FileBeanIndex;import com.lucene.index.util.FileUtil; public class TestIndex &#123; public static void main(String[] args) &#123; try &#123; List&lt;FileBean&gt; fileBeans = FileUtil.getFolderFiles("C:\\Users\\lenovo\\Desktop\\lucene\\lucene-5.1.0"); int totalCount = fileBeans.size(); int perThreadCount = 3000; System.out.println("查询到的数据总数是"+fileBeans.size()); int threadCount = totalCount/perThreadCount + (totalCount%perThreadCount == 0 ? 0 : 1); ExecutorService pool = Executors.newFixedThreadPool(threadCount); CountDownLatch countDownLatch1 = new CountDownLatch(1); CountDownLatch countDownLatch2 = new CountDownLatch(threadCount); System.out.println(fileBeans.size()); for(int i = 0; i &lt; threadCount; i++) &#123; int start = i*perThreadCount; int end = (i+1) * perThreadCount &lt; totalCount ? (i+1) * perThreadCount : totalCount; List&lt;FileBean&gt; subList = fileBeans.subList(start, end); Runnable runnable = new FileBeanIndex("index",i, countDownLatch1, countDownLatch2, subList); //子线程交给线程池管理 pool.execute(runnable); &#125; countDownLatch1.countDown(); System.out.println("开始创建索引"); //等待所有线程都完成 countDownLatch2.await(); //线程全部完成工作 System.out.println("所有线程都创建索引完毕"); //释放线程池资源 pool.shutdown(); &#125; catch (Exception e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125; &#125; &#125; 以上即是多线程多目录索引，大家有什么疑问的欢迎交流；]]></content>
      <categories>
        <category>lucene</category>
      </categories>
      <tags>
        <tag>lucene</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[lucene的索引构建原理]]></title>
    <url>%2F2019%2F03%2F16%2FLucene%2F5%E3%80%81lucene%E7%B4%A2%E5%BC%95%E6%9E%84%E5%BB%BA%E5%8E%9F%E7%90%86%2F</url>
    <content type="text"><![CDATA[lucene（5）—lucene的索引构建原理lucene创建索引的原理IndexWriter的addDocument方法详解今天看了IndexWriter类的addDocument方法，IndexWriter对此方法的说明如下： 12345Adds a document to this index. Note that if an Exception is hit (for example disk full) then the index will be consistent, but this document may not have been added. Furthermore, it&apos;s possible the index will have one segment in non-compound format even when using compound files (when a merge has partially succeeded).This method periodically flushes pending documents to the Directory (see above), and also periodically triggers segment merges in the index according to the MergePolicy in use.Merges temporarily consume space in the directory. The amount of space required is up to 1X the size of all segments being merged, when no readers/searchers are open against the index, and up to 2X the size of all segments being merged when readers/searchers are open against the index (see forceMerge(int) for details). The sequence of primitive merge operations performed is governed by the merge policy. Note that each term in the document can be no longer than MAX_TERM_LENGTH in bytes, otherwise an IllegalArgumentException will be thrown. 大意如下： 此方法向索引中添加一个document； 需要注意的是如果执行过程中发生异常（比如磁盘空间不足）的时候索引会保持一致性，但是这个document也许并没有被添加，此外，即使使用符合文件也有可能索引包含一个非复合格式的segment（当合并索引有部分成功的时候） 此方法会定期的flush索引文件目录，并且会根据合并策略定期去触发索引文件中segment的合并操作； 刚方法会对合并临时的索引空间，当没有reader或者searcher读取或写入索引文件的时候所需要占用的磁盘空间至少要超过需要合并的segments文件的一倍，反之将会占用两倍以上的空间；序列的合并操作的优化取决于合并策略‘ 要确保document中的每一个term占用的字节长度都不能超过MAX_TERM_LENGTH，否则会抛出IllegalArgumentException异常； 其实际的执行方法为： 继续跟进updateDocument方法，其实现如下 可以看见updateDocument是先从索引中删除包含相同term的document然后重新添加document到索引中； 此操作需要确保IndexWriter没有被关闭，其实现是先有DocumentsWriter类的updateDocument方法判断，这里先判断将根据term找到对应的document，并先放到待删除的document队列中，然后从队列中读取document，再将要flush的documents写入磁盘，同时更新flush队列中的索引状态； 相关源码如下 在此期间有一个ThreadState类型的读写锁，lucene判断ThreadState的状态，如果此锁被激活，从内存中获取document并更新到索引文件且重置内存中索引的数量和状态，最后释放相关的资源。 此即为IndexWriter的索引构建过程，看代码晕头转向的，以后为大家带来一点干货，明天带来lucene索引优化之多线程创建索引。]]></content>
      <categories>
        <category>lucene</category>
      </categories>
      <tags>
        <tag>lucene</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[lucene的analysis相关和自定义分词器]]></title>
    <url>%2F2019%2F03%2F16%2FLucene%2F3%E3%80%81lucene%E7%9A%84analysis%E7%9B%B8%E5%85%B3%E5%92%8C%E8%87%AA%E5%AE%9A%E4%B9%89%E5%88%86%E8%AF%8D%E5%99%A8%2F</url>
    <content type="text"><![CDATA[lucene（3）—lucene的analysis相关和自定义分词器analysis说明lucene ananlysis应用场景lucene提供了analysis用来将文本转换到索引文件或提供给IndexSearcher查询索引； 对于lucene而言，不管是索引还是检索，都是针对于纯文本输入来讲的； 通过lucene的强大类库我们可以访问各种格式的文档，如HTML、XML、PDF、Word、TXT等， 我们需要传递给lucene的只是文件中的纯文本内容； lucene的词语切分lucene的索引和检索前提是其对文本内容的分析和词组的切分；比如，文档中有一句话叫“Hello World,Welcome to China” 我们想找到包含这段话的文档，而用户输入的查询条件又不尽详细（可能只是hello） 这里我们就需要用到lucene索引该文档的时候预先对文档内容进行切分，将词源和文本对应起来。 有时候对词语进行简单切分还远远不够，我们还需要对字符串进行深度切分，lucene不仅能够对索引内容预处理还可以对请求参数进行切分； 使用analyzerlucene的索引使用如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546package com.lucene.analysis; import java.io.IOException;import java.io.StringReader; import org.apache.lucene.analysis.Analyzer;import org.apache.lucene.analysis.TokenStream;import org.apache.lucene.analysis.standard.StandardAnalyzer;import org.apache.lucene.analysis.tokenattributes.OffsetAttribute;import org.junit.Test; public class AnalysisTest &#123; @Test public void tokenTest() &#123; Analyzer analyzer = new StandardAnalyzer(); // or any other analyzer TokenStream ts = null; try &#123; ts = analyzer.tokenStream("myfield", new StringReader( "some text goes here")); OffsetAttribute offsetAtt = ts.addAttribute(OffsetAttribute.class); ts.reset(); // Resets this stream to the beginning. (Required) while (ts.incrementToken()) &#123; // Use AttributeSource.reflectAsString(boolean) // for token stream debugging. System.out.println("token: " + ts.reflectAsString(true)); System.out.println("token start offset: " + offsetAtt.startOffset()); System.out.println("token end offset: " + offsetAtt.endOffset()); &#125; ts.end(); &#125; catch (IOException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125; finally &#123; try &#123; ts.close(); &#125; catch (IOException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125; &#125; &#125; &#125; 自定义Analyzer和实现自己的analysis模块1.要实现自己的analyzer，我们需要继承Analyzer并重写其中的分词模块。 2.维护停止词词典 3.重写TokenStreamComponents方法，选择合适的分词方法，对词语进行过滤 示例代码如下 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970package com.lucene.analysis.self; import java.io.IOException; import org.apache.lucene.analysis.Analyzer;import org.apache.lucene.analysis.TokenStream;import org.apache.lucene.analysis.Tokenizer;import org.apache.lucene.analysis.core.LowerCaseTokenizer;import org.apache.lucene.analysis.core.StopAnalyzer;import org.apache.lucene.analysis.core.StopFilter;import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;import org.apache.lucene.analysis.util.CharArraySet; public class MyAnalyzer extends Analyzer &#123; private CharArraySet stopWordSet;//停止词词典 public CharArraySet getStopWordSet() &#123; return stopWordSet; &#125; public void setStopWordSet(CharArraySet stopWordSet) &#123; this.stopWordSet = stopWordSet; &#125; public MyAnalyzer() &#123; super(); this.stopWordSet = StopAnalyzer.ENGLISH_STOP_WORDS_SET;//可在此基础上拓展停止词 &#125; /**扩展停止词 * @param stops */ public MyAnalyzer(String[] stops) &#123; this(); stopWordSet.addAll(StopFilter.makeStopSet(stops)); &#125; @Override protected TokenStreamComponents createComponents(String fieldName) &#123; //正则匹配分词 Tokenizer source = new LowerCaseTokenizer(); return new TokenStreamComponents(source, new StopFilter(source, stopWordSet)); &#125; public static void main(String[] args) &#123; Analyzer analyzer = new MyAnalyzer(); String words = "A AN yuyu"; TokenStream stream = null; try &#123; stream = analyzer.tokenStream("myfield", words); stream.reset(); CharTermAttribute offsetAtt = stream.addAttribute(CharTermAttribute.class); while (stream.incrementToken()) &#123; System.out.println(offsetAtt.toString()); &#125; stream.end(); &#125; catch (IOException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125;finally&#123; try &#123; stream.close(); &#125; catch (IOException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125; &#125; &#125;&#125; 运行结果如下： 1yuyu 说明该分词器对a an 进行了过滤，这些过滤的词在stopWordSet中 添加字长过滤器有时候我们需要对字符串中的短字符进行过滤，比如welcome to BeiJIng中过滤掉长度小于2的字符串，我们期望的结果就变成了Welcome BeiJing,我们仅需要重新实现createComponents方法，相关代码如下: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172package com.lucene.analysis.self; import java.io.IOException; import org.apache.lucene.analysis.Analyzer;import org.apache.lucene.analysis.TokenStream;import org.apache.lucene.analysis.Tokenizer;import org.apache.lucene.analysis.core.LowerCaseTokenizer;import org.apache.lucene.analysis.core.StopAnalyzer;import org.apache.lucene.analysis.core.StopFilter;import org.apache.lucene.analysis.core.WhitespaceTokenizer;import org.apache.lucene.analysis.miscellaneous.LengthFilter;import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;import org.apache.lucene.analysis.util.CharArraySet; public class LengFilterAanlyzer extends Analyzer &#123; private int len; public int getLen() &#123; return len; &#125; public void setLen(int len) &#123; this.len = len; &#125; public LengFilterAanlyzer() &#123; super(); &#125; public LengFilterAanlyzer(int len) &#123; super(); this.len = len; &#125; @Override protected TokenStreamComponents createComponents(String fieldName) &#123; final Tokenizer source = new WhitespaceTokenizer(); TokenStream result = new LengthFilter(source, len, Integer.MAX_VALUE); return new TokenStreamComponents(source,result); &#125; public static void main(String[] args) &#123; Analyzer analyzer = new LengFilterAanlyzer(2); String words = "I am a java coder"; TokenStream stream = null; try &#123; stream = analyzer.tokenStream("myfield", words); stream.reset(); CharTermAttribute offsetAtt = stream.addAttribute(CharTermAttribute.class); while (stream.incrementToken()) &#123; System.out.println(offsetAtt.toString()); &#125; stream.end(); &#125; catch (IOException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125;finally&#123; try &#123; stream.close(); &#125; catch (IOException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125; &#125; &#125;&#125; 程序的执行结果如下： 123amjavacoder 说明小于2个字符的文本被过滤了。]]></content>
      <categories>
        <category>lucene</category>
      </categories>
      <tags>
        <tag>lucene</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[lucene的中文分词器jcseg和IKAnalyzer分词器及其使用说明]]></title>
    <url>%2F2019%2F03%2F16%2FLucene%2F4%E3%80%81lucene%E7%9A%84%E4%B8%AD%E6%96%87%E5%88%86%E8%AF%8D%E5%99%A8jcseg%E5%92%8CIK%20Analyzer%E5%88%86%E8%AF%8D%E5%99%A8%E5%8F%8A%E5%85%B6%E4%BD%BF%E7%94%A8%E8%AF%B4%E6%98%8E%2F</url>
    <content type="text"><![CDATA[lucene（4）—lucene的中文分词器jcseg和IK Analyzer分词器及其使用说明为什么要使用lucene中文分词器在lucene的开发过程中，我们常会遇到分词时中文识别的问题，lucene提供了 lucene-analyzers-common-5.0.0.jar包来支持分词，但多的是对英国，法国，意大利等过语言的支持， 因此我们需要引入中文分词的概念。 各种中文分词器及其对比jcseg中文分词器jcseg是使用Java开发的一款开源的中文分词器, 使用mmseg算法. 分词准确率高达98.4%, 支持中文人名识别, 同义词匹配, 停止词过滤… jcseg支持三种切分模式：(1).简易模式：FMM算法，适合速度要求场合。(2).复杂模式-MMSEG四种过滤算法，具有较高的岐义去除，分词准确率达到了98.41%。(3).检测模式：只返回词库中已有的词条，很适合某些应用场合。(1.9.4开始) 就分词效率而言，简易模式速度最快 jcseg词库配置丰富，自我感觉功能最强大，详见jcseg开发文档； jcseg现版本不兼容lucene5，我修改了其analyzer包，相关示例代码如下 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051package com.lucene.analyzer; import java.io.IOException; import org.apache.lucene.analysis.Analyzer;import org.apache.lucene.analysis.TokenStream;import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;import org.junit.Test;import org.lionsoul.jcseg.analyzer.JcsegAnalyzer5X;import org.lionsoul.jcseg.core.JcsegTaskConfig; public class JcsegAnalyzerTest &#123; @Test public void tokenTest() &#123; Analyzer analyzer = new JcsegAnalyzer5X(JcsegTaskConfig.SIMPLE_MODE); //非必须(用于修改默认配置): 获取分词任务配置实例 JcsegAnalyzer5X jcseg = (JcsegAnalyzer5X) analyzer; JcsegTaskConfig config = jcseg.getTaskConfig(); //追加同义词到分词结果中, 需要在jcseg.properties中配置jcseg.loadsyn=1 config.setAppendCJKSyn(true); //追加拼音到分词结果中, 需要在jcseg.properties中配置jcseg.loadpinyin=1 config.setAppendCJKPinyin(true); //更多配置, 请查看com.webssky.jcseg.core.JcsegTaskConfig类 String words = "中华人民共和国"; TokenStream stream = null; try &#123; stream = analyzer.tokenStream("myfield", words); stream.reset(); CharTermAttribute offsetAtt = stream.addAttribute(CharTermAttribute.class); while (stream.incrementToken()) &#123; System.out.println(offsetAtt.toString()); &#125; stream.end(); &#125; catch (IOException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125;finally&#123; try &#123; if(stream != null) stream.close(); &#125; catch (IOException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125; &#125;&#125; &#125; 运行结果如下： 12中华人民共和国 IKAnalyzerIK Analyzer是一个开源的，基亍java语言开发的轻量级的中文分词工具包。 采用了特有的“正向迭代最细粒度切分算法“，支持细粒度和智能分词两种切分模式；在系统环境：Core2 i7 3.4G双核，4G内存，window 7 64位， Sun JDK 1.6_29 64位 普通pc环境测试，IK2012具有160万字/秒（3000KB/S）的高速处理能力。2012版本的智能分词模式支持简单的分词排歧义处理和数量词合并输出。采用了多子处理器分析模式，支持：英文字母、数字、中文词汇等分词处理，兼容韩文、日文字符优化的词典存储，更小的内存占用。支持用户词典扩展定义。特别的，在2012版本，词典支持中文，英文，数字混合词语。 IK Analyzer支持细粒度切分和智能切分两种分词模式; 在细粒度切分下，词语分解到很细的力度，比如“一个苹果”，会被切分成如下 1234一个一个苹果 在智能切分模式下，则会分词如下： 12一个苹果 和jcseg相同，现版本的IK Analyzer只兼容至lucene4版本，我修改了相关源码，使其提供了对lucene5的支持。 IK Analyzer示例代码如下： 1234567891011121314151617181920212223242526272829303132333435363738package com.lucene.analyzer;import java.io.IOException;import org.apache.lucene.analysis.Analyzer;import org.apache.lucene.analysis.TokenStream;import org.apache.lucene.analysis.tokenattributes.CharTermAttribute;import org.junit.Test;import org.wltea.analyzer.lucene.IKAnalyzer; public class IKAnalyzerTest &#123; @Test public void tokenTest() &#123; Analyzer analyzer = new IKAnalyzer(); String words = "中华人民共和国"; TokenStream stream = null; try &#123; stream = analyzer.tokenStream("myfield", words); stream.reset(); CharTermAttribute offsetAtt = stream.addAttribute(CharTermAttribute.class); while (stream.incrementToken()) &#123; System.out.println(offsetAtt.toString()); &#125; stream.end(); &#125; catch (IOException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125;finally&#123; try &#123; stream.close(); &#125; catch (IOException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125; &#125;&#125; &#125; 其运行结果如下： 123456789中华人民共和国中华人民中华华人人民共和国人民共和国共和国]]></content>
      <categories>
        <category>lucene</category>
      </categories>
      <tags>
        <tag>lucene</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[lucene的各种Field及其排序]]></title>
    <url>%2F2019%2F03%2F16%2FLucene%2F2%E3%80%81lucene%E7%9A%84%E5%90%84%E7%A7%8DField%E5%8F%8A%E5%85%B6%E6%8E%92%E5%BA%8F%2F</url>
    <content type="text"><![CDATA[lucene（2）—lucene的各种Field及其排序Lucene的Field说明Lucene存储对象是以document为存储单元，对象中相关的属性值则存放到Field中； lucene中所有Field都是IndexableField接口的实现 123org.apache.lucene.index.IndexableField Represents a single field for indexing. IndexWriter consumes Iterable&lt;IndexableField&gt; as a document. IndexableField接口提供了一些方法，主要是对field相关属性的获取，包括 12/** 获取field的名称 */public String name(); 12/** 获取field的类型fieldType */public IndexableFieldType fieldType(); 12/** *获取当前field的权重（评分值） 只有Field有评分的概念，如果我们想对document进行评分值的设定 必须预先对document中对应的field值进行评分设设定*/ public float boost(); 12/** 如果此Filed为二进制类型的，返回相应的值*/public BytesRef binaryValue(); 1234/** * 创建一个用户索引此Field的TokenStream */public TokenStream tokenStream(Analyzer analyzer, TokenStream reuse) throws IOException; 所有的Field均是org.apache.lucene.document.Field的子类； 项目中我们常用的Field类型主要有IntField, LongField, FloatField, DoubleField, BinaryDocValuesField, NumericDocValuesField, SortedDocValuesField, StringField, TextField, StoredField. lucene常见FieldIntField 主要对int类型的字段进行存储，需要注意的是如果需要对InfField进行排序使用SortField.Type.INT来比较，如果进范围查询或过滤，需要采用NumericRangeQuery.newIntRange() LongField 主要处理Long类型的字段的存储，排序使用SortField.Type.Long,如果进行范围查询或过滤利用NumericRangeQuery.newLongRange()，LongField常用来进行时间戳的排序，保存System.currentTimeMillions() FloatField 对Float类型的字段进行存储，排序采用SortField.Type.Float,范围查询采用NumericRangeQuery.newFloatRange() BinaryDocVluesField 只存储不共享值，如果需要共享值可以用SortedDocValuesField NumericDocValuesField 用于数值类型的Field的排序(预排序)，需要在要排序的field后添加一个同名的NumericDocValuesField SortedDocValuesField 用于String类型的Field的排序，需要在StringField后添加同名的SortedDocValuesField StringField 用户String类型的字段的存储，StringField是只索引不分词 TextField 对String类型的字段进行存储，TextField和StringField的不同是TextField既索引又分词 StoredField 存储Field的值，可以用IndexSearcher.doc和IndexReader.document来获取此Field和存储的值 IntField使用12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182package com.lucene.field; import java.io.IOException; import org.apache.lucene.document.Document;import org.apache.lucene.document.Field;import org.apache.lucene.document.IntField;import org.apache.lucene.document.NumericDocValuesField;import org.apache.lucene.index.IndexWriter;import org.apache.lucene.search.IndexSearcher;import org.apache.lucene.search.MatchAllDocsQuery;import org.apache.lucene.search.Query;import org.apache.lucene.search.ScoreDoc;import org.apache.lucene.search.Sort;import org.apache.lucene.search.SortField;import org.apache.lucene.search.TopFieldDocs;import org.junit.Test; import com.lucene.index.IndexUtil;import com.lucene.search.SearchUtil; public class IntFieldTest &#123; /** * 保存一个intField */ @Test public void testIndexIntFieldStored() &#123; Document document = new Document(); document.add(new IntField("intValue", 30, Field.Store.YES)); //要排序必须加同名的field，且类型为NumericDocValuesField document.add(new NumericDocValuesField("intValue", 30)); Document document1 = new Document(); document1.add(new IntField("intValue", 40, Field.Store.YES)); document1.add(new NumericDocValuesField("intValue", 40)); IndexWriter writer = null; try &#123; writer = IndexUtil.getIndexWriter("intFieldPath", false); writer.addDocument(document); writer.addDocument(document1); &#125; catch (IOException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125;finally&#123; try &#123; writer.commit(); writer.close(); &#125; catch (IOException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125; &#125; &#125; /** * 测试intField排序 */ @Test public void testIntFieldSort()&#123; try &#123; IndexSearcher searcher = SearchUtil.getIndexSearcher("intFieldPath", null); //构建排序字段 SortField[] sortField = new SortField[1]; sortField[0] = new SortField("intValue",SortField.Type.INT,true); Sort sort = new Sort(sortField); //查询所有结果 Query query = new MatchAllDocsQuery(); TopFieldDocs docs = searcher.search(query, 2, sort); ScoreDoc[] scores = docs.scoreDocs; //遍历结果 for (ScoreDoc scoreDoc : scores) &#123; System.out.println(searcher.doc(scoreDoc.doc));; &#125; //searcher.search(query, results); &#125; catch (IOException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125; &#125; &#125; 测试排序结果如下 12Document&lt;stored&lt;intValue:40&gt;&gt;Document&lt;stored&lt;intValue:30&gt;&gt; 如果修改NumericDocValuesField对应的值，结果会随着其值的大小而改变 LongField使用123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081package com.lucene.field; import java.io.IOException; import org.apache.lucene.document.Document;import org.apache.lucene.document.Field;import org.apache.lucene.document.LongField;import org.apache.lucene.document.NumericDocValuesField;import org.apache.lucene.index.IndexWriter;import org.apache.lucene.search.IndexSearcher;import org.apache.lucene.search.MatchAllDocsQuery;import org.apache.lucene.search.Query;import org.apache.lucene.search.ScoreDoc;import org.apache.lucene.search.Sort;import org.apache.lucene.search.SortField;import org.apache.lucene.search.TopFieldDocs;import org.junit.Test; import com.lucene.index.IndexUtil;import com.lucene.search.SearchUtil; public class LongFieldTest &#123; /** * 保存一个longField */ @Test public void testIndexLongFieldStored() &#123; Document document = new Document(); document.add(new LongField("longValue", 50L, Field.Store.YES)); document.add(new NumericDocValuesField("longValue", 50L)); Document document1 = new Document(); document1.add(new LongField("longValue", 80L, Field.Store.YES)); document1.add(new NumericDocValuesField("longValue", 80L)); IndexWriter writer = null; try &#123; writer = IndexUtil.getIndexWriter("longFieldPath", false); writer.addDocument(document); writer.addDocument(document1); &#125; catch (IOException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125;finally&#123; try &#123; writer.commit(); writer.close(); &#125; catch (IOException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125; &#125; &#125; /** * 测试longField排序 */ @Test public void testLongFieldSort()&#123; try &#123; IndexSearcher searcher = SearchUtil.getIndexSearcher("longFieldPath", null); //构建排序字段 SortField[] sortField = new SortField[1]; sortField[0] = new SortField("longValue",SortField.Type.LONG,true); Sort sort = new Sort(sortField); //查询所有结果 Query query = new MatchAllDocsQuery(); TopFieldDocs docs = searcher.search(query, 2, sort); ScoreDoc[] scores = docs.scoreDocs; //遍历结果 for (ScoreDoc scoreDoc : scores) &#123; //System.out.println(searcher.doc(scoreDoc.doc));; Document doc = searcher.doc(scoreDoc.doc); System.out.println(doc.getField("longValue").numericValue()); &#125; //searcher.search(query, results); &#125; catch (IOException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125; &#125;&#125; 运行结果如下： 12Document&lt;stored&lt;longValue:80&gt;&gt;Document&lt;stored&lt;longValue:50&gt;&gt; FloatField使用123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081package com.lucene.field; import java.io.IOException; import org.apache.lucene.document.Document;import org.apache.lucene.document.Field;import org.apache.lucene.document.FloatDocValuesField;import org.apache.lucene.document.FloatField;import org.apache.lucene.index.IndexWriter;import org.apache.lucene.search.IndexSearcher;import org.apache.lucene.search.MatchAllDocsQuery;import org.apache.lucene.search.Query;import org.apache.lucene.search.ScoreDoc;import org.apache.lucene.search.Sort;import org.apache.lucene.search.SortField;import org.apache.lucene.search.TopFieldDocs;import org.junit.Test; import com.lucene.index.IndexUtil;import com.lucene.search.SearchUtil; public class FloatFieldTest &#123; /** * 保存一个floatField */ @Test public void testIndexFloatFieldStored() &#123; Document document = new Document(); document.add(new FloatField("floatValue", 9.1f, Field.Store.YES)); document.add(new FloatDocValuesField("floatValue", 82.0f)); Document document1 = new Document(); document1.add(new FloatField("floatValue", 80.1f, Field.Store.YES)); document1.add(new FloatDocValuesField("floatValue", 80.1f)); IndexWriter writer = null; try &#123; writer = IndexUtil.getIndexWriter("floatFieldPath", false); writer.addDocument(document); writer.addDocument(document1); &#125; catch (IOException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125;finally&#123; try &#123; writer.commit(); writer.close(); &#125; catch (IOException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125; &#125; &#125; /** * 测试intField排序 */ @Test public void testFloatFieldSort()&#123; try &#123; IndexSearcher searcher = SearchUtil.getIndexSearcher("floatFieldPath", null); //构建排序字段 SortField[] sortField = new SortField[1]; sortField[0] = new SortField("floatValue",SortField.Type.FLOAT,true); Sort sort = new Sort(sortField); //查询所有结果 Query query = new MatchAllDocsQuery(); TopFieldDocs docs = searcher.search(query, 2, sort); ScoreDoc[] scores = docs.scoreDocs; //遍历结果 for (ScoreDoc scoreDoc : scores) &#123; //System.out.println(searcher.doc(scoreDoc.doc));; Document doc = searcher.doc(scoreDoc.doc); System.out.println(doc.getField("floatValue").numericValue()); &#125; //searcher.search(query, results); &#125; catch (IOException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125; &#125;&#125; 结果如下： 12Document&lt;stored&lt;floatValue:9.1&gt;&gt;Document&lt;stored&lt;floatValue:80.1&gt;&gt; BinaryDocValuesField使用12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485package com.lucene.field; import java.io.IOException; import org.apache.lucene.document.BinaryDocValuesField;import org.apache.lucene.document.Document;import org.apache.lucene.document.Field;import org.apache.lucene.document.FloatDocValuesField;import org.apache.lucene.document.FloatField;import org.apache.lucene.document.IntField;import org.apache.lucene.document.LongField;import org.apache.lucene.document.NumericDocValuesField;import org.apache.lucene.index.IndexWriter;import org.apache.lucene.search.IndexSearcher;import org.apache.lucene.search.MatchAllDocsQuery;import org.apache.lucene.search.Query;import org.apache.lucene.search.ScoreDoc;import org.apache.lucene.search.Sort;import org.apache.lucene.search.SortField;import org.apache.lucene.search.TopFieldDocs;import org.apache.lucene.util.BytesRef;import org.junit.Test; import com.lucene.index.IndexUtil;import com.lucene.search.SearchUtil; public class BinaryDocValuesFieldTest &#123; /** * 保存一个BinaryDocValuesField */ @Test public void testIndexLongFieldStored() &#123; Document document = new Document(); document.add(new BinaryDocValuesField("binaryValue",new BytesRef("1234".getBytes()))); Document document1 = new Document(); document1.add(new BinaryDocValuesField("binaryValue",new BytesRef("2345".getBytes()))); IndexWriter writer = null; try &#123; writer = IndexUtil.getIndexWriter("binaryValueFieldPath", false); writer.addDocument(document); writer.addDocument(document1); &#125; catch (IOException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125;finally&#123; try &#123; writer.commit(); writer.close(); &#125; catch (IOException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125; &#125; &#125; /** * 测试BinaryDocValuesField排序 */ @Test public void testBinaryDocValuesFieldSort()&#123; try &#123; IndexSearcher searcher = SearchUtil.getIndexSearcher("binaryValueFieldPath", null); //构建排序字段 SortField[] sortField = new SortField[1]; sortField[0] = new SortField("binaryValue",SortField.Type.STRING_VAL,true); Sort sort = new Sort(sortField); //查询所有结果 Query query = new MatchAllDocsQuery(); TopFieldDocs docs = searcher.search(query, 2, sort); ScoreDoc[] scores = docs.scoreDocs; //遍历结果 for (ScoreDoc scoreDoc : scores) &#123; //System.out.println(searcher.doc(scoreDoc.doc));; Document doc = searcher.doc(scoreDoc.doc); System.out.println(doc); //System.out.println(doc.getField("binaryValue").numericValue()); &#125; //searcher.search(query, results); &#125; catch (IOException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125; &#125;&#125; 运行结果： 12Document&lt;&gt;Document&lt;&gt; 为什么这样呢，这是跟BinaryDocValuesField的特性决定的，只索引不存值！ StringField使用1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889package com.lucene.field; import java.io.IOException; import org.apache.lucene.document.BinaryDocValuesField;import org.apache.lucene.document.Document;import org.apache.lucene.document.Field;import org.apache.lucene.document.FloatDocValuesField;import org.apache.lucene.document.FloatField;import org.apache.lucene.document.IntField;import org.apache.lucene.document.LongField;import org.apache.lucene.document.NumericDocValuesField;import org.apache.lucene.document.SortedDocValuesField;import org.apache.lucene.document.StringField;import org.apache.lucene.index.IndexWriter;import org.apache.lucene.search.IndexSearcher;import org.apache.lucene.search.MatchAllDocsQuery;import org.apache.lucene.search.Query;import org.apache.lucene.search.ScoreDoc;import org.apache.lucene.search.Sort;import org.apache.lucene.search.SortField;import org.apache.lucene.search.TopFieldDocs;import org.apache.lucene.util.BytesRef;import org.junit.Test; import com.lucene.index.IndexUtil;import com.lucene.search.SearchUtil; public class StringFieldTest &#123; /** * 保存一个StringField */ @Test public void testIndexLongFieldStored() &#123; Document document = new Document(); document.add(new StringField("stringValue","12445", Field.Store.YES)); document.add(new SortedDocValuesField("stringValue", new BytesRef("12445".getBytes()))); Document document1 = new Document(); document1.add(new StringField("stringValue","23456", Field.Store.YES)); document1.add(new SortedDocValuesField("stringValue", new BytesRef("23456".getBytes()))); IndexWriter writer = null; try &#123; writer = IndexUtil.getIndexWriter("stringFieldPath", false); writer.addDocument(document); writer.addDocument(document1); &#125; catch (IOException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125;finally&#123; try &#123; writer.commit(); writer.close(); &#125; catch (IOException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125; &#125; &#125; /** * 测试StringField排序 */ @Test public void testStringFieldSort()&#123; try &#123; IndexSearcher searcher = SearchUtil.getIndexSearcher("stringFieldPath", null); //构建排序字段 SortField[] sortField = new SortField[1]; sortField[0] = new SortField("stringVal",SortField.Type.STRING,true); Sort sort = new Sort(sortField); //查询所有结果 Query query = new MatchAllDocsQuery(); TopFieldDocs docs = searcher.search(query, 2, sort); ScoreDoc[] scores = docs.scoreDocs; //遍历结果 for (ScoreDoc scoreDoc : scores) &#123; //System.out.println(searcher.doc(scoreDoc.doc));; Document doc = searcher.doc(scoreDoc.doc); System.out.println(doc); //System.out.println(doc.getField("binaryValue").numericValue()); &#125; //searcher.search(query, results); &#125; catch (IOException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125; &#125;&#125; 运行结果如下： 12Document&lt;stored,indexed,tokenized,omitNorms,indexOptions=DOCS&lt;stringValue:12445&gt;&gt;Document&lt;stored,indexed,tokenized,omitNorms,indexOptions=DOCS&lt;stringValue:23456&gt;&gt; TextField使用123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990package com.lucene.field; import java.io.IOException; import org.apache.lucene.document.BinaryDocValuesField;import org.apache.lucene.document.Document;import org.apache.lucene.document.Field;import org.apache.lucene.document.FloatDocValuesField;import org.apache.lucene.document.FloatField;import org.apache.lucene.document.IntField;import org.apache.lucene.document.LongField;import org.apache.lucene.document.NumericDocValuesField;import org.apache.lucene.document.SortedDocValuesField;import org.apache.lucene.document.StringField;import org.apache.lucene.document.TextField;import org.apache.lucene.index.IndexWriter;import org.apache.lucene.search.IndexSearcher;import org.apache.lucene.search.MatchAllDocsQuery;import org.apache.lucene.search.Query;import org.apache.lucene.search.ScoreDoc;import org.apache.lucene.search.Sort;import org.apache.lucene.search.SortField;import org.apache.lucene.search.TopFieldDocs;import org.apache.lucene.util.BytesRef;import org.junit.Test; import com.lucene.index.IndexUtil;import com.lucene.search.SearchUtil; public class TextFieldTest &#123; /** * 保存一个StringField */ @Test public void testIndexLongFieldStored() &#123; Document document = new Document(); document.add(new TextField("textValue","12345", Field.Store.YES)); document.add(new SortedDocValuesField("textValue", new BytesRef("12345".getBytes()))); Document document1 = new Document(); document1.add(new TextField("textValue","23456", Field.Store.YES)); document1.add(new SortedDocValuesField("textValue", new BytesRef("23456".getBytes()))); IndexWriter writer = null; try &#123; writer = IndexUtil.getIndexWriter("textFieldPath", false); writer.addDocument(document); writer.addDocument(document1); &#125; catch (IOException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125;finally&#123; try &#123; writer.commit(); writer.close(); &#125; catch (IOException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125; &#125; &#125; /** * 测试StringField排序 */ @Test public void testStringFieldSort()&#123; try &#123; IndexSearcher searcher = SearchUtil.getIndexSearcher("textFieldPath", null); //构建排序字段 SortField[] sortField = new SortField[1]; sortField[0] = new SortField("textValue",SortField.Type.STRING,true); Sort sort = new Sort(sortField); //查询所有结果 Query query = new MatchAllDocsQuery(); TopFieldDocs docs = searcher.search(query, 2, sort); ScoreDoc[] scores = docs.scoreDocs; //遍历结果 for (ScoreDoc scoreDoc : scores) &#123; //System.out.println(searcher.doc(scoreDoc.doc));; Document doc = searcher.doc(scoreDoc.doc); System.out.println(doc); //System.out.println(doc.getField("binaryValue").numericValue()); &#125; //searcher.search(query, results); &#125; catch (IOException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125; &#125;&#125; 运行结果如下： 12Document&lt;stored,indexed,tokenized&lt;textValue:23456&gt;&gt;Document&lt;stored,indexed,tokenized&lt;textValue:12345&gt;&gt; 源码下载地址lucene field使用源码]]></content>
      <categories>
        <category>lucene</category>
      </categories>
      <tags>
        <tag>lucene</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[lucene的多样化查询]]></title>
    <url>%2F2019%2F03%2F16%2FLucene%2F21%E3%80%81lucene%E7%9A%84%E5%A4%9A%E6%A0%B7%E5%8C%96%E6%9F%A5%E8%AF%A2%2F</url>
    <content type="text"><![CDATA[lucene（21）—lucene的多样化查询 查询类 说明 TermQuery 通过项进行搜索 TermRangeQuery 在指定的项范围内进行搜索 PrefixQuery 通过字符串搜索 BooleanQuery 组合查询 PhraseQuery 通过短语搜索 WildcardQuery 通配符查询 FuzzyQuery 搜索类似项 MatchAllDocsQuery 匹配所有文档 MatchNoDocsQuery 不用匹配文档 QueryParser 解析查询表达式 MultiPhraseQuery 多短语查询 NumericRangeQuery 数字范围查询，一般在价格、时间域的查询 在 Lucene4 以后，组合查询只有一个构造方法，并没有无参构造方法，而是多了一个静态内部类 Builder。所以组合查如下： 1Query booleanQuery = new BooleanQuery.Builder().add(query1,BooleanClause.Occur.MUST).add(query1,BooleanClause.Occur.MUST).build(); BooleanClause.Occur 提供了一下四种： 名称 作用 MUST 相当于 SQL 中的 and FILTER SHOULD 相当于 SQL 中的 in MUST_NOT 当它们同事使用的情况： 高级搜索lucene 包含了 一个建立在 SpanQuery 类基础上的整套查询体系，大致反映了 Lucene 的 Query 类体系。SpanQuery 是指域中的起始词汇单元和终止词汇单元的位置。SpanQuery 有一些常用的子类，如下所示： FieldMaskingSpanQuery用于在多个域之间查询，即把另一个域看成某个域，从而看起来像是在同个域中查询，因为 Lucene 默认某个条件只能作用在单个域上，不支持跨域查询，只能在同一个域中查询，所以有了FieldMaskingSpanQuery。 SpanTermQuery和其他跨度查询类型结合使用，单独使用时，相当于 Term,slop 为跨度因子，用来限制两个 Trem 之间的最大跨度。还有一个 inOrder 参数，它用来设置是否允许尽心倒叙跨度。即 TremA 到 TramB 不一定从左到右去匹配也可以从右到左，从右到左就是倒叙，inOrder 为 true 即表示 order (顺序) 很重要不能倒叙去匹配必须正向去匹配，false 返之。注意，停用词不在 slop 统计范围内 SpanFirstQuery表示对出现在一个域中的 [0,n] 范围内的 Term 项进行的匹配查询，关键是 n 指定了查询的 term 出现范围的上限。 SpanContainingQuery返回在另一个范围内的查询匹配结果，big 和 little 的句子可以是任何 span 类型查询。在包含 little 匹配中从 big 匹配跨度返回。例如 “a beautiful and boring world” , big 查询是 SpanNearQuery(SpanTermQuery(“beautiful “),SpanTermQuery(“world”)).setSlop(2),而 little 查询是 SpanTermQuery(“boring”) ，则该 Doc 命中，并 big 匹配跨度返回，即 big 优先级高。]]></content>
      <categories>
        <category>lucene</category>
      </categories>
      <tags>
        <tag>lucene</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[lucene 综合应用实例]]></title>
    <url>%2F2019%2F03%2F16%2FLucene%2F20%E3%80%81lucene%E7%9A%84%E7%BB%BC%E5%90%88%E4%BD%BF%E7%94%A8%E5%AE%9E%E4%BE%8B%2F</url>
    <content type="text"><![CDATA[lucene（20）—lucene 综合应用实例最近因项目需求的需要，完成一个”会话检索”功能。该功能是把录音转写成文字，对转写后的文本进行关键字检索。因为该功能对检索条件类型的使用比较完整（例如：时间范围、关键字、语速等不同类型）以及使用的注意点也比较多，所以在这里给大家分享一下。希望可以帮到你。 功能说明会话检索，支持 多个文件夹同时检索，支持的索引大小为 1300 MB 左右（大约是45万条数据），支持 多个条件进行 and 检索。 功能依赖lucene 使用的版本是 5.5.3 ，相对来说还是比较老的，当前最新的版本是 7.7.0 。 123456789101112131415161718192021222324252627282930313233343536373839404142434445&lt;dependency&gt; &lt;groupId&gt;org.apache.lucene&lt;/groupId&gt; &lt;artifactId&gt;lucene-core&lt;/artifactId&gt; &lt;version&gt;5.5.3&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.lucene&lt;/groupId&gt; &lt;artifactId&gt;lucene-analyzers-common&lt;/artifactId&gt; &lt;version&gt;5.5.3&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.lucene&lt;/groupId&gt; &lt;artifactId&gt;lucene-queryparser&lt;/artifactId&gt; &lt;version&gt;5.5.3&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.lucene&lt;/groupId&gt; &lt;artifactId&gt;lucene-queries&lt;/artifactId&gt; &lt;version&gt;5.5.3&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.lucene&lt;/groupId&gt; &lt;artifactId&gt;lucene-backward-codecs&lt;/artifactId&gt; &lt;version&gt;5.5.3&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.lucene&lt;/groupId&gt; &lt;artifactId&gt;lucene-memory&lt;/artifactId&gt; &lt;version&gt;5.5.3&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.lucene&lt;/groupId&gt; &lt;artifactId&gt;lucene-highlighter&lt;/artifactId&gt; &lt;version&gt;5.5.3&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.lucene&lt;/groupId&gt; &lt;artifactId&gt;lucene-spatial&lt;/artifactId&gt; &lt;version&gt;5.5.3&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.lucene&lt;/groupId&gt; &lt;artifactId&gt;lucene-analyzers-smartcn&lt;/artifactId&gt; &lt;version&gt;5.5.3&lt;/version&gt;&lt;/dependency&gt; 实现过程编码过程分页处理类 Page 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245public class Page&lt;T&gt; &#123; /** 当前第几页(从1开始计算) */ private int currentPage; /** 每页显示几条 */ private int pageSize; /** 总记录数 */ private int totalRecord; /** 总页数 */ private int totalPage; /** 分页数据集合[用泛型T来限定集合元素类型] */ private Collection&lt;T&gt; items; /** 当前显示起始索引(从零开始计算) */ private int startIndex; /** 当前显示结束索引(从零开始计算) */ private int endIndex; /** 一组最多显示几个页码[比如Google一组最多显示10个页码] */ private int groupSize; /** 左边偏移量 */ private int leftOffset = 5; /** 右边偏移量 */ private int rightOffset = 4; /** 当前页码范围 */ private String[] pageRange; /** 分页数据 */ private List&lt;Document&gt; docList; /** 上一页最后一个ScoreDoc对象 */ private ScoreDoc afterDoc; /** 上一页最后一个ScoreDoc对象的Document对象ID */ private int afterDocId; public void setRangeIndex() &#123; int groupSize = getGroupSize(); int totalPage = getTotalPage(); if (totalPage &lt; 2) &#123; startIndex = 0; endIndex = totalPage - startIndex; &#125; else &#123; int currentPage = getCurrentPage(); if (groupSize &gt;= totalPage) &#123; startIndex = 0; endIndex = totalPage - startIndex - 1; &#125; else &#123; int leftOffset = getLeftOffset(); int middleOffset = getMiddleOffset(); if (-1 == middleOffset) &#123; startIndex = 0; endIndex = groupSize - 1; &#125; else if (currentPage &lt;= leftOffset) &#123; startIndex = 0; endIndex = groupSize - 1; &#125; else &#123; startIndex = currentPage - leftOffset - 1; if (currentPage + rightOffset &gt; totalPage) &#123; endIndex = totalPage - 1; &#125; else &#123; endIndex = currentPage + rightOffset - 1; &#125; &#125; &#125; &#125; &#125; public int getCurrentPage() &#123; if (currentPage &lt;= 0) &#123; currentPage = 1; &#125; else &#123; int totalPage = getTotalPage(); if (totalPage &gt; 0 &amp;&amp; currentPage &gt; getTotalPage()) &#123; currentPage = totalPage; &#125; &#125; return currentPage; &#125; public void setCurrentPage(int currentPage) &#123; this.currentPage = currentPage; &#125; public int getPageSize() &#123; if (pageSize &lt;= 0) &#123; pageSize = 10; &#125; return pageSize; &#125; public void setPageSize(int pageSize) &#123; this.pageSize = pageSize; &#125; public int getTotalRecord() &#123; return totalRecord; &#125; public void setTotalRecord(int totalRecord) &#123; this.totalRecord = totalRecord; &#125; public int getTotalPage() &#123; int totalRecord = getTotalRecord(); if (totalRecord == 0) &#123; totalPage = 0; &#125; else &#123; int pageSize = getPageSize(); totalPage = totalRecord % pageSize == 0 ? totalRecord / pageSize : (totalRecord / pageSize) + 1; &#125; return totalPage; &#125; public void setTotalPage(int totalPage) &#123; this.totalPage = totalPage; &#125; public int getStartIndex() &#123; return startIndex; &#125; public void setStartIndex(int startIndex) &#123; this.startIndex = startIndex; &#125; public int getEndIndex() &#123; return endIndex; &#125; public void setEndIndex(int endIndex) &#123; this.endIndex = endIndex; &#125; public int getGroupSize() &#123; if (groupSize &lt;= 0) &#123; groupSize = 10; &#125; return groupSize; &#125; public void setGroupSize(int groupSize) &#123; this.groupSize = groupSize; &#125; public int getLeftOffset() &#123; leftOffset = getGroupSize() / 2; return leftOffset; &#125; public void setLeftOffset(int leftOffset) &#123; this.leftOffset = leftOffset; &#125; public int getRightOffset() &#123; int groupSize = getGroupSize(); if (groupSize % 2 == 0) &#123; rightOffset = (groupSize / 2) - 1; &#125; else &#123; rightOffset = groupSize / 2; &#125; return rightOffset; &#125; public void setRightOffset(int rightOffset) &#123; this.rightOffset = rightOffset; &#125; /** 中心位置索引[从1开始计算] */ public int getMiddleOffset() &#123; int groupSize = getGroupSize(); int totalPage = getTotalPage(); if (groupSize &gt;= totalPage) &#123; return -1; &#125; return getLeftOffset() + 1; &#125; public String[] getPageRange() &#123; setRangeIndex(); int size = endIndex - startIndex + 1; if (size &lt;= 0) &#123; return new String[0]; &#125; if (totalPage == 1) &#123; return new String[] &#123; "1" &#125;; &#125; pageRange = new String[size]; for (int i = 0; i &lt; size; i++) &#123; pageRange[i] = (startIndex + i + 1) + ""; &#125; return pageRange; &#125; public void setPageRange(String[] pageRange) &#123; this.pageRange = pageRange; &#125; public void setItems(Collection&lt;T&gt; items) &#123; this.items = items; &#125; public Collection&lt;T&gt; getItems() &#123; return items; &#125; public void setDocList(List&lt;Document&gt; docList) &#123; this.docList = docList; &#125; public List&lt;Document&gt; getDocList() &#123; return docList; &#125; public void setAfterDoc(ScoreDoc afterDoc) &#123; this.afterDoc = afterDoc; &#125; public ScoreDoc getAfterDoc() &#123; setAfterDocId(afterDocId); return afterDoc; &#125; public void setAfterDocId(int afterDocId) &#123; this.afterDocId = afterDocId; if (null == afterDoc) &#123; this.afterDoc = new ScoreDoc(afterDocId, 1.0f); &#125; &#125; public int getAfterDocId() &#123;return afterDocId;&#125; /*构造方法*/ public Page() &#123;&#125; public Page(int currentPage, int pageSize) &#123; this.currentPage = currentPage; this.pageSize = pageSize; &#125; public Page(int currentPage, int pageSize, Collection&lt;T&gt; items) &#123; this.currentPage = currentPage; this.pageSize = pageSize; this.items = items; &#125; public Page(int currentPage, int pageSize, Collection&lt;T&gt; items, int groupSize) &#123; this.currentPage = currentPage; this.pageSize = pageSize; this.items = items; this.groupSize = groupSize; &#125; public Page(int currentPage, int pageSize, int groupSize, int afterDocId) &#123; this.currentPage = currentPage; this.pageSize = pageSize; this.groupSize = groupSize; this.afterDocId = afterDocId; &#125; &#125; 会话记录 IQCConversationInfoBean 实体类 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455public class IQCConversationInfoBean extends EntityBean &#123; // serialVersionUID private static final long serialVersionUID = -4459013070304617092L; private String serialNo;// 通话编号 private Date callTime; // 呼入时间 private String callDate; // 会话日期 private Date hangupTime; // 挂机时间 private String language; // 语种 private String agentContent; // 座席通话内容 private String custContent; // 客户通话内容 private String allContent; //全部通话内容 private String agentFirst; // 座席首句 private String agentLast; // 座席尾句 private String custFirst; // 客户首句 private String custLast; // 客户尾句 private Float agentMaxSpeed=0f; // 座席最大语速 private Float agentMinSpeed=0f; // 座席最小语速 private Float agentAvgSpeed=0f; // 座席平均语速 private Float custMaxSpeed=0f; // 座席最大语速 private Float custMinSpeed=0f; // 座席最小语速 private Float custAvgSpeed=0f; // 座席平均语速 private Integer silenceSeconds; // 静音时长 private String sumNo; //小结编号 private Float silencePercent; // 静音占比 private String mediaType; // 会话类型 private Integer maxSilenceSeconds; // 最大静音时长 private String accountCode; // 座席用户号 private String empName; // 座席姓名 private Integer talkSeconds; // 通话时长 private Integer minTalkSeconds; // 最小通话时长 private Integer maxTalkSeconds; // 最大通话时长 private Float custMaxEmotion=0f; // 客户最大情绪 private Float custMinEmotion=0f; // 客户最小情绪 private Float custAvgEmotion=0f; // 客户平均情绪 private Float agentMaxEmotion=0f; // 座席最大情绪 private Float agentMinEmotion=0f; // 座席最小情绪 private Float agentAvgEmotion=0f; // 座席平均情绪 private Integer silenceCount; // 静音次数 private String overLap; // 是否重叠音 private String callNo; // 来电号码 private String custNo; // 客户号 private String custGender; // 客户性别 private String businessGroupCode; // 座席组别 private String satisfiedType; // 满意度 private String custName; // 客户姓名 private String taskCode; // 任务编号 private String analysisResult; // 分析结果 private String channelCode; // 数据渠道 //扩展字段 private String filePath; // 文件路径 private String beginTime; // 查询条件开始时间 private String endTime; // 查询条件结束时间 .....&#125; lucene 工具类 LuceneUtils 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889public class LuceneUtils &#123; // 打开索引目录 public static FSDirectory openFSDirectory(String luceneDir) &#123; FSDirectory directory = null; try &#123; directory = FSDirectory.open(Paths.get(luceneDir)); /** * 注意：isLocked方法内部会试图去获取Lock,如果获取到Lock， 会关闭它，否则return * false表示索引目录没有被锁， 这也就是为什么unlock方法被从IndexWriter类中移除的原因 */ IndexWriter.isLocked(directory); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; return directory; &#125; // 创建索引阅读器（多目录） public static MultiReader getMultiReader(List&lt;String&gt; dirPathList) throws IOException &#123; List&lt;IndexReader&gt; indexReaders = new ArrayList&lt;IndexReader&gt;(); for(String dirPath : dirPathList)&#123; Directory directory = openFSDirectory(dirPath); if(DirectoryReader.indexExists(directory))&#123; IndexReader reader = DirectoryReader.open(directory); indexReaders.add(reader); &#125; &#125; if(indexReaders.size()&gt;0)&#123; return new MultiReader(indexReaders.toArray(new IndexReader[indexReaders.size()])); &#125;else&#123; return null; &#125; &#125; // 创建索引查询器（多目录） public static IndexSearcher getMultiIndexSearcher (MultiReader multiReader, ExecutorService executor) &#123; if(null != executor) &#123; return new IndexSearcher(multiReader, executor); &#125;else&#123; return new IndexSearcher(multiReader); &#125; &#125; // 获取符合条件的总记录数 public static ScoreDoc[] searchTotalRecord(IndexSearcher search, Query query) &#123; ScoreDoc[] docs = null; try &#123; TopDocs topDocs = search.search(query, Integer.MAX_VALUE); if (topDocs==null || topDocs.scoreDocs==null || topDocs.scoreDocs.length==0) &#123; return docs; &#125; docs = topDocs.scoreDocs; &#125;catch (IOException e) &#123; e.printStackTrace(); &#125; return docs; &#125; // Lucene多目录分页查询 public static void pageQuery(IndexSearcher searcher, Query query, Page&lt;Document&gt; page) throws IOException &#123; ScoreDoc[] scoreDocs = searchTotalRecord(searcher, query); if (null != scoreDocs) &#123; // 设置总记录数 page.setTotalRecord(scoreDocs.length); ScoreDoc afterDoc = null; if (page.getCurrentPage() &gt; 1) &#123; afterDoc = scoreDocs[(page.getCurrentPage() - 1) * page.getPageSize() - 1]; &#125; TopDocs topDocs = searcher.searchAfter(afterDoc, query, page.getPageSize()); List&lt;Document&gt; docList = new ArrayList&lt;Document&gt;(); ScoreDoc[] docs = topDocs.scoreDocs; int index = 0; for (ScoreDoc scoreDoc : docs) &#123; int docID = scoreDoc.doc; Document document = searcher.doc(docID); if (index == docs.length - 1) &#123; page.setAfterDoc(scoreDoc); page.setAfterDocId(docID); &#125; docList.add(document); index++; &#125; page.setItems(docList); &#125; &#125; &#125; service 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157@Service("iqcConversationIndexService")public class IQCConversationIndexService&#123; private SimpleDateFormat formatter = new SimpleDateFormat("yyyyMM"); // 获取索引目录 private static final String INDEX_ROOT_PATH = SysConstant.config.getProperty("indexRootPath"); public ResultBean&lt;IQCConversationInfoBean&gt; getIQCConversationInfoPageByBean (IQCConversationInfoBean bean, int currentPage, int pageSize) throws Exception &#123; ResultBean&lt;IQCConversationInfoBean&gt; rb = new ResultBean&lt;IQCConversationInfoBean&gt;() ; ExecutorService _executorService = Executors.newFixedThreadPool(10); MultiReader multiReader = null; IndexSearcher searcher = null; try &#123; Page&lt;Document&gt; page = new Page&lt;Document&gt;(currentPage, pageSize); // 根据时间获取检索文件夹 Date beginDate = formatter.parse(bean.getBeginTime().substring(0,6)); Date endDate = formatter.parse(bean.getEndTime().substring(0,6)); String dirName = ""; //计算时间区间内每个日期文件夹并解析其中的文件 Calendar tempStart = Calendar.getInstance(); tempStart.setTime(endDate); List&lt;String&gt; indexDirecorys = new ArrayList&lt;String&gt;(); while(beginDate.getTime() &lt;= endDate.getTime())&#123; dirName = JCalendar.getDateStr(endDate, "yyyyMM"); indexDirecorys.add(INDEX_ROOT_PATH + File.separator + dirName); tempStart.add(Calendar.MONTH, -1); endDate = tempStart.getTime(); &#125; multiReader = LuceneUtils.getMultiReader(indexDirecorys); if(null!=multiReader)&#123; searcher = LuceneUtils.getMultiIndexSearcher(multiReader,_executorService); BooleanQuery booleanQuery = dealBooleanQueryTerms(bean); LuceneUtils.pageQuery(searcher, booleanQuery, page); if (page==null || page.getItems()==null || page.getItems().size()==0) &#123; log.debug("未检索到记录"); rb.setTotal(0l); &#125;else&#123; for(Document doc : page.getItems())&#123; IQCConversationInfoExtBean temp = new IQCConversationInfoExtBean(); for(IndexableField field : doc.getFields())&#123; setConversationValue(temp, field); &#125; rb.getRows().add(temp); &#125; rb.setTotal((long) page.getTotalRecord()); &#125; &#125;else&#123; log.info("不存在索引会话记录【"+bean.getBeginTime()+"】至【"+bean.getEndTime()+"】"); rb.setTotal(0l); &#125; rb.setReturnCode(SysConstant.SYS_RETURN_SUCCESS_CODE); rb.setReturnMessage(SysConstant.SYS_RETURN_SUCCESS_MESSAGE); &#125;catch (Exception e) &#123; log.error("会话检索异常", e); rb.setReturnCode(SysConstant.SYS_RETURN_EXCEPTION_CODE); rb.setReturnMessage("检索异常："+e.getMessage()); &#125;finally&#123; if(!_executorService.isShutdown())&#123; _executorService.shutdown(); &#125; if(null!=searcher)&#123; searcher.getIndexReader().close(); &#125; if(null!=multiReader)&#123; multiReader.close(); &#125; &#125; return rb; &#125; // 多种条件组合检索 private BooleanQuery dealBooleanQueryTerms (IQCConversationInfoBean bean) throws Exception&#123; BooleanQuery.Builder booleanQueryBuilder = new BooleanQuery.Builder(); //关键词 if(StringUtil.isNotEmpty(bean.getAllContent()))&#123; Term t = new Term("allContent", ".*"+bean.getAllContent()+".*"); Query query = new RegexQuery(t); booleanQueryBuilder.add(query, BooleanClause.Occur.MUST); &#125; //客户语速 if(null!=bean.getCustMinSpeed() &amp;&amp; null!=bean.getCustMaxSpeed())&#123; Query query = NumericRangeQuery.newFloatRange( "custMaxSpeed", bean.getCustMinSpeed(), bean.getCustMaxSpeed(), true, true); booleanQueryBuilder.add(query, BooleanClause.Occur.MUST); &#125; //来电号码 if(StringUtil.isNotEmpty(bean.getCallNo()))&#123; Query query = new TermQuery(new Term("callNo", bean.getCallNo())); booleanQueryBuilder.add(query, BooleanClause.Occur.MUST); &#125; //呼入时间 if(null!=bean.getBeginTime() &amp;&amp; null!=bean.getEndTime())&#123; long beginTime = JCalendar.getDate(bean.getBeginTime(), "yyyyMMddHHmmss").getTime(); long endTime = JCalendar.getDate(bean.getEndTime(), "yyyyMMddHHmmss").getTime(); Query query = NumericRangeQuery.newLongRange( "callTime", beginTime,endTime, true, true); booleanQueryBuilder.add(query, BooleanClause.Occur.MUST); &#125; //通话时长 if(null!=bean.getMinTalkSeconds() &amp;&amp; null!=bean.getMaxTalkSeconds())&#123; Query query = NumericRangeQuery.newIntRange( "maxTalkSeconds", bean.getMinTalkSeconds(), bean.getMaxTalkSeconds(), true, true); booleanQueryBuilder.add(query, BooleanClause.Occur.MUST); &#125; &#125; // 通过反射设置对象的值-单层不考虑继承 public static void setConversationValue(IQCConversationInfoBean iqcConversationInfoBean, IndexableField indexableField)&#123; String fieldName = indexableField.name(); if (null != iqcConversationInfoBean &amp;&amp; !fieldName.equals("serialVersionUID")) &#123; try &#123; Class&lt;?&gt; clazz1 = IQCConversationInfoBean.class; Field[] fields = clazz1.getDeclaredFields(); for(Field field : fields)&#123; if(field.getName().equals(fieldName))&#123; String name=firstLetterUpperCase(field.getName()); String setmethodName="set"+name; Method m = clazz1.getDeclaredMethod(setmethodName, field.getType()); switch(field.getType().getSimpleName())&#123; case "String": m.invoke(iqcConversationInfoBean, indexableField.stringValue()); break; case "Float": m.invoke(iqcConversationInfoBean, indexableField.numericValue()); break; case "Long": m.invoke(iqcConversationInfoBean, indexableField.numericValue()); break; case "Date": m.invoke(iqcConversationInfoBean, new Date((long) indexableField.numericValue())); break; &#125; break; &#125; &#125; &#125;catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; &#125; // public static String firstLetterUpperCase(String str)&#123; if(str==null||str.length()&lt;2)&#123; return str; &#125;else&#123; String first=str.substring(0, 1).toUpperCase(); return first+str.substring(1,str.length()); &#125; &#125; &#125; 检索流程根据关键词解析（queryParser）出查询条件query（Termquery）,利用检索工具（indexSearcher）去索引库获取文档的id,然后再根据文档 id去文档信息库获取文档信息。 分词器不同，建立的索引数据就不同；比较通用的一个中文分词器IKAnalyzer的用法。 结果展示 注意事项使用多线程。在使用多线程时，只需要创建线程池即可。事实上，Lucene 在 IndexSearcher 中 判断是否有 executor ,如果 IndexSearcher 有 executor ，则会由每个线程控制一部分索引的读取，而且查询的过程采用的是 future 机制，这种方式是边读边往结果集里边追加数据，这样异步处理机制提升了效率。具体源码可看 IndexSearcher 的 search。 控制检索文件夹。如果同时检索的文件夹太多的时，会增加 GC 负担 在你能承受的范围内设置更多的内存。以免造成内存溢出 总结本文是对 Lucene 多条件检索的记录。实现多目录多线程的检索方式；实现分页功能；实现多种类型的条件查询以及数据量较大时检索的注意点进行记录。为了更好的使用 Lucene 后面将总结如何提高 Lucene 的检索效率。 全文检索，lucene 在 匹配效果、速度和效率是极大的优于数据库的。]]></content>
      <categories>
        <category>lucene</category>
      </categories>
      <tags>
        <tag>lucene</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[lucene增量更新和NRT(near-real-time)Query近实时查询]]></title>
    <url>%2F2019%2F03%2F16%2FLucene%2F19%E3%80%81lucene%E5%A2%9E%E9%87%8F%E6%9B%B4%E6%96%B0%E5%92%8CNRT(near-real-time)Query%E8%BF%91%E5%AE%9E%E6%97%B6%E6%9F%A5%E8%AF%A2%2F</url>
    <content type="text"><![CDATA[lucene（19）—lucene增量更新和NRT(near-real-time)Query近实时查询有时候我们创建完索引之后，数据源可能有更新的内容，而我们又想像数据库那样能直接体现在查询中，这里就是我们所说的增量索引。对于这样的需求我们怎么来实现呢？lucene内部是没有提供这种增量索引的实现的； 这里我们一般可能会想到，将之前的索引全部删除，然后进行索引的重建。对于这种做法，如果数据源的条数不是特别大的情况下倒还可以，如果数据源的条数特别大的话，势必会造成查询数据耗时，同时索引的构建也是比较耗时的，几相叠加，势必可能造成查询的时候数据缺失的情况，这势必严重影响用户的体验； 比较常见的增量索引的实现是： 设置一个定时器，定时从数据源中读取比现有索引文件中新的内容或是数据源中带有更新标示的数据。 对数据转换成需要的document并进行索引 这样做较以上的那种全删除索引然后重建的好处在于： 数据源查询扫描的数据量小 相应的更新索引的条数也少，减少了大量的IndexWriter的commit和close这些耗时操作 以上解决了增量的问题，但是实时性的问题还是存在的： 索引的变更只有在IndexWriter的commit执行之后才可以体现出来 那么我们怎样对实时性有个提升呢，大家都知道lucene索引可以以文件索引和内存索引两种方式存在，相较于文件索引，内存索引的执行效率要高于文件索引的构建，因为文件索引是要频繁的IO操作的；结合以上的考虑，我们采用文件索引+内存索引的形式来进行lucene的增量更新；其实现机制如下： 定时任务扫描数据源的变更 对获得的数据源列表放在内存中 内存中的document达到数量限制的时候，以队列的方式删除内存中的索引，并将之添加到文件索引 查询的时候采用文件+内存索引联合查询的方式以达到NRT效果 定时任务调度器java内置了TimerTask，此类是可以提供定时任务的，但是有一点就是TimerTask的任务是无状态的，我们还需要对任务进行并行的设置；了解到quartz任务调度框架提供了有状态的任务StatefulJob，即在本次调度任务没有执行完毕时，下次任务不会执行； 常见的我们启动一个quartz任务的方式如下： 12345678Date runTime = DateBuilder.evenSecondDate(new Date()); StdSchedulerFactory sf = new StdSchedulerFactory(); Scheduler scheduler = sf.getScheduler(); JobDetail job = JobBuilder.newJob(XXX.class).build(); Trigger trigger = TriggerBuilder.newTrigger().startAt(runTime).withSchedule(SimpleScheduleBuilder.simpleSchedule().withIntervalInSeconds(3).repeatForever()).forJob(job).build(); scheduler.scheduleJob(job, trigger); scheduler.start();&lt;/span&gt; 以上我们是设置了每三秒执行一次定时任务，而任务类是XXX 任务类通用方法这里我定义了一个XXX的父类，其定义如下： 12345678910111213141516171819202122232425262728293031323334package com.chechong.lucene.indexcreasement; import java.util.List;import java.util.TimerTask; import org.apache.lucene.store.RAMDirectory;import org.quartz.Job;import org.quartz.StatefulJob; /**有状态的任务：串行执行，即不允许上次执行没有完成即开始本次如果需要并行给接口改为Job即可 * @author lenovo * */public abstract class BaseInCreasementIndex implements StatefulJob &#123; /** * 内存索引 */ private RAMDirectory ramDirectory; public BaseInCreasementIndex() &#123; &#125; public BaseInCreasementIndex(RAMDirectory ramDirectory) &#123; super(); this.ramDirectory = ramDirectory; &#125; /**更新索引 * @throws Exception */ public abstract void updateIndexData() throws Exception; /**消费数据 * @param list */ public abstract void consume(List list) throws Exception;&#125; 任务类相关实现,以下方法是获取待添加索引的数据源XXXInCreasementIndex 12345678910@Override public void execute(JobExecutionContext context) throws JobExecutionException &#123; try &#123; XXXInCreasementIndex index = new XXXInCreasementIndex(Constants.XXX_INDEX_PATH, XXXDao.getInstance(), RamDirectoryControl.getRAMDireactory()); index.updateIndexData(); &#125; catch (Exception e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125; &#125; 123456789@Override public void updateIndexData() throws Exception &#123; int maxBeanID = SearchUtil.getLastIndexBeanID(); System.out.println(maxBeanID); List&lt;XXX&gt; sources = XXXDao.getListInfoBefore(maxBeanID);、、 if (sources != null &amp;&amp; sources.size() &gt; 0) &#123; this.consume(sources); &#125; &#125; 这里，XXX代表我们要获取数据的实体类对象 consume方法主要是做两件事： 数据存放到内存索引 判断内存索引数量，超出限制的话以队列方式取出超出的数量，并将之存放到文件索引 12345@Override public void consume(List list) throws Exception &#123; IndexWriter writer = RamDirectoryControl.getRAMIndexWriter(); RamDirectoryControl.consume(writer,list); &#125; 上边我们将内存索引和队列的实现放在了RamDirectoryControl中 内存索引控制器首先我们对内存索引的IndexWriter进行初始化，在初始化的时候需要注意先执行一次commit，否则会提示no segments的异常 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051private static IndexWriter ramIndexWriter; private static RAMDirectory directory; static&#123; directory = new RAMDirectory(); try &#123; ramIndexWriter = getRAMIndexWriter(); &#125; catch (Exception e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125; &#125; public static RAMDirectory getRAMDireactory()&#123; return directory; &#125; public static IndexSearcher getIndexSearcher() throws IOException&#123; IndexReader reader = null; IndexSearcher searcher = null; try &#123; reader = DirectoryReader.open(directory); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; searcher = new IndexSearcher(reader); return searcher; &#125; /**单例模式获取ramIndexWriter * @return * @throws Exception */ public static IndexWriter getRAMIndexWriter() throws Exception&#123; if(ramIndexWriter == null)&#123; synchronized (IndexWriter.class) &#123; Analyzer analyzer = new IKAnalyzer(); IndexWriterConfig iwConfig = new IndexWriterConfig(analyzer); iwConfig.setOpenMode(OpenMode.CREATE_OR_APPEND); try &#123; ramIndexWriter = new IndexWriter(directory, iwConfig); ramIndexWriter.commit(); ramIndexWriter.close(); iwConfig = new IndexWriterConfig(analyzer); iwConfig.setOpenMode(OpenMode.CREATE_OR_APPEND); ramIndexWriter = new IndexWriter(directory, iwConfig); &#125; catch (IOException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125; &#125; &#125; return ramIndexWriter; &#125; 定义一个获取内存索引中数据条数的方法 1234567891011121314151617181920212223242526272829/**根据查询器、查询条件、每页数、排序条件进行查询 * @param query 查询条件 * @param first 起始值 * @param max 最大值 * @param sort 排序条件 * @return */ public static TopDocs getScoreDocsByPerPageAndSortField(IndexSearcher searcher,Query query, int first,int max, Sort sort)&#123; try &#123; if(query == null)&#123; System.out.println(" Query is null return null "); return null; &#125; TopFieldCollector collector = null; if(sort != null)&#123; collector = TopFieldCollector.create(sort, first+max, false, false, false); &#125;else&#123; SortField[] sortField = new SortField[1]; sortField[0] = new SortField("createTime",SortField.Type.STRING,true); Sort defaultSort = new Sort(sortField); collector = TopFieldCollector.create(defaultSort,first+max, false, false, false); &#125; searcher.search(query, collector); return collector.topDocs(first, max); &#125; catch (IOException e) &#123; // TODO Auto-generated catch block &#125; return null; &#125; 此方法返回结果为TopDocs，我们根据TopDocs的totalHits来获取内存索引中的数据条数，以此来鉴别内存占用，防止内存溢出。 consume方法的实现如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354/**消费数据 * @param docs * @param listSize * @param writer * @param list * @throws Exception */ public static void consume(IndexWriter writer, List list) throws Exception &#123; Query query = new MatchAllDocsQuery(); IndexSearcher searcher = getIndexSearcher(); System.out.println(directory); TopDocs topDocs = getScoreDocsByPerPageAndSortField(searcher,query, 1, 1, null); int currentTotal = topDocs.totalHits; if(currentTotal+list.size() &gt; Constants.XXX_RAM_LIMIT)&#123; //超出内存限制 int pulCount = Constants.XXX_RAM_LIMIT - currentTotal; List&lt;Document&gt; docs = new LinkedList&lt;Document&gt;(); if(pulCount &lt;= 0)&#123; //直接处理集合的内容 TopDocs allDocs = SearchUtil.getScoreDocsByPerPageAndSortField(searcher, query, 0,currentTotal, null); ScoreDoc[] scores = allDocs.scoreDocs; for(int i = 0 ;i &lt; scores.length ; i ++)&#123; //取出内存中的数据 Document doc1 = searcher.doc(scores[i].doc); Integer pollId = Integer.parseInt(doc1.get("id")); Document doc = delDocumentFromRAMDirectory(pollId); if(doc != null)&#123; XXX carSource = (XXX) BeanTransferUtil.doc2Bean(doc, XXX.class); Document doc2 = carSource2Document(carSource); if(doc2 != null)&#123; docs.add(doc2); &#125; &#125; &#125; addDocumentToFSDirectory(docs); writer = getRAMIndexWriter(); consume(writer, list); &#125;else&#123; //先取出未达到内存的部分 List subProcessList = list.subList(0, pulCount); consume(writer, subProcessList); List leaveList = list.subList(pulCount, list.size()); consume(writer, leaveList); &#125; &#125;else&#123;//未超出限制，直接存放到内存 int listSize = list.size(); if(listSize &gt; 0)&#123; //存放到内存 &#125; &#125; &#125; 上边的逻辑为： 根据getScoreDocsByPerPageAndSortField获取当前内存中的数据条数根据内存中数据数量A和本次获取的数据源的总数B和内存中限制的数量C进行比较如果A+B&lt;=C则未超出内存索引的限制，所有数据均存放到内存反之，判断当前内存中的数据是否已经达到限制，如果已经超出，则直接处理取出内存中的内容，然后回调此方法。如果未达到限制，先取出未达到限制的部分，然后对剩余的进行回调。 这里我们的BeanTransferUtil是根据document转换成对应的bean的方法，此处用到了反射和commons-beanutils.jar 12345678910111213141516171819202122232425262728293031323334package com.chechong.util; import java.lang.reflect.Field;import java.lang.reflect.InvocationTargetException; import org.apache.commons.beanutils.BeanUtils;import org.apache.lucene.document.Document; public class BeanTransferUtil &#123; public static Object doc2Bean(Document doc, Class clazz) &#123; try &#123; Object obj = clazz.newInstance(); Field[] fields = clazz.getDeclaredFields(); for (Field field : fields) &#123; field.setAccessible(true); String fieldName = field.getName(); BeanUtils.setProperty(obj, fieldName, doc.get(fieldName)); &#125; return obj; &#125; catch (InstantiationException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125; catch (IllegalAccessException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125; catch (InvocationTargetException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125; return null; &#125;&#125; 从内存索引中读取索引的方法如下： 12345678910111213141516171819202122232425/**从内存索引中删除指定的doc * @param pollId * @throws IOException */ private static Document delDocumentFromRAMDirectory(Integer pollId) throws IOException &#123; Document doc = null; Query query = SearchUtil.getQuery("id", "int", pollId+"", false); IndexSearcher searcher = getIndexSearcher(); try &#123; TopDocs queryDoc = SearchUtil.getScoreDocsByPerPageAndSortField(searcher, query, 0, 1, null); ScoreDoc[] docs = queryDoc.scoreDocs; System.out.println(docs.length); if(docs.length &gt; 0)&#123; doc = searcher.doc(docs[0].doc); System.out.println(doc); ramIndexWriter.deleteDocuments(query); ramIndexWriter.commit(); &#125; return doc; &#125; catch (IOException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125; return null; &#125; 此处是根据id来读取内存索引中的内容，然后将它转换成document同时删除内存中的对应记录。 NRT近实时查询的实现对于上边的索引我们要采用适当的查询方法，这里查询时候为了达到近实时的效果，需要将内存索引添加到查询的范围中，即IndexReader中。 这里的IndexSearcher的获取方法如下： 12345678910111213141516171819202122232425262728/**多目录多线程查询 * @param parentPath 父级索引目录 * @param service 多线程查询 * @param isAddRamDirectory 是否增加内存索引查询 * @return * @throws IOException */ public static IndexSearcher getMultiSearcher(String parentPath,ExecutorService service, boolean isAddRamDirectory) throws IOException&#123; File file = new File(parentPath); File[] files = file.listFiles(); IndexReader[] readers = null; if(!isAddRamDirectory)&#123; readers = new IndexReader[files.length]; &#125;else&#123; readers = new IndexReader[files.length+1]; &#125; for (int i = 0 ; i &lt; files.length ; i ++) &#123; readers[i] = DirectoryReader.open(FSDirectory.open(Paths.get(files[i].getPath(), new String[0]))); &#125; if(isAddRamDirectory)&#123; readers[files.length] = DirectoryReader.open(RamDirectoryControl.getRAMDireactory()); &#125; MultiReader multiReader = new MultiReader(readers); IndexSearcher searcher = new IndexSearcher(multiReader,service); return searcher; &#125; 如此，我们就可以在查询的时候既从文件索引中读取，也从内存索引中检索数据了；]]></content>
      <categories>
        <category>lucene</category>
      </categories>
      <tags>
        <tag>lucene</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[lucene的IndexWriter对象创建和索引策略的选择]]></title>
    <url>%2F2019%2F03%2F16%2FLucene%2F1%E3%80%81lucene%E7%9A%84IndexWriter%E5%AF%B9%E8%B1%A1%E5%88%9B%E5%BB%BA%E5%92%8C%E7%B4%A2%E5%BC%95%E7%AD%96%E7%95%A5%E7%9A%84%E9%80%89%E6%8B%A9%2F</url>
    <content type="text"><![CDATA[lucene（1）—lucene的IndexWriter对象创建和索引策略的选择因工作的需要（数据量大造成原有系统查询效率低），最近做了搜索引擎相关的内容，选择了lucene5版本（15年发布的）。 lucene是一个开放源代码的全文搜索引擎开发工具包，提供了简单强大的搜索引擎接口，其优点如下： 数据以索引文件的形式存储，索引文件可以跨平台，只要保证索引完整，复制到任何机器或者磁盘空间均可以查询索引内容； 在传统全文检索引擎的倒排索引的基础上，实现了分块索引，能够针对新的文件建立小文件索引，提升索引速度。然后通过与原有索引的合并，达到优化的目的； 索引的构建和查询都十分简洁，有强大的类库实现相关功能； 开发源代码，论坛和资源十分丰富。 索引的构建过程描述如下： 1）判断JRE版本是否为64位和是否支持堆外内存，并创建 ​ 1.1 如果满足条件，创建MMapDirectory，此种Directory可以有效的利用虚拟机内存地址空间 ； ​ 1.2 如果不满足以上条件，判断系统是否是windows,如果满足条件，创建SimpleFSDirectory，此种directory提供了性能不太高的多线程支持，lucene推荐使用NIOFSDirectory或者MMapDirectory来替代之； ​ 1.3 如果以上均不满足，创建NIOFSDirectory对象，此种directory的英文说明为 1An FSDirectory implementation that uses java.nio's FileChannel's positional read, which allows multiple threads to read from the same file without synchronizing 大意是一个利用了java nio中FileChannel的FSDirectory实现，允许无syschronized的对同一文件进行多线程读 2）词库分析器Analyzer创建（需要注意的是使用哪种Analyzer进行索引查询，创建的时候也要使用对应的索引器，否则查询结果有问题） 3）IndexWriterConfig对象创建,并获取IndexWriter对象 ​ 3.1 判断是覆盖索引还是追加索引，如果是覆盖索引indexWriterConfig.setOpenMode(IndexWriterConfig.OpenMode.CREATE); ​ 3.2 如果追加indexWriterConfig.setOpenMode(IndexWriterConfig.OpenMode.CREATE_OR_APPEND); ​ 4) 遍历根据要索引的对象列表，对单个对象的field进行lucene相关field构建，添加到Document对象中 ​ 5）IndexWriter对索引进行写入； ​ 6）IndexWriter执行commit()和close()结束索引创建过程 以lucene5为例，索引器的创建如下： 123456789101112131415161718 /**创建索引写入器 * @param indexPath * @param create * @return * @throws IOException */public static IndexWriter getIndexWriter(String indexPath,boolean create) throws IOException&#123; Directory dir = FSDirectory.open(Paths.get(indexPath, new String[0])); Analyzer analyzer = new StandardAnalyzer(); IndexWriterConfig iwc = new IndexWriterConfig(analyzer); if (create)&#123; iwc.setOpenMode(IndexWriterConfig.OpenMode.CREATE); &#125;else &#123; iwc.setOpenMode(IndexWriterConfig.OpenMode.CREATE_OR_APPEND); &#125; IndexWriter writer = new IndexWriter(dir, iwc); return writer;&#125; 下面给出当时工作需要的创建索引测试例子 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243public class MultiThreadIndexTest &#123; private static Pattern p_html = Pattern.compile("&lt;[^&gt;]+&gt;", Pattern.CASE_INSENSITIVE); public static void main(String[] args) throws IllegalArgumentException, IllegalAccessException, ParseException &#123; String indexPath = "D:\\LuceneIndex\\IQC\\ag\\voice\\20190201"; System.out.println("开始创建索引"); IQCConversationInfoBean iqcConversationInfoBean = new IQCConversationInfoBean(); iqcConversationInfoBean.setMediaType("voice"); iqcConversationInfoBean.setCallDate("20190201"); iqcConversationInfoBean.setCallTime(JCalendar.getDate("20190201145454", "yyyyMMddHHmmss")); iqcConversationInfoBean.setHangupTime(JCalendar.getDate("20190201145959", "yyyyMMddHHmmss")); // 挂机时间 iqcConversationInfoBean.setChannelCode("ag"); iqcConversationInfoBean.setAccountCode("Admin"); iqcConversationInfoBean.setEmpName("超级管理员"); iqcConversationInfoBean.setLanguage("1"); iqcConversationInfoBean.setCustNo("123456"); iqcConversationInfoBean.setCustName("洪尼玛"); iqcConversationInfoBean.setCallNo("18565279427"); iqcConversationInfoBean.setSatisfiedType("1"); // 满意度 // 座席通话内容 iqcConversationInfoBean.setAgentContent("您好，请问有什么可以帮到您请问是陈女士吗啊，对从您实名号，您是通过什么渠道转的？手机银行的您，是同行转账，还是跨行转账？那真没有到账的话，要以系统处理为准呢？他这些要以系统处理为准的，今天是年30系统的。我这边帮您看一下，嗯撤销不了哇，因为您当时选择的普通转账那这个你这什么时候到账，就看那边的系统处理为准的那您可以用此到家吗，你干嘛用普通到账呢啊啊我这边看到您用的是普通转账来的是系统转帐，"); // agentFirst iqcConversationInfoBean.setAgentFirst("您好，请问有什么可以帮到您请问是陈女士吗啊，对从您实名号，您是通过什么渠道转的？手机银行的您，是同行转账，还是跨行转账？那真没有到账的话，要以系统处理为准呢？他这些要以系统处理为准的，今天是年30系统的。我这边帮您看一下，嗯撤销不了哇，因为您当时选择的普通转账那这个你这什么时候到账，就看那边的系统处理为准的那您可以用此到家吗，你干嘛用普通到账呢啊啊我这边看到您用的是普通转账来的是系统转帐，"); // agentLast iqcConversationInfoBean.setAgentLast("已经登记好了"); // 客户通话内容 iqcConversationInfoBean.setCustContent("你好，我想问一下，我这将是广州农农商银行，现在我回来到这边那个密码，搞忘记了，可不可以在我换地方改密改密码呢，还还找密码呢"); // 客户首句 iqcConversationInfoBean.setCustFirst("你好，我想问一下，我这将是广州农农商银行，现在我回来到这边那个密码，搞忘记了，可不可以在我换地方改密改密码呢，还还找密码呢"); // 客户尾句 iqcConversationInfoBean.setCustLast("结果登记"); // 全部通话内容 for(int i=0;i&lt;5000;i++)&#123; iqcConversationInfoBean.setAllContent("&lt;li start=13070 end=14870 emotion=5.0 speed=4.0 &gt;坐席：您好，请问有什么可以帮您&lt;/li&gt;" + "&lt;li start=14880 end=21470 emotion=6.0 speed=3.0 &gt;客户：然后你们这个银行，这里呀，那柜员机老是故障啊，嗯，也没用过的维修的&lt;/li&gt;" + "&lt;li start=0 silence=6 class=\"silences\" &gt; 6S. &lt;/li&gt;" + "&lt;li start=21480 end=23270 emotion=6.0 speed=2.68 &gt;坐席：嗯，哪个柜员机呀&lt;/li&gt;" + "&lt;li start=23280 end=27780 emotion=6.0 speed=1.46 &gt;客户：我们广州市白云区江高镇&lt;/li&gt;" + "&lt;li start=29000 end=30200 emotion=6.0 speed=3.0 &gt;客户：然后，新楼村&lt;/li&gt;" + "&lt;li start=0 silence=6 class=\"silences\" &gt; 6S. &lt;/li&gt;" + "&lt;li start=30210 end=32250 emotion=6.0 speed=4.11 &gt;坐席：嗯桂圆是有没有那个订单号啊？&lt;/li&gt;" + "&lt;li start=32890 end=36190 emotion=6.0 speed=3.81 &gt;坐席：在屏幕上方有一个本机终端号的，有没有看到？&lt;/li&gt;" + "&lt;li start=36200 end=38590 emotion=6.0 speed=3.51 &gt;客户：没看，但是我我报你帮我查一下&lt;/li&gt;" + "&lt;li start=38600 end=40690 emotion=6.0 speed=4.01 &gt;坐席：那个地址在哪里啊，刚刚在哪里&lt;/li&gt;" + "&lt;li start=40700 end=45490 emotion=6.0 speed=2.12 &gt;客户：呃，江高镇，然后新楼村新楼路31号&lt;/li&gt;" + "&lt;li start=0 silence=4 class=\"silences\" &gt; 4S. &lt;/li&gt;" + "&lt;li start=45500 end=46630 emotion=5.0 speed=1.06 &gt;坐席：嗯好&lt;/li&gt;" + "&lt;li start=0 silence=11 class=\"silences\" &gt; 11S. &lt;/li&gt;" + "&lt;li start=58500 end=60550 emotion=6.0 speed=3.51 &gt;坐席：他旁边没有网点的他，旁边&lt;/li&gt;" + "&lt;li start=61420 end=64720 emotion=6.0 speed=2.36 &gt;客户：没有就这个柜员机，我们学校&lt;/li&gt;" + "&lt;li start=0 silence=4 class=\"silences\" &gt; 4S. &lt;/li&gt;" + "&lt;li start=64730 end=66520 emotion=6.0 speed=2.34 &gt;坐席：嗯，什么学校啊&lt;/li&gt;" + "&lt;li start=66530 end=68230 emotion=6.0 speed=2.47 &gt;客户：广东、江南理工&lt;/li&gt;" + "&lt;li start=68870 end=73970 emotion=6.0 speed=3.17 &gt;坐席：学校里面的嘛，对江南理工就是那个江西的，在南方的南吗？&lt;/li&gt;" + "&lt;li start=73980 end=79170 emotion=6.0 speed=1.38 &gt;客户：然后是江南江南是南方的南&lt;/li&gt;" + "&lt;li start=0 silence=6 class=\"silences\" &gt; 6S. &lt;/li&gt;" + "&lt;li start=80140 end=84040 emotion=6.0 speed=3.69 &gt;坐席：江南理工没有看到他这个有柜员，机的地方啊，他是在&lt;/li&gt;" + "&lt;li start=84050 end=86900 emotion=6.0 speed=2.1 &gt;客户：哦，江南理工技工学校&lt;/li&gt;" + "&lt;li start=0 silence=4 class=\"silences\" &gt; 4S. &lt;/li&gt;" + "&lt;li start=88210 end=89410 emotion=6.0 speed=3.0 &gt;坐席：在哪个区的？&lt;/li&gt;" + "&lt;li start=89420 end=91140 emotion=6.0 speed=2.09 &gt;客户：白云区江高镇&lt;/li&gt;" + "&lt;li start=98540 end=99740 emotion=6.0 speed=3.0 &gt;客户：然后那个金融&lt;/li&gt;" + "&lt;li start=0 silence=10 class=\"silences\" &gt; 10S. &lt;/li&gt;" + "&lt;li start=99750 end=105570 emotion=6.0 speed=2.47 &gt;坐席：他是在那个往港、煤炭地质局对面，那个技工学校吗？&lt;/li&gt;" + "&lt;li start=106250 end=109250 emotion=6.0 speed=3.0 &gt;客户：不是我们是白云区江高镇新楼村的&lt;/li&gt;" + "&lt;li start=0 silence=3 class=\"silences\" &gt; 3S. &lt;/li&gt;" + "&lt;li start=109260 end=111950 emotion=6.0 speed=2.23 &gt;坐席：新农村新是哪个新啊？&lt;/li&gt;" + "&lt;li start=111960 end=113150 emotion=6.0 speed=2.52 &gt;客户：新中国的新&lt;/li&gt;" + "&lt;li start=113160 end=114350 emotion=4.0 speed=0.5 &gt;坐席：嗯&lt;/li&gt;" + "&lt;li start=114360 end=116880 emotion=6.0 speed=3.33 &gt;客户：楼市大龙的龙村，是村庄的村。&lt;/li&gt;" + "&lt;li start=0 silence=3 class=\"silences\" &gt; 3S. &lt;/li&gt;" + "&lt;li start=118240 end=120340 emotion=6.0 speed=3.14 &gt;坐席：没有看到他这个地点有啊&lt;/li&gt;" + "&lt;li start=120350 end=123340 emotion=6.0 speed=3.01 &gt;客户：那我，们在银行的奇怪呢，是因为&lt;/li&gt;" + "&lt;li start=0 silence=3 class=\"silences\" &gt; 3S. &lt;/li&gt;" + "&lt;li start=123350 end=126640 emotion=6.0 speed=3.1 &gt;坐席：您要看一下，他去柜员机的位置才行啊&lt;/li&gt;" + "&lt;li start=126650 end=129340 emotion=6.0 speed=4.01 &gt;客户：我们会员就直接就是在我们学校"+i+"里面吗？&lt;/li&gt;" + "&lt;li start=129350 end=135630 emotion=7.0 speed=3.53 &gt;坐席：是什么学校啊，刚才跟你说了广东将的，其实没有跟他有这个学校，有没有全称呢？&lt;/li&gt;" + "&lt;li start=136300 end=139210 emotion=7.0 speed=2.26 &gt;客户：广东、江南理工技工学校&lt;/li&gt;" + "&lt;li start=0 silence=6 class=\"silences\" &gt; 6S. &lt;/li&gt;" + "&lt;li start=142190 end=148190 emotion=6.0 speed=2.8 &gt;坐席：但是，我们收他技工，学校就只有出来刚刚那个旺港煤炭地质局&lt;/li&gt;" + "&lt;li start=148200 end=158090 emotion=6.0 speed=2.73 &gt;客户：您的商品一双鞋呢，不，是旺旺，我们是在广州市白云区江高镇新楼村新楼路31号，这个详细的地址&lt;/li&gt;" + "&lt;li start=0 silence=9 class=\"silences\" &gt; 9S. &lt;/li&gt;" + "&lt;li start=158100 end=162340 emotion=6.0 speed=4.1 &gt;坐席：嗯，您学校地址，但是，他柜员机不，他不一定是这么登记的吗？&lt;/li&gt;" + "&lt;li start=162980 end=172900 emotion=6.0 speed=3.81 &gt;客户：那你这个是你们的问题了，你像我这个我是我现在就是就是现在要要告诉你们呢，这里有我们这个，这个地址啊，你们所学校有因为台风的呀啊&lt;/li&gt;" + "&lt;li start=0 silence=11 class=\"silences\" &gt; 11S. &lt;/li&gt;" + "&lt;li start=173540 end=174440 emotion=6.0 speed=3.33 &gt;坐席：好评返现。&lt;/li&gt;" + "&lt;li start=174450 end=178640 emotion=7.0 speed=3.57 &gt;客户：我整天啊，有问题一直，搞错了，你们要反映上去，上面&lt;/li&gt;" + "&lt;li start=0 silence=4 class=\"silences\" &gt; 4S. &lt;/li&gt;" + "&lt;li start=178650 end=184940 emotion=7.0 speed=3.52 &gt;坐席：但是，您这样去把您那个柜员机上面，它有一个模板套，您的会员区发布宝贝订单号&lt;/li&gt;" + "&lt;li start=185550 end=186360 emotion=7.0 speed=2.22 &gt;坐席：好不好&lt;/li&gt;" + "&lt;li start=184950 end=185540 emotion=6.0 speed=1.01 &gt;客户：嗯&lt;/li&gt;" + "&lt;li start=187000 end=190300 emotion=6.0 speed=3.81 &gt;坐席：但是，我们确实现在没有看到您这个地方，有啊&lt;/li&gt;" + "&lt;li start=190310 end=192100 emotion=7.0 speed=3.68 &gt;客户：那您说一下怎么办，因为&lt;/li&gt;" + "&lt;li start=192110 end=193600 emotion=6.0 speed=4.83 &gt;坐席：我们没有查到我们就没有办&lt;/li&gt;" + "&lt;li start=193610 end=196850 emotion=7.0 speed=4.44 &gt;客户：反馈到我想问一下，你，说你，你说让我现在怎么办？&lt;/li&gt;" + "&lt;li start=0 silence=3 class=\"silences\" &gt; 3S. &lt;/li&gt;" + "&lt;li start=197550 end=201450 emotion=6.0 speed=4.76 &gt;坐席：所以，您看一下您那个柜员机，他旁边是只有一台吗，还是什么情况啊&lt;/li&gt;" + "&lt;li start=201460 end=205500 emotion=6.0 speed=2.97 &gt;客户：有一台机黑屏，什么都看不到，整天出故障啊&lt;/li&gt;" + "&lt;li start=0 silence=5 class=\"silences\" &gt; 5S. &lt;/li&gt;" + "&lt;li start=206990 end=208360 emotion=6.0 speed=3.06 &gt;坐席：嗯，稍等一下啊&lt;/li&gt;" + "&lt;li start=209060 end=211320 emotion=6.0 speed=3.71 &gt;客户：你们现在真的老子真的有问题啊&lt;/li&gt;" + "&lt;li start=213040 end=220180 emotion=6.0 speed=3.78 &gt;客户：你报一个详细地址了，还问我是在哪个，你说的那个不是在那里，这里你们就往那low爆了，知道吧&lt;/li&gt;" + "&lt;li start=220820 end=223260 emotion=6.0 speed=2.7 &gt;客户：你要找人过来核实修改呀&lt;/li&gt;" + "&lt;li start=0 silence=33 class=\"silences\" &gt; 33S. &lt;/li&gt;" + "&lt;li start=256910 end=257430 emotion=4.0 speed=1.15 &gt;客户：嗯&lt;/li&gt;" + "&lt;li start=258590 end=259330 emotion=5.0 speed=1.62 &gt;客户：是吧&lt;/li&gt;"); iqcConversationInfoBean.setSerialNo(UUID.randomUUID().toString()); iqcConversationInfoBean.setAgentMaxSpeed((float)Math.round(Math.random()*100)); iqcConversationInfoBean.setAgentMinSpeed((float)Math.round(Math.random()*100)); iqcConversationInfoBean.setAgentAvgSpeed((float)Math.round(Math.random()*100)); iqcConversationInfoBean.setAgentMaxEmotion((float)Math.round(Math.random()*100)); iqcConversationInfoBean.setAgentMinEmotion((float)Math.round(Math.random()*100)); iqcConversationInfoBean.setAgentAvgEmotion((float)Math.round(Math.random()*100)); iqcConversationInfoBean.setCustMaxSpeed((float)Math.round(Math.random()*100)); iqcConversationInfoBean.setCustMinSpeed((float)Math.round(Math.random()*100)); iqcConversationInfoBean.setCustAvgSpeed((float)Math.round(Math.random()*100)); iqcConversationInfoBean.setCustMaxEmotion((float)Math.round(Math.random()*100)); iqcConversationInfoBean.setCustMinEmotion((float)Math.round(Math.random()*100)); iqcConversationInfoBean.setCustAvgEmotion((float)Math.round(Math.random()*100)); Map&lt;String, Object&gt; textFiled = ReflectUtil.reflectObjectToMap(iqcConversationInfoBean, true); Document doc = createDoc(textFiled); if(null!=doc)&#123; addDoc(indexPath, doc, null); &#125; //System.out.println(i+"索引创建完毕"); &#125; System.out.println("创建索引完毕"); &#125; private static Document createDoc(Map&lt;String, Object&gt; textFiled) &#123; Document doc = null; try &#123; doc = null; if (textFiled != null &amp;&amp; textFiled.size() &gt; 0) &#123; doc = new Document(); // 遍历需要增加到索引的属性 List&lt;String&gt; NOT_ANALYZED = new ArrayList&lt;String&gt;();// 非分析域，但是需要保存 for (String Not : "id;status;update;agentFirst;agentLast;agentContent;custFirst;custLast;custContent;allContent;".split(";")) &#123;// for (String Not : "id;status;update;mediaType;callDate;channelCode;accountCode;empName;language;custNo;custName;callNo;satisfiedType;agentFirst;agentLast;agentContent;custFirst;custLast;custContent;allContent;".split(";")) &#123; NOT_ANALYZED.add(Not); &#125; for (String keyName : textFiled.keySet()) &#123; if (keyName != null &amp;&amp; !keyName.isEmpty()) &#123; // if // (keyName.equals(SysConstant.config.getProperty("kbmsID"))||keyName.indexOf(SysConstant.config.getProperty("DIM"))&gt;=0) // &#123; if (keyName.equals("serialNo")) &#123; doc.add(LuceneHelper.getnotanalyzedField(keyName, textFiled.get(keyName))); &#125; else if (NOT_ANALYZED.contains(keyName)) &#123; Matcher m_html = p_html.matcher((String) textFiled.get(keyName)); doc.add(new Field(keyName, m_html.replaceAll(""), Field.Store.YES, Field.Index.NOT_ANALYZED)); &#125; else &#123; doc.add(LuceneHelper.getField(keyName, textFiled.get(keyName))); &#125; &#125; &#125; &#125; else &#123; System.out.println("无法初始化doc"); &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; return doc; &#125; public static Field getField(String fieldName, Object value) &#123; FieldType fieldType = new FieldType(); fieldType.setOmitNorms(true); fieldType.setIndexOptions(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS); fieldType.setStored(true); fieldType.setTokenized(false); if (value instanceof Integer) &#123; return new IntField(fieldName, Integer.parseInt(value.toString()), fieldType); // return new // NumericDocValuesField(fieldName,Long.parseLong(value.toString())); &#125; else if (value instanceof Long) &#123; return new LongField(fieldName, (Long) value, fieldType); &#125; else if (value instanceof Float) &#123; return new FloatField(fieldName, (Float) value, fieldType); &#125; else if (value instanceof Date) &#123; return new LongField(fieldName, ((Date) value).getTime(), fieldType); &#125; else &#123; return new TextField(fieldName, value.toString(), Field.Store.YES); &#125; &#125; private static boolean addDoc(String indexPath, Document doc, String message) &#123; boolean res = false; // 索引配置器 IndexWriterConfig iwc = null; IndexWriter indexWriter = null; try &#123; if (indexPath != null &amp;&amp; !indexPath.isEmpty() &amp;&amp; doc != null) &#123; File indexDir = new File(indexPath); if (!indexDir.exists()) indexDir.mkdirs(); if (indexDir.exists()) &#123; /* 创建索引文件 */ iwc = new IndexWriterConfig(ChineseAnalyzerUtil.getAnalyzer()); // 创建索引文件对象 Directory dir = FSDirectory.open(indexDir.toPath()); boolean isNeedCreate = indexDir.listFiles().length &gt; 0 ? false : true; if (isNeedCreate) &#123;// 是否需要创建,默认 CREATE_OR_APPEND iwc.setOpenMode(OpenMode.CREATE); System.out.println("创建新索引,OpenMode==&gt;&gt;CREATE"); &#125; else &#123; &#125; // 写入索引文件对象 indexWriter = new IndexWriter(dir, iwc); // 加载到索引中 // 遍历需要增加到索引的属性 // 加载到索引文档 // indexWriter.addDocument(doc); IndexableField kbmsID = doc.getField("serialNo"); if (kbmsID != null) &#123; String termId = kbmsID.name(); String termValue = kbmsID.stringValue(); indexWriter.updateDocument(new Term(termId, termValue), doc); &#125; else &#123; indexWriter.addDocument(doc); &#125; // 提交到索引 indexWriter.commit(); indexWriter.close(); res = true; &#125; else &#123; message = "contentPath 参数为空，正文无法读取，无法创建索引"; &#125; &#125; else &#123; message = "indexPath:" + indexPath + " 目录不存在，无法创建索引"; &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); System.out.println("创建Lucene索引异常"); &#125; if (message != null &amp;&amp; !message.isEmpty()) System.out.println("message-" + message); return res; &#125;&#125; 注意：创建索引的内用是对一通通话的内容进行]]></content>
      <categories>
        <category>lucene</category>
      </categories>
      <tags>
        <tag>lucene</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[lucene索引时join和查询时join使用示例]]></title>
    <url>%2F2019%2F03%2F16%2FLucene%2F18%E3%80%81lucene%E7%B4%A2%E5%BC%95%E6%97%B6join%E5%92%8C%E6%9F%A5%E8%AF%A2%E6%97%B6join%E4%BD%BF%E7%94%A8%E7%A4%BA%E4%BE%8B%2F</url>
    <content type="text"><![CDATA[lucene（18）—lucene索引时join和查询时join使用示例了解sql的朋友都知道，我们在查询的时候可以采用join查询，即对有一定关联关系的对象进行联合查询来对多维的数据进行整理。这个联合查询的方式挺方便的，跟我们现实生活中的托人找关系类似，我们想要完成一件事，先找自己的熟人，然后通过熟人在一次找到其他，最终通过这种手段找到想要联系到的人。有点类似于”世间万物皆有联系“的感觉。 lucene的join包提供了索引时join和查询时join的功能； Index-time join大意是索引时join提供了查询时join的支持，且IndexWriter.addDocuments()方法调用时被join的documents以单个document块存储索引。索引时join对普通文本内容（如xml文档或数据库表）是方便可用的。特别是对类似于数据库的那种多表关联的情况，我们需要对提供关联关系的列提供join支持； 在索引时join的时候，索引中的documents被分割成parent documents（每个索引块的最后一个document）和child documents (除了parent documents外的所有documents). 由于lucene并不记录doc块的信息，我们需要提供一个Filter来标示parent documents。 在搜索结果的时候，我们利用ToParentBlockJoinQuery来从child query到parent document space来remap/join对应的结果。 如果我们只关注匹配查询条件的parent documents，我们可以用任意的collector来采集匹配到的parent documents；如果我们还想采集匹配parent document查询条件的child documents，我们就需要利用ToParentBlockJoinCollector来进行查询；一旦查询完成，我们可以利用ToParentBlockJoinCollector.getTopGroups()来获取匹配条件的TopGroups. Query-time joins查询时join是基于索引词，其实现有两步： 第一步先从匹配fromQuery的fromField中采集所有的数据； 从第一步得到的数据中筛选出所有符合条件的documents 查询时join接收一下输入参数： fromField：fromField的名称，即要join的documents中的字段； formQuery: 用户的查询条件 multipleValuesPerDocument： fromField在document是否是多个值 scoreMode：定义other join side中score是如何被使用的。如果不关注scoring，我们只需要设置成ScoreMode.None，此种方式会忽略评分因此会更高效和节约内存； toField：toField的名称，即要join的toField的在对应的document中的字段 通常查询时join的实现类似于如下： 123456789String fromField = "from"; // Name of the from fieldboolean multipleValuesPerDocument = false; // Set only yo true in the case when your fromField has multiple values per document in your indexString toField = "to"; // Name of the to fieldScoreMode scoreMode = ScoreMode.Max // Defines how the scores are translated into the other side of the join.Query fromQuery = new TermQuery(new Term("content", searchTerm)); // Query executed to collect from values to join to the to values Query joinQuery = JoinUtil.createJoinQuery(fromField, multipleValuesPerDocument, toField, fromQuery, fromSearcher, scoreMode);TopDocs topDocs = toSearcher.search(joinQuery, 10); // Note: toSearcher can be the same as the fromSearcher// Render topDocs... 查询示例这里我们模拟6组数据，示例代码如下 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129package com.lucene.index.test; import static org.junit.Assert.assertEquals; import java.nio.file.Paths; import org.apache.lucene.analysis.Analyzer;import org.apache.lucene.analysis.standard.StandardAnalyzer;import org.apache.lucene.document.Document;import org.apache.lucene.document.Field;import org.apache.lucene.document.SortedDocValuesField;import org.apache.lucene.document.TextField;import org.apache.lucene.index.DirectoryReader;import org.apache.lucene.index.IndexReader;import org.apache.lucene.index.IndexWriter;import org.apache.lucene.index.IndexWriterConfig;import org.apache.lucene.index.IndexWriterConfig.OpenMode;import org.apache.lucene.index.Term;import org.apache.lucene.search.IndexSearcher;import org.apache.lucene.search.Query;import org.apache.lucene.search.TermQuery;import org.apache.lucene.search.TopDocs;import org.apache.lucene.search.join.JoinUtil;import org.apache.lucene.search.join.ScoreMode;import org.apache.lucene.store.Directory;import org.apache.lucene.store.FSDirectory;import org.apache.lucene.util.BytesRef;import org.junit.Test; public class TestJoin &#123; @Test public void testSimple() throws Exception &#123; final String idField = "id"; final String toField = "productId"; Directory dir = FSDirectory.open(Paths.get("index")); Analyzer analyzer = new StandardAnalyzer(); IndexWriterConfig config = new IndexWriterConfig(analyzer); config.setOpenMode(OpenMode.CREATE); IndexWriter w = new IndexWriter(dir, config); // 0 Document doc = new Document(); doc.add(new TextField("description", "random text", Field.Store.YES)); doc.add(new TextField("name", "name1", Field.Store.YES)); doc.add(new TextField(idField, "1", Field.Store.YES)); doc.add(new SortedDocValuesField(idField, new BytesRef("1"))); w.addDocument(doc); // 1 Document doc1 = new Document(); doc1.add(new TextField("price", "10.0", Field.Store.YES)); doc1.add(new TextField(idField, "2", Field.Store.YES)); doc1.add(new SortedDocValuesField(idField, new BytesRef("2"))); doc1.add(new TextField(toField, "1", Field.Store.YES)); doc1.add(new SortedDocValuesField(toField, new BytesRef("1"))); w.addDocument(doc1); // 2 Document doc2 = new Document(); doc2.add(new TextField("price", "20.0", Field.Store.YES)); doc2.add(new TextField(idField, "3", Field.Store.YES)); doc2.add(new SortedDocValuesField(idField, new BytesRef("3"))); doc2.add(new TextField(toField, "1", Field.Store.YES)); doc2.add(new SortedDocValuesField(toField, new BytesRef("1"))); w.addDocument(doc2); // 3 Document doc3 = new Document(); doc3.add(new TextField("description", "more random text", Field.Store.YES)); doc3.add(new TextField("name", "name2", Field.Store.YES)); doc3.add(new TextField(idField, "4", Field.Store.YES)); doc3.add(new SortedDocValuesField(idField, new BytesRef("4"))); w.addDocument(doc3); // 4 Document doc4 = new Document(); doc4.add(new TextField("price", "10.0", Field.Store.YES)); doc4.add(new TextField(idField, "5", Field.Store.YES)); doc4.add(new SortedDocValuesField(idField, new BytesRef("5"))); doc4.add(new TextField(toField, "4", Field.Store.YES)); doc4.add(new SortedDocValuesField(toField, new BytesRef("4"))); w.addDocument(doc4); // 5 Document doc5 = new Document(); doc5.add(new TextField("price", "20.0", Field.Store.YES)); doc5.add(new TextField(idField, "6", Field.Store.YES)); doc5.add(new SortedDocValuesField(idField, new BytesRef("6"))); doc5.add(new TextField(toField, "4", Field.Store.YES)); doc5.add(new SortedDocValuesField(toField, new BytesRef("4"))); w.addDocument(doc5); //6 Document doc6 = new Document(); doc6.add(new TextField(toField, "4", Field.Store.YES)); doc6.add(new SortedDocValuesField(toField, new BytesRef("4"))); w.addDocument(doc6); w.commit(); w.close(); IndexReader reader = DirectoryReader.open(dir); IndexSearcher indexSearcher = new IndexSearcher(reader); // Search for product Query joinQuery = JoinUtil.createJoinQuery(idField, false, toField, new TermQuery(new Term("name", "name2")), indexSearcher, ScoreMode.None); System.out.println(joinQuery); TopDocs result = indexSearcher.search(joinQuery, 10); System.out.println("查询到的匹配数据："+result.totalHits); joinQuery = JoinUtil.createJoinQuery(idField, false, toField, new TermQuery(new Term("name", "name1")), indexSearcher, ScoreMode.None); result = indexSearcher.search(joinQuery, 10); System.out.println("查询到的匹配数据："+result.totalHits); // Search for offer joinQuery = JoinUtil.createJoinQuery(toField, false, idField, new TermQuery(new Term("id", "5")), indexSearcher, ScoreMode.None); result = indexSearcher.search(joinQuery, 10); System.out.println("查询到的匹配数据："+result.totalHits); indexSearcher.getIndexReader().close(); dir.close(); &#125; &#125; 程序的运行结果如下： 123查询到的匹配数据：3查询到的匹配数据：2查询到的匹配数据：1 以第一个查询为例： 我们在查询的时候先根据name=name2这个查询条件找到记录为doc3的document,由于查询的是toField匹配的，我们在根据doc3找到其toField的值为4，然后查询条件变为productId:4，找出除本条记录外的其他数据，结果正好为3，符合条件。]]></content>
      <categories>
        <category>lucene</category>
      </categories>
      <tags>
        <tag>lucene</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[lucene搜索之expressions表达式处理]]></title>
    <url>%2F2019%2F03%2F16%2FLucene%2F17%E3%80%81lucene%E6%90%9C%E7%B4%A2%E4%B9%8Bexpressions%E8%A1%A8%E8%BE%BE%E5%BC%8F%E5%A4%84%E7%90%86%2F</url>
    <content type="text"><![CDATA[lucene（17）—lucene搜索之expressions表达式处理有时候我们在做lucene的结果展示的时候可能需要对多个列的内容进行计算，根据多个field对应的值做数值方面的运算。 lucene自4.6版本起，提供了用于运算的expression模块； expression分为两部分： org.apache.lucene.expressions：提供了字段绑定和相关的表达式参数传递的功能； org.apache.lucene.expressions.js:提供了表达式定义的功能。 Expression类使用示例Expression是提供document的运算的支持类； 我们的运算表达式和其绑定内容通常类似于如下： 123456789101112// compile an expression:Expression expr = JavascriptCompiler.compile("sqrt(_score) + ln(popularity)");// SimpleBindings just maps variables to SortField instancesSimpleBindings bindings = new SimpleBindings(); bindings.add(new SortField("_score", SortField.Type.SCORE));bindings.add(new SortField("popularity", SortField.Type.INT));// create a sort field and sort by it (reverse order)Sort sort = new Sort(expr.getSortField(bindings, true));Query query = new TermQuery(new Term("body", "contents"));searcher.search(query, null, 10, sort); 如上所示，我们对document中的_score和popularity两个字段进行值的运算，这里是对_score开平方之后和popularity的对数运算求和，运算方式的定义在第一行； 下边有定义了SimpleBindings，binding主要是对运算的数据进行数据绑定； 最终的查询结果是根据以上的运算结果采取倒排序的方式表达式说明表达式的构造可以采用如下的几种来进行组合： 数值型的 加减乘除取模（+-*/%）等运算符 移位运算符：| &amp; ^ ~ &lt;&lt; &gt;&gt; &gt;&gt;&gt; 布尔运算符（包括三目运算符）： &amp;&amp; || ! ?: 比较运算符：&lt; &lt;= == &gt;= &gt; 数学运算函数：abs ceil exp floor ln log10 logn max min sqrt pow 三角运算函数：acosh acos asinh asin atanh atan atan2 cosh cos sinh sin tanh tan haversin公式 min,max函数 代码示例： 我写了一个测试程序，模拟长方形的运算并排序； 面积倒序排序；当面积相同时，按宽度倒序，长度倒序； 周长倒序排序；周长相同时，按宽度倒序，长度倒序； 示例程序如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132package com.lucene.expression; import java.io.IOException;import java.nio.file.Paths;import java.text.ParseException; import org.apache.lucene.analysis.Analyzer;import org.apache.lucene.analysis.standard.StandardAnalyzer;import org.apache.lucene.document.Document;import org.apache.lucene.document.Field;import org.apache.lucene.document.IntField;import org.apache.lucene.document.NumericDocValuesField;import org.apache.lucene.expressions.Expression;import org.apache.lucene.expressions.SimpleBindings;import org.apache.lucene.expressions.js.JavascriptCompiler;import org.apache.lucene.index.DirectoryReader;import org.apache.lucene.index.IndexReader;import org.apache.lucene.index.IndexWriter;import org.apache.lucene.index.IndexWriterConfig;import org.apache.lucene.index.IndexWriterConfig.OpenMode;import org.apache.lucene.search.IndexSearcher;import org.apache.lucene.search.MatchAllDocsQuery;import org.apache.lucene.search.Query;import org.apache.lucene.search.ScoreDoc;import org.apache.lucene.search.Sort;import org.apache.lucene.search.SortField;import org.apache.lucene.search.TopDocs;import org.apache.lucene.store.Directory;import org.apache.lucene.store.FSDirectory;import org.junit.Test; public class LuceneExpressionTest&#123; @Test public void index()&#123; try &#123; Directory directory = FSDirectory.open(Paths.get("index")); Analyzer analyzer = new StandardAnalyzer(); IndexWriterConfig config = new IndexWriterConfig(analyzer); config.setOpenMode(OpenMode.CREATE_OR_APPEND); IndexWriter writer = new IndexWriter(directory, config); Document doc = new Document(); //模拟长方形 doc.add(new IntField("width", 3,Field.Store.YES)); doc.add(new IntField("longth", 4,Field.Store.YES)); doc.add(new NumericDocValuesField("width", 3)); doc.add(new NumericDocValuesField("longth", 4)); writer.addDocument(doc); Document doc1 = new Document(); doc1.add(new IntField("width", 2,Field.Store.YES)); doc1.add(new IntField("longth", 5,Field.Store.YES)); doc1.add(new NumericDocValuesField("width", 2)); doc1.add(new NumericDocValuesField("longth", 5)); writer.addDocument(doc1); Document doc2 = new Document(); doc2.add(new IntField("width", 2,Field.Store.YES)); doc2.add(new IntField("longth", 6,Field.Store.YES)); doc2.add(new NumericDocValuesField("width", 2)); doc2.add(new NumericDocValuesField("longth", 6)); writer.addDocument(doc2); writer.commit(); writer.close(); &#125; catch (IOException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125; &#125; /** * 面积比较 */ @Test public void testAcreage()&#123; try &#123; Expression expr = JavascriptCompiler.compile("width*longth"); SimpleBindings bindings = new SimpleBindings(); bindings.add(new SortField("width", SortField.Type.INT)); bindings.add(new SortField("longth", SortField.Type.INT)); Sort sort = new Sort(expr.getSortField(bindings, true)); Query query = new MatchAllDocsQuery(); Directory directory = FSDirectory.open(Paths.get("index")); IndexReader reader = DirectoryReader.open(directory); IndexSearcher searcher = new IndexSearcher(reader); TopDocs docs = searcher.search(query, 10, sort); for (ScoreDoc scoreDoc : docs.scoreDocs) &#123; System.out.println(searcher.doc(scoreDoc.doc)); &#125; &#125; catch (ParseException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125; catch (IOException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125; &#125; /** * 周长比较 */ @Test public void testCircum()&#123; try &#123; Expression expr = JavascriptCompiler.compile("width+longth+sqrt(pow(width,2)+pow(longth,2))"); SimpleBindings bindings = new SimpleBindings(); bindings.add(new SortField("width", SortField.Type.INT)); bindings.add(new SortField("longth", SortField.Type.INT)); Sort sort = new Sort(expr.getSortField(bindings, true)); Query query = new MatchAllDocsQuery(); Directory directory = FSDirectory.open(Paths.get("index")); IndexReader reader = DirectoryReader.open(directory); IndexSearcher searcher = new IndexSearcher(reader); TopDocs docs = searcher.search(query, 10, sort); for (ScoreDoc scoreDoc : docs.scoreDocs) &#123; System.out.println(searcher.doc(scoreDoc.doc)); &#125; &#125; catch (ParseException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125; catch (IOException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125; &#125;&#125; 面积的比较运行结果为： 123Document&lt;stored&lt;width:3&gt; stored&lt;longth:4&gt;&gt;Document&lt;stored&lt;width:2&gt; stored&lt;longth:6&gt;&gt;Document&lt;stored&lt;width:2&gt; stored&lt;longth:5&gt;&gt; 同样的，周长比较的运行结果为： 123Document&lt;stored&lt;width:2&gt; stored&lt;longth:6&gt;&gt;Document&lt;stored&lt;width:2&gt; stored&lt;longth:5&gt;&gt;Document&lt;stored&lt;width:3&gt; stored&lt;longth:4&gt;&gt; 以上是lucene的expression的应用,源代码下载地址： http://download.csdn.net/detail/wuyinggui10000/8762423]]></content>
      <categories>
        <category>lucene</category>
      </categories>
      <tags>
        <tag>lucene</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[lucene搜索之facet查询查询示例（2）]]></title>
    <url>%2F2019%2F03%2F16%2FLucene%2F16%E3%80%81lucene%E6%90%9C%E7%B4%A2%E4%B9%8Bfacet%E6%9F%A5%E8%AF%A2%E6%9F%A5%E8%AF%A2%E7%A4%BA%E4%BE%8B%EF%BC%882%EF%BC%89%2F</url>
    <content type="text"><![CDATA[lucene（16）—lucene搜索之facet查询查询示例（2）lucene（14）—lucene搜索之facet索引原理和facet查询实例，上篇主要是统计facet的dim和每个种类对应的数量，个人感觉这个跟lucene的group不同的在于facet的存储类似于hash（key-field-value）形式的，而group则是单一的map（key-value）形式的，虽然都可以统计某一品类的数量，显然facet更具扩展性。 key-field-value查询facet可以对某一个维度的满足某个条件的结果进行统计，如下： 123456789101112131415161718 @Test public void testDrillDownSlide()&#123; try &#123; DirectoryReader indexReader = DirectoryReader.open(directory); IndexSearcher searcher = new IndexSearcher(indexReader); DrillSideways ds = new DrillSideways(searcher, config, taxoReader); DrillDownQuery ddq = new DrillDownQuery(config); ddq.add("filePath", "ik"); DrillSidewaysResult r = ds.search(ddq, 10); TopDocs hits = r.hits; for (ScoreDoc scoreDoc : hits.scoreDocs) &#123; Document doc = searcher.doc(scoreDoc.doc); System.out.println(doc.get("path")); &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; 这里我们搜索的dim是filePath，查找的范围是ik相关联的数据，对应的查询结果就是所有包含在IK文件夹下的数据 12345678910C:\Users\lenovo\Desktop\lucene\IK-Analyzer-2012FF\dist\IKAnalyzer.cfg.xmlC:\Users\lenovo\Desktop\lucene\IK-Analyzer-2012FF\dist\IKAnalyzer2012FF_u1.jarC:\Users\lenovo\Desktop\lucene\IK-Analyzer-2012FF\dist\IKAnalyzer2015.jarC:\Users\lenovo\Desktop\lucene\IK-Analyzer-2012FF\dist\LICENSE.txtC:\Users\lenovo\Desktop\lucene\IK-Analyzer-2012FF\dist\NOTICE.txtC:\Users\lenovo\Desktop\lucene\IK-Analyzer-2012FF\dist\stopword.dicC:\Users\lenovo\Desktop\lucene\IK-Analyzer-2012FF\doc\allclasses-frame.htmlC:\Users\lenovo\Desktop\lucene\IK-Analyzer-2012FF\doc\allclasses-noframe.htmlC:\Users\lenovo\Desktop\lucene\IK-Analyzer-2012FF\doc\constant-values.htmlC:\Users\lenovo\Desktop\lucene\IK-Analyzer-2012FF\doc\deprecated-list.html range查询facet还支持range查询，range查询的类型包括DoubleRange和LongRange；其对应的Facets为DoubleRangeFacets和LongRangeFacets; 以LongRangeFacetCounts为例，LongRangeFacetCounts可以对long类型的数值进行整理查询 这里我们对每个文档的单词数量进行区间的分组，range查询示例如下： 123456789101112131415161718@Testpublic void testOverlappedEndStart()&#123; try &#123; IndexReader reader = DirectoryReader.open(directory); FacetsCollector fc = new FacetsCollector(); IndexSearcher s = new IndexSearcher(reader); s.search(new MatchAllDocsQuery(), fc); Facets facets = new LongRangeFacetCounts("contentLength", fc, new LongRange("0-100", 0L, true, 100L, true), new LongRange("100-200", 100L, true, 200L, true), new LongRange("200-300", 200L, true, 300L, true), new LongRange("300-400", 300L, true, 400L, true)); FacetResult result = facets.getTopChildren(10, "contentLength"); System.out.println(result.toString()); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125;&#125; 其执行结果为： 12345dim=contentLength path=[] value=22 childCount=4 0-100 (7) 100-200 (9) 200-300 (3) 300-400 (3) 多个dim查询facet里DrillSideways可以定义多个facetCount的查询，这时返回的结果为各个facet对应的统计数 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849@Testpublic void testMixedRangeAndNonRangeTaxonomy()&#123; try &#123; IndexReader reader = DirectoryReader.open(directory); IndexSearcher s = new IndexSearcher(reader); DrillSideways ds = new DrillSideways(s, config, taxoReader)&#123; @Override protected Facets buildFacetsResult(FacetsCollector drillDowns, FacetsCollector[] drillSideways, String[] drillSidewaysDims) throws IOException &#123; FacetsCollector fieldFC = drillDowns; FacetsCollector dimFC = drillDowns; if (drillSideways != null) &#123; for(int i=0;i&lt;drillSideways.length;i++) &#123; String dim = drillSidewaysDims[i]; if (dim.equals("contentLength")) &#123; fieldFC = drillSideways[i]; &#125; else &#123; dimFC = drillSideways[i]; &#125; &#125; &#125; Map&lt;String,Facets&gt; byDim = new HashMap&lt;String,Facets&gt;(); byDim.put("contentLength",new LongRangeFacetCounts("contentLength", fieldFC, new LongRange("less than 100", 0L, true, 100L, false), new LongRange("between 100 and 200", 100L, true, 200L, false), new LongRange("over 200", 200L, true, Integer.MAX_VALUE, false))); byDim.put("dim", new FastTaxonomyFacetCounts(taxoReader, config, dimFC)); return new MultiFacets(byDim); &#125; @Override protected boolean scoreSubDocsAtOnce() &#123; return false; &#125; &#125;; DrillDownQuery ddq = new DrillDownQuery(config); DrillSidewaysResult dsr = ds.search(ddq, 10); Facets facet = dsr.facets; List&lt;FacetResult&gt; results = facet.getAllDims(reader.maxDoc()); for (FacetResult facetResult : results) &#123; System.out.println(facetResult.dim); LabelAndValue[] values = facetResult.labelValues; for (LabelAndValue labelAndValue : values) &#123; System.out.println("\t"+labelAndValue.label +" "+labelAndValue.value); &#125; &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125;&#125; 程序运行结果如下： 1234567dim odd 126 even 119contentLength less than 100 7 between 100 and 200 9 over 200 229 对单个range的列表查询支持facet支持单个range的区间查询，这样可以查询出此range对饮的TopDocs列表，等同于返回了document对象列表； 这里我们查询内容长度在0到100之间的数据 12345678910111213141516@Testpublic void testDrillDownQueryWithRange()&#123; try &#123; IndexReader reader = DirectoryReader.open(directory); IndexSearcher s = new IndexSearcher(reader); DrillDownQuery ddq = new DrillDownQuery(config); ddq.add("contentLength", NumericRangeQuery.newLongRange("contentLength", 0l, 100l, true, false));//; TopDocs docs = s.search(ddq, reader.maxDoc()); System.out.println("查询到的数据总数："+docs.totalHits); for (ScoreDoc scoreDoc : docs.scoreDocs) &#123; System.out.println(s.doc(scoreDoc.doc).get("path")); &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125;&#125; 其运行结果如下： 12345678查询到的数据总数：7C:\Users\lenovo\Desktop\lucene\jcseg\DONATE.txtC:\Users\lenovo\Desktop\lucene\jcseg\jcseg-elasticsearch\src\main\resources\es-plugin.propertiesC:\Users\lenovo\Desktop\lucene\jcseg\lexicon\lex-autoload.todoC:\Users\lenovo\Desktop\lucene\jcseg\lexicon\lex-en-pun.lexC:\Users\lenovo\Desktop\lucene\jcseg\lexicon\lex-ln-adorn.lexC:\Users\lenovo\Desktop\lucene\IK-Analyzer-2012FF\doc\resources\inherit.gifC:\Users\lenovo\Desktop\lucene\IK-Analyzer-2012FF\src\ext.dic 本节内容都是示例，个人觉得这种会比较直观些，facet涉及的面比较广，这里没有facet的sort和其他相关操作，会在后续补上，希望大家持续关注。]]></content>
      <categories>
        <category>lucene</category>
      </categories>
      <tags>
        <tag>lucene</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java读取word excel pdf及lucene搜索之正则表达式查询RegExQuery和手机邮箱查询示例]]></title>
    <url>%2F2019%2F03%2F16%2FLucene%2F15%E3%80%81java%E8%AF%BB%E5%8F%96word%20excel%20pdf%E5%8F%8Alucene%E6%90%9C%E7%B4%A2%E4%B9%8B%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F%E6%9F%A5%E8%AF%A2RegExQuery%E5%92%8C%E6%89%8B%E6%9C%BA%E9%82%AE%E7%AE%B1%E6%9F%A5%E8%AF%A2%E7%A4%BA%E4%BE%8B%2F</url>
    <content type="text"><![CDATA[lucene（15）—java读取word excel pdf及lucene搜索之正则表达式查询RegExQuery和手机邮箱查询示例读取文本文件中的内容，找出文件中的手机号和邮箱，我自己写了一个读取文档的内容的正则查询示例，用于匹配文件中是否含有邮箱或者手机号，这个等于是对之前的文本处理工具的一个梳理，同时结合lucene内部提供的正则匹配查询RegexQuery； 废话不多说了，直接上代码，这里先对文件内容读取分类处理，分为pdf word excel 和普通文本四类，不同的种类读取文本内容不一样 pdf利用pdfbox读取内容，word和excel利用poi进行读取内容，文本文档利用jdk自带的读取 读取pdf、word、excel和普通文本文档内容（支持word excel 2007）这里代码做了一点调整， 主要是对excel格式的空行和空列的过滤 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175package com.lucene.index.util;import java.io.File;import java.io.FileInputStream;import java.io.FileNotFoundException;import java.io.IOException;import java.io.InputStream;import java.nio.charset.Charset;import java.nio.file.Files;import java.nio.file.Paths;import java.util.LinkedList;import java.util.List;import org.apache.pdfbox.PDFReader;import org.apache.pdfbox.pdmodel.PDDocument;import org.apache.pdfbox.util.PDFTextStripper;import org.apache.poi.EncryptedDocumentException;import org.apache.poi.POIXMLDocument;import org.apache.poi.POIXMLTextExtractor;import org.apache.poi.hssf.usermodel.HSSFCell;import org.apache.poi.hssf.usermodel.HSSFRow;import org.apache.poi.hwpf.extractor.WordExtractor;import org.apache.poi.openxml4j.exceptions.InvalidFormatException;import org.apache.poi.openxml4j.exceptions.OpenXML4JException;import org.apache.poi.openxml4j.opc.OPCPackage;import org.apache.poi.ss.usermodel.Cell;import org.apache.poi.ss.usermodel.CellStyle;import org.apache.poi.ss.usermodel.Sheet;import org.apache.poi.ss.usermodel.Workbook;import org.apache.poi.ss.usermodel.WorkbookFactory;import org.apache.poi.xssf.usermodel.XSSFCell;import org.apache.poi.xssf.usermodel.XSSFRow;import org.apache.poi.xssf.usermodel.XSSFSheet;import org.apache.poi.xssf.usermodel.XSSFWorkbook;import org.apache.poi.xwpf.extractor.XWPFWordExtractor;import org.apache.xmlbeans.XmlException;import com.lucene.bean.FileBean; public class FileUtil &#123; /**读取文件信息和下属文件夹 * @param folder * @return * @throws IOException * @throws OpenXML4JException * @throws XmlException */ public static List&lt;FileBean&gt; getFolderFiles(String folder) throws Exception &#123; List&lt;FileBean&gt; fileBeans = new LinkedList&lt;FileBean&gt;(); File file = new File(folder); if(file.isDirectory())&#123; File[] files = file.listFiles(); if(files != null)&#123; for (File file2 : files) &#123; fileBeans.addAll(getFolderFiles(file2.getAbsolutePath())); &#125; &#125; &#125;else&#123; FileBean bean = new FileBean(); String filePath = file.getAbsolutePath(); bean.setPath(file.getAbsolutePath()); bean.setModified(file.lastModified()); String content = ""; if(filePath.endsWith(".doc") || filePath.endsWith(".docx"))&#123; content = readDoc(file); &#125;else if(filePath.endsWith(".xls") || filePath.endsWith(".xlsx"))&#123; content = readExcel(file); &#125;else if(filePath.endsWith(".pdf"))&#123; content = readPdf(file); &#125;else&#123; content = new String(Files.readAllBytes(Paths.get(folder))); &#125; bean.setContent(content); fileBeans.add(bean); &#125; return fileBeans; &#125; /**讀取excel文件 * @param file * @return * @throws IOException * @throws InvalidFormatException * @throws EncryptedDocumentException */ public static String readExcel(File file) throws Exception &#123; String filePath = file.getAbsolutePath(); StringBuffer content = new StringBuffer(""); if(filePath.endsWith(".xls"))&#123; InputStream inp = new FileInputStream(filePath); Workbook wb = WorkbookFactory.create(inp); Sheet sheet = wb.getSheetAt(0); for(int i = sheet.getFirstRowNum();i&lt;= sheet.getPhysicalNumberOfRows();i++)&#123; HSSFRow row = (HSSFRow) sheet.getRow(i); if (row == null) &#123; continue; &#125; for (int j = row.getFirstCellNum(); j &lt;= row.getLastCellNum(); j++) &#123; if(j &lt; 0)&#123; continue;//增加下标判断 &#125; HSSFCell cell = row.getCell(j); if (cell == null) &#123; continue; &#125; content.append(cell.getStringCellValue()); &#125; &#125; wb.close(); inp.close(); &#125;else&#123; XSSFWorkbook xwb = new XSSFWorkbook(file.getAbsolutePath()); XSSFSheet sheet = xwb.getSheetAt(0); // 定义 row、cell XSSFRow row; String cell; // 循环输出表格中的内容 for (int i = sheet.getFirstRowNum(); i &lt; sheet.getPhysicalNumberOfRows(); i++) &#123; row = sheet.getRow(i); if(row == null)&#123; continue; &#125; for (int j = row.getFirstCellNum(); j &lt; row.getPhysicalNumberOfCells(); j++) &#123; // 通过 row.getCell(j).toString() 获取单元格内容， if(j&lt;0)&#123; continue; &#125; XSSFCell xfcell = row.getCell(j); if(xfcell == null)&#123; continue; &#125; xfcell.setCellType(Cell.CELL_TYPE_STRING);//数值型的转成文本型 cell = xfcell.getStringCellValue(); content.append(cell+" "); &#125; &#125; &#125; return content.toString(); &#125; /**讀取word內容 * @param file * @return * @throws IOException * @throws OpenXML4JException * @throws XmlException */ public static String readDoc(File file) throws IOException, XmlException, OpenXML4JException &#123; String filePath = file.getAbsolutePath(); if(filePath.endsWith(".doc"))&#123; InputStream is = new FileInputStream(file); WordExtractor ex = new WordExtractor(is); String text2003 = ex.getText(); ex.close(); is.close(); return text2003; &#125;else&#123; OPCPackage opcPackage = POIXMLDocument.openPackage(filePath); POIXMLTextExtractor extractor = new XWPFWordExtractor(opcPackage); String text2007 = extractor.getText(); extractor.close(); return text2007; &#125; &#125; /**讀取pdf內容 * @param file * @return * @throws IOException */ public static String readPdf(File file) throws IOException&#123; PDDocument doc = PDDocument.load(file.getAbsolutePath()); PDFTextStripper stripper = new PDFTextStripper(); String content = stripper.getText(doc); doc.close(); return content; &#125;&#125; 正则查询query构建在原有 lucene 查询的工具类的基础上加入正则查询的构建 1234567891011 /**获取regexQuery对象 * @param field * @param regex * @return */public static Query getRegexExpQuery(String field,String regex)&#123; Query query = null; Term term = new Term(field, regex); query = new RegexpQuery(term); return query;&#125; 最终的searchUtil的内容为 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235package com.lucene.search;import java.io.File;import java.io.IOException;import java.nio.file.Paths;import java.util.concurrent.ExecutorService;import org.apache.lucene.analysis.Analyzer;import org.apache.lucene.analysis.standard.StandardAnalyzer;import org.apache.lucene.document.Document;import org.apache.lucene.index.DirectoryReader;import org.apache.lucene.index.IndexReader;import org.apache.lucene.index.MultiReader;import org.apache.lucene.index.Term;import org.apache.lucene.search.BooleanClause.Occur;import org.apache.lucene.search.BooleanQuery;import org.apache.lucene.search.IndexSearcher;import org.apache.lucene.search.MatchAllDocsQuery;import org.apache.lucene.search.NumericRangeQuery;import org.apache.lucene.search.Query;import org.apache.lucene.search.RegexpQuery;import org.apache.lucene.search.ScoreDoc;import org.apache.lucene.search.Sort;import org.apache.lucene.search.SortField;import org.apache.lucene.search.SortField.Type;import org.apache.lucene.search.TermQuery;import org.apache.lucene.search.TopDocs;import org.apache.lucene.search.TopFieldCollector;import org.apache.lucene.store.FSDirectory;import org.wltea.analyzer.lucene.IKAnalyzer;import com.lucene.index.IndexUtil; public class SearchUtil &#123; public static final Analyzer analyzer = new IKAnalyzer(); /**获取IndexSearcher对象（适合单索引目录查询使用） * @param indexPath 索引目录 * @return * @throws IOException * @throws InterruptedException */ public static IndexSearcher getIndexSearcher(String indexPath,ExecutorService service,boolean realtime) throws IOException, InterruptedException&#123; DirectoryReader reader = DirectoryReader.open(IndexUtil.getIndexWriter(indexPath, true), realtime); IndexSearcher searcher = new IndexSearcher(reader,service); if(service != null)&#123; service.shutdown(); &#125; return searcher; &#125; /**多目录多线程查询 * @param parentPath 父级索引目录 * @param service 多线程查询 * @return * @throws IOException * @throws InterruptedException */ public static IndexSearcher getMultiSearcher(String parentPath,ExecutorService service,boolean realtime) throws IOException, InterruptedException&#123; MultiReader multiReader; File file = new File(parentPath); File[] files = file.listFiles(); IndexReader[] readers = new IndexReader[files.length]; if(!realtime)&#123; for (int i = 0 ; i &lt; files.length ; i ++) &#123; readers[i] = DirectoryReader.open(FSDirectory.open(Paths.get(files[i].getPath(), new String[0]))); &#125; &#125;else&#123; for (int i = 0 ; i &lt; files.length ; i ++) &#123; readers[i] = DirectoryReader.open(IndexUtil.getIndexWriter(files[i].getPath(), true), true); &#125; &#125; multiReader = new MultiReader(readers); IndexSearcher searcher = new IndexSearcher(multiReader,service); if(service != null)&#123; service.shutdown(); &#125; return searcher; &#125; /**从指定配置项中查询 * @return * @param analyzer 分词器 * @param field 字段 * @param fieldType 字段类型 * @param queryStr 查询条件 * @param range 是否区间查询 * @return */ public static Query getQuery(String field,String fieldType,String queryStr,boolean range)&#123; Query q = null; if(queryStr != null &amp;&amp; !"".equals(queryStr))&#123; if(range)&#123; String[] strs = queryStr.split("\\|"); if("int".equals(fieldType))&#123; int min = new Integer(strs[0]); int max = new Integer(strs[1]); q = NumericRangeQuery.newIntRange(field, min, max, true, true); &#125;else if("double".equals(fieldType))&#123; Double min = new Double(strs[0]); Double max = new Double(strs[1]); q = NumericRangeQuery.newDoubleRange(field, min, max, true, true); &#125;else if("float".equals(fieldType))&#123; Float min = new Float(strs[0]); Float max = new Float(strs[1]); q = NumericRangeQuery.newFloatRange(field, min, max, true, true); &#125;else if("long".equals(fieldType))&#123; Long min = new Long(strs[0]); Long max = new Long(strs[1]); q = NumericRangeQuery.newLongRange(field, min, max, true, true); &#125; &#125;else&#123; if("int".equals(fieldType))&#123; q = NumericRangeQuery.newIntRange(field, new Integer(queryStr), new Integer(queryStr), true, true); &#125;else if("double".equals(fieldType))&#123; q = NumericRangeQuery.newDoubleRange(field, new Double(queryStr), new Double(queryStr), true, true); &#125;else if("float".equals(fieldType))&#123; q = NumericRangeQuery.newFloatRange(field, new Float(queryStr), new Float(queryStr), true, true); &#125;else&#123; Term term = new Term(field, queryStr); q = new TermQuery(term); &#125; &#125; &#125;else&#123; q= new MatchAllDocsQuery(); &#125; System.out.println(q); return q; &#125; /**多条件查询类似于sql in * @param querys * @return */ public static Query getMultiQueryLikeSqlIn(Query ... querys)&#123; BooleanQuery query = new BooleanQuery(); for (Query subQuery : querys) &#123; query.add(subQuery,Occur.SHOULD); &#125; return query; &#125; /**获取regexQuery对象 * @param field * @param regex * @return */ public static Query getRegexExpQuery(String field,String regex)&#123; Query query = null; Term term = new Term(field, regex); query = new RegexpQuery(term); return query; &#125; /**多条件查询类似于sql and * @param querys * @return */ public static Query getMultiQueryLikeSqlAnd(Query ... querys)&#123; BooleanQuery query = new BooleanQuery(); for (Query subQuery : querys) &#123; query.add(subQuery,Occur.MUST); &#125; return query; &#125; /**对多个条件进行排序构建排序条件 * @param fields * @param type * @param reverses * @return */ public static Sort getSortInfo(String[] fields,Type[] types,boolean[] reverses)&#123; SortField[] sortFields = null; int fieldLength = fields.length; int typeLength = types.length; int reverLength = reverses.length; if(!(fieldLength == typeLength) || !(fieldLength == reverLength))&#123; return null; &#125;else&#123; sortFields = new SortField[fields.length]; for (int i = 0; i &lt; fields.length; i++) &#123; sortFields[i] = new SortField(fields[i], types[i], reverses[i]); &#125; &#125; return new Sort(sortFields); &#125; /**根据查询器、查询条件、每页数、排序条件进行查询 * @param query 查询条件 * @param first 起始值 * @param max 最大值 * @param sort 排序条件 * @return */ public static TopDocs getScoreDocsByPerPageAndSortField(IndexSearcher searcher,Query query, int first,int max, Sort sort)&#123; try &#123; if(query == null)&#123; System.out.println(" Query is null return null "); return null; &#125; TopFieldCollector collector = null; if(sort != null)&#123; collector = TopFieldCollector.create(sort, first+max, false, false, false); &#125;else&#123; sort = new Sort(new SortField[]&#123;new SortField("modified", SortField.Type.LONG)&#125;); collector = TopFieldCollector.create(sort, first+max, false, false, false); &#125; searcher.search(query, collector); return collector.topDocs(first, max); &#125; catch (IOException e) &#123; // TODO Auto-generated catch block &#125; return null; &#125; /**获取上次索引的id,增量更新使用 * @return */ public static Integer getLastIndexBeanID(IndexReader multiReader)&#123; Query query = new MatchAllDocsQuery(); IndexSearcher searcher = null; searcher = new IndexSearcher(multiReader); SortField sortField = new SortField("id", SortField.Type.INT,true); Sort sort = new Sort(new SortField[]&#123;sortField&#125;); TopDocs docs = getScoreDocsByPerPageAndSortField(searcher,query, 0, 1, sort); ScoreDoc[] scoreDocs = docs.scoreDocs; int total = scoreDocs.length; if(total &gt; 0)&#123; ScoreDoc scoreDoc = scoreDocs[0]; Document doc = null; try &#123; doc = searcher.doc(scoreDoc.doc); &#125; catch (IOException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125; return new Integer(doc.get("id")); &#125; return 0; &#125;&#125; 正则查询测试正则查询测试类，主要是测试是否包含手机号或邮箱号，这里的手机号验证有点粗糙，希望不要介意 1234567891011121314151617181920212223242526272829303132333435package com.lucene.index.test;import java.io.IOException;import java.util.concurrent.Executors;import org.apache.lucene.document.Document;import org.apache.lucene.search.IndexSearcher;import org.apache.lucene.search.Query;import org.apache.lucene.search.ScoreDoc;import org.apache.lucene.search.TopDocs;import com.lucene.search.SearchUtil;public class TestSearch &#123; public static void main(String[] args) &#123; try &#123; IndexSearcher searcher = SearchUtil.getMultiSearcher("index", Executors.newCachedThreadPool(), false); Query phoneQuery = SearchUtil.getRegexExpQuery("content", "1[0-9]&#123;10&#125;"); Query mailQuery = SearchUtil.getRegexExpQuery("content", "([a-z0-9A-Z]+[-_|\\.]?)+[a-z0-9A-Z]*@([a-z0-9A-Z]+(-[a-z0-9A-Z]+)?\\.)+[a-zA-Z]&#123;2,&#125;"); Query finaQuery = SearchUtil.getMultiQueryLikeSqlIn(new Query[]&#123;phoneQuery,mailQuery&#125;); TopDocs topDocs = SearchUtil.getScoreDocsByPerPageAndSortField(searcher, finaQuery, 0, 20, null); System.out.println("符合条件的数据总数："+topDocs.totalHits); System.out.println("本次查询到的数目为："+topDocs.scoreDocs.length); ScoreDoc[] scoreDocs = topDocs.scoreDocs; for (ScoreDoc scoreDoc : scoreDocs) &#123; Document doc = searcher.doc(scoreDoc.doc); System.out.println(doc.get("path")+" "+doc.get("content")); &#125; &#125; catch (IOException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125; catch (InterruptedException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125; &#125; &#125; 最终测试结果如下： 123456789101112content:/1[0-9]&#123;10&#125;/content:/([a-z0-9A-Z]+[-_|\.]?)+[a-z0-9A-Z]*@([a-z0-9A-Z]+(-[a-z0-9A-Z]+)?\.)+[a-zA-Z]&#123;2,&#125;/符合条件的数据总数：6本次查询到的数目为：6D:\hadoop\lucene_regexSearch\testDir\2.txt.txt 电话号码：18519237811D:\hadoop\lucene_regexSearch\testDir\3.txt.txt 电子邮箱yinggui_Wu@163.comD:\hadoop\lucene_regexSearch\testDir\1.docx 邮箱内容yinggui_Wu@163.com D:\hadoop\lucene_regexSearch\testDir\1.pdf 邮箱内容 yinggui_Wu@163.com D:\hadoop\lucene_regexSearch\testDir\1.xlsx 1 2 3 18510539956 D:\hadoop\lucene_regexSearch\testDir\1.txt.txt &lt;a target=_blank href=&quot;mailto:fanyi@qq.com&quot;&gt;fanyi@qq.com&lt;/a&gt; 代码下载地址 http://download.csdn.net/detail/wuyinggui10000/8746407]]></content>
      <categories>
        <category>lucene</category>
      </categories>
      <tags>
        <tag>lucene</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[lucene搜索之facet查询原理和facet查询实例]]></title>
    <url>%2F2019%2F03%2F16%2FLucene%2F14%E3%80%81lucene%E6%90%9C%E7%B4%A2%E4%B9%8Bfacet%E6%9F%A5%E8%AF%A2%E5%8E%9F%E7%90%86%E5%92%8Cfacet%E6%9F%A5%E8%AF%A2%E5%AE%9E%E4%BE%8B%2F</url>
    <content type="text"><![CDATA[lucene（14）—lucene搜索之facet查询原理和facet查询实例Facet说明我们在浏览网站的时候，经常会遇到按某一类条件查询的情况，这种情况尤以电商网站最多，以天猫商城为例，我们选择某一个品牌，系统会将该品牌对应的商品展示出来，效果图如下： 如上图，我们关注的是品牌，选购热点等方面，对于类似的功能我们用lucene的term查询当然可以，但是在数据量特别大的情况下还用普通查询来实现显然会因为FSDirectory.open等耗时的操作造成查询效率的低下，同时普通查询是全部document都扫描一遍，这样显然造成了查询效率低； lucene提供了facet查询用于对同一类的document进行聚类化，这样在查询的时候先关注某一个方面，这种显然缩小了查询范围，进而提升了查询效率； facet模块提供了多个用于处理facet的统计和值处理的方法； 要实现facet的功能，我们需要了解facetField,FacetField定义了dim和此field对应的path,需要特别注意的是我们在做facetField索引的时候，需要事先调用FacetsConfig.build(Document); FacetField的indexOptions设置为了DOCS_AND_FREQS_AND_POSITIONS的,即既索引又统计出现的频次和出现的位置，这样做主要是为了方便查询和统计； 相应的在存储的时候我们需要利用FacetsConfig和DirectoryTaxonomyWriter； DirectoryTaxonomyWriter用来利用Directory来存储Taxono信息到硬盘； DirectoryTaxonomyWriter的构造器如下: 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758public DirectoryTaxonomyWriter(Directory directory, OpenMode openMode, TaxonomyWriterCache cache) throws IOException &#123; dir = directory; IndexWriterConfig config = createIndexWriterConfig(openMode); indexWriter = openIndexWriter(dir, config); // verify (to some extent) that merge policy in effect would preserve category docids assert !(indexWriter.getConfig().getMergePolicy() instanceof TieredMergePolicy) : "for preserving category docids, merging none-adjacent segments is not allowed"; // after we opened the writer, and the index is locked, it's safe to check // the commit data and read the index epoch openMode = config.getOpenMode(); if (!DirectoryReader.indexExists(directory)) &#123; indexEpoch = 1; &#125; else &#123; String epochStr = null; Map&lt;String, String&gt; commitData = readCommitData(directory); if (commitData != null) &#123; epochStr = commitData.get(INDEX_EPOCH); &#125; // no commit data, or no epoch in it means an old taxonomy, so set its epoch to 1, for lack // of a better value. indexEpoch = epochStr == null ? 1 : Long.parseLong(epochStr, 16); &#125; if (openMode == OpenMode.CREATE) &#123; ++indexEpoch; &#125; FieldType ft = new FieldType(TextField.TYPE_NOT_STORED); ft.setOmitNorms(true); parentStreamField = new Field(Consts.FIELD_PAYLOADS, parentStream, ft); fullPathField = new StringField(Consts.FULL, "", Field.Store.YES); nextID = indexWriter.maxDoc(); if (cache == null) &#123; cache = defaultTaxonomyWriterCache(); &#125; this.cache = cache; if (nextID == 0) &#123; cacheIsComplete = true; // Make sure that the taxonomy always contain the root category // with category id 0. addCategory(new FacetLabel()); &#125; else &#123; // There are some categories on the disk, which we have not yet // read into the cache, and therefore the cache is incomplete. // We choose not to read all the categories into the cache now, // to avoid terrible performance when a taxonomy index is opened // to add just a single category. We will do it later, after we // notice a few cache misses. cacheIsComplete = false; &#125; &#125; 由上述代码可知，DirectoryTaxonomyWriter先打开一个IndexWriter,在确保indexWriter打开和locked的前提下，读取directory对应的segments中需要提交的内容，如果读取到的内容为空，说明是上次的内容，设置indexEpoch为1，接着对cache进行设置；判断directory中是否还包含有document，如果有设置cacheIsComplete为false,反之为true; 时候不早了，今天先写到这里，明天会在此基础上补充，大家见谅 编程实践我对之前的读取文件夹内容的做了个facet索引的例子 对BaseIndex修改了facet的设置，相关代码如下 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175package com.lucene.index; import java.io.File;import java.io.IOException;import java.nio.file.Paths;import java.text.ParseException;import java.util.List;import java.util.concurrent.CountDownLatch; import org.apache.lucene.document.Document;import org.apache.lucene.document.Field;import org.apache.lucene.facet.FacetResult;import org.apache.lucene.facet.Facets;import org.apache.lucene.facet.FacetsCollector;import org.apache.lucene.facet.FacetsConfig;import org.apache.lucene.facet.taxonomy.FastTaxonomyFacetCounts;import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyReader;import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyWriter;import org.apache.lucene.index.IndexOptions;import org.apache.lucene.index.IndexWriter;import org.apache.lucene.index.Term;import org.apache.lucene.search.IndexSearcher;import org.apache.lucene.search.MatchAllDocsQuery;import org.apache.lucene.search.Query;import org.apache.lucene.store.Directory;import org.apache.lucene.store.FSDirectory;import org.apache.lucene.store.RAMDirectory; import com.lucene.search.SearchUtil; public abstract class BaseIndex&lt;T&gt; implements Runnable&#123; /** * 父级索引路径 */ private String parentIndexPath; /** * 索引编写器 */ private IndexWriter writer; private int subIndex; /** * 主线程 */ private final CountDownLatch countDownLatch1; /** *工作线程 */ private final CountDownLatch countDownLatch2; /** * 对象列表 */ private List&lt;T&gt; list; /** * facet查询 */ private String facet; protected final FacetsConfig config = new FacetsConfig(); protected final static String indexPath = "index1"; protected final static DirectoryTaxonomyWriter taxoWriter; static&#123; try &#123; Directory directory = FSDirectory.open(Paths.get(indexPath, new String[0])); taxoWriter = new DirectoryTaxonomyWriter(directory); &#125; catch (IOException e) &#123; throw new ExceptionInInitializerError("BaseIndex initializing error"); &#125; &#125; public BaseIndex(IndexWriter writer,CountDownLatch countDownLatch1, CountDownLatch countDownLatch2, List&lt;T&gt; list, String facet)&#123; super(); this.writer = writer; this.countDownLatch1 = countDownLatch1; this.countDownLatch2 = countDownLatch2; this.list = list; this.facet = facet; &#125; public BaseIndex(String parentIndexPath, int subIndex, CountDownLatch countDownLatch1, CountDownLatch countDownLatch2, List&lt;T&gt; list) &#123; super(); this.parentIndexPath = parentIndexPath; this.subIndex = subIndex; try &#123; //多目录索引创建 File file = new File(parentIndexPath+"/index"+subIndex); if(!file.exists())&#123; file.mkdir(); &#125; this.writer = IndexUtil.getIndexWriter(parentIndexPath+"/index"+subIndex, true); &#125; catch (IOException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125;; this.subIndex = subIndex; this.countDownLatch1 = countDownLatch1; this.countDownLatch2 = countDownLatch2; this.list = list; &#125; public BaseIndex(String path,CountDownLatch countDownLatch1, CountDownLatch countDownLatch2, List&lt;T&gt; list) &#123; super(); try &#123; //单目录索引创建 File file = new File(path); if(!file.exists())&#123; file.mkdir(); &#125; this.writer = IndexUtil.getIndexWriter(path,true); &#125; catch (IOException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125;; this.countDownLatch1 = countDownLatch1; this.countDownLatch2 = countDownLatch2; this.list = list; &#125; /**创建索引 * @param writer * @param carSource * @param create * @throws IOException * @throws ParseException */ public abstract void indexDoc(IndexWriter writer,T t) throws Exception; /**批量索引创建 * @param writer * @param t * @throws Exception */ public void indexDocs(IndexWriter writer,List&lt;T&gt; t) throws Exception&#123; for (T t2 : t) &#123; indexDoc(writer,t2); &#125; &#125; /**带group的索引创建 * @param writer * @param docs * @throws IOException */ public void indexDocsWithGroup(IndexWriter writer,String groupFieldName,String groupFieldValue,List&lt;Document&gt; docs) throws IOException&#123; Field groupEndField = new Field(groupFieldName, groupFieldValue, Field.Store.NO, Field.Index.NOT_ANALYZED); docs.get(docs.size()-1).add(groupEndField); // writer.updateDocuments(new Term(groupFieldName, groupFieldValue),docs); writer.commit(); writer.close(); &#125; @Override public void run() &#123; try &#123; countDownLatch1.await(); System.out.println(writer); indexDocs(writer,list); &#125; catch (InterruptedException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125; catch (Exception e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125;finally&#123; countDownLatch2.countDown(); try &#123; writer.commit(); writer.close(); &#125; catch (IOException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125; &#125; &#125;&#125; 相应得，document的索引需要利用DirectoryTaxonomyWriter来进行原有document的处理 12345678910111213141516171819202122232425262728293031323334353637383940414243444546package com.lucene.index; import java.util.List;import java.util.concurrent.CountDownLatch; import org.apache.lucene.document.Document;import org.apache.lucene.document.Field;import org.apache.lucene.document.LongField;import org.apache.lucene.document.StringField;import org.apache.lucene.document.TextField;import org.apache.lucene.facet.FacetField;import org.apache.lucene.index.IndexWriter;import org.apache.lucene.index.IndexWriterConfig;import org.apache.lucene.index.Term; import com.lucene.bean.FileBean; public class FileBeanIndex extends BaseIndex&lt;FileBean&gt;&#123; private static String facet; public FileBeanIndex(IndexWriter writer, CountDownLatch countDownLatch12, CountDownLatch countDownLatch1, List&lt;FileBean&gt; fileBeans, String facet1) &#123; super(writer, countDownLatch12, countDownLatch1, fileBeans, facet); facet = facet1; &#125; @Override public void indexDoc(IndexWriter writer, FileBean t) throws Exception &#123; Document doc = new Document(); String path = t.getPath(); System.out.println(t.getPath()); doc.add(new StringField("path", path, Field.Store.YES)); doc.add(new LongField("modified", t.getModified(), Field.Store.YES)); doc.add(new TextField("content", t.getContent(), Field.Store.YES)); doc.add(new FacetField("filePath", new String[]&#123;facet&#125;)); //doc = config.build(taxoWriter,doc); if (writer.getConfig().getOpenMode() == IndexWriterConfig.OpenMode.CREATE)&#123; //writer.addDocument(doc); writer.addDocument(this.config.build(taxoWriter, doc)); &#125;else&#123; writer.updateDocument(new Term("path", t.getPath()), this.config.build(taxoWriter, doc)); &#125; taxoWriter.commit(); &#125; &#125; 测试facet功能的测试类： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869package com.lucene.search; import java.io.IOException;import java.nio.file.Paths;import java.util.ArrayList;import java.util.List; import org.apache.lucene.facet.FacetResult;import org.apache.lucene.facet.Facets;import org.apache.lucene.facet.FacetsCollector;import org.apache.lucene.facet.FacetsConfig;import org.apache.lucene.facet.LabelAndValue;import org.apache.lucene.facet.taxonomy.FastTaxonomyFacetCounts;import org.apache.lucene.facet.taxonomy.TaxonomyReader;import org.apache.lucene.facet.taxonomy.directory.DirectoryTaxonomyReader;import org.apache.lucene.index.DirectoryReader;import org.apache.lucene.search.IndexSearcher;import org.apache.lucene.search.MatchAllDocsQuery;import org.apache.lucene.search.Query;import org.apache.lucene.store.Directory;import org.apache.lucene.store.FSDirectory;import org.junit.Test; public class TestSearchFacet &#123; public static Directory directory; public static Directory taxoDirectory; public static TaxonomyReader taxoReader; protected final static FacetsConfig config = new FacetsConfig(); static &#123; try &#123; directory = FSDirectory.open(Paths.get("index", new String[0])); taxoDirectory = FSDirectory.open(Paths.get("index1", new String[0])); taxoReader = new DirectoryTaxonomyReader(taxoDirectory); &#125; catch (IOException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125; &#125; public static void testSearchFacet() &#123; try &#123; DirectoryReader indexReader = DirectoryReader.open(directory); IndexSearcher searcher = new IndexSearcher(indexReader); FacetsCollector fc = new FacetsCollector(); FacetsCollector.search(searcher, new MatchAllDocsQuery(), indexReader.maxDoc(), fc); Facets facets = new FastTaxonomyFacetCounts(taxoReader, config, fc); List&lt;FacetResult&gt; results =facets.getAllDims(100); for (FacetResult facetResult : results) &#123; System.out.println(facetResult.dim); LabelAndValue[] values = facetResult.labelValues; for (LabelAndValue labelAndValue : values) &#123; System.out.println("\t"+labelAndValue.label +" "+labelAndValue.value); &#125; &#125; indexReader.close(); taxoReader.close(); &#125; catch (IOException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125; &#125; public static void main(String[] args) &#123; testSearchFacet(); &#125; &#125; 相关代码下载 http://download.csdn.net/detail/wuyinggui10000/8738651]]></content>
      <categories>
        <category>lucene</category>
      </categories>
      <tags>
        <tag>lucene</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[lucene搜索之自定义排序的实现原理和编写自己的自定义排序工具]]></title>
    <url>%2F2019%2F03%2F16%2FLucene%2F13%E3%80%81%E8%87%AA%E5%AE%9A%E4%B9%89%E6%8E%92%E5%BA%8F%E7%9A%84%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86%E5%92%8C%E7%BC%96%E5%86%99%E8%87%AA%E5%B7%B1%E7%9A%84%E8%87%AA%E5%AE%9A%E4%B9%89%E6%8E%92%E5%BA%8F%E5%B7%A5%E5%85%B7%2F</url>
    <content type="text"><![CDATA[lucene（13）—lucene搜索之自定义排序的实现原理和编写自己的自定义排序工具自定义排序说明我们在做lucene搜索的时候，可能会需要排序功能，虽然lucene内置了多种类型的排序，但是如果在需要先进行某些值的运算然后在排序的时候就有点显得无能为力了； 要做自定义查询，我们就要研究lucene已经实现的排序功能，lucene的所有排序都是要继承FieldComparator,然后重写内部实现，这里以IntComparator为例子来查看其实现； IntComparator相关实现其类的声明为 public static class IntComparator extends NumericComparator,这里说明IntComparator接收的是Integer类型的参数，即只处理IntField的排序； IntComparator声明的参数为： 123private final int[] values; private int bottom; // Value of bottom of queue private int topValue; 查看copy方法可知 values随着类初始化而初始化其长度 values用于存储NumericDocValues中读取到的内容 具体实现如下： values的初始化 12345678/** * Creates a new comparator based on &#123;@link Integer#compare&#125; for &#123;@code numHits&#125;. * When a document has no value for the field, &#123;@code missingValue&#125; is substituted. */ public IntComparator(int numHits, String field, Integer missingValue) &#123; super(field, missingValue); values = new int[numHits]; &#125; values值填充(此为IntComparator的处理方式) 1234567891011@Override public void copy(int slot, int doc) &#123; int v2 = (int) currentReaderValues.get(doc); // Test for v2 == 0 to save Bits.get method call for // the common case (doc has value and value is non-zero): if (docsWithField != null &amp;&amp; v2 == 0 &amp;&amp; !docsWithField.get(doc)) &#123; v2 = missingValue; &#125; values[slot] = v2; &#125; 这些实现都是类似的，我们的应用实现自定义排序的时候需要做的是对binaryDocValues或NumericDocValues的值进行计算，然后实现FieldComparator内部方法，对应IntComparator就是如上的值copy操作； 然后我们需要实现compareTop、compareBottom和compare，IntComparator的实现为： 12345678910111213141516@Overridepublic int compare(int slot1, int slot2) &#123; return Integer.compare(values[slot1], values[slot2]);&#125; @Overridepublic int compareBottom(int doc) &#123; int v2 = (int) currentReaderValues.get(doc); // Test for v2 == 0 to save Bits.get method call for // the common case (doc has value and value is non-zero): if (docsWithField != null &amp;&amp; v2 == 0 &amp;&amp; !docsWithField.get(doc)) &#123; v2 = missingValue; &#125; return Integer.compare(bottom, v2);&#125; 12345678910 @Override public int compareTop(int doc) &#123; int docValue = (int) currentReaderValues.get(doc); // Test for docValue == 0 to save Bits.get method call for // the common case (doc has value and value is non-zero): if (docsWithField != null &amp;&amp; docValue == 0 &amp;&amp; !docsWithField.get(doc)) &#123; docValue = missingValue; &#125; return Integer.compare(topValue, docValue); &#125; 实现自己的FieldComparator要实现FieldComparator，需要对接收参数进行处理，定义处理值的集合，同时定义BinaryDocValues和接收的参数等，这里我写了一个通用的比较器，代码如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980package com.lucene.search; import java.io.IOException; import org.apache.lucene.index.BinaryDocValues;import org.apache.lucene.index.DocValues;import org.apache.lucene.index.LeafReaderContext;import org.apache.lucene.search.SimpleFieldComparator; import com.lucene.util.ObjectUtil; /**自定义comparator * @author lenovo * */public class SelfDefineComparator extends SimpleFieldComparator&lt;String&gt; &#123; private Object[] values;//定义的Object[]，同IntComparator private Object bottom; private Object top; private String field; private BinaryDocValues binaryDocValues;//接收的BinaryDocValues,同IntComparator中的NumericDocValues private ObjectUtil objectUtil;//这里为了便于拓展用接口代替抽象类 private Object[] params;//接收的参数 public SelfDefineComparator(String field, int numHits, Object[] params,ObjectUtil objectUtil) &#123; values = new Object[numHits]; this.objectUtil = objectUtil; this.field = field; this.params = params; &#125; @Override public void setBottom(int slot) &#123; this.bottom = values[slot]; &#125; @Override public int compareBottom(int doc) throws IOException &#123; Object distance = getValues(doc); return (bottom.toString()).compareTo(distance.toString()); &#125; @Override public int compareTop(int doc) throws IOException &#123; Object distance = getValues(doc); return objectUtil.compareTo(top,distance); &#125; @Override public void copy(int slot, int doc) throws IOException &#123; values[slot] = getValues(doc); &#125; private Object getValues(int doc) &#123; Object instance = objectUtil.getValues(doc,params,binaryDocValues) ; return instance; &#125; @Override protected void doSetNextReader(LeafReaderContext context) throws IOException &#123; binaryDocValues = DocValues.getBinary(context.reader(), field);//context.reader().getBinaryDocValues(field); &#125; @Override public int compare(int slot1, int slot2) &#123; return objectUtil.compareTo(values[slot1],values[slot2]); &#125; @Override public void setTopValue(String value) &#123; this.top = value; &#125; @Override public String value(int slot) &#123; return values[slot].toString(); &#125; &#125; 其中ObjectUtil是一个接口，定义了值处理的过程，最终是要服务于comparator的compare方法的，同时对comparator的内部compare方法进行了定义 ObjectUtil接口定义如下： 12345678910111213141516171819202122package com.lucene.util; import org.apache.lucene.index.BinaryDocValues; public interface ObjectUtil &#123; /**自定义的获取处理值的方法 * @param doc * @param params * @param binaryDocValues * @return */ public abstract Object getValues(int doc, Object[] params, BinaryDocValues binaryDocValues) ; /**compare比较器实现 * @param object * @param object2 * @return */ public abstract int compareTo(Object object, Object object2); &#125; 我们不仅要提供比较器和comparator，同时还要提供接收用户输入的FiledComparatorSource 12345678910111213141516171819202122232425262728293031323334353637383940414243444546package com.lucene.search; import java.io.IOException; import org.apache.lucene.search.FieldComparator;import org.apache.lucene.search.FieldComparatorSource; import com.lucene.util.ObjectUtil; /**comparator用于接收用户原始输入，继承自FieldComparatorSource实现了自定义comparator的构建 * @author lenovo * */public class SelfDefineComparatorSource extends FieldComparatorSource &#123; private Object[] params;//接收的参数 private ObjectUtil objectUtil;//这里为了便于拓展用接口代替抽象类 public Object[] getParams() &#123; return params; &#125; public void setParams(Object[] params) &#123; this.params = params; &#125; public ObjectUtil getObjectUtil() &#123; return objectUtil; &#125; public void setObjectUtil(ObjectUtil objectUtil) &#123; this.objectUtil = objectUtil; &#125; public SelfDefineComparatorSource(Object[] params, ObjectUtil objectUtil) &#123; super(); this.params = params; this.objectUtil = objectUtil; &#125; @Override public FieldComparator&lt;?&gt; newComparator(String fieldname, int numHits, int sortPos, boolean reversed) throws IOException &#123; //实际比较由SelfDefineComparator实现 return new SelfDefineComparator(fieldname, numHits, params, objectUtil); &#125;&#125; 相关测试程序，这里我们模拟一个StringComparator，对String值进行排序 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768package com.lucene.search; import org.apache.lucene.analysis.Analyzer;import org.apache.lucene.analysis.standard.StandardAnalyzer;import org.apache.lucene.document.BinaryDocValuesField;import org.apache.lucene.document.Document;import org.apache.lucene.document.Field;import org.apache.lucene.document.StringField;import org.apache.lucene.index.DirectoryReader;import org.apache.lucene.index.IndexReader;import org.apache.lucene.index.IndexWriter;import org.apache.lucene.index.IndexWriterConfig;import org.apache.lucene.index.IndexWriterConfig.OpenMode;import org.apache.lucene.index.Term;import org.apache.lucene.search.IndexSearcher;import org.apache.lucene.search.MatchAllDocsQuery;import org.apache.lucene.search.Query;import org.apache.lucene.search.ScoreDoc;import org.apache.lucene.search.Sort;import org.apache.lucene.search.SortField;import org.apache.lucene.search.TermQuery;import org.apache.lucene.search.TopDocs;import org.apache.lucene.search.TopFieldDocs;import org.apache.lucene.store.RAMDirectory;import org.apache.lucene.util.BytesRef; import com.lucene.util.CustomerUtil;import com.lucene.util.ObjectUtil;import com.lucene.util.StringComparaUtil; /** * * @author 吴莹桂 * */public class SortTest &#123; public static void main(String[] args) throws Exception &#123; RAMDirectory directory = new RAMDirectory(); Analyzer analyzer = new StandardAnalyzer(); IndexWriterConfig indexWriterConfig = new IndexWriterConfig(analyzer); indexWriterConfig.setOpenMode(OpenMode.CREATE_OR_APPEND); IndexWriter indexWriter = new IndexWriter(directory, indexWriterConfig); addDocument(indexWriter, "B"); addDocument(indexWriter, "D"); addDocument(indexWriter, "A"); addDocument(indexWriter, "E"); indexWriter.commit(); indexWriter.close(); IndexReader reader = DirectoryReader.open(directory); IndexSearcher searcher = new IndexSearcher(reader); Query query = new MatchAllDocsQuery(); ObjectUtil util = new StringComparaUtil(); Sort sort = new Sort(new SortField("name",new SelfDefineComparatorSource(new Object[]&#123;&#125;,util),true)); TopDocs topDocs = searcher.search(query, Integer.MAX_VALUE, sort); ScoreDoc[] docs = topDocs.scoreDocs; for(ScoreDoc doc : docs)&#123; Document document = searcher.doc(doc.doc); System.out.println(document.get("name")); &#125; &#125; private static void addDocument(IndexWriter writer,String name) throws Exception&#123; Document document = new Document(); document.add(new StringField("name",name,Field.Store.YES)); document.add(new BinaryDocValuesField("name", new BytesRef(name.getBytes()))); writer.addDocument(document); &#125; &#125; 其对应的ObjectUtil实现如下： 12345678910111213141516171819202122package com.lucene.util; import org.apache.lucene.index.BinaryDocValues;import org.apache.lucene.util.BytesRef; public class StringComparaUtil implements ObjectUtil &#123; @Override public Object getValues(int doc, Object[] params, BinaryDocValues binaryDocValues) &#123; BytesRef bytesRef = binaryDocValues.get(doc); String value = bytesRef.utf8ToString(); return value; &#125; @Override public int compareTo(Object object, Object object2) &#123; // TODO Auto-generated method stub return object.toString().compareTo(object2.toString()); &#125; &#125;]]></content>
      <categories>
        <category>lucene</category>
      </categories>
      <tags>
        <tag>lucene</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[lucene搜索之分组处理group查询]]></title>
    <url>%2F2019%2F03%2F16%2FLucene%2F12%E3%80%81lucene%E6%90%9C%E7%B4%A2%E4%B9%8B%E5%88%86%E7%BB%84%E5%A4%84%E7%90%86group%E6%9F%A5%E8%AF%A2%2F</url>
    <content type="text"><![CDATA[lucene（12）—lucene搜索之分组处理group查询grouping介绍我们在做lucene搜索的时候，可能会用到对某个条件的数据进行统计，比如统计有多少个省份，在sql查询中我们可以用distinct来完成类似的功能，也可以用group by来对查询的列进行分组查询。在lucene中我们实现类似的功能怎么做呢，比较费时的做法时我们查询出所有的结果，然后对结果里边的省份对应的field查询出来，往set里边放，显然这种做法效率低，不可取；lucene为了解决上述问题，提供了用于分组操作的模块group，group主要用户处理不同lucene中含有某个相同field值的不同document的分组统计。 Grouping可以接收如下参数： groupField：要分组的字段；比如我们对省份（province）进行分组，要传入对应的值为province，要注意的是如果groupField在document中不存在，会返回一个null的分组； groupSort：分组是怎么排序的，排序字段决定了分组内容展示的先后顺序； topNGroups：分组展示的数量，只计算0到topNGroup条记录； groupOffset：从第几个TopGroup开始算起，举例来说groupOffset为3的话，会展示从3到topNGroup对应的记录，此数值我们可以用于分页查询； withinGroupSort：每组内怎么排序； maxDocsPerGroup：每组处理多少个document； withinGroupOffset：每组显示的document初始位置； group的实现需要两步： 第一步：利用TermFirstPassGroupingCollector来收集top groups； 第二步：用TermSecondPassGroupingCollector处理每个group对应的documents group模块定义了group和group的采集方式；所有的grouping colletor,所有的grouping collector都是抽象类并且提供了基于term的实现； 实现group的前提： 要group的field必须是必须是SortedDocValuesField类型的； solr尽管也提供了grouping by的相关方法实现，但是对group的抽象实现还是由该模块实现； 暂不支持sharding,我们需要自己提供groups和每个group的documents的合并 group示例12345678910111213141516171819202122232425262728293031323334353637383940414243444546package com.lucene.search; import java.io.IOException; import org.apache.lucene.search.IndexSearcher;import org.apache.lucene.search.MatchAllDocsQuery;import org.apache.lucene.search.Query;import org.apache.lucene.search.Sort;import org.apache.lucene.search.SortField;import org.apache.lucene.search.grouping.GroupDocs;import org.apache.lucene.search.grouping.GroupingSearch;import org.apache.lucene.search.grouping.TopGroups;import org.apache.lucene.util.BytesRef; public class GroupSearchTest &#123; public static void main(String[] args) &#123; GroupingSearch groupingSearch = new GroupingSearch("province"); SortField sortField = new SortField("city", SortField.Type.STRING_VAL); Sort sort = new Sort(sortField); groupingSearch.setGroupSort(sort); groupingSearch.setFillSortFields(true); groupingSearch.setCachingInMB(4.0, true); groupingSearch.setAllGroups(true); IndexSearcher searcher; try &#123; searcher = SearchUtil.getIndexSearcherByIndexPath("index", null); Query query = new MatchAllDocsQuery(); TopGroups&lt;BytesRef&gt; result = groupingSearch.search(searcher,query, 0, searcher.getIndexReader().maxDoc()); // Render groupsResult... GroupDocs&lt;BytesRef&gt;[] docs = result.groups; for (GroupDocs&lt;BytesRef&gt; groupDocs : docs) &#123; System.out.println(new String(groupDocs.groupValue.bytes)); &#125; int totalGroupCount = result.totalGroupCount; System.out.println(totalGroupCount); &#125; catch (IOException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125; &#125; &#125; 利用BlockGroupingCollector 我们有时候想要在索引的时候就将group字段存入以方便search，我们可以在确保docs被索引的前提下，先查询出来每个要group的term对应的documents,然后在最后的document插入一个标记分组的field,我们可以如此做： 123456789101112/**带group的索引创建 * @param writer * @param docs * @throws IOException */ public void indexDocsWithGroup(IndexWriter writer,String groupFieldName,String groupFieldValue,List&lt;Document&gt; docs) throws IOException&#123; Field groupEndField = new Field(groupFieldName, groupFieldValue, Field.Store.NO, Field.Index.NOT_ANALYZED); docs.get(docs.size()-1).add(groupEndField); writer.updateDocuments(new Term(groupFieldName, groupFieldValue),docs); writer.commit(); writer.close(); &#125; 在分组查询的时候，我们可以 123456789101112131415161718192021222324/**group查询，适用于对group字段已经进行分段索引的情况 * @param searcher * @param groupEndQuery * @param query * @param sort * @param withinGroupSort * @param groupOffset * @param topNGroups * @param needsScores * @param docOffset * @param docsPerGroup * @param fillFields * @return * @throws IOException */ public static TopGroups&lt;BytesRef&gt; getTopGroupsByGroupTerm(IndexSearcher searcher,Query groupEndQuery,Query query,Sort sort,Sort withinGroupSort,int groupOffset,int topNGroups,boolean needsScores,int docOffset,int docsPerGroup,boolean fillFields) throws IOException&#123; @SuppressWarnings("deprecation") Filter groupEndDocs = new CachingWrapperFilter(new QueryWrapperFilter(groupEndQuery)); BlockGroupingCollector c = new BlockGroupingCollector(sort, groupOffset+topNGroups, needsScores, groupEndDocs); searcher.search(query, c); @SuppressWarnings("unchecked") TopGroups&lt;BytesRef&gt; groupsResult = (TopGroups&lt;BytesRef&gt;) c.getTopGroups(withinGroupSort, groupOffset, docOffset, docOffset+docsPerGroup, fillFields); return groupsResult; &#125; 我们也可以直接进行group的查询，此为通用的实现 查询方法1234567891011121314151617181920/** * @param searcher * @param query * @param groupFieldName * @param sort * @param maxCacheRAMMB * @param page * @param perPage * @return * @throws IOException */ public static TopGroups&lt;BytesRef&gt; getTopGroups(IndexSearcher searcher,Query query,String groupFieldName,Sort sort,double maxCacheRAMMB,int page,int perPage) throws IOException&#123; GroupingSearch groupingSearch = new GroupingSearch(groupFieldName); groupingSearch.setGroupSort(sort); groupingSearch.setFillSortFields(true); groupingSearch.setCachingInMB(maxCacheRAMMB, true); groupingSearch.setAllGroups(true); TopGroups&lt;BytesRef&gt; result = groupingSearch.search(searcher,query, (page-1)*perPage, page*perPage); return result; &#125; 以下是查询的工具类 查询工具类123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335package com.lucene.search; import java.io.File;import java.io.IOException;import java.nio.file.Paths;import java.util.Set;import java.util.concurrent.ExecutorService; import org.apache.lucene.analysis.Analyzer;import org.apache.lucene.analysis.standard.StandardAnalyzer;import org.apache.lucene.document.Document;import org.apache.lucene.index.DirectoryReader;import org.apache.lucene.index.IndexReader;import org.apache.lucene.index.MultiReader;import org.apache.lucene.index.Term;import org.apache.lucene.queryparser.classic.ParseException;import org.apache.lucene.queryparser.classic.QueryParser;import org.apache.lucene.search.BooleanQuery;import org.apache.lucene.search.CachingWrapperFilter;import org.apache.lucene.search.Filter;import org.apache.lucene.search.IndexSearcher;import org.apache.lucene.search.MatchAllDocsQuery;import org.apache.lucene.search.NumericRangeQuery;import org.apache.lucene.search.Query;import org.apache.lucene.search.QueryWrapperFilter;import org.apache.lucene.search.ScoreDoc;import org.apache.lucene.search.Sort;import org.apache.lucene.search.SortField;import org.apache.lucene.search.TermQuery;import org.apache.lucene.search.TopDocs;import org.apache.lucene.search.BooleanClause.Occur;import org.apache.lucene.search.grouping.BlockGroupingCollector;import org.apache.lucene.search.grouping.GroupDocs;import org.apache.lucene.search.grouping.GroupingSearch;import org.apache.lucene.search.grouping.TopGroups;import org.apache.lucene.search.highlight.Highlighter;import org.apache.lucene.search.highlight.InvalidTokenOffsetsException;import org.apache.lucene.search.highlight.QueryScorer;import org.apache.lucene.search.highlight.SimpleFragmenter;import org.apache.lucene.search.highlight.SimpleHTMLFormatter;import org.apache.lucene.store.FSDirectory;import org.apache.lucene.util.BytesRef; /**lucene索引查询工具类 * @author lenovo * */public class SearchUtil &#123; /**获取IndexSearcher对象 * @param indexPath * @param service * @return * @throws IOException */ public static IndexSearcher getIndexSearcherByParentPath(String parentPath,ExecutorService service) throws IOException&#123; MultiReader reader = null; //设置 try &#123; File[] files = new File(parentPath).listFiles(); IndexReader[] readers = new IndexReader[files.length]; for (int i = 0 ; i &lt; files.length ; i ++) &#123; readers[i] = DirectoryReader.open(FSDirectory.open(Paths.get(files[i].getPath(), new String[0]))); &#125; reader = new MultiReader(readers); &#125; catch (IOException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125; return new IndexSearcher(reader,service); &#125; /**多目录多线程查询 * @param parentPath 父级索引目录 * @param service 多线程查询 * @return * @throws IOException */ public static IndexSearcher getMultiSearcher(String parentPath,ExecutorService service) throws IOException&#123; File file = new File(parentPath); File[] files = file.listFiles(); IndexReader[] readers = new IndexReader[files.length]; for (int i = 0 ; i &lt; files.length ; i ++) &#123; readers[i] = DirectoryReader.open(FSDirectory.open(Paths.get(files[i].getPath(), new String[0]))); &#125; MultiReader multiReader = new MultiReader(readers); IndexSearcher searcher = new IndexSearcher(multiReader,service); return searcher; &#125; /**根据索引路径获取IndexReader * @param indexPath * @return * @throws IOException */ public static DirectoryReader getIndexReader(String indexPath) throws IOException&#123; return DirectoryReader.open(FSDirectory.open(Paths.get(indexPath, new String[0]))); &#125; /**根据索引路径获取IndexSearcher * @param indexPath * @param service * @return * @throws IOException */ public static IndexSearcher getIndexSearcherByIndexPath(String indexPath,ExecutorService service) throws IOException&#123; IndexReader reader = getIndexReader(indexPath); return new IndexSearcher(reader,service); &#125; /**如果索引目录会有变更用此方法获取新的IndexSearcher这种方式会占用较少的资源 * @param oldSearcher * @param service * @return * @throws IOException */ public static IndexSearcher getIndexSearcherOpenIfChanged(IndexSearcher oldSearcher,ExecutorService service) throws IOException&#123; DirectoryReader reader = (DirectoryReader) oldSearcher.getIndexReader(); DirectoryReader newReader = DirectoryReader.openIfChanged(reader); return new IndexSearcher(newReader, service); &#125; /**多条件查询类似于sql in * @param querys * @return */ public static Query getMultiQueryLikeSqlIn(Query ... querys)&#123; BooleanQuery query = new BooleanQuery(); for (Query subQuery : querys) &#123; query.add(subQuery,Occur.SHOULD); &#125; return query; &#125; /**多条件查询类似于sql and * @param querys * @return */ public static Query getMultiQueryLikeSqlAnd(Query ... querys)&#123; BooleanQuery query = new BooleanQuery(); for (Query subQuery : querys) &#123; query.add(subQuery,Occur.MUST); &#125; return query; &#125; /**从指定配置项中查询 * @return * @param analyzer 分词器 * @param field 字段 * @param fieldType 字段类型 * @param queryStr 查询条件 * @param range 是否区间查询 * @return */ public static Query getQuery(String field,String fieldType,String queryStr,boolean range)&#123; Query q = null; try &#123; if(queryStr != null &amp;&amp; !"".equals(queryStr))&#123; if(range)&#123; String[] strs = queryStr.split("\\|"); if("int".equals(fieldType))&#123; int min = new Integer(strs[0]); int max = new Integer(strs[1]); q = NumericRangeQuery.newIntRange(field, min, max, true, true); &#125;else if("double".equals(fieldType))&#123; Double min = new Double(strs[0]); Double max = new Double(strs[1]); q = NumericRangeQuery.newDoubleRange(field, min, max, true, true); &#125;else if("float".equals(fieldType))&#123; Float min = new Float(strs[0]); Float max = new Float(strs[1]); q = NumericRangeQuery.newFloatRange(field, min, max, true, true); &#125;else if("long".equals(fieldType))&#123; Long min = new Long(strs[0]); Long max = new Long(strs[1]); q = NumericRangeQuery.newLongRange(field, min, max, true, true); &#125; &#125;else&#123; if("int".equals(fieldType))&#123; q = NumericRangeQuery.newIntRange(field, new Integer(queryStr), new Integer(queryStr), true, true); &#125;else if("double".equals(fieldType))&#123; q = NumericRangeQuery.newDoubleRange(field, new Double(queryStr), new Double(queryStr), true, true); &#125;else if("float".equals(fieldType))&#123; q = NumericRangeQuery.newFloatRange(field, new Float(queryStr), new Float(queryStr), true, true); &#125;else&#123; Analyzer analyzer = new StandardAnalyzer(); q = new QueryParser(field, analyzer).parse(queryStr); &#125; &#125; &#125;else&#123; q= new MatchAllDocsQuery(); &#125; System.out.println(q); &#125; catch (ParseException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125; return q; &#125; /**根据field和值获取对应的内容 * @param fieldName * @param fieldValue * @return */ public static Query getQuery(String fieldName,Object fieldValue)&#123; Term term = new Term(fieldName, new BytesRef(fieldValue.toString())); return new TermQuery(term); &#125; /**根据IndexSearcher和docID获取默认的document * @param searcher * @param docID * @return * @throws IOException */ public static Document getDefaultFullDocument(IndexSearcher searcher,int docID) throws IOException&#123; return searcher.doc(docID); &#125; /**根据IndexSearcher和docID * @param searcher * @param docID * @param listField * @return * @throws IOException */ public static Document getDocumentByListField(IndexSearcher searcher,int docID,Set&lt;String&gt; listField) throws IOException&#123; return searcher.doc(docID, listField); &#125; /**分页查询 * @param page 当前页数 * @param perPage 每页显示条数 * @param searcher searcher查询器 * @param query 查询条件 * @return * @throws IOException */ public static TopDocs getScoreDocsByPerPage(int page,int perPage,IndexSearcher searcher,Query query) throws IOException&#123; TopDocs result = null; if(query == null)&#123; System.out.println(" Query is null return null "); return null; &#125; ScoreDoc before = null; if(page != 1)&#123; TopDocs docsBefore = searcher.search(query, (page-1)*perPage); ScoreDoc[] scoreDocs = docsBefore.scoreDocs; if(scoreDocs.length &gt; 0)&#123; before = scoreDocs[scoreDocs.length - 1]; &#125; &#125; result = searcher.searchAfter(before, query, perPage); return result; &#125; public static TopDocs getScoreDocs(IndexSearcher searcher,Query query) throws IOException&#123; TopDocs docs = searcher.search(query, getMaxDocId(searcher)); return docs; &#125; /**高亮显示字段 * @param searcher * @param field * @param keyword * @param preTag * @param postTag * @param fragmentSize * @return * @throws IOException * @throws InvalidTokenOffsetsException */ public static String[] highlighter(IndexSearcher searcher,String field,String keyword,String preTag, String postTag,int fragmentSize) throws IOException, InvalidTokenOffsetsException&#123; Term term = new Term("content",new BytesRef("lucene")); TermQuery termQuery = new TermQuery(term); TopDocs docs = getScoreDocs(searcher, termQuery); ScoreDoc[] hits = docs.scoreDocs; QueryScorer scorer = new QueryScorer(termQuery); SimpleHTMLFormatter simpleHtmlFormatter = new SimpleHTMLFormatter(preTag,postTag);//设定高亮显示的格式&lt;B&gt;keyword&lt;/B&gt;,此为默认的格式 Highlighter highlighter = new Highlighter(simpleHtmlFormatter,scorer); highlighter.setTextFragmenter(new SimpleFragmenter(fragmentSize));//设置每次返回的字符数 Analyzer analyzer = new StandardAnalyzer(); String[] result = new String[hits.length]; for (int i = 0; i &lt; result.length ; i++) &#123; Document doc = searcher.doc(hits[i].doc); result[i] = highlighter.getBestFragment(analyzer, field, doc.get(field)); &#125; return result; &#125; /**统计document的数量,此方法等同于matchAllDocsQuery查询 * @param searcher * @return */ public static int getMaxDocId(IndexSearcher searcher)&#123; return searcher.getIndexReader().maxDoc(); &#125; /**group查询，适用于对group字段已经进行分段索引的情况 * @param searcher * @param groupEndQuery * @param query * @param sort * @param withinGroupSort * @param groupOffset * @param topNGroups * @param needsScores * @param docOffset * @param docsPerGroup * @param fillFields * @return * @throws IOException */ public static TopGroups&lt;BytesRef&gt; getTopGroupsByGroupTerm(IndexSearcher searcher,Query groupEndQuery,Query query,Sort sort,Sort withinGroupSort,int groupOffset,int topNGroups,boolean needsScores,int docOffset,int docsPerGroup,boolean fillFields) throws IOException&#123; @SuppressWarnings("deprecation") Filter groupEndDocs = new CachingWrapperFilter(new QueryWrapperFilter(groupEndQuery)); BlockGroupingCollector c = new BlockGroupingCollector(sort, groupOffset+topNGroups, needsScores, groupEndDocs); searcher.search(query, c); @SuppressWarnings("unchecked") TopGroups&lt;BytesRef&gt; groupsResult = (TopGroups&lt;BytesRef&gt;) c.getTopGroups(withinGroupSort, groupOffset, docOffset, docOffset+docsPerGroup, fillFields); return groupsResult; &#125; /**通用的进行group查询 * @param searcher * @param query * @param groupFieldName * @param sort * @param maxCacheRAMMB * @param page * @param perPage * @return * @throws IOException */ public static TopGroups&lt;BytesRef&gt; getTopGroups(IndexSearcher searcher,Query query,String groupFieldName,Sort sort,double maxCacheRAMMB,int page,int perPage) throws IOException&#123; GroupingSearch groupingSearch = new GroupingSearch(groupFieldName); groupingSearch.setGroupSort(sort); groupingSearch.setFillSortFields(true); groupingSearch.setCachingInMB(maxCacheRAMMB, true); groupingSearch.setAllGroups(true); TopGroups&lt;BytesRef&gt; result = groupingSearch.search(searcher,query, (page-1)*perPage, page*perPage); return result; &#125;&#125;]]></content>
      <categories>
        <category>lucene</category>
      </categories>
      <tags>
        <tag>lucene</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[lucene搜索之高亮显示highlighter]]></title>
    <url>%2F2019%2F03%2F16%2FLucene%2F11%E3%80%81lucene%E6%90%9C%E7%B4%A2%E4%B9%8B%E9%AB%98%E4%BA%AE%E6%98%BE%E7%A4%BAhighlighter%2F</url>
    <content type="text"><![CDATA[lucene（11）—lucene搜索之高亮显示highlighterhighlighter介绍我们在做查询的时候，希望对我们自己的搜索结果与搜索内容相近的地方进行着重显示，就如下面的效果 这里我们搜索的内容是“区块”，搜索引擎展示的结果中对用户的输入信息进行了配色方面的处理，这种区分正常文本和输入内容的效果即是高亮显示； 这样做的好处： 视觉上让人便于查找有搜索对应的文本块；界面展示更友好；lucene提供了highlighter插件来体现类似的效果； highlighter对查询关键字高亮处理； highlighter包包含了用于处理结果页查询内容高亮显示的功能，其中Highlighter类highlighter包的核心组件，借助Fragmenter, fragment Scorer, 和Formatter等类来支持用户自定义高亮展示的功能； 示例程序这里边我利用了之前的做的目录文件索引 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556package com.lucene.search.util; import java.io.IOException;import java.io.StringReader;import java.util.concurrent.ExecutorService;import java.util.concurrent.Executors; import org.apache.lucene.analysis.Analyzer;import org.apache.lucene.analysis.TokenStream;import org.apache.lucene.analysis.standard.StandardAnalyzer;import org.apache.lucene.document.Document;import org.apache.lucene.index.Term;import org.apache.lucene.search.IndexSearcher;import org.apache.lucene.search.ScoreDoc;import org.apache.lucene.search.TermQuery;import org.apache.lucene.search.TopDocs;import org.apache.lucene.search.highlight.Highlighter;import org.apache.lucene.search.highlight.InvalidTokenOffsetsException;import org.apache.lucene.search.highlight.QueryScorer;import org.apache.lucene.search.highlight.SimpleFragmenter;import org.apache.lucene.search.highlight.SimpleHTMLFormatter;import org.apache.lucene.util.BytesRef; public class HighlighterTest &#123; public static void main(String[] args) &#123; IndexSearcher searcher; TopDocs docs; ExecutorService service = Executors.newCachedThreadPool(); try &#123; searcher = SearchUtil.getMultiSearcher("index", service); Term term = new Term("content",new BytesRef("lucene")); TermQuery termQuery = new TermQuery(term); docs = SearchUtil.getScoreDocsByPerPage(1, 30, searcher, termQuery); ScoreDoc[] hits = docs.scoreDocs; QueryScorer scorer = new QueryScorer(termQuery); SimpleHTMLFormatter simpleHtmlFormatter = new SimpleHTMLFormatter("&lt;B&gt;","&lt;/B&gt;");//设定高亮显示的格式&lt;B&gt;keyword&lt;/B&gt;,此为默认的格式 Highlighter highlighter = new Highlighter(simpleHtmlFormatter,scorer); highlighter.setTextFragmenter(new SimpleFragmenter(20));//设置每次返回的字符数 Analyzer analyzer = new StandardAnalyzer(); for(int i=0;i&lt;hits.length;i++)&#123; Document doc = searcher.doc(hits[i].doc); String str = highlighter.getBestFragment(analyzer, "content", doc.get("content")) ; System.out.println(str); &#125; &#125; catch (IOException e1) &#123; // TODO Auto-generated catch block e1.printStackTrace(); &#125; catch (InvalidTokenOffsetsException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125;finally&#123; service.shutdown(); &#125; &#125;&#125; lucene的highlighter高亮展示的原理： 根据Formatter和Scorer创建highlighter对象，formatter定义了高亮的显示方式，而scorer定义了高亮的评分； 评分的算法是先根据term的评分值获取对应的document的权重，在此基础上对文本的内容进行轮询,获取对应的文本出现的次数，和它在term对应的文本中出现的位置（便于高亮处理），评分并分词的算法为： 1234567891011121314151617181920212223242526public float getTokenScore() &#123; position += posIncAtt.getPositionIncrement();//记录出现的位置 String termText = termAtt.toString(); WeightedSpanTerm weightedSpanTerm; if ((weightedSpanTerm = fieldWeightedSpanTerms.get( termText)) == null) &#123; return 0; &#125; if (weightedSpanTerm.positionSensitive &amp;&amp; !weightedSpanTerm.checkPosition(position)) &#123; return 0; &#125; float score = weightedSpanTerm.getWeight();//获取权重 // found a query term - is it unique in this doc? if (!foundTerms.contains(termText)) &#123;//结果排重处理 totalScore += score; foundTerms.add(termText); &#125; return score; &#125; formatter的原理为：对搜索的文本进行判断，如果scorer获取的totalScore不小于0，即查询内容在对应的term中存在，则按照格式拼接成preTag+查询内容+postTag的格式 详细算法如下： 12345678910111213public String highlightTerm(String originalText, TokenGroup tokenGroup) &#123; if (tokenGroup.getTotalScore() &lt;= 0) &#123; return originalText; &#125; // Allocate StringBuilder with the right number of characters from the // beginning, to avoid char[] allocations in the middle of appends. StringBuilder returnBuffer = new StringBuilder(preTag.length() + originalText.length() + postTag.length()); returnBuffer.append(preTag); returnBuffer.append(originalText); returnBuffer.append(postTag); return returnBuffer.toString(); &#125; 其默认格式为“”的形式； Highlighter根据scorer和formatter，对document进行分析，查询结果调用getBestTextFragments,TokenStream tokenStream,String text,boolean mergeContiguousFragments,int maxNumFragments)，其过程为scorer首先初始化查询内容对应的出现位置的下标，然后对tokenstream添加PositionIncrementAttribute，此类记录单词出现的位置；对文本内容进行轮询，判断查询的文本长度是否超出限制，如果超出文本长度提示过长内容；如果获取到指定的文本，先对单次查询的内容进行内容的截取（截取值根据setTextFragmenter指定的值决定），再调用formatter的highlightTerm方法对文本进行重新构建在本次轮询和下次单词出现之前对文本内容进行处理 查询工具类123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282package com.lucene.search.util; import java.io.File;import java.io.IOException;import java.nio.file.Paths;import java.util.Set;import java.util.concurrent.ExecutorService; import org.apache.lucene.analysis.Analyzer;import org.apache.lucene.analysis.standard.StandardAnalyzer;import org.apache.lucene.document.Document;import org.apache.lucene.index.DirectoryReader;import org.apache.lucene.index.IndexReader;import org.apache.lucene.index.MultiReader;import org.apache.lucene.index.Term;import org.apache.lucene.queryparser.classic.ParseException;import org.apache.lucene.queryparser.classic.QueryParser;import org.apache.lucene.search.BooleanQuery;import org.apache.lucene.search.IndexSearcher;import org.apache.lucene.search.MatchAllDocsQuery;import org.apache.lucene.search.NumericRangeQuery;import org.apache.lucene.search.Query;import org.apache.lucene.search.ScoreDoc;import org.apache.lucene.search.TermQuery;import org.apache.lucene.search.TopDocs;import org.apache.lucene.search.BooleanClause.Occur;import org.apache.lucene.search.highlight.Highlighter;import org.apache.lucene.search.highlight.InvalidTokenOffsetsException;import org.apache.lucene.search.highlight.QueryScorer;import org.apache.lucene.search.highlight.SimpleFragmenter;import org.apache.lucene.search.highlight.SimpleHTMLFormatter;import org.apache.lucene.store.FSDirectory;import org.apache.lucene.util.BytesRef; /**lucene索引查询工具类 * @author lenovo * */public class SearchUtil &#123; /**获取IndexSearcher对象 * @param indexPath * @param service * @return * @throws IOException */ public static IndexSearcher getIndexSearcherByParentPath(String parentPath,ExecutorService service) throws IOException&#123; MultiReader reader = null; //设置 try &#123; File[] files = new File(parentPath).listFiles(); IndexReader[] readers = new IndexReader[files.length]; for (int i = 0 ; i &lt; files.length ; i ++) &#123; readers[i] = DirectoryReader.open(FSDirectory.open(Paths.get(files[i].getPath(), new String[0]))); &#125; reader = new MultiReader(readers); &#125; catch (IOException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125; return new IndexSearcher(reader,service); &#125; /**多目录多线程查询 * @param parentPath 父级索引目录 * @param service 多线程查询 * @return * @throws IOException */ public static IndexSearcher getMultiSearcher(String parentPath,ExecutorService service) throws IOException&#123; File file = new File(parentPath); File[] files = file.listFiles(); IndexReader[] readers = new IndexReader[files.length]; for (int i = 0 ; i &lt; files.length ; i ++) &#123; readers[i] = DirectoryReader.open(FSDirectory.open(Paths.get(files[i].getPath(), new String[0]))); &#125; MultiReader multiReader = new MultiReader(readers); IndexSearcher searcher = new IndexSearcher(multiReader,service); return searcher; &#125; /**根据索引路径获取IndexReader * @param indexPath * @return * @throws IOException */ public static DirectoryReader getIndexReader(String indexPath) throws IOException&#123; return DirectoryReader.open(FSDirectory.open(Paths.get(indexPath, new String[0]))); &#125; /**根据索引路径获取IndexSearcher * @param indexPath * @param service * @return * @throws IOException */ public static IndexSearcher getIndexSearcherByIndexPath(String indexPath,ExecutorService service) throws IOException&#123; IndexReader reader = getIndexReader(indexPath); return new IndexSearcher(reader,service); &#125; /**如果索引目录会有变更用此方法获取新的IndexSearcher这种方式会占用较少的资源 * @param oldSearcher * @param service * @return * @throws IOException */ public static IndexSearcher getIndexSearcherOpenIfChanged(IndexSearcher oldSearcher,ExecutorService service) throws IOException&#123; DirectoryReader reader = (DirectoryReader) oldSearcher.getIndexReader(); DirectoryReader newReader = DirectoryReader.openIfChanged(reader); return new IndexSearcher(newReader, service); &#125; /**多条件查询类似于sql in * @param querys * @return */ public static Query getMultiQueryLikeSqlIn(Query ... querys)&#123; BooleanQuery query = new BooleanQuery(); for (Query subQuery : querys) &#123; query.add(subQuery,Occur.SHOULD); &#125; return query; &#125; /**多条件查询类似于sql and * @param querys * @return */ public static Query getMultiQueryLikeSqlAnd(Query ... querys)&#123; BooleanQuery query = new BooleanQuery(); for (Query subQuery : querys) &#123; query.add(subQuery,Occur.MUST); &#125; return query; &#125; /**从指定配置项中查询 * @return * @param analyzer 分词器 * @param field 字段 * @param fieldType 字段类型 * @param queryStr 查询条件 * @param range 是否区间查询 * @return */ public static Query getQuery(String field,String fieldType,String queryStr,boolean range)&#123; Query q = null; try &#123; if(queryStr != null &amp;&amp; !"".equals(queryStr))&#123; if(range)&#123; String[] strs = queryStr.split("\\|"); if("int".equals(fieldType))&#123; int min = new Integer(strs[0]); int max = new Integer(strs[1]); q = NumericRangeQuery.newIntRange(field, min, max, true, true); &#125;else if("double".equals(fieldType))&#123; Double min = new Double(strs[0]); Double max = new Double(strs[1]); q = NumericRangeQuery.newDoubleRange(field, min, max, true, true); &#125;else if("float".equals(fieldType))&#123; Float min = new Float(strs[0]); Float max = new Float(strs[1]); q = NumericRangeQuery.newFloatRange(field, min, max, true, true); &#125;else if("long".equals(fieldType))&#123; Long min = new Long(strs[0]); Long max = new Long(strs[1]); q = NumericRangeQuery.newLongRange(field, min, max, true, true); &#125; &#125;else&#123; if("int".equals(fieldType))&#123; q = NumericRangeQuery.newIntRange(field, new Integer(queryStr), new Integer(queryStr), true, true); &#125;else if("double".equals(fieldType))&#123; q = NumericRangeQuery.newDoubleRange(field, new Double(queryStr), new Double(queryStr), true, true); &#125;else if("float".equals(fieldType))&#123; q = NumericRangeQuery.newFloatRange(field, new Float(queryStr), new Float(queryStr), true, true); &#125;else&#123; Analyzer analyzer = new StandardAnalyzer(); q = new QueryParser(field, analyzer).parse(queryStr); &#125; &#125; &#125;else&#123; q= new MatchAllDocsQuery(); &#125; System.out.println(q); &#125; catch (ParseException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125; return q; &#125; /**根据field和值获取对应的内容 * @param fieldName * @param fieldValue * @return */ public static Query getQuery(String fieldName,Object fieldValue)&#123; Term term = new Term(fieldName, new BytesRef(fieldValue.toString())); return new TermQuery(term); &#125; /**根据IndexSearcher和docID获取默认的document * @param searcher * @param docID * @return * @throws IOException */ public static Document getDefaultFullDocument(IndexSearcher searcher,int docID) throws IOException&#123; return searcher.doc(docID); &#125; /**根据IndexSearcher和docID * @param searcher * @param docID * @param listField * @return * @throws IOException */ public static Document getDocumentByListField(IndexSearcher searcher,int docID,Set&lt;String&gt; listField) throws IOException&#123; return searcher.doc(docID, listField); &#125; /**分页查询 * @param page 当前页数 * @param perPage 每页显示条数 * @param searcher searcher查询器 * @param query 查询条件 * @return * @throws IOException */ public static TopDocs getScoreDocsByPerPage(int page,int perPage,IndexSearcher searcher,Query query) throws IOException&#123; TopDocs result = null; if(query == null)&#123; System.out.println(" Query is null return null "); return null; &#125; ScoreDoc before = null; if(page != 1)&#123; TopDocs docsBefore = searcher.search(query, (page-1)*perPage); ScoreDoc[] scoreDocs = docsBefore.scoreDocs; if(scoreDocs.length &gt; 0)&#123; before = scoreDocs[scoreDocs.length - 1]; &#125; &#125; result = searcher.searchAfter(before, query, perPage); return result; &#125; public static TopDocs getScoreDocs(IndexSearcher searcher,Query query) throws IOException&#123; TopDocs docs = searcher.search(query, getMaxDocId(searcher)); return docs; &#125; /**高亮显示字段 * @param searcher * @param field * @param keyword * @param preTag * @param postTag * @param fragmentSize * @return * @throws IOException * @throws InvalidTokenOffsetsException */ public static String[] highlighter(IndexSearcher searcher,String field,String keyword,String preTag, String postTag,int fragmentSize) throws IOException, InvalidTokenOffsetsException&#123; Term term = new Term("content",new BytesRef("lucene")); TermQuery termQuery = new TermQuery(term); TopDocs docs = getScoreDocs(searcher, termQuery); ScoreDoc[] hits = docs.scoreDocs; QueryScorer scorer = new QueryScorer(termQuery); SimpleHTMLFormatter simpleHtmlFormatter = new SimpleHTMLFormatter(preTag,postTag);//设定高亮显示的格式&lt;B&gt;keyword&lt;/B&gt;,此为默认的格式 Highlighter highlighter = new Highlighter(simpleHtmlFormatter,scorer); highlighter.setTextFragmenter(new SimpleFragmenter(fragmentSize));//设置每次返回的字符数 Analyzer analyzer = new StandardAnalyzer(); String[] result = new String[hits.length]; for (int i = 0; i &lt; result.length ; i++) &#123; Document doc = searcher.doc(hits[i].doc); result[i] = highlighter.getBestFragment(analyzer, field, doc.get(field)); &#125; return result; &#125; /**统计document的数量,此方法等同于matchAllDocsQuery查询 * @param searcher * @return */ public static int getMaxDocId(IndexSearcher searcher)&#123; return searcher.getIndexReader().maxDoc(); &#125; &#125; 源码下载地址http://download.csdn.net/detail/wuyinggui10000/8726407]]></content>
      <categories>
        <category>lucene</category>
      </categories>
      <tags>
        <tag>lucene</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[lucene搜索之联想词提示之suggest原理和应用]]></title>
    <url>%2F2019%2F03%2F16%2FLucene%2F10%E3%80%81lucene%E6%90%9C%E7%B4%A2%E4%B9%8B%E8%81%94%E6%83%B3%E8%AF%8D%E6%8F%90%E7%A4%BA%E4%B9%8Bsuggest%E5%8E%9F%E7%90%86%E5%92%8C%E5%BA%94%E7%94%A8%2F</url>
    <content type="text"><![CDATA[lucene（10）—lucene搜索之联想词提示之suggest原理和应用昨天了解了suggest包中的spell相关的内容，主要是拼写检查和相似度查询提示； 今天准备了解下关于联想词的内容，lucene的联想词是在org.apache.lucene.search.suggest包下边，提供了自动补全或者联想提示功能的支持； InputIterator说明InputIterator是一个支持枚举term,weight,payload三元组的供suggester使用的接口，目前仅支持AnalyzingSuggester,FuzzySuggester andAnalyzingInfixSuggester 三种suggester支持payloads； InputIterator的实现类有以下几种： BufferedInputIterator：对二进制类型的输入进行轮询； DocumentInputIterator：从索引中被store的field中轮询； FileIterator：从文件中每次读出单行的数据轮询，以\t进行间隔（且\t的个数最多为2个）； HighFrequencyIterator：从索引中被store的field轮询，忽略长度小于设定值的文本； InputIteratorWrapper：遍历BytesRefIterator并且返回的内容不包含payload且weight均为1； SortedInputIterator：二进制类型的输入轮询且按照指定的comparator算法进行排序； InputIterator提供的方法如下： weight():此方法设置某个term的权重，设置的越高suggest的优先级越高； payload():每个suggestion对应的元数据的二进制表示，我们在传输对象的时候需要转换对象或对象的某个属性为BytesRef类型，相应的suggester调用lookup的时候会返回payloads信息； hasPayload()：判断iterator是否有payloads； contexts():获取某个term的contexts,用来过滤suggest的内容，如果suggest的列表为空，返回null hasContexts():获取iterator是否有contexts; Suggester查询工具Lookup类说明此类提供了字符串的联想查询功能 Lookup类提供了一个CharSequenceComparator，此comparator主要是用来对CharSequence进行排序，按字符顺序排序； 内置LookupResult，用于返回suggest的结果，同时也是按照CharSequenceComparator进行key的排序； 内置了LookupPriorityQueue，用以存储LookupResult; LookUp提供的方法build(Dictionary dict) ： 从指定directory进行build; load(InputStream input) ： 将InputStream转成DataInput并执行load(DataInput)方法； store(OutputStream output) ： 将OutputStream转成DataOutput并执行store(DataOutput)方法； getCount() ： 获取lookup的build的项的数量； build(InputIterator inputIterator) ： 根据指定的InputIterator构建Lookup对象； lookup(CharSequence key, boolean onlyMorePopular, int num) ：根据key查询可能的结果返回值为List; Lookup的相关实现如下： 编写自己的suggest模块注意：在suggest的时候我们需要导入lucene-misc-5.1.0.jar否则系统会提示类SortedMergePolicy没有找到； 首先我们定义自己的实体类： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950package com.lucene.suggest; import java.io.Serializable; public class Product implements Serializable &#123; private static final long serialVersionUID = 1L; private String name; private String image; private String[] regions; private int numberSold; public Product(String name, String image, String[] regions, int numberSold) &#123; this.name = name; this.image = image; this.regions = regions; this.numberSold = numberSold; &#125; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; public String getImage() &#123; return image; &#125; public void setImage(String image) &#123; this.image = image; &#125; public String[] getRegions() &#123; return regions; &#125; public void setRegions(String[] regions) &#123; this.regions = regions; &#125; public int getNumberSold() &#123; return numberSold; &#125; public void setNumberSold(int numberSold) &#123; this.numberSold = numberSold; &#125;&#125; 然后定义InputIterator这里定义消费者是List，并对list进行遍历放入payload中： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778package com.lucene.suggest; import java.io.ByteArrayOutputStream;import java.io.IOException;import java.io.ObjectOutputStream;import java.io.UnsupportedEncodingException;import java.util.Comparator;import java.util.HashSet;import java.util.Iterator;import java.util.Set; import org.apache.lucene.search.suggest.InputIterator;import org.apache.lucene.util.BytesRef; public class ProductIterator implements InputIterator &#123; private Iterator&lt;Product&gt; productIterator; private Product currentProduct; ProductIterator(Iterator&lt;Product&gt; productIterator) &#123; this.productIterator = productIterator; &#125; public boolean hasContexts() &#123; return true; &#125; /** * 是否有设置payload信息 */ public boolean hasPayloads() &#123; return true; &#125; public Comparator&lt;BytesRef&gt; getComparator() &#123; return null; &#125; public BytesRef next() &#123; if (productIterator.hasNext()) &#123; currentProduct = productIterator.next(); try &#123; return new BytesRef(currentProduct.getName().getBytes("UTF8")); &#125; catch (UnsupportedEncodingException e) &#123; throw new RuntimeException("Couldn't convert to UTF-8",e); &#125; &#125; else &#123; return null; &#125; &#125; public BytesRef payload() &#123; try &#123; ByteArrayOutputStream bos = new ByteArrayOutputStream(); ObjectOutputStream out = new ObjectOutputStream(bos); out.writeObject(currentProduct); out.close(); return new BytesRef(bos.toByteArray()); &#125; catch (IOException e) &#123; throw new RuntimeException("Well that's unfortunate."); &#125; &#125; public Set&lt;BytesRef&gt; contexts() &#123; try &#123; Set&lt;BytesRef&gt; regions = new HashSet&lt;BytesRef&gt;(); for (String region : currentProduct.getRegions()) &#123; regions.add(new BytesRef(region.getBytes("UTF8"))); &#125; return regions; &#125; catch (UnsupportedEncodingException e) &#123; throw new RuntimeException("Couldn't convert to UTF-8"); &#125; &#125; public long weight() &#123; return currentProduct.getNumberSold(); &#125;&#125; 编写测试类 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374package com.lucene.suggest; import java.io.ByteArrayInputStream;import java.io.IOException;import java.io.ObjectInputStream;import java.nio.file.Paths;import java.util.ArrayList;import java.util.HashSet;import java.util.List; import org.apache.lucene.analysis.standard.StandardAnalyzer;import org.apache.lucene.search.suggest.Lookup.LookupResult;import org.apache.lucene.search.suggest.analyzing.AnalyzingInfixSuggester;import org.apache.lucene.store.Directory;import org.apache.lucene.store.FSDirectory;import org.apache.lucene.util.BytesRef; public class SuggestProducts &#123; private static void lookup(AnalyzingInfixSuggester suggester, String name, String region) throws IOException &#123; HashSet&lt;BytesRef&gt; contexts = new HashSet&lt;BytesRef&gt;(); contexts.add(new BytesRef(region.getBytes("UTF8"))); List&lt;LookupResult&gt; results = suggester.lookup(name, contexts, 2, true, false); System.out.println("-- \"" + name + "\" (" + region + "):"); for (LookupResult result : results) &#123; System.out.println(result.key); BytesRef bytesRef = result.payload; ObjectInputStream is = new ObjectInputStream(new ByteArrayInputStream(bytesRef.bytes)); Product product = null; try &#123; product = (Product)is.readObject(); &#125; catch (ClassNotFoundException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125; System.out.println("product-Name:" + product.getName()); System.out.println("product-regions:" + product.getRegions()); System.out.println("product-image:" + product.getImage()); System.out.println("product-numberSold:" + product.getNumberSold()); &#125; System.out.println(); &#125; public static void main(String[] args) &#123; try &#123; Directory indexDir = FSDirectory.open(Paths.get("suggestPath", new String[0])); StandardAnalyzer analyzer = new StandardAnalyzer(); AnalyzingInfixSuggester suggester = new AnalyzingInfixSuggester(indexDir, analyzer); ArrayList&lt;Product&gt; products = new ArrayList&lt;Product&gt;(); products.add(new Product("Electric Guitar", "http://images.example/electric-guitar.jpg", new String[] &#123; "US", "CA" &#125;, 100)); products.add(new Product("Electric Train", "http://images.example/train.jpg", new String[] &#123; "US", "CA" &#125;, 100)); products.add(new Product("Acoustic Guitar", "http://images.example/acoustic-guitar.jpg", new String[] &#123; "US", "ZA" &#125;, 80)); products.add(new Product("Guarana Soda", "http://images.example/soda.jpg", new String[] &#123; "ZA", "IE" &#125;, 130)); suggester.build(new ProductIterator(products.iterator())); lookup(suggester, "Gu", "US"); lookup(suggester, "Gu", "ZA"); lookup(suggester, "Gui", "CA"); lookup(suggester, "Electric guit", "US"); suggester.refresh(); &#125; catch (IOException e) &#123; System.err.println("Error!"); &#125; &#125;&#125;]]></content>
      <categories>
        <category>lucene</category>
      </categories>
      <tags>
        <tag>lucene</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[阿里智能客服中心培训-1]]></title>
    <url>%2F2019%2F03%2F02%2F%E5%AE%A2%E6%9C%8D%E4%B8%AD%E5%BF%83%E5%9F%B9%E8%AE%AD%E7%AC%AC%E4%B8%80%E5%A4%A9%2F</url>
    <content type="text"><![CDATA[阿里智能客服中心培训-1引言首先还是非常感谢公司给我这次的培训机会，对于我来说还是收获到很多知识和方案。在第一天的五节课里，老师们讲的内容也包含了我很感兴趣的内容，比如:机器人小蜜的技术方案、智能导航和智能质检。阿里的老师在有限的时间内还是能把内容都有序给我们将完，给我的印象也是比较深刻，也让我真正的领略到它们丰富的项目经验和产品成果。 培训提纲： 科目 讲师 云小蜜技术方案介绍 王璨 智能语音技术方案介绍 李征 智能外呼技术方案介绍 梁进 智能导航技术方案介绍 山崎 智能质检技术方案介绍 镇远 知识云配置进阶及答疑 张瑄 对话工厂配置和脚本编程 北菜 OpenAPI实操介绍 送志 政务智能服务方案介绍 橙语 云小蜜技术方案产品能力大图 云小蜜技术框架 会话管理最佳实践 知识库问答流程 知识的存储结构 一条FAQ知识/机器阅读只能归属一个目录，一个目录可以拥有多条FAQ知识/机器阅读； 一条FAQ知识可拥有多个答案，一个答案只能属于一个FAQ知识； 一条答案拥有多个视角，一个视角属于多个答案； 一条FAQ知识可能拥有多个相似问法； 对话工厂会话流程 机器阅读传统思路：拆解问题与答案 长篇文档拆解困难 活动时效性短，更新频发 答案粒度难以控制，解决能力差 让机器直接阅读文章，自动找答案 减少人工阅读、拆解、配置知识点的工作量 提升问题理解的泛化能力，自动理解相关文法，无需穷举关键词 什么样的场景适合机器阅读 拥有海量的存量业务文档 知识点数量多，不易整理、维护 知识图谱问答基于结构化的精准问答、多轮会话与推理 知识结构化：知识图谱构建 知识结构化：语言表示 知识结构化：知识展示 知识图谱构建 语言表示 知识展示 使用场景 可归类实体数量大 知识点数量大 KBQA解决方案 会话能力选型 智能语音技术方案整体情况名词解释ASR 语音识别技术，也称为自动语音识别（Automatic Speech Recognition），简称 ASR，其目标是将人类语音中的词汇内容转换为可读的文字。 TTS 语音合成技术，也称为自动语音合成（Text To Speech）,简称 TTS ,其目标是将文字转换成人的声音。 NLU 自然语言理解技术（Natural Languager Understanding），简称 NLU (ULP)，即研究如何让计算机读懂人类语言。 IVR 交互式语音应答技术（Interactive Voice Response），简称 IVR ，在这里叫呼叫中心（CallCenter,CC）统一成为 IVR。一般来说，由 IVR 通过 SDM 服务调用 ASR、TTS、NLU 能力。 SDM 语音对话管理服务（Speech Dialogue Mangerment），简称 SDM ，是 MRCP 协议的服务端实现，也即 MRCP-SERVER，对外用以和各类呼叫平台（如：华为、avaya）进行对接，对内集成了 ASR、TTS、NLU能力。 ASR基础能力 支持普通话、粤语、四川话、东北话、英语识别 支持一句话识别 支持实时语音识别 支持 RESTful 识别 支持录音识别 相关概念BAD 语音活动检测（Voice Activity Detection ,VAD）又称语音端点检测，语音边界检测。目的是从声音信号流里识别和消除长时间的静音期。切出用户语音的开始说话时间和结束说话时间，加快语音引擎的识别速度。 声学模型 对发声的建模，它能够本语音输入转换成声学表示的输出，更准确的说是给出语音引擎的识别速度。 语言模型 可以理解为在声学模型给出发音序列之后，从后选的的文字序列中找出概率最大的字符串序列 一个 ASR 引擎至少有一个声学模型和语言模型 阿里的 ASR 引擎有一个基础语言模型和多个定制语言模型 专有云架构语音识别模块：实时语音识别 语音识别模块：一句话识别 自学平台 热词 类热词（需要类语言模型配合） 定制语言模型 简单原则 优先使用定制语言模型，效果好，不易出错 对于有明显数量限制（较少，比如100个左右）的人名、地名、使用类热词功能，否则使用热词功能 TTS基础能力 语速控制（快慢） 音量控制（高低） 音色控制（几十个定制发音人） 普通话、粤语、英语合成 中英文混读 数字串发音 合成方案（参数、拼接） 标签（SSML） 音库建设 发音人 录音环境 录音文案 质量控制 数据闭环 Neural TTS效果访问:https://ai.aliyun.com/nls/tts SDM(MRCP-SERVER)架构 SDM = 网络层（TCP、UDP）+ 协议层（SIP、RTP、MRCP）+ 逻辑层（ASR、TTS、NLU） 如果是 gateway 是语音服务的网关层、那么 SDM 是客户领域中各类 AI 能力的网关 SDM 承上启下，对外和 IVR 基于标准协议对接；对内完成各类 AI 服务的私有协议的转换 MRCP 协议 为什么选择 MRCP 协议 客服领域（比如通信、金融领域）大多有呼叫中心、设计硬件、历史悠久、升级换代很难 客户无法、可能没有能力、主观上也不愿意集成各个语音厂商API。 集成难度大 有的呼叫中心设备甚至无法集成第三方 API 好处 客户呼叫中心升级更简单（呼叫中心大多直接或间接支持 MRCP） 客户不在关注语音厂商 SDK 细节，仅仅参照 MRCP 标准 和呼叫中心的对接方案1 方案2 方案3 智能外呼技术方案智能客服 语音导航的进步 语音导航过程 智能助手 人员流动性大，培训成本高 V S 智能辅助解决新手上岗难问题 坐席和用户一对一交互，无监督无状态 V S 无缝融合第三方参与人人交互全过程 功能架构 适用场景金融、教育、旅游、网络推广、地产、客户回访、4s店、中国电信、联通、移动、保险、美容等行业。 典型智能外呼客服组织网专有云公有云智能导航技术方案概述 智能语音导航是什么？ 基于 ASR （语音识别）、NLU（自然语言理解）、TTS（语音合成）技术实现的智能语音产品/解决方案。 通常用于在电话端接待用户，为用户提供智能问答或功能跳转。 eg: 自助查水费 不是 不是 Sirt，无法解答未配置在 NLU 中的问题 不是学习机器，无法在没有人工干预的情况下自行学会新的知识或听懂方言 阿里云智能语音导航依赖一下原子产品 达摩院ASR/TTS/MRCP 模块 智能客服-云小蜜 提供适配 IVR 的渠道特征性： 个性化问候语 拒识处理 重听 访问提示 在多轮会话间共享上下文 私有化版本集成架构 IVR 与 MRCP 进行对接，通过MRCP 服务实现对 ASR 和 TTS 的调用 IVR 与智能导航 OpenAPI 进行对接 开启会话接口 会话接口 收号完成接口 核身完成（可选） 私有化版本部署指南逻辑架构 高可用架构 部署步骤 准备中间件 MySQL + Redis 初始化数据表 部署智能语义导航应用 初始化租户，生成调用 OpengAPI 的 AccessKey 在云小蜜中创建机器人 初始化导航实例，关联云小蜜机器人 云小蜜配置指南 具体是使用 API 以阿里提供的为准，这里只是简单的提一下。 总结智能语音导航=云小蜜+ASR+TTS+MRCP+智能导航OpenAPI+IVR基础配置+回复单元定义 拒识语、访问提示在导航 config 中配置；话术在小蜜中配置；重听、转人工在智能 IVR 基础配置对话流中配置； 智能质检技术方案规则配置 常用算子 关键词-词 正则-复杂关键词 语义-句子 算子与条件 条件间可以配置依赖关系 一个条件包含一个算子 系统集成 通过 FTP 不需要过多开发 适用于存量录音 没有客户姓名、号码、技能组等额外信息 通过 API 需要集成开发 信息全 uploadAudioDataWithRules 实时质检 准确率 转写准确率 准确率：&gt;85% 自学习平台 热词：适用于专有名词 语言模型：常规对话；量要多（&gt;1M）；一行一句（只需要话术，不需要角色，eg:客服：您好，请问，有什么可以帮到您？） 录音改造 分轨准确率 错误率：8%-15% 严重错误率: &lt;10% 双轨录音：从录音源头改造]]></content>
      <categories>
        <category>培训总结</category>
      </categories>
      <tags>
        <tag>培训总结</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[文件的上传和下载]]></title>
    <url>%2F2018%2F08%2F05%2F%E6%96%87%E4%BB%B6%E4%B8%8A%E4%BC%A0%E5%92%8C%E4%B8%8B%E8%BD%BD%2F</url>
    <content type="text"><![CDATA[文件的上传和下载文件上传选择 文件之后回显 html12345678910&lt;script type="text/javascript" src="../../../sunriseui.js"&gt;&lt;/script&gt;&lt;script type="text/javascript"&gt; load('ajaxfileupload');&lt;/script&gt;....&lt;div id="divPreview"&gt;&lt;input id="iconPic1" name="picUrl" type="hidden"/&gt;&lt;img id="imgHeadPhoto" src="" style="width: 320px; height: 130px; border: solid 1px #d2e2e2;" alt="" /&gt;&lt;/div&gt;&lt;input id="files" name="files" type="file" size="20" onchange="PreviewImage(this,'imgHeadPhoto','divPreview');"/&gt; js123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475//js本地图片预览，兼容ie[6-9]、火狐、Chrome17+、Opera11+、Maxthon3function PreviewImage(fileObj, imgPreviewId, divPreviewId) &#123; //允许上传文件的后缀名 var allowExtention = ".jpg,.bmp,.gif,.png"; var extention = fileObj.value.substring(fileObj.value.lastIndexOf(".") + 1).toLowerCase(); var browserVersion = window.navigator.userAgent.toUpperCase(); if (allowExtention.indexOf(extention) &gt; -1) &#123; //HTML5实现预览，兼容chrome、火狐7+等 if (fileObj.files) &#123; if (window.FileReader) &#123; var reader = new FileReader(); reader.onload = function (e) &#123; /*var pcBase64Text = e.target.result; var blob = dataURLtoBlob(pcBase64Text); console.log(e.target.result);*/ $("#iconPic1").val(e.target.result); document.getElementById(imgPreviewId).setAttribute("src",e.target.result); &#125; //读取文件把问价转换成 Base64 格式 reader.readAsDataURL(fileObj.files[0]); &#125;else if (browserVersion.indexOf("SAFARI") &gt; -1) &#123; alert("不支持Safari6.0以下浏览器的图片预览!"); &#125; &#125;else if (browserVersion.indexOf("MSIE") &gt; -1) &#123; // 兼容 IE6 浏览器 if (browserVersion.indexOf("MSIE 6") &gt; -1) &#123; document.getElementById(imgPreviewId).setAttribute("src", fileObj.value); &#125;else&#123; // 兼容 IE[7-9] 浏览器 fileObj.select(); if(browserVersion.indexOf("MSIE 9") &gt; -1)&#123; //不加上document.selection.createRange().text在ie9会拒绝访问 fileObj.blur(); &#125; var newPreview = document.getElementById(divPreviewId + "New"); if(newPreview == null)&#123; newPreview = document.createElement("div"); newPreview.setAttribute("id", divPreviewId + "New"); newPreview.style.width = document.getElementById(imgPreviewId).width + "px"; newPreview.style.height = document.getElementById(imgPreviewId).height + "px"; newPreview.style.height = document.getElementById(imgPreviewId).height + "px"; &#125; newPreview.style.filter = "progid:DXImageTransform.Microsoft.AlphaImageLoader(sizingMethod='scale', src='" + document.selection.createRange().text + "')"; var tempDivPreview = document.getElementById(divPreviewId); tempDivPreview.parentNode.insertBefore(newPreview, tempDivPreview); tempDivPreview.style.display = "none"; &#125; &#125;else if (browserVersion.indexOf("FIREFOX") &gt; -1) &#123; var firefoxVersion = parseFloat(browserVersion.toLowerCase().match(/firefox\/([\d.]+)/)[1]); //firefox7以下版本 if (firefoxVersion &lt; 7) &#123; document.getElementById(imgPreviewId). setAttribute("src", fileObj.files[0].getAsDataURL()); &#125;else &#123;//firefox7.0+ document.getElementById(imgPreviewId). setAttribute("src", window.URL.createObjectURL(fileObj.files[0])); &#125; &#125;else &#123; document.getElementById(imgPreviewId).setAttribute("src", fileObj.value); &#125; &#125;else &#123; alert("仅支持" + allowExtention + "为后缀名的文件!"); fileObj.value = ""; //清空选中文件 if (browserVersion.indexOf("MSIE") &gt; -1) &#123; fileObj.select(); document.selection.clear(); &#125; fileObj.outerHTML = fileObj.outerHTML; &#125; return fileObj.value; //返回路径&#125; 这段代码里面我只用到 第一个判断 兼容chrome、火狐7+ ，使用文件内改变事件。 使用 ajaxfileupload.js 进行异步上传 1234567891011121314151617181920//后台请求function uploadFile()&#123; return getHost(HostAddress.uomp) + '/uomp/keeperAdvertInfo/uploadFile';&#125;$.ajaxFileUpload(&#123;url:uploadFile(),type:'post',secureuri : false, // 是否需要安全协议，一般设置为falseid : "filesform",fileElementId : 'files', // 文件上传域的IDdataType : 'JSON', // 返回值类型 一般设置为jsondata : &#123; // 请求后台接口要带的参数 uploadpath:'keeper', fileName:fn&#125;,async : false,success : function(data) &#123;&#125;,error : function(data, status) &#123;&#125;&#125;); 后台文件上传的上传的实现 文件辅助类 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277import java.io.BufferedInputStream;import java.io.BufferedOutputStream;import java.io.BufferedReader;import java.io.File;import java.io.FileInputStream;import java.io.FileOutputStream;import java.io.InputStream;import java.io.InputStreamReader;import java.io.OutputStream;import java.util.List;import org.apache.commons.logging.Log;import org.apache.commons.logging.LogFactory;import com.eyuninfo.framework.common.SysConstant;public class FileUtil &#123; private static Log log = LogFactory.getLog(FileUtil.class); public static boolean uploadfile(InputStream in, String filePath) &#123; boolean res = false; try &#123; if (in != null) &#123; File file = new File(filePath); int bytesRead = 0; OutputStream os = new FileOutputStream(file); byte[] buffer = new byte[8192]; while ((bytesRead = in.read(buffer, 0, 8192)) != -1) &#123; os.write(buffer, 0, bytesRead); &#125; os.close(); in.close(); res = true; &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; return res; &#125; /** * 读取文本文件 * * @param txtPath 文本文件路径 * @return 文本内容 * @author michael 20161103 from kbms */ public static String readTxt(String txtPath) &#123; StringBuilder content = new StringBuilder(); InputStreamReader read = null; try &#123; if (txtPath != null &amp;&amp; !txtPath.isEmpty()) &#123; // String encoding=codeString(txtPath); File file = new File(txtPath); if (file.isFile() &amp;&amp; file.exists()) &#123; // 判断文件是否存在 read = new InputStreamReader(new FileInputStream(file));// 考虑到编码格式 BufferedReader bufferedReader = new BufferedReader(read); String lineTxt = null; while ((lineTxt = bufferedReader.readLine()) != null) &#123; content.append(lineTxt); &#125; read.close(); &#125; else &#123; log.info("找不到指定的文件"); &#125; &#125; else &#123; log.debug("txtPath 不能为空"); &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); log.error(e.getMessage()); &#125; return content.toString(); &#125; /** * 获取文件编码 * * @param fileName * @return * @throws Exception * @author michael 20161103 from kbms */ public static String codeString(String fileName) throws Exception &#123; BufferedInputStream bin = new BufferedInputStream(new FileInputStream(fileName)); int p = (bin.read() &lt;&lt; 8) + bin.read(); String code = null; // 其中的 0xefbb、0xfffe、0xfeff、0x5c75这些都是这个文件的前面两个字节的16进制数 switch (p) &#123; case 0xefbb: code = "UTF-8"; break; case 0xfffe: code = "Unicode"; break; case 0xfeff: code = "UTF-16BE"; break; case 0x5c75: code = "ANSI|ASCII"; break; default: code = "GBK"; &#125; return code; &#125; /** * 创建目录 * * @param dir 目录 * @return boolean 创建结果 * @author michael 20161103 from kbms */ public static boolean mkDir(File dir) &#123; if (!dir.getParentFile().exists()) &#123; mkDir(dir.getParentFile()); &#125; return dir.mkdir(); &#125; /** * 获取某文件夹下的所有文件（递归,含子文件） * * @param file * @param resultFileName * @return 文件名称列表 */ public static List&lt;String&gt; getFiles(File file, List&lt;String&gt; resultFileName) &#123; try &#123; File[] files = file.listFiles(); if (files == null) return resultFileName;// 判断目录下是不是空的 for (File f : files) &#123; if (f.isDirectory()) &#123;// 判断是否文件夹 resultFileName.add(f.getPath()); getFiles(f, resultFileName);// 调用自身,查找子目录 &#125; else &#123; resultFileName.add(f.getPath()); &#125; &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; return resultFileName; &#125; /** * 文件复制 * * @param source 原文件 * @param target 目标文件 * @author michael 20161103 from kbms */ public static void copy(File source, File target) &#123; InputStream fis = null; OutputStream fos = null; try &#123; fis = new BufferedInputStream(new FileInputStream(source)); fos = new BufferedOutputStream(new FileOutputStream(target)); byte[] buf = new byte[4096]; int i; while ((i = fis.read(buf)) != -1) &#123; fos.write(buf, 0, i); &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); log.error(e.getMessage()); &#125; finally &#123; try &#123; fis.close(); fos.close(); log.info("已经复制文件:" + source.getPath() + " 到 " + target.getPath()); &#125; catch (Exception ioe) &#123; ioe.printStackTrace(); log.error(ioe.getMessage()); &#125; &#125; &#125; /** * 删除文件夹里面所有内容 * @param path * @return * @author michael 20161103 from kbms */ public static boolean delAllFile(String path) &#123; boolean flag = false; File file = new File(path); if (!file.exists()) &#123; return flag; &#125; if (!file.isDirectory()) &#123; return flag; &#125; String[] tempList = file.list(); File temp = null; for (int i = 0; i &lt; tempList.length; i++) &#123; if (path.endsWith(File.separator)) &#123; temp = new File(path + tempList[i]); &#125; else &#123; temp = new File(path + File.separator + tempList[i]); &#125; if (temp.isFile()) &#123; temp.delete(); &#125; if (temp.isDirectory()) &#123; //先删除文件夹里面的文件 delAllFile(path + SysConstant.FILE_SEPARATOR + tempList[i]); flag = true; &#125; &#125; return flag; &#125; /** * 创建Txt文件 * @param name 文件名 * @param text * @return */ public static String createTxtFile(String name,String text)&#123; try&#123; String docPath=SysConstant.config.getProperty("txtPath"); if(docPath!=null)&#123; boolean re=true; docPath = docPath+SysConstant.FILE_SEPARATOR+DateUtil.getDate(""); File docFile=new File(docPath); if(!docFile.exists())&#123; re= docFile.mkdirs(); &#125; if(re)&#123; //结构化数据保存为txt文档用于创建文件索引 docPath+=SysConstant.FILE_SEPARATOR+name+".txt"; InputStream inputStream = InputStreamUtils.StringToInputStream(text); if(uploadDoc(inputStream,docPath)) return docPath;else&#123; return null; &#125; &#125;else&#123; log.error("docPath不存在"+docPath); return null; &#125; &#125;else&#123; log.error("配置文件SysConstant不存在 docPath 参数"); return null; &#125; &#125;catch(Exception e)&#123; e.printStackTrace(); return null; &#125; &#125; private static boolean uploadDoc(InputStream in,String filePath)&#123; boolean res= false; try&#123; if(in!=null)&#123; File file = new File(filePath); if(file.exists())&#123; file.createNewFile(); &#125; log.info("docPath:::&gt;&gt;"+filePath); int bytesRead = 0; OutputStream os = new FileOutputStream(file); byte[] buffer = new byte[8192]; while ((bytesRead = in.read(buffer, 0, 8192)) != -1) &#123; os.write(buffer, 0, bytesRead); &#125; os.close(); in.close(); res= true; &#125; &#125;catch(Exception e)&#123; e.printStackTrace(); log.error(e.getMessage()); &#125; return res; &#125; &#125; controller 1234567891011121314151617181920212223242526272829303132333435@RequestMapping(value = "/uploadFile", method = RequestMethod.POST,produces="text/plain;charset=UTF-8")@ResponseBodypublic ResultBean&lt;LogImportBean&gt; uploadFile(Locale locale,@RequestParam("uploadpath") String uploadpath,@RequestParam("fileName") String fileName,@RequestParam MultipartFile[] files, Model model) &#123; ResultBean&lt;LogImportBean&gt; rb=new ResultBean&lt;LogImportBean&gt;(); try &#123; String rootpatch= SysConstant.config.getProperty("rootuplaodfile")+File.separator+uploadpath; File rootfile=new File(rootpatch); if(!rootfile.exists())&#123; if(rootfile.mkdirs())&#123; log.info("创建文件夹成功："+rootpatch); &#125;else&#123; log.info("创建文件夹失败："+rootpatch); &#125; &#125; if(new File(rootpatch).exists())&#123; String filepath=rootpatch+File.separator+fileName; if(FileUtil.uploadfile(files[0].getInputStream(),filepath))&#123; rb.setBean(new LogImportBean(0, "保存附件成功！")); rb.setReturnCode(SysConstant.SYS_RETURN_SUCCESS_CODE); rb.setReturnMessage(SysConstant.SYS_RETURN_SUCCESS_MESSAGE); &#125;else&#123; rb.setBean(new LogImportBean(0, "保存附件失败！")); rb.setReturnCode(SysConstant.SYS_RETURN_FAILED_CODE); &#125; &#125;else&#123; rb.setReturnCode(SysConstant.SYS_RETURN_FAILED_CODE); rb.setReturnCode("rootpatch文件夹不存在："+rootpatch); &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; return rb;&#125; 文件下载 controller 123456789101112131415161718192021222324@RequestMapping(value = "/downloadFile", method = RequestMethod.GET)public ResponseEntity&lt;byte[]&gt; downloadFile(Locale locale, @RequestParam("uploadpath") String uploadpath, @RequestParam("fileName") String fileName, Model model)&#123; try &#123; String rootpatch= SysConstant.config.getProperty("rootuplaodfile")+File.separator+uploadpath; String filepath=rootpatch+File.separator+fileName; HttpHeaders headers = new HttpHeaders(); MediaType mt=new MediaType("application","octet-stream"); headers.setContentType(mt); fileName = new String(fileName.getBytes("UTF-8"),"ISO8859-1"); headers.setContentDispositionFormData("attachment",fileName); File file=new File(filepath); if(file.exists())&#123; return new ResponseEntity&lt;byte[]&gt;(FileUtils.readFileToByteArray(file), headers, HttpStatus.OK); &#125;else&#123; return new ResponseEntity&lt;byte[]&gt;(null, headers, HttpStatus.FAILED_DEPENDENCY); &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); return new ResponseEntity&lt;byte[]&gt;(null, null, HttpStatus.FAILED_DEPENDENCY); &#125;&#125;]]></content>
      <categories>
        <category>Java 高级</category>
      </categories>
      <tags>
        <tag>Java 高级</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[图片上传base64]]></title>
    <url>%2F2018%2F08%2F05%2F%E5%9B%BE%E7%89%87%E4%B8%8A%E4%BC%A0base64%2F</url>
    <content type="text"><![CDATA[图片上传base64 选择图片并转换html 页面12345&lt;div id="divPreview"&gt;&lt;input id="iconPic1" name="iconPic" type="hidden"/&gt;&lt;img id="imgHeadPhoto" src="" style="width: 120px; height: 130px; border: solid 1px #d2e2e2;" alt="" /&gt;&lt;/div&gt;&lt;input type="file" onchange="PreviewImage(this,'imgHeadPhoto','divPreview');" size="20" /&gt; js123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475//js本地图片预览，兼容ie[6-9]、火狐、Chrome17+、Opera11+、Maxthon3function PreviewImage(fileObj, imgPreviewId, divPreviewId) &#123; //允许上传文件的后缀名 var allowExtention = ".jpg,.bmp,.gif,.png"; var extention = fileObj.value.substring(fileObj.value.lastIndexOf(".") + 1).toLowerCase(); var browserVersion = window.navigator.userAgent.toUpperCase(); if (allowExtention.indexOf(extention) &gt; -1) &#123; //HTML5实现预览，兼容chrome、火狐7+等 if (fileObj.files) &#123; if (window.FileReader) &#123; var reader = new FileReader(); reader.onload = function (e) &#123; /*var pcBase64Text = e.target.result; var blob = dataURLtoBlob(pcBase64Text); console.log(e.target.result);*/ $("#iconPic1").val(e.target.result); document.getElementById(imgPreviewId).setAttribute("src",e.target.result); &#125; //读取文件把问价转换成 Base64 格式 reader.readAsDataURL(fileObj.files[0]); &#125;else if (browserVersion.indexOf("SAFARI") &gt; -1) &#123; alert("不支持Safari6.0以下浏览器的图片预览!"); &#125; &#125;else if (browserVersion.indexOf("MSIE") &gt; -1) &#123; // 兼容 IE6 浏览器 if (browserVersion.indexOf("MSIE 6") &gt; -1) &#123; document.getElementById(imgPreviewId).setAttribute("src", fileObj.value); &#125;else&#123; // 兼容 IE[7-9] 浏览器 fileObj.select(); if(browserVersion.indexOf("MSIE 9") &gt; -1)&#123; //不加上document.selection.createRange().text在ie9会拒绝访问 fileObj.blur(); &#125; var newPreview = document.getElementById(divPreviewId + "New"); if(newPreview == null)&#123; newPreview = document.createElement("div"); newPreview.setAttribute("id", divPreviewId + "New"); newPreview.style.width = document.getElementById(imgPreviewId).width + "px"; newPreview.style.height = document.getElementById(imgPreviewId).height + "px"; newPreview.style.height = document.getElementById(imgPreviewId).height + "px"; &#125; newPreview.style.filter = "progid:DXImageTransform.Microsoft.AlphaImageLoader(sizingMethod='scale', src='" + document.selection.createRange().text + "')"; var tempDivPreview = document.getElementById(divPreviewId); tempDivPreview.parentNode.insertBefore(newPreview, tempDivPreview); tempDivPreview.style.display = "none"; &#125; &#125;else if (browserVersion.indexOf("FIREFOX") &gt; -1) &#123; var firefoxVersion = parseFloat(browserVersion.toLowerCase().match(/firefox\/([\d.]+)/)[1]); //firefox7以下版本 if (firefoxVersion &lt; 7) &#123; document.getElementById(imgPreviewId). setAttribute("src", fileObj.files[0].getAsDataURL()); &#125;else &#123;//firefox7.0+ document.getElementById(imgPreviewId). setAttribute("src", window.URL.createObjectURL(fileObj.files[0])); &#125; &#125;else &#123; document.getElementById(imgPreviewId).setAttribute("src", fileObj.value); &#125; &#125;else &#123; alert("仅支持" + allowExtention + "为后缀名的文件!"); fileObj.value = ""; //清空选中文件 if (browserVersion.indexOf("MSIE") &gt; -1) &#123; fileObj.select(); document.selection.clear(); &#125; fileObj.outerHTML = fileObj.outerHTML; &#125; return fileObj.value; //返回路径&#125; 这段代码里面我只用到 第一个判断 兼容chrome、火狐7+ ，使用文件内容改变事件。 结果 关于 FileReader 的几种转化因为我保存到数据的类型是 Clob ,所以我把 FileReader 解析的 Base64 内容直接以文本的形式保存到数据库。这样的好处是，在显示图片的时候直接给 img 标签的 src 赋值就好了，不用转。 假如你的图片要保存在类型是 Blob 的话，那就需要把图片转成二进制（原因，reader.readAsDataURL转化的是十六进制Blob 只能保存二进制内容）。下面的两个方法可以实现，Base64 转 二进制，二进制 转 Base64 。 js 1234567891011121314151617function dataURLtoBlob(dataurl) &#123; var arr = dataurl.split(','), mime = arr[0].match(/:(.*?);/)[1], bstr = atob(arr[1]), n = bstr.length, u8arr = new Uint8Array(n); while(n--)&#123; u8arr[n] = bstr.charCodeAt(n); &#125; return new Blob([u8arr], &#123;type:mime&#125;);&#125;function readBlobAsDataURL(blob, callback) &#123; var a = new FileReader(); a.onload = function(e) &#123;callback(e.target.result);&#125;; a.readAsDataURL(blob);&#125;// 测试readBlobAsDataURL(blob, function (dataurl)&#123; console.log(dataurl);&#125;); 存储图片到数据库实体类123456789101112131415161718192021222324252627282930313233343536373839package com.eyuninfo.bean.uomp.keeperManager;import javax.persistence.Basic;import javax.persistence.Column;import javax.persistence.FetchType;import javax.persistence.Lob;import com.eyuninfo.bean.EntityBean;/** * @author: shenwenfang * @CreateDate: 2018-06-25 */public class KeeperManage extends EntityBean &#123; private static final long serialVersionUID = 7125755338853860747L; // 我想办code private String code; // 功能名称 private String name; // 页面跳转路径 private String url; // 渠道：1表示移动银行 2表示直销银行 private String channel; // 状态：0：正常 1表示删除 private String status; /** * 存放图片base64 */ @Lob private String iconPic; // 省略了 set 和 get 方法 &#125; 注意：在保存 base64 的字段加上 JPA 的 @Lob 注解。假如在查询字段报转换错误的话。 service 层12345678910111213package com.eyuninfo.uomp.web.keepermanager.service;import com.eyuninfo.bean.uomp.keeperManager.KeeperManage;import com.github.pagehelper.PageInfo;import java.util.List;public interface IKeperDealManageService &#123; /** * 添加 * @param bean * @return */ int addKeperDealManage(KeeperManage bean) throws Exception;&#125; 1234567891011121314package com.eyuninfo.uomp.web.keepermanager.service;import com.github.pagehelper.PageInfo;import com.eyuninfo.bean.uomp.keeperManager.KeeperManage;import com.eyuninfo.framework.base.service.impl.BaseService;import java.util.List;import org.springframework.stereotype.Service;@Service("keperDealManageService")public class KeperDealManageService extends BaseService implements IKeperDealManageService &#123; @Override public int addKeperDealManage(KeeperManage bean) &#123; return super.insert("addKeperDealManage", bean); &#125; &#125; controller 层1234567891011121314151617181920212223242526272829303132333435363738394041424344454647package com.eyuninfo.uomp.web.keepermanager.controller;import com.eyuninfo.bean.ResultBean;import com.eyuninfo.bean.uomp.keeperManager.KeeperManage;import org.springframework.web.bind.annotation.RequestParam;import org.springframework.web.bind.annotation.ResponseBody;import javax.annotation.Resource;@Controller@RequestMapping(value = "/keperDealManage")public class KeperDealManageController extends BaseController &#123; @Resource private IKeperDealManageService keperDealManageService; /** * 添加 * @param locale * @param bean * @return */ @RequestMapping(value = "/addKeperDealManage", method = &#123; RequestMethod.POST &#125;) @ResponseBody public ResultBean&lt;KeeperManage&gt; addKeperDealManage(Locale locale, @ModelAttribute("KeperDealManage") KeeperManage bean) &#123; ResultBean&lt;KeeperManage&gt; rb = new ResultBean&lt;KeeperManage&gt;(); try &#123; // 生成主键 bean.setCode(DateUtil.getCurDataFormat("yyyyMMddHHmmssSSS")); String date = DateUtil.getCurDataFormat("yyyyMMddHHmmss"); bean.setCreatetime(date); bean.setUpdateTime(date); int newKey = keperDealManageService.addKeperDealManage(bean); if(newKey &gt; 0)&#123; rb.setReturnCode(SysConstant.SYS_RETURN_SUCCESS_CODE); rb.setReturnMessage(SysConstant.SYS_RETURN_SUCCESS_MESSAGE); &#125; else &#123; rb.setReturnCode(SysConstant.SYS_RETURN_FAILED_CODE); rb.setReturnMessage(SysConstant.SYS_RETURN_FAILED_MESSAGE); &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); log.error("异常："+e.getMessage()); rb.setReturnCode(SysConstant.SYS_RETURN_EXCEPTION_CODE); rb.setReturnMessage("异常："+e.getMessage()); &#125; return rb; &#125; &#125; XML12345678910111213141516171819202122232425262728293031&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;!DOCTYPE mapper PUBLIC "-//mybatis.org//DTD Mapper 3.0//EN" "http://mybatis.org/dtd/mybatis-3-mapper.dtd"&gt;&lt;mapper namespace="com.eyuninfo.uomp.web.keepermanager.service.IKeperDealManageService"&gt; insert into keeper_deal_manage ( code, name, url, channel, creator, createtime, operator, update_time, sort, icon_pic, funtype ) values ( #&#123;code,jdbcType=VARCHAR&#125;, #&#123;name,jdbcType=VARCHAR&#125;, #&#123;url,jdbcType=VARCHAR&#125;, #&#123;channel,jdbcType=VARCHAR&#125;, #&#123;creator,jdbcType=VARCHAR&#125;, #&#123;createtime,jdbcType=VARCHAR&#125;, #&#123;operator,jdbcType=VARCHAR&#125;, #&#123;updateTime,jdbcType=VARCHAR&#125;, #&#123;sort,jdbcType=DECIMAL&#125;, #&#123;iconPic,jdbcType=CLOB&#125;, #&#123;funType,jdbcType=VARCHAR&#125; )&lt;/mapper&gt; 结果 保存的是一段很长得字符串（十六进制），把它放到浏览器的访问就是一张图片。 关联知识点FileList 对象和 file 对象HTML 中的 input[type=”file”] 标签有个 multiple 属性，允许用户选择多个文件，FileList对象则就是表示用户选择的文件列表。这个列表中的每一个文件，就是一个 file 对象。 实例12345678910111213&lt;input type="file" id="files" multiple&gt;&lt;script&gt; var elem = document.getElementById('files'); elem.onchange = function (event) &#123; var files = event.target.files; for (var i = 0; i &lt; files.length; i++) &#123; // 文件类型为 image 并且文件大小小于 200kb if(files[i].type.indexOf('image/') !== -1 &amp;&amp; files[i].size &lt; 204800)&#123; console.log(files[i].name); &#125; &#125; &#125;&lt;/script&gt; input 中有个 accept 属性，可以用来规定能够通过文件上传进行提交的文件类型。 accept=”image/*” 可以用来限制只允许上传图像格式。但是在 Webkit 浏览器下却出现了响应滞慢的问题，要等上好几秒才弹出文件选择框。 解决方法就是将 * 通配符改为指定的 MIME 类型。 1&lt;input type="file" accept="image/gif,image/jpeg,image/jpg,image/png"&gt; Blob 对象Blob 对象相当于一个容器，可以用于存放二进制数据。它有两个属性，size 属性表示字节长度，type 属性表示 MIME 类型。 创建1var blob = new Blob(['hello'], &#123;type:"text/plain"&#125;); Blob 构造函数中的第一个参数是一个数组，可以存放 ArrayBuffer对象、ArrayBufferView 对象、Blob对象和字符串。 Blob 对象可以通过 slice() 方法来返回一个新的 Blob 对象。 1var newblob = blob.slice(0,5, &#123;type:"text/plain"&#125;); slice() 方法使用三个参数，均为可选。第一个参数代表要从Blob对象中的二进制数据的起始位置开始复制，第二个参数代表复制的结束位置，第三个参数为 Blob 对象的 MIME 类型。 canvas.toBlob() 也可以创建 Blob 对象。toBlob() 使用三个参数，第一个为回调函数，第二个为图片类型，默认为 image/png，第三个为图片质量，值在0到1之间。 12var canvas = document.getElementById('canvas');canvas.toBlob(function(blob)&#123; console.log(blob); &#125;, "image/jpeg", 0.5); 下载Blod 对象可以通过 window.URL 对象生成一个网络地址，结合 a 标签的 download 属性来实现下载文件功能。 比如把 canvas 下载为一个图片文件。 123456789101112var canvas = document.getElementById('canvas');canvas.toBlob(function(blob)&#123; // 使用 createObjectURL 生成地址，格式为 blob:null/fd95b806-db11-4f98-b2ce-5eb16b38ba36 var url = URL.createObjectURL(blob); var a = document.createElement('a'); a.download = 'canvas'; a.href = url; // 模拟a标签点击进行下载 a.click(); // 下载后告诉浏览器不再需要保持这个文件的引用了 URL.revokeObjectURL(url);&#125;);]]></content>
      <categories>
        <category>Java 高级</category>
      </categories>
      <tags>
        <tag>Java 高级</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[实现 xls 数据的导入]]></title>
    <url>%2F2018%2F08%2F05%2F%E6%95%B0%E6%8D%AE%E5%AF%BC%E5%85%A5%2F</url>
    <content type="text"><![CDATA[实现 xls 数据的导入 html12345678&lt;a href="#" class="easyui-linkbutton" data-options="iconCls:'icon-add',plain:true" onclick="batchAddClient()"&gt;导入信息&lt;/a&gt;&lt;!-- 导入框 --&gt;&lt;div id="importCheckWin" class="easyui-dialog" data-options="onBeforeOpen:getDialogCheckUrl,closed:true" style="width:489px; height:268px;"&gt; &lt;iframe frameborder="0" id = "impIframeCheck" width="100%" height="100%" scrolling="no" src=""&gt;&lt;/iframe&gt; &lt;/div&gt; js 方法1234function getDialogCheckUrl()&#123; var url = getHost(HostAddress.uomp)+'/uomp/cdaBaseDataRemarks/importCheckWin'; $('#impIframeCheck').attr('src',url);&#125; controller123456789101112131415@Controller@RequestMapping(value = "/cdaBaseDataRemarks")public class CdaBaseDataRemarksController extends BaseController &#123; /** * 打开批量检查导入窗口 * @param locale * @param model * @return */ @RequestMapping(value = "/importCheckWin", method = RequestMethod.GET) public String importCheckWin(Locale locale, Model model) &#123; model.addAttribute("uihost", SysConstant.config.getProperty("uihost")); return "importBatchRemarksCheck"; &#125;&#125; jsp 页面123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990&lt;%@ page language="java" contentType="text/html; charset=UTF-8" pageEncoding="UTF-8"%&gt;&lt;%@ taglib prefix="c" uri="http://java.sun.com/jsp/jstl/core"%&gt;&lt;!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd"&gt;&lt;html&gt;&lt;head&gt;&lt;meta http-equiv="Content-Type" content="text/html; charset=UTF-8"&gt;&lt;title&gt;批量检查客户窗口&lt;/title&gt;&lt;script type="text/javascript" src="$&#123;uihost&#125;/sunriseui/sunriseui.js"&gt;&lt;/script&gt;&lt;script type="text/javascript"&gt; load('ajaxfileupload')&lt;/script&gt;&lt;script type="text/javascript"&gt; var map;var index=0; var mapLenght=0;/*map长度 */ var successNum=0;/*成功条数*/ var failNum='';/*错误行号*/ //存在跨域问题 不能够获取到返回值 -- 批量检查提交 function coreSelect() &#123; $('#tipgrid').empty(); $.ajaxFileUpload(&#123; url : getHost(HostAddress.uomp) + '/uomp/cdaBaseDataRemarks/importBatchClientCheckData', secureuri : false, // 是否需要安全协议，一般设置为false id : "filesform", fileElementId : 'files', // 文件上传域的ID dataType : 'json', // 返回值类型 一般设置为json data : &#123; &#125;, async : false, success : function(data)&#123;// 服务器成功响应处理函数 if(data.returnCode==ReturnCode.success)&#123; $('#importRotaff').form('reset'); $('#tipgrid').datagrid(&#123; data:data, title:'导入结果', fitColumns:false, border:false, columns:[[ &#123;field:'relustMsg',title:'备注信息导入结果',width:500&#125; ]] &#125;); $('#tipgrid').append("&lt;span&gt;"+data.returnMessage+'&lt;/span&gt;'); &#125;else&#123; $.messager.alert('警告', data.returnMessage,'warning'); &#125; &#125; &#125;); &#125; function fileChange(target)&#123; var fileSize = 0; if(!target.files)&#123; var filePath = target.value; var fileSystem = new ActiveXObject("Scriting.FileSystemObject"); var file = fileSystem.GetFile(filePath); fileSize = file.Size; &#125;else&#123; fileSize = target.files[0].size; &#125; var size = fileSize/1024; if(size&gt;3000)&#123; $.messager.alert('警告','导入数据超过3M，请分批导入!','warning'); $('#subBtn').attr('disable'); return; &#125;else&#123; $('#subBtn').linkbutton('enable'); &#125; &#125;&lt;/script&gt;&lt;/head&gt;&lt;body class="easyui-layout" data-options="fit:true"&gt; &lt;div data-options="region:'center'" style="background: #eee;"&gt; &lt;form id="importRotaff" action="" enctype="multipart/form-data"&gt; &lt;table style="width: 100%"&gt; &lt;tr&gt; &lt;td colspan="2"&gt; &lt;input type="file" name="files" onchange="fileChange(this)" id="files" style="width: 200px"&gt; &lt;a id="subBtn" href="#" class="easyui-linkbutton" iconCls="icon-search" onclick="coreSelect()"&gt;导入信息&lt;/a&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="center" id="thistd"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/table&gt; &lt;/form&gt; &lt;div id="tipgrid"&gt;&lt;/div&gt; &lt;/div&gt;&lt;/body&gt;&lt;/html&gt; 数据导入 controller1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859@Controller@RequestMapping(value = "/cdaBaseDataRemarks")public class CdaBaseDataRemarksController extends BaseController &#123; /** * 读取批量检查Excel导入 * @param locale * @param model * @return */ @RequestMapping(value = "/importBatchClientCheckData", method = RequestMethod.POST) @ResponseBody public ResultBean&lt;LogImportBean&gt; importBatchClientCheckData(Locale locale, @RequestParam MultipartFile[] files,@ModelAttribute("CdaBaseDataRemarks") CdaBaseDataRemarks bean) throws Exception&#123; ResultBean&lt;LogImportBean&gt; rb = new ResultBean&lt;LogImportBean&gt;(); //返回导入结果明细信息列表 List&lt;LogImportBean&gt; logs = new ArrayList&lt;LogImportBean&gt;(); rb.setRows(logs); if(files[0].getSize()==0)&#123; rb.setReturnCode(SysConstant.SYS_RETURN_FAILED_CODE); rb.setReturnMessage("未选择文件，请选择您要导入的execl文件"); return rb; &#125; InputStream is = files[0].getInputStream(); Workbook book = Workbook.getWorkbook(is); Sheet sheet = book.getSheet(0); int rownum = sheet.getRows(); // 解析到表格中的数量，判断是否超过导入个数限制 if (rownum &gt; 1) &#123; rb.setReturnCode(SysConstant.SYS_RETURN_SUCCESS_CODE); rb.setReturnMessage(SysConstant.SYS_RETURN_SUCCESS_MESSAGE); for (int i = 1; i &lt; rownum; i++) &#123; //第一个参数是列，第二个参数是行 String custId = sheet.getCell(0, i).getContents(); String certNo = sheet.getCell(1, i).getContents(); String certType = sheet.getCell(2, i).getContents(); String remarks = sheet.getCell(3, i).getContents(); // 判断字符串是否在大于20 if(remarks.length()&gt;20)&#123; logs.add(new LogImportBean("备注内容不能大于20个字")); &#125; // 新增自己的业务代码 &#125;// for 循环结束 &#125;else&#123; rb.setReturnCode(SysConstant.SYS_RETURN_FAILED_CODE); rb.setReturnMessage("表格不能为空！"); &#125; return rb; &#125;&#125;]]></content>
      <categories>
        <category>Java 高级</category>
      </categories>
      <tags>
        <tag>Java 高级</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[webstorm 配置 Apache 服务器]]></title>
    <url>%2F2018%2F08%2F05%2Fwebstorm%E9%85%8D%E7%BD%AEApache%E6%9C%8D%E5%8A%A1%E5%99%A8%2F</url>
    <content type="text"><![CDATA[webstorm配置Apache服务器今天尝试了在 apache 上部署静态工程，顺便记录一下，希望能帮到需要的同学！ 添加 apache 服务 ​ 服务的类型选择 Local or mounted folder 。 配置 Connection 配置 Mappings 1处，选择你工程所在的位置，选择到工程直接访问的文件夹。（我的工程名称是 com.eyuninfo.web,但是我所有的页面都在 WebContent 的目录下）。 2处，htdocs 是你安装的 Apache 的目录下的一个目录，在这个目录下面放的是你在 webstorm 的 Tools-&gt;Deployment-&gt;Upload to Default Server 的所上传的文件，也就是部署到服务器路径下面的目录，如下： 3处，sunriseui 是工程的上下文。 访问的时候就是 http://127.0.0.1/sunriseui/sunrise/pages/home/tophome_w.html 这样了。 保存，完成。]]></content>
      <categories>
        <category>webstorm</category>
      </categories>
      <tags>
        <tag>开发工具</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[你的面试题]]></title>
    <url>%2F2018%2F06%2F21%2F%E4%BD%A0%E7%9A%84%E9%9D%A2%E8%AF%95%E9%A2%98%2F</url>
    <content type="text"><![CDATA[你的面试题引言找工作对于每个人来说对都都意味着你将接受企业对你的考验。在接受考验之前我们应该做好准备，才能提高自己的信心进而坦然的迎接每一次考验。不断的积累经验，不气馁、不抱怨、不放弃直到找到属于自己的岗位，并为之努力。 java中有哪些集合，主要方法有哪些主要有LinkedList，ArrayList，Vector等。下面是详细：Collection├List│├LinkedList│├ArrayList│└Vector│ └Stack└SetMap├Hashtable├HashMap└WeakHashMapList 的具体实现包括 ArrayList 和 Vector，它们是可变大小的列表，比较适合构建、存储和操作任何类型对象的元素列表。 List 适用于按数值索引访问元素的情形。 Map 提供了一个更通用的元素存储方法。 Map 集合类用于存储元素对（称作“键”和“值”）其中每个键映射到一个值。 List、Set、Map是否继承自Collection接口List、Set 是，Map 不是。Map是键值对映射容器，与List和Set有明显的区别，而Set存储的零散的元素且不允许有重复元素，List是线性结构的容器，适用于按数值索引访问元素的情形。 List、Map、Set接口，存取元素时各自特点List 以特定次序来持有元素，可有重复元素。Set 无法拥有重复元素,内部排序。Map 保存key-value值，value可多值。 List的遍历： List接口有size()和get()方法，用这两个方法可以实现对List的遍历。size()方法得到List中的元素个数。get()方法取得某个位置上的元素 List 和 setList：元素是有序的，元素可以重复，因为该集合体系有索引Set：元素是无序的，元素不可以重复（存入和取出的顺序不一定一致） List中常见的三个子类 ArrayList ：底层的数据使用的是数组结构。 特点：查询速度很快，但是增删稍慢。线程不同步，效率高 。可变长度数组，默认容量为10的空列表，如果超过了，则50%的增加 LinkedList ：底层的数据使用的是链表数据结构。 特点：增删数度很快，但是查询稍慢。 Vector：底层使用的是数组结构。枚举是Vector特有的取出方式是同步的，效率较低，被ArrayList替代。最早出现的。默认容量为10的空列表，如果超过了，则100%的增加. ArrayList 与 LinkedList 的区别最明显的区别是 ArrrayList 底层的数据结构是数组，支持随机访问，而 LinkedList 的底层数据结构是链表，不支持随机访问。使用下标访问一个元素，ArrayList 的时间复杂度是 O(1)，而 LinkedList 是 O(n)。1.LinkedList内部存储的是Node，不仅要维护数据域，还要维护prev和next，如果LinkedList中的结点特别多，则LinkedList比ArrayList更占内存。插入删除操作效率：2.LinkedList在做插入和删除操作时，插入或删除头部或尾部时是高效的，操作越靠近中间位置的元素时，需要遍历查找，速度相对慢一些，如果在数据量较大时，每次插入或删除时遍历查找比较费时。所以LinkedList插入与删除，慢在遍历查找，快在只需要更改相关结点的引用地址。ArrayList在做插入和删除操作时，插入或删除尾部时也一样是高效的，操作其他位置，则需要批量移动元素，所以ArrayList插入与删除，快在遍历查找，慢在需要批量移动元素。3.循环遍历效率：由于ArrayList实现了RandomAccess随机访问接口，所以使用for(int i = 0; i &lt; size; i++)遍历会比使用Iterator迭代器来遍历快而由于LinkedList未实现RandomAccess接口，所以推荐使用Iterator迭代器来遍历数据。因此，如果我们需要频繁在列表的中部改变插入或删除元素时，建议使用LinkedList，否则，建议使用ArrayList，因为ArrayList遍历查找元素较快，并且只需存储元素的数据域，不需要额外记录其他数据的位置信息，可以节省内存空间。 Java 中的 LinkedList 是单向链表还是双向链表是双向链表。 Vector和ArrayList的区别 首先看这两类都实现List接口，而List接口一共有三个实现类，分别是 ArrayList、Vector 和 LinkedList 。List 用于存放多个元素，能够维护元素的次序，并且允许元素的重复。3个具体实现类的相关区别如下： 1.ArrayList是最常用的List实现类，内部是通过数组实现的，它允许对元素进行快速随机访问。数组的缺点是每个元素之间不能有间隔，当数组大小不满足时需要增加存储能力，就要将已经有数组的数据复制到新的存储空间中。当从 ArrayList 的中间位置插入或者删除元素时，需要对数组进行复制、移动、代价比较高。因此，它适合随机查找和遍历，不适合插入和删除。2.Vector与ArrayList一样，也是通过数组实现的，不同的是它支持线程的同步，即某一时刻只有一个线程能够写Vector，避免多线程同时写而引起的不一致性，但实现同步需要很高的花费，因此，访问它比访问ArrayList慢。3.LinkedList是用链表结构存储数据的，很适合数据的动态插入和删除，随机访问和遍历速度比较慢。另外，他还提供了List接口中没有定义的方法，专门用于操作表头和表尾元素，可以当作堆栈、队列和双向队列使用。 HashMap与HashTable的区别1、HashMap 是非线程安全的，HashTable 是线程安全的。 2、HashMap 的键和值都允许有 null 值存在，而 HashTable 则不行。 3、因为线程安全的问题，HashMap 效率比 HashTable 的要高。HashMap 的实现机制：维护一个每个元素是一个链表的数组，而且链表中的每个节点是一个 Entry[] 键值对的数据结构。实现了 数组+链表 的特性，查找快，插入删除也快。对于每个 key , 他对应的数组索引下标是 int i = hash(key.hashcode)&amp;(len-1);每个新加入的节点放在链表首，然后该新加入的节点指向原链表首 HashMap，ConcurrentHashMap与LinkedHashMap的区别ConcurrentHashMap是使用了锁分段技术技术来保证线程安全的，锁分段技术：首先将数据分成一段一段的存储，然后给每一段数据配一把锁，当一个线程占用锁访问其中一个段数据的时候，其他段的数据也能被其他线程访问ConcurrentHashMap 是在每个段（segment）中线程安全的LinkedHashMap维护一个双链表，可以将里面的数据按写入的顺序读出 ConcurrentHashMap应用场景1：ConcurrentHashMap 的应用场景是高并发，但是并不能保证线程安全，而同步的 HashMap 和 HashMap 的是锁住整个容器，而加锁之后 ConcurrentHashMap 不需要锁住整个容器，只需要锁住对应的 Segment 就好了，所以可以保证高并发同步访问，提升了效率。2：可以多线程写。ConcurrentHashMap把HashMap分成若干个Segmenet1.get时，不加锁，先定位到segment然后在找到头结点进行读取操作。而value是volatile变量，所以可以保证在竞争条件时保证读取最新的值，如果读到的value是null，则可能正在修改，那么久调用ReadValueUnderLock函数，加锁保证读到的数据是正确的。 2.Put时会加锁，一律添加到hash链的头部。 3.Remove时也会加锁，由于next是final类型不可改变，所以必须把删除的节点之前的节点都复制一遍。 4.ConcurrentHashMap允许多个修改操作并发进行，其关键在于使用了锁分离技术。它使用了多个锁来控制对Hash表的不同Segment进行的修改。ConcurrentHashMap的应用场景是高并发，但是并不能保证线程安全，而同步的HashMap和HashTable的是锁住整个容器，而加锁之后ConcurrentHashMap不需要锁住整个容器，只需要锁住对应的segment就好了，所以可以保证高并发同步访问，提升了效率。 HashSet 和 HashMap 区别 HashSet： HashSet实现了Set接口，它不允许集合中出现重复元素。当我们提到HashSet时，第一件事就是在将对象存储在 HashSet之前，要确保重写hashCode（）方法和equals（）方法，这样才能比较对象的值是否相等，确保集合中没有储存相同的对象。如果不重写上述两个方法，那么将使用下面方法默认实现： public boolean add(Object obj)方法用在Set添加元素时，如果元素值重复时返回 “false”，如果添加成功则返回”true” HashMap： HashMap实现了Map接口，Map接口对键值对进行映射。Map中不允许出现重复的键（Key）。Map接口有两个基本的实现TreeMap和HashMap。TreeMap保存了对象的排列次序，而HashMap不能。HashMap可以有空的键值对（Key（null）-Value（null））HashMap是非线程安全的（非Synchronize），要想实现线程安全，那么需要调用collections类的静态方法synchronizeMap（）实现。 public Object put(Object Key,Object value)方法用来将元素添加到map中。 总结： HashMap 实现了 Map 接口；存储键值对；调用put（）向map中添加元素；HashMap使用键（Key）计算Hashcode；HashMap相对于HashSet较快，因为它是使用唯一的键获取对象。 HashSet 实现了 Set 接口；仅存储对象；调用add（）方法向Set中添加元素；HashSet使用成员对象来计算hashcode值，对于两个对象来说hashcode可能相同，所以equals()方法用来判断对象的相等性，如果两个对象不同的话，那么返回false；HashSet较HashMap来说比较慢。 重载和重写的区别 重载：允许存在一个以上的同名函数，只要它们的参数个数或参数类型不同即可。 重写：当子类继承父类，沿袭了父类的功能到子类中，子类虽具备该功能，但功能内容不一致，这是使用覆盖特性，保留父类的功能定义，并重写功能内容。 forward 和redirect的区别forward是服务器请求资源，服务器直接访问目标地址的URL，把那个URL的响应内容读取过来，然后把这些内容再发给浏览器，浏览器根本不知道服务器发送的内容是从哪儿来的，所以它的地址栏中还是原来的地址。 redirect就是服务端根据逻辑,发送一个状态码,告诉浏览器重新去请求那个地址，一般来说浏览器会用刚才请求的所有参数重新请求，所以session,request参数都可以获取。 int 和 Integer 有什么区别Java 提供两种不同的类型：引用类型和原始类型（或内置类型）。Int 是 java 的原始数据类型，Integer 是 java为int提供的封装类。Java为每个原始类型提供了封装类。原始类型封装类，booleanBoolean,charCharacter,byteByte,shortShort,intInteger,longLong,floatFloat,doubleDouble引用类型和原始类型的行为完全不同，并且它们具有不同的语义。引用类型和原始类型具有不同的特征和用法，它们包括：大小和速度问题，这种类型以哪种类型的数据结构存储，当引用类型和原始类型用作某个类的实例数据时所指定的缺省值。对象引用实例变量的缺省值为 null，而原始类型实例变量的缺省值与它们的类型有关 error和exception有什么区别Error和Exception都是java错误处理机制的一部分，都继承了Throwable类。 Exception表示的异常，异常可以通过程序来捕捉，或者优化程序来避免。 Error表示的是系统错误，不能通过程序来进行错误处理 throw 和 throws 的区别throw 用于抛出 java.lang.Throwable 类的一个实例化对象，意思是说你可以通过关键字 throw 抛出一个 Error 或者 一个Exception，如：throw new IllegalArgumentException(“size must be multiple of 2″) ；而throws 的作用是作为方法声明和签名的一部分，方法被抛出相应的异常以便调用者能处理。Java 中，任何未处理的受检查异常强制在 throws 子句中声明。 简单回答： Throw写在代码块内，throw后面跟的是一个具体的异常实例Throws写在方法前面后面，throws后面跟的是异常类，异常类可以出现多个 常见Runtime异常 ArithmeticException：算术异常 InstantiationException：对象初始化异常 IllegalArgumentException：参数不匹配异常 ArrayIndexOutOfBoundsException：数组下标越界 NullPointerException：空指针异常 NumberFormatException：数字转换异常 java 属性的可见性有那些属性的可见性有：公有的（public） 保护的（protected） 默认的（default）私有的（private） 属性的访问权限 权限 public protected defalut private 同一个类中 OK OK OK OK 同一个包中 OK OK OK 子类（不同包） OK OK 不同包中 OK 线程与进程的区别进程是系统进行资源分配和调度的一个独立单位，线程是CPU调度和分派的基本单位 进程和线程的关系： 一个线程只能属于一个进程，而一个进程可以有多个线程，但至少有一个线程。 资源分配给进程，同一进程的所有线程共享该进程的所有资源。 线程在执行过程中，需要协作同步。不同进程的线程间要利用消息通信的办法实现同步。 线程是指进程内的一个执行单元，也是进程内的可调度实体。 线程与进程的区别： 调度：线程作为调度和分配的基本单位，进程作为拥有资源的基本单位。 并发性：不仅进程之间可以并发执行，同一个进程的多个线程之间也可以并发执行。 拥有资源：进程是拥有资源的一个独立单位，线程不拥有系统资源，但可以访问隶属于进程的资源。 系统开销：在创建或撤销进程的时候，由于系统都要为之分配和回收资源，导致系统的明显大于创建或撤销线程时的开销。但进程有独立的地址空间，进程崩溃后，在保护模式下不会对其他的进程产生影响，而线程只是一个进程中的不同的执行路径。线程有自己的堆栈和局部变量，但线程之间没有单独的地址空间，一个线程死掉就等于整个进程死掉，所以多进程的程序要比多线程的程序健壮，但是在进程切换时，耗费的资源较大，效率要差些。 创建线程的两种方式 继承Thread类。 定义类继承Thread； 复写父类中的方法；目的：将自定义代码存储在run方法中，让线程运行。 调用线程的start方法，该方法有两个作用：启动线程，调用run方法 实现Runnable接口 定义类实现Runnable接口。 覆盖Runnable接口中的run方法。 通过Thread类建立线程对象。 将Runnable接口的子类对象作为实际参数传递给Thread类的构造函数。 调用Thread类的start方法开启线程并调用Runnable接口子类的run方法。 创建线程的实现方式和继承方式有什么区别 实现方式相比继承方式的好处： 避免了单继承的局限性（单继承只能继承一个父类）。在定义线程时，建议使用实现方式。 存放代码的位置不一样： 继承Thread：线程代码存放Thread子类的run方法中 实现Runnable，线程代码存在接口的子类的run方法。 实现Runnable接口的好处 将线程的任务从线程的子类中分离出来，进行了单独的封装。 按照面向对象的思想将任务的封装成对象。 避免了java单继承的局限性。 Runnable接口和Callable接口的区别这是一个优点深度的问题。 Runnable接口中的run()方法的返回值是void，它做的事情只是纯粹地去执行run()方法中的代码而已；Callable接口中的call()方法是有返回值的，是一个泛型，和Future、FutureTask配合可以用来获取异步执行的结果。 这其实是很有用的一个特性，因为多线程相比单线程更难、更复杂的一个重要原因就是因为多线程充满着未知性，某条线程是否执行了？某条线程执行了多久？某条线程执行的时候我们期望的数据是否已经赋值完毕？无法得知，我们能做的只是等待这条多线程的任务执行完毕而已。而Callable+Future/FutureTask却可以获取多线程运行的结果，可以在等待时间太长没获取到需要的数据的情况下取消该线程的任务，真的是非常有用。 线程同步的方法 wait():让线程等待。将线程存储到一个线程池中。 notify()：唤醒被等待的线程。通常都唤醒线程池中的第一个。让被唤醒的线程处于临时阻塞状态。 notifyAll(): 唤醒所有的等待线程。将线程池中的所有线程都唤醒。 启动一个线程是用run()还是start()启动一个线程是调用start()方法，使线程所代表的虚拟处理机处于可运行状态，这意味着它可以由JVM调度并执行。这并不意味着线程就会立即运行。run()方法可以产生必须退出的标志来停止一个线程。 Java的数据结构有那些 线性表（ArrayList） 链表（LinkedList） 栈（Stack） 队列（Queue） 图（Map） 树（Tree） Java中有几种数据类型 整型：byte,short,int,long 浮点型：float,double 字符型：char 布尔型：boolean Java中的包装类都是那些 byte：Byte short：Short int：Integer long：Long float：Float double：Double char：Character boolean：Boolean Java最顶级的父类是哪个Object Object 的常用方有哪些clone()、equals()、hashCode()、notify()、notifyAll()、toString()、wait()、finalize() 数组实例化有几种方式静态实例化：创建数组的时候已经指定数组中的元素,int[] a=new int[]{1,3,3} 动态实例化：实例化数组的时候，只指定了数组程度，数组中所有元素都是数组类型的默认值 实例化数组后，能不能改变数组长度呢不能，数组一旦实例化，它的长度就是固定的 String是最基本的数据类型吗java.lang.String类是final类型的，因此不可以继承这个类、不能修改这个类。为了提高效率节省空间，我们应该用StringBuffer类。 是否可以继承String类String 类是final类，不可以被继承。 补充：继承String本身就是一个错误的行为，对String类型最好的重用方式是关联关系（Has-A）和依赖关系（Use-A）而不是继承关系（Is-A） String类的常用方法有那些charAt：返回指定索引处的字符indexOf()：返回指定字符的索引replace()：字符串替换trim()：去除字符串两端空白split()：分割字符串，返回一个分割后的字符串数组getBytes()：返回字符串的byte类型数组length()：返回字符串长度toLowerCase()：将字符串转成小写字母toUpperCase()：将字符串转成大写字符substring()：截取字符串format()：格式化字符串equals()：字符串比较 ==与equlas有什么区别==可以判断基本数据类型值是否相等，也可以判断两个对象指向的内存地址是否相同，也就是说判断两个对象是否是同一个对象，Equlas通常用来做字符串比较。 final关键字的作用 可以修饰类、函数、变量； 被final修饰的类不可以被继承。为了避免被继承，被子类复写。final class Demo { } 被final修饰的方法不可以被复写。final void show () { } 被final 修饰的变量是一个常量，只能赋值一次。 内部类定义在类中的局部位置上时，只能访问该局部被final修饰的局部变量。 static关键字有哪些作用static 修饰变量、修饰方法;静态块;静态内部类;静态导包; abstract class和interface有什么区别抽象类与接口的区别：1.接口可以多重继承 ，抽象类不可以2.接口定义方法，不给实现；而抽象类可以实现部分方法3.接口中基本数据类型的数据成员，都默认为static和final，抽象类则不是如果事先知道某种东西会成为基础类，那么第一个选择就是把它变成一个接口。只有在必须使用方法定义或者成员变量的时候，才应考虑采用抽象类。 final,finally,finalize的区别final—修饰符（关键字）如果一个类被声明为final，意味着它不能再派生出新的子类，不能作为父类被继承。因此一个类不能既被声明为 abstract的，又被声明为final的。将变量或方法声明为final，可以保证它们在使用中不被改变。被声明为final的变量必须在声明时给定初值，而在以后的引用中只能读取，不可修改。被声明为final的方法也同样只能使用，不能重写。 finally—再异常处理时提供 finally 块来执行任何清除操作。如果抛出一个异常，那么相匹配的 catch 子句就会执行，然后控制就会进入 finally 块（如果有的话）。 finalize—方法名。Java 技术允许使用 finalize() 方法在垃圾收集器将对象从内存中清除出去之前做必要的清理工作。这个方法是由垃圾收集器在确定这个对象没有被引用时对这个对象调用的。它是在 Object 类中定义的，因此所有的类都继承了它。子类覆盖 finalize() 方法以整理系统资源或者执行其他清理工作。finalize() 方法是在垃圾收集器删除对象之前对这个对象调用的。 HTTP请求方法get和post有什么区别 Post传输数据时，不需要在URL中显示出来，而Get方法要在URL中显示。 Post传输的数据量大，可以达到2M，而Get方法由于受到URL长度限制,只能传递大约1024字节. Post就是为了将数据传送到服务器段,Get就是为了从服务器段取得数据.而Get之所以也能传送数据,只是用来设计告诉服务器,你到底需要什么样的数据.Post的信息作为http请求的内容，而Get是在Http头部传输的。 其他 HTTP 请求方法 HEAD 与 GET 相同，但只返回 HTTP 报头，不返回文档主体。 PUT上传指定的 URI 表示。DELETE 删除指定资源。 OPTIONS 返回服务器支持的 HTTP 方法 CONNECT 把请求连接转换到透明的 TCP/IP 通道。 session 与 cookie 区别 cookie数据存放在客户的浏览器上，session数据放在服务器上。 cookie不是很安全，别人可以分析存放在本地的COOKIE并进行COOKIE欺骗考虑到安全应当使用session。 session会在一定时间内保存在服务器上。当访问增多，会比较占用你服务器的性能考虑到减轻服务器性能方面，应当使用COOKIE。 单个cookie保存的数据不能超过4K，很多浏览器都限制一个站点最多保存20个cookie。 所以个人建议： 将登陆信息等重要信息存放为SESSION 其他信息如果需要保留，可以放在COOKIE中 Mysql 的分页 SQL 语句select * from tablename limit m,n(n是指从第m+1条开始，取n条) Mysql 优化1.如果明确知道只有一条结果返回，limit1能够提高效率 2.把计算放在业务层而不是数据库层，除了节省数据的 CPU ,还有意想不到的查询缓存优化效果。 3.强制类型转换会全表扫描 4.在属性上进行计算不能命中索引 5.使用 ENUM 而不是字符串 6.数据分区度不大的字段不宜使用索引 7.负向查询和前导模糊查询不能使用索引 8.用TRUNCATE替代DELETE 9.删除重复记录 10.用Where子句替换HAVING子句 11.用EXISTS替代IN、用NOT EXISTS替代NOT IN 12.用索引提高效率 13.用EXISTS替换DISTINCT 14.用&gt;=替代&gt; 15.用IN来替换OR String s = new String(“xyz”);创建了几个字符串对象两个对象，一个是静态区的”xyz”，一个是用new创建在堆上的对象。 用Java写一个单例类123456789101112131415161718192021222324252627282930313233饿汉式单例:public class Singleton &#123; private Singleton()&#123;&#125; private static Singleton instance = new Singleton(); public static Singleton getInstance()&#123; return instance; &#125;&#125;懒汉式单例（线程安全）:public class Singleton &#123; private static Singleton instance = null; private Singleton() &#123;&#125; public static synchronized Singleton getInstance()&#123; if (instance == null) instance ＝ new Singleton(); return instance; &#125;&#125; 用Java写一个冒泡排序冒泡排序几乎是个程序员都写得出来，但是面试的时候如何写一个逼格高的冒泡排序却不是每个人都能做到 123456789101112131415161718192021import java.util.Comparator;/** * 排序器接口(策略模式: 将算法封装到具有共同接口的独立的类中使得它们可以相互替换) * */public interface Sorter &#123; /** * 排序 * @param list 待排序的数组 */ public &lt;T extends Comparable&lt;T&gt;&gt; void sort(T[] list); /** * 排序 * @param list 待排序的数组 * @param comp 比较两个对象的比较器 */ public &lt;T&gt; void sort(T[] list, Comparator&lt;T&gt; comp);&#125; 12345678910111213141516171819202122232425262728293031323334353637383940import java.util.Comparator;/** * 冒泡排序 * */public class BubbleSorter implements Sorter &#123; @Override public &lt;T extends Comparable&lt;T&gt;&gt; void sort(T[] list) &#123; boolean swapped = true; for (int i = 1, len = list.length; i &lt; len &amp;&amp; swapped; ++i) &#123; swapped = false; for (int j = 0; j &lt; len - i; ++j) &#123; if (list[j].compareTo(list[j + 1]) &gt; 0) &#123; T temp = list[j]; list[j] = list[j + 1]; list[j + 1] = temp; swapped = true; &#125; &#125; &#125; &#125; @Override public &lt;T&gt; void sort(T[] list, Comparator&lt;T&gt; comp) &#123; boolean swapped = true; for (int i = 1, len = list.length; i &lt; len &amp;&amp; swapped; ++i) &#123; swapped = false; for (int j = 0; j &lt; len - i; ++j) &#123; if (comp.compare(list[j], list[j + 1]) &gt; 0) &#123; T temp = list[j]; list[j] = list[j + 1]; list[j + 1] = temp; swapped = true; &#125; &#125; &#125; &#125;&#125; Java解析XML的四种方法 DOM4J生成和解析XML文档 DOM生成和解析XML文档 SAX生成和解析XML文档 JDOM生成和解析XML Maven有哪些优点优点如下：简化了项目依赖管理：易于上手，对于新手可能一个”mvn clean package”命令就可能满足他的工作便于与持续集成工具（jenkins）整合便于项目升级，无论是项目本身升级还是项目使用的依赖升级。有助于多模块项目的开发，一个模块开发好后，发布到仓库，依赖该模块时可以直接从仓库更新，而不用自己去编译。maven有很多插件，便于功能扩展，比如生产站点，自动发布版本等 Spring有哪些优点1.轻量级：Spring在大小和透明性方面绝对属于轻量级的，基础版本的Spring框架大约只有2MB。2.控制反转(IOC)：Spring使用控制反转技术实现了松耦合。依赖被注入到对象，而不是创建或寻找依赖对象。3.面向切面编程(AOP)： Spring支持面向切面编程，同时把应用的业务逻辑与系统的服务分离开来。4.容器：Spring包含并管理应用程序对象的配置及生命周期。5.MVC框架：Spring的web框架是一个设计优良的web MVC框架，很好的取代了一些web框架。6.事务管理：Spring对下至本地业务上至全局业务(JAT)提供了统一的事务管理接口。7.异常处理：Spring提供一个方便的API将特定技术的异常(由JDBC, Hibernate, 或JDO抛出)转化为一致的、Unchecked异常。 spring 主要使用了哪些 ，IOC和AOP实现原理是什么spring主要功能有IOC，AOP，MVC等，IOC实现原理：先反射生成实例，然后调用时主动注入。AOP原理：主要使用java动态代理。 什么是IOC容器其优点Spring IOC负责创建对象、管理对象(通过依赖注入)、整合对象、配置对象以及管理这些对象的生命周期。优点:IOC或依赖注入减少了应用程序的代码量。它使得应用程序的测试很简单，因为在单元测试中不再需要单例或JNDI查找机制。简单的实现以及较少的干扰机制使得松耦合得以实现。IOC容器支持勤性单例及延迟加载服务。 什么是 AOPSpring 面向切面编程（AOP）,利用AOP可以对业务逻辑的各个部分隔离，从而使的业务逻辑各部分的耦合性降低，提高程序的可重用性，踢开开发效率，主要功能：日志记录，性能统计，安全控制，事务处理，异常处理等。AOP实现原理是java动态代理，但是jdk的动态代理必须实现接口，所以 Spring 的 AOP 是用 cglib 这个库实现的，cglis 使用里 asm 这个直接操纵字节码的框架，所以可以做到不使用接口的情况下实现动态代理。 AOP与OOP的区别OOP 面向对象编程，针对业务处理过程的实体及其属性和行为进行抽象封装，以获得更加清晰高效的逻辑单元划分。而AOP则是针对业务处理过程中的切面进行提取，它所面对的是处理过程的某个步骤或阶段，以获得逻辑过程的中各部分之间低耦合的隔离效果。这两种设计思想在目标上有着本质的差异。 举例： 对于“雇员”这样一个业务实体进行封装，自然是OOP的任务，我们可以建立一个“Employee”类，并将“雇员”相关的属性和行为封装其中。而用AOP 设计思想对“雇员”进行封装则无从谈起。 同样，对于“权限检查”这一动作片段进行划分，则是AOP的目标领域。 OOP面向名次领域，AOP面向动词领域。 总之AOP可以通过预编译方式和运行期动态代理实现在不修改源码的情况下，给程序动态同意添加功能的一项技术。 Spring 的依赖注入方式有哪一些Spring 的依赖注入可以有两种方式来完成:setter 方法注入和构造方法注入。构造器依赖注入：构造器依赖注入在容器触发构造器的时候完成，该构造器有一系列的参数，每个参数代表注入的对象。Setter方法依赖注入：首先容器会触发一个无参构造函数或无参静态工厂方法实例化对象，之后容器调用bean中的setter方法完成Setter方法依赖注入。 SpringMVC运行原理 客户端请求提交到DispatcherServlet 由DispatcherServlet控制器查询HandlerMapping，找到并分发到指定的Controller中。 Controller调用业务逻辑处理后，返回ModelAndView DispatcherServlet查询一个或多个ViewResoler视图解析器，找到ModelAndView指定的视图 视图负责将结果显示到客户端 关于职业规划关于我的职业规划，我把它分成三个阶段： 第一个阶段是三年，在这三年里，我要学会快速融入到公司的团队中，知道如何团结协作、如何使用项目管理工具、项目版本如何控制、自己写的代码如何测试如何在线上运行、如何去优化自己的代码。能够快速的理解项目相关的业务，对于复杂的业务能够考虑到可能会出现的多种情况以及解决方案。积累一定的开发经验。除了完成工作以外，在空余的时间里，研究各种技术实现细节， 对JDK 的源码和用到的框架进行深入的研究并且多做总结，写一些博客，在 Github 上分享技术。多看书，系统的学习。 第二个阶段是五年，也就是又过了两年，我在要具备在技术上独挡一面的能力并且清楚自己未来的方向，从一个写代码逐步走向系统分析师或是架构师，成为项目组中不可或缺的人物。 第三个阶段是十年，成为一名对行业有着深入认识、对技术有着深入认识、能从零开始对一个产品进行分析。 离职原因个人发展原因。因为目前的工作太安逸，没有挑战，公司整体上都很稳定，也没有什么发展空间，我希望有一份比较有挑战的工作，毕竟我现在还年轻，是拼搏的年华，也需要成长。]]></content>
      <categories>
        <category>基础面试题</category>
      </categories>
      <tags>
        <tag>基础面试题</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[面试感悟：工作经验java程序员应有的技能]]></title>
    <url>%2F2018%2F06%2F03%2F%E9%9D%A2%E8%AF%95%E6%84%9F%E6%82%9F%EF%BC%9A%E5%B7%A5%E4%BD%9C%E7%BB%8F%E9%AA%8Cjava%E7%A8%8B%E5%BA%8F%E5%91%98%E5%BA%94%E6%9C%89%E7%9A%84%E6%8A%80%E8%83%BD%2F</url>
    <content type="text"><![CDATA[面试感悟：工作经验java程序员应有的技能前言因为和同事有约定再加上LZ自己也喜欢做完一件事之后进行总结，因此有了这篇文章。这篇文章大部分内容都是面向整个程序员群体的，当然因为LZ本身是做Java开发的，因此有一部分内容也是专门面向咱们Java程序员的。 简单先说一下，LZ坐标杭州，13届本科毕业，算上年前在阿里巴巴B2B事业部的面试，一共有面试了有6家公司（因为LZ不想请假，因此只是每个晚上去其他公司面试，所以面试的公司比较少），其中成功的有4家，另外两家失败的原因在于： 1、阿里巴巴B2B事业部的面试，两轮技术面试都过了，最后一轮面试是对方的主管，由于听说技术面试过了基本上90%都面试成功了，所以LZ在和主管的交谈中也是毫无顾忌，说得天花乱坠，很多自己介于知道和不知道的东西都直接脱口而出了，结果多次被对方一反问就问得哑口无言。事后想来，模棱两可的答案是面试中最忌讳的，这次的失败也让LZ认真地对待后面的每一次面试 2、另外一家失败的是一家小公司，也就20来个人吧，整个团队是支付宝出来创业的，非常厉害。面试完LZ多方了解了一下，对方认为我基本功什么的都不错，但是实际项目经验还是欠缺一些，因为对方是创业型公司，需要人上手就能干活，因此我在这个时候还不是特别适合他们团队 至于其他成功的四家公司，给LZ的面试评价都挺高的貌似，但LZ也不想记流水账，因此就不一一列举每家公司的面试过程了，下面LZ主要谈谈作为一名工作三年左右的Java程序员应该具备的一些技能以及个人的一些其他感悟。 关于程序员的几个阶段每个程序员、或者说每个工作者都应该有自己的职业规划，如果看到这里的朋友没有自己的职业规划，希望你可以思考一下自己的将来。 LZ常常思考自己的未来，也从自己的思考中总结出了一些东西，作为第一部分来谈谈。LZ认为一名程序员应该有几个阶段（以下时间都算上实习期）： 第一阶段：三年 我认为三年对于程序员来说是第一个门槛，这个阶段将会淘汰掉一批不适合写代码的人。这一阶段，我们走出校园，迈入社会，成为一名程序员，正式从书本上的内容迈向真正的企业级开发。我们知道如何团队协作、如何使用项目管理工具、项目版本如何控制、我们写的代码如何测试如何在线上运行等等，积累了一定的开发经验，也对代码有了一定深入的认识，是一个比较纯粹的Coder的阶段 第二阶段：五年 五年又是区分程序员的第二个门槛。有些人在三年里，除了完成工作，在空余时间基本不会研究别的东西，这些人永远就是个Coder，年纪大一些势必被更年轻的人给顶替；有些人在三年里，除了写代码之外，还热衷于研究各种技术实现细节、看了N多好书、写一些博客、在Github上分享技术，这些人在五年后必然具备在技术上独当一面的能力并且清楚自己未来的发展方向，从一个Coder逐步走向系统分析师或是架构师，成为项目组中不可或缺的人物 第三阶段：十年 十年又是另一个门槛了，转行或是继续做一名程序员就在这个节点上。如果在前几年就抱定不转行的思路并且为之努力的话，那么在十年的这个节点上，有些人必然成长为一名对行业有着深入认识、对技术有着深入认识、能从零开始对一个产品进行分析的程序员，这样的人在公司基本担任的都是CTO、技术专家、首席架构师等最关键的职位，这对于自己绝对是一件荣耀的事，当然老板在经济上也绝不会亏待你 第一部分总结一下，我认为，随着你工作年限的增长、对生活对生命认识的深入，应当不断思考三个问题： 1、我到底适不适合当一名程序员？ 2、我到底应不应该一辈子以程序员为职业？ 3、我对编程到底持有的是一种什么样的态度，是够用就好呢还是不断研究？ 最终，明确自己的职业规划，对自己的规划负责并为之努力。 关于项目经验LZ在网上经常看到一些别的朋友有提出项目经验的问题，依照LZ面试的感觉来说，面试主要看几点：项目经验+基本技术+个人潜力（也就是值不值得培养）。 关于项目经验，我认为并发编程网的创始人方腾飞老师讲的一段话非常好： 介绍产品时面试官会考察应聘者的沟通能力和思考能力，我们大部分情况都是做产品的一个功能或一个模块，但是即使是这样，自己有没有把整个系统架构或产品搞清楚，并能介绍清楚，为什么做这个系统？这个系统的价值是什么？这个系统有哪些功能？优缺点有哪些？如果让你重新设计这个系统你会如何设计？ 我觉得这就已经足以概括了。也许你仅仅工作一年，也许你做的是项目中微不足道的模块，当然这些一定是你的劣势且无法改变，但是如何弥补这个劣势，从方老师的话中我总结几点： 1、明确你的项目到底是做什么的，有哪些功能 2、明确你的项目的整体架构，在面试的时候能够清楚地画给面试官看并且清楚地指出从哪里调用到哪里、使用什么方式调用 3、明确你的模块在整个项目中所处的位置及作用 4、明确你的模块用到了哪些技术，更好一些的可以再了解一下整个项目用到了哪些技术 在你无法改变自己的工作年限、自己的不那么有说服力的项目经验的情况下（这一定是扣分项），可以通过这种方式来一定程度上地弥补并且增进面试官对你的好感度。 关于专业技能写完项目接着写写一名3年工作经验的Java程序员应该具备的技能，这可能是Java程序员们比较关心的内容。我这里要说明一下，以下列举的内容不是都要会的东西—-但是如果你掌握得越多，最终能得到的评价、拿到的薪水势必也越高。 1、基本语法 这包括static、final、transient等关键字的作用，foreach循环的原理等等。今天面试我问你static关键字有哪些作用，如果你答出static修饰变量、修饰方法我会认为你合格，答出静态块，我会认为你不错，答出静态内部类我会认为你很好，答出静态导包我会对你很满意，因为能看出你非常热衷研究技术。 最深入的一次，LZ记得面试官直接问到了我Volatile关键字的底层实现原理（顺便插一句，面试和被面试本身就是相对的，面试官能问这个问题同时也让面试者感觉到面试官也是一个喜爱研究技术的人，增加了面试者对公司的好感，LZ最终选择的就是问了这个问题的公司），不要觉得这太吹毛求疵了—-越简单的问题越能看出一个人的水平，别人对你技术的考量绝大多数都是以深度优先、广度次之为标准的，切记。 2、集合 非常重要，也是必问的内容。基本上就是List、Map、Set，问的是各种实现类的底层实现原理，实现类的优缺点。 集合要掌握的是ArrayList、LinkedList、Hashtable、HashMap、ConcurrentHashMap、HashSet的实现原理，能流利作答，当然能掌握CopyOnWrite容器和Queue是再好不过的了。另外多说一句，ConcurrentHashMap的问题在面试中问得特别多，大概是因为这个类可以衍生出非常多的问题，关于ConcurrentHashMap，我给网友朋友们提供三点回答或者是研究方向： （1）ConcurrentHashMap的锁分段技术 （2）ConcurrentHashMap的读是否要加锁，为什么 （3）ConcurrentHashMap的迭代器是强一致性的迭代器还是弱一致性的迭代器 3、设计模式 本来以为蛮重要的一块内容，结果只在阿里巴巴B2B事业部面试的时候被问了一次，当时问的是装饰器模式。 当然咱们不能这么功利，为了面试而学习，设计模式在工作中还是非常重要、非常有用的，23种设计模式中重点研究常用的十来种就可以了，面试中关于设计模式的问答主要是三个方向： （1）你的项目中用到了哪些设计模式，如何使用 （2）知道常用设计模式的优缺点 （3）能画出常用设计模式的UML图 4、多线程 这也是必问的一块了。因为三年工作经验，所以基本上不会再问你怎么实现多线程了，会问得深入一些比如说Thread和Runnable的区别和联系、多次start一个线程会怎么样、线程有哪些状态。当然这只是最基本的，出乎意料地，几次面试几乎都被同时问到了一个问题，问法不尽相同，总结起来是这么一个意思： 假如有Thread1、Thread2、ThreaD3、Thread4四条线程分别统计C、D、E、F四个盘的大小，所有线程都统计完毕交给Thread5线程去做汇总，应当如何实现？ 聪明的网友们对这个问题是否有答案呢？不难，java.util.concurrent下就有现成的类可以使用。 另外，线程池也是比较常问的一块，常用的线程池有几种？这几种线程池之间有什么区别和联系？线程池的实现原理是怎么样的？实际一些的，会给你一些具体的场景，让你回答这种场景该使用什么样的线程池比较合适。 最后，虽然这次面试问得不多，但是多线程同步、锁这块也是重点。synchronized和ReentrantLock的区别、synchronized锁普通方法和锁静态方法、死锁的原理及排查方法等等，关于多线程，我在之前有些过文章总结过多线程的40个问题，可以参看40个Java多线程问题总结。 5、JDK源码 要想拿高工资，JDK源码不可不读。上面的内容可能还和具体场景联系起来，JDK源码就是实打实地看你平时是不是爱钻研了。LZ面试过程中被问了不少JDK源码的问题，其中最刁钻的一个问了LZ，String的hashCode()方法是怎么实现的，幸好LZ平时String源代码看得多，答了个大概。JDK源码其实没什么好总结的，纯粹看个人，总结一下比较重要的源码： （1）List、Map、Set实现类的源代码 （2）ReentrantLock、AQS的源代码 （3）AtomicInteger的实现原理，主要能说清楚CAS机制并且AtomicInteger是如何利用CAS机制实现的 （4）线程池的实现原理 （5）Object类中的方法以及每个方法的作用 这些其实要求蛮高的，LZ去年一整年基本把JDK中重要类的源代码研究了个遍，真的花费时间、花费精力，当然回头看，是值得的—-不仅仅是为了应付面试。 6、框架 老生常谈，面试必问的东西。一般来说会问你一下你们项目中使用的框架，然后给你一些场景问你用框架怎么做，比如我想要在Spring初始化bean的时候做一些事情该怎么做、想要在bean销毁的时候做一些事情该怎么做、MyBatis中$和#的区别等等，这些都比较实际了，平时积累得好、有多学习框架的使用细节自然都不成问题。 如果上面你的问题答得好，面试官往往会深入地问一些框架的实现原理。问得最多的就是Spring AOP的实现原理，当然这个很简单啦，两句话就搞定的的事儿，即使你不会准备一下就好了。LZ遇到的最变态的是让LZ画一下Spring的Bean工厂实现的UML图，当然面对这样一个有深度的问题，LZ是绝对答不出来的/(ㄒoㄒ)/~~ 7、数据库 数据库十有八九也都会问到。一些基本的像union和union all的区别、left join、几种索引及其区别就不谈了，比较重要的就是数据库性能的优化，如果对于数据库的性能优化一窍不通，那么有时间，还是建议你在面试前花一两天专门把SQL基础和SQL优化的内容准备一下。 不过数据库倒是不用担心，一家公司往往有很多部门，如果你对数据库不熟悉而基本技术又非常好，九成都是会要你的，估计会先把你放到对数据库使用不是要求非常高的部门锻炼一下。 8、数据结构和算法分析 数据结构和算法分析，对于一名程序员来说，会比不会好而且在工作中绝对能派上用场。数组、链表是基础，栈和队列深入一些但也不难，树挺重要的，比较重要的树AVL树、红黑树，可以不了解它们的具体实现，但是要知道什么是二叉查找树、什么是平衡树，AVL树和红黑树的区别。记得某次面试，某个面试官和我聊到了数据库的索引，他问我： 你知道索引使用的是哪种数据结构实现吗？ LZ答到用的Hash表吧，答错。他又问，你知道为什么要使用树吗？LZ答到因为Hash表可能会出现比较多的冲突，在千万甚至是上亿级别的数据面前，会大大增加查找的时间复杂度。而树比较稳定，基本保证最多二三十次就能找到想要的数据，对方说不完全对，最后我们还是交流了一下这个问题，我也明白了为什么要使用树，这里不说，网友朋友们觉得索引为什么要使用树来实现呢？ 至于算法分析，不会、不想研究就算了，记得某次面试对方问我，Collections.sort方法使用的是哪种排序方法，额，吐血三升。当然为了显示LZ的博学，对算法分析也有一定的研究(⊙﹏⊙)b，LZ还是硬着头皮说了一句可能是冒泡排序吧。当然答案肯定不是，有兴趣的网友朋友们可以去看一下Collections.sort方法的源代码，用的是一种叫做TimSort的排序法，也就是增强型的归并排序法。 9、Java虚拟机 出乎LZ的意料，Java虚拟机应该是很重要的一块内容，结果在这几家公司中被问到的概率几乎为0。要知道，LZ去年可是花了大量的时间去研究Java虚拟机的，光周志明老师的《深入理解Java虚拟机：JVM高级特性与最佳实践》，LZ就读了不下五遍。 言归正传，虽然Java虚拟机没问到，但我觉得还是有必要研究的，LZ就简单地列一个提纲吧，谈谈Java虚拟机中比较重要的内容： （1）Java虚拟机的内存布局 （2）GC算法及几种垃圾收集器 （3）类加载机制，也就是双亲委派模型 （4）Java内存模型 （5）happens-before规则 （6）volatile关键字使用规则 也许面试无用，但在走向大牛的路上，不可不会。 10、Web方面的一些问题 Java主要面向Web端，因此Web的一些问题也是必问的。LZ碰到过问得最多的两个问题是： 谈谈分布式Session的几种实现方式 常用的四种能答出来自然是让面试官非常满意的，另外一个常问的问题是： 讲一下Session和Cookie的区别和联系以及Session的实现原理 这两个问题之外，web.xml里面的内容是重点，Filter、Servlet、Listener，不说对它们的实现原理一清二楚吧，至少能对它们的使用知根知底。另外，一些细节的方面比如get/post的区别、forward/重定向的区别、HTTPS的实现原理也都可能会被考察到。 噢，想起来了，一致性Hash算法貌似也被问到了几次，这个LZ以前专门深入研究过并且写了两篇博文，因此问到这个问题LZ自然是答得毫不费力。文章是MemCache超详细解读和对一致性Hash算法，Java代码实现的深入研究，特别说明，LZ真的不是在为自已以前写的文章打广告啊啊啊啊啊啊。 最后，如果有兴趣有时间，建议学习、研究一下SOA和RPC，面向服务体系，大型分布式架构必备，救命良方、包治百病、屡试不爽。 关于HR面试如果你过五关斩六将，成功地通过了所有的技术面，那么恭喜你，你离升职加薪、出任CEO、迎娶白富美、走向人生巅峰又进了一步。但是还没有到谈薪资待遇的时候，最后还有一个考验：HR面试。基本所有的大公司都有这一轮的面试，不要小看HR面试，很多公司的HR对于面试者都有一票否决权的—-即使前面的面试对你的评价再高。 所以，这轮的面试也必须重视起来，HR面试主要问的是几点： 1、简历中写的过去工作经历的离职原因 2、当前公司薪资待遇 3、期望能到怎样的一家公司 4、个人未来的发展方向 我专门提一下第2点。可能有人比较排斥也不想说这个，我个人倒是持开放状态，问了就说了，当然一些的夸大还是必要的，当前公司薪资待遇多报个一千块钱完全没问题（毕竟是一家互联网公司总多多少少有些补贴啊什么的嘛）。因为这和你在新公司能拿到的薪水关系不大，新公司能拿到的薪水的决定因素是整个公司的薪资情况以及根据你的面试情况在公司的定位，都是有固定的薪资范围的。HR问这个主要也就是心里有个数并且看你是否诚信—-有些公司入职时会要求你提供最近一家单位的银行流水号。 HR面试就说到这里了，总结起来其实就是四个字：滴水不漏。整个面试过程态度积极向上，不要有任何悲观消极的态度（尤其在谈到以前公司情况的时候，即使有再多的不满），就不会有问题。 关于面试心态这个嘛，LZ其实在公司也面试过几个人，一半以上的面试者回答问题的时候都属于那种双腿发抖、声音颤抖的类型。在LZ看来这大可不必并且这还是扣分项，回答问题的时候最最基本的两个要求： 1、不紧不慢，平心静气 2、条理清晰 表达能力绝对是面试的时候重要的考察项目。咱们做的是程序员这一行，讲究的是团队协作，不是写作、画画，一支笔、一个人就行了，一个表达能力不行的程序员，要来又有什么用呢？ 除此之外，就是保持良好的心态。古语说得好，只要功夫深，铁杵磨成针，面试的成功与否，在于平时的积累，临时抱抱佛脚，看两道面试题是没有用的，只要平时足够努力，成功是水到渠成的事情，平时不怎么研究技术的，那也就是个听天由命的事情，只要充分地展示平时自己的所学就可以了。 因此在我看来，不要把面试当作面试，当做一次技术交流，把面试的心态从我要找到一份工作转变为我要通过面试去发现不足、提升自己，这样就会平和多了，即使失败也不会有太多失望的感觉。 另外，如果平时自己热衷于研究技术的朋友，真的要有自信，不要觉得别人面试你别人就比你厉害。面试官未必比你优秀，他问的问题往往都是他平时研究得比较多的问题，你一样有很多自己的研究面试官未必知道。 关于Java网上常看到一种说法：Java比较简单。某种程度上这会打击Java程序员的信心—-原来咱们平时用的是这种小儿科的玩意儿啊，在我看来这种想法大可不必，这一部分我来讲讲对于这个话题的看法。 这种说法有些片面，得分开两部分来看，我用四个自总结一下就是：易学难精。 1、易学部分 Java易学我认为有两部分的原因： （1）很多培训公司包括大四的学生找工作都会学习Java，绝大多数是因为易学。Java从C/C++发展而来，感谢前人的智慧，它消除了C/C++中最复杂和让人困惑的语法、它消除了平台的差异性、它不需要用户手动释放内存空间、它避免了Java程序员和本地语言的交互，让程序员只需要专注于语法层面和应用层面。 （2）Java作为一门面向对象的语言，在企业级开发中体现出了它无与伦比的特性，整个开发流程比较固定化、模块化，需求分析起来也相对容易。我举个自己以前的例子吧，我在大一学习C语言的时候，用C语言写了一个图书管理系统写了2000+的代码，大四学了C++之后，用面向对象的语言C++取代面向过程的语言C语言重新写了一个功能相似的图书管理系统，只写了1100行的样子，这就是面向对象的优势。 2、难精部分 接着咱们聊聊难精的部分。 Java语言的设计者帮助Java程序员做了这么多事情，这有利也有弊。有利的部分前面已经说过了，让Java易学，不过有弊的部分同样明显。假如在应用运行过程中遇到了语法层面和应用层面之外的错误，应当如何处理？比如线上环境出现内存溢出怎么办？GC时间过长怎么办？IO长时间没反应怎么办？方法抛出莫名其妙的异常怎么办？ 凡此种种，绝不是一名只会写几个if…else…的Java程序员就可以解决的，这需要大量的经历、大量的实践、大量对Java底层实现细节的研究，而这往往是最难、最考验Java程序员的部分，一些人根本就不想往深去研究，另外一些人研究了一点点就研究不下去了。 Java为什么难精？就是这个原因。除非你水平特别高，否则五年工作经验以下的Java程序员在简历上写”精通Java”绝对是一件非常愚蠢的事情。 结语文章写到这里，感觉有点像鸡汤文了，那就以最后的鸡汤作为结尾吧。 在以前博客园的一篇文章中，讲到了奔三程序员的困惑，大致说的是三十岁之后程序员要转行之类的云云，LZ在博文中留下了如下的评论： 就以这段话自勉、共勉吧。越努力、越幸运，如果你不是官二代、富二代、红二代，那么请记住：勤奋才是改变你命运的唯一捷径。]]></content>
      <categories>
        <category>基础面试题</category>
      </categories>
      <tags>
        <tag>基础面试题</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[节点操作]]></title>
    <url>%2F2018%2F06%2F03%2F%E8%8A%82%E7%82%B9%E6%93%8D%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[节点操作节点关系children()方法 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head lang="en"&gt; &lt;meta charset="UTF-8"&gt; &lt;title&gt;&lt;/title&gt; &lt;style&gt; .div1,.div2,.div3&#123; width: 500px; height: 150px; border: 1px solid black; margin: 20px; &#125; div p&#123; width: 100px; height: 100px; background-color: skyblue; float: left; margin: 5px; &#125; div .teshu&#123; background-color: orange; &#125; &lt;/style&gt;&lt;/head&gt;&lt;body&gt;&lt;div class="div1"&gt; &lt;p class="teshu"&gt;teshu&lt;/p&gt; &lt;p&gt;&lt;/p&gt; &lt;p&gt;&lt;span&gt;span&lt;/span&gt;&lt;/p&gt;&lt;/div&gt;&lt;div class="div2"&gt; &lt;p&gt;&lt;/p&gt; &lt;p class="teshu"&gt;teshu&lt;/p&gt; &lt;p class="teshu"&gt;teshu&lt;/p&gt;&lt;/div&gt;&lt;div class="div3"&gt; &lt;p&gt;&lt;/p&gt; &lt;p class="teshu"&gt;teshu&lt;/p&gt; &lt;p&gt;&lt;/p&gt;&lt;/div&gt;&lt;script src="js/jquery-1.12.3.min.js"&gt;&lt;/script&gt;&lt;script&gt; $('div').click(function () &#123; $(this).children().css('background-color','red'); // div 的直接子元素为 teshu 变红 //$(this).children('.teshu').css('background-color','red'); &#125;);&lt;/script&gt;&lt;/body&gt;&lt;/html&gt; find()方法由于children()只能查找子元素，如果是孙子元素是找不到的。所以jQuery提供了find()”寻找”的方法。 作用：在某个节点中查找符合选择器要求的后代节点 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head lang="en"&gt; &lt;meta charset="UTF-8"&gt; &lt;title&gt;&lt;/title&gt; &lt;style&gt; .div1,.div2,.div3&#123; width: 500px; height: 150px; border: 1px solid black; margin: 20px; &#125; div p&#123; width: 100px; height: 100px; background-color: skyblue; float: left; margin: 5px; &#125; div .teshu&#123; background-color: orange; &#125; &lt;/style&gt;&lt;/head&gt;&lt;body&gt;&lt;div class="div1"&gt; &lt;p class="teshu"&gt;teshu&lt;/p&gt; &lt;p&gt;&lt;/p&gt; &lt;p&gt;&lt;span&gt;span&lt;/span&gt;&lt;/p&gt;&lt;/div&gt;&lt;div class="div2"&gt; &lt;p&gt;&lt;/p&gt; &lt;p class="teshu"&gt;teshu&lt;/p&gt; &lt;p class="teshu"&gt;teshu&lt;/p&gt;&lt;/div&gt;&lt;div class="div3"&gt; &lt;p&gt;&lt;/p&gt; &lt;p class="teshu"&gt;teshu&lt;/p&gt; &lt;p&gt;&lt;/p&gt;&lt;/div&gt;&lt;script src="js/jquery-1.12.3.min.js"&gt;&lt;/script&gt;&lt;script&gt; $('div').click(function () &#123; $(this).find('p').css('background-color','red'); //$(this).find('.teshu').css('background-color','red'); // $(this).find('span').css('background-color','red'); &#125;);&lt;/script&gt;&lt;/body&gt;&lt;/html&gt; parent()方法作用：表示查找当前节点的直属父节点 12345678910111213141516171819202122232425262728293031323334353637383940414243444546&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head lang="en"&gt; &lt;meta charset="UTF-8"&gt; &lt;title&gt;&lt;/title&gt; &lt;style&gt; .div1,.div2,.div3&#123; width: 500px; height: 150px; border: 1px solid black; margin: 20px; &#125; div p&#123; width: 100px; height: 100px; background-color: skyblue; float: left; margin: 5px; &#125; div .teshu&#123; background-color: orange; &#125; &lt;/style&gt;&lt;/head&gt;&lt;body&gt;&lt;div class="div1"&gt; &lt;p class="teshu"&gt;teshu&lt;/p&gt; &lt;p&gt;&lt;/p&gt; &lt;p&gt;&lt;span&gt;span&lt;/span&gt;&lt;/p&gt;&lt;/div&gt;&lt;div class="div2"&gt; &lt;p&gt;&lt;/p&gt; &lt;p class="teshu"&gt;teshu&lt;/p&gt; &lt;p class="teshu"&gt;teshu&lt;/p&gt;&lt;/div&gt;&lt;div class="div3"&gt; &lt;p&gt;&lt;/p&gt; &lt;p class="teshu"&gt;teshu&lt;/p&gt; &lt;p&gt;&lt;/p&gt;&lt;/div&gt;&lt;script src="js/jquery-1.12.3.min.js"&gt;&lt;/script&gt;&lt;script&gt; $('p').click(function () &#123; //$(this).parent('.div3').css('border','10px solid orange'); $(this).parent().css('border','10px solid orange'); &#125;);&lt;/script&gt;&lt;/body&gt;&lt;/html&gt; parents()方法作用：表示查找当前节点的所有祖先节点，直到html节点为止。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head lang="en"&gt; &lt;meta charset="UTF-8"&gt; &lt;title&gt;&lt;/title&gt; &lt;style&gt; .div1,.div2,.div3&#123; width: 500px; height: 150px; border: 1px solid black; margin: 20px; &#125; div p&#123; width: 100px; height: 100px; background-color: skyblue; float: left; margin: 5px; &#125; div .teshu&#123; background-color: orange; &#125; &lt;/style&gt;&lt;/head&gt;&lt;body&gt;&lt;div class="div1"&gt; &lt;p class="teshu"&gt;teshu&lt;/p&gt; &lt;p&gt;&lt;/p&gt; &lt;p&gt;&lt;span&gt;span&lt;/span&gt;&lt;/p&gt;&lt;/div&gt;&lt;div class="div2"&gt; &lt;p&gt;&lt;/p&gt; &lt;p class="teshu"&gt;teshu&lt;/p&gt; &lt;p class="teshu"&gt;teshu&lt;/p&gt;&lt;/div&gt;&lt;div class="div3"&gt; &lt;p&gt;&lt;/p&gt; &lt;p class="teshu"&gt;teshu&lt;/p&gt; &lt;p&gt;&lt;/p&gt;&lt;/div&gt;&lt;script src="js/jquery-1.12.3.min.js"&gt;&lt;/script&gt;&lt;script&gt; $('p').click(function () &#123; // $(this).parents('body').css('border','10px solid orange'); $(this).parents().css('border','10px solid orange'); &#125;);&lt;/script&gt;&lt;/body&gt;&lt;/html&gt; siblings()方法作用：访问当前节点的所有兄弟节点（除本身之外） 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head lang="en"&gt; &lt;meta charset="UTF-8"&gt; &lt;title&gt;&lt;/title&gt; &lt;style&gt; .div1,.div2,.div3&#123; width: 500px; height: 150px; border: 1px solid black; margin: 20px; &#125; div p&#123; width: 100px; height: 100px; background-color: skyblue; float: left; margin: 5px; &#125; div .teshu&#123; background-color: orange; &#125; &lt;/style&gt;&lt;/head&gt;&lt;body&gt;&lt;div class="div1"&gt; &lt;p class="teshu"&gt;teshu&lt;/p&gt; &lt;p&gt;&lt;/p&gt; &lt;p&gt;&lt;span&gt;span&lt;/span&gt;&lt;/p&gt;&lt;/div&gt;&lt;div class="div2"&gt; &lt;p&gt;&lt;/p&gt; &lt;p class="teshu"&gt;teshu&lt;/p&gt; &lt;p class="teshu"&gt;teshu&lt;/p&gt;&lt;/div&gt;&lt;div class="div3"&gt; &lt;p&gt;&lt;/p&gt; &lt;p class="teshu"&gt;teshu&lt;/p&gt; &lt;p&gt;&lt;/p&gt;&lt;/div&gt;&lt;script src="js/jquery-1.12.3.min.js"&gt;&lt;/script&gt;&lt;script&gt; // $('p').click(function () &#123; // $(this).css('background-color','yellow').siblings().css('background-color','pink'); // &#125;); // 方法1： // $('p').mouseenter(function () &#123; // $(this).css('background-color','yellow').siblings().css('background-color','pink'); // &#125;).mouseleave(function () &#123; // $(this).css('background-color','pink'); // &#125;); $('p').hover(function () &#123; $(this).css('background-color','yellow').siblings().css('background-color','pink'); &#125;,function () &#123; $(this).css('background-color','pink'); &#125;);&lt;/script&gt;&lt;/body&gt;&lt;/html&gt; next()、prev()、nextAll()和prevAll()方法next() 后一个亲兄弟 prev() 前一个亲兄弟 nextAll() 后所有亲兄弟 prevAll() 前所有亲兄弟 手风琴效果例子12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head lang="en"&gt; &lt;meta charset="UTF-8"&gt; &lt;title&gt;lesson4&lt;/title&gt; &lt;style&gt; *&#123; margin: 0; padding: 0; &#125; .sfq&#123; width: 500px; border: 1px solid black; margin: 80px auto; &#125; .sfq ul&#123; list-style: none; &#125; .sfq ul li&#123; border-bottom: 1px dotted #333; &#125; .sfq ul li div&#123; display: none; border: 1px dotted lightgray; &#125; &lt;/style&gt;&lt;/head&gt;&lt;body&gt;&lt;div class="sfq"&gt; &lt;ul&gt; &lt;li&gt; &lt;h2&gt;这是标题&lt;/h2&gt; &lt;div class="info"&gt; 这是内容这是内容这是内容这是内容这是内容这是内容这是内容这是内容这是内容这是内容这是内容这是内容这是内容这是内容这是内容这是内容这是内容 &lt;/div&gt; &lt;/li&gt; &lt;li&gt; &lt;h2&gt;这是标题&lt;/h2&gt; &lt;div class="info"&gt; 这是内容这是内容这是内容这是内容这是内容这是内容这是内容这是内容这是内容这是内容这是内容这是内容这是内容这是内容这是内容这是内容这是内容 &lt;/div&gt; &lt;/li&gt; &lt;li&gt; &lt;h2&gt;这是标题&lt;/h2&gt; &lt;div class="info"&gt; 这是内容这是内容这是内容这是内容这是内容这是内容这是内容这是内容这是内容这是内容这是内容这是内容这是内容这是内容这是内容这是内容这是内容 &lt;/div&gt; &lt;/li&gt; &lt;/ul&gt;&lt;/div&gt;&lt;script src="js/jquery-1.12.3.min.js"&gt;&lt;/script&gt;&lt;script&gt; $('h2').click(function () &#123; // :visible 表示可见的 if($(this).siblings().is(':visible'))&#123; $(this).siblings().slideUp(); &#125;else&#123; //在当前这个点击的h2的兄弟展开之前，先将所有的收起 $('.info').slideUp(); $(this).siblings().slideDown(); &#125; &#125;);&lt;/script&gt;&lt;/body&gt;&lt;/html&gt; 节点关系综合查找在jQuery中可以通过连续打点来调用节点的关系方法。但是这个操作必须存在一个前提，那就是：一定要知道当前正在操作的元素是谁。 节点顺序和遍历index()方法index()方法用来获取当前元素在其兄弟节点中的排名，从0开始。 123456789101112131415161718192021222324252627282930313233343536373839404142434445&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head lang="en"&gt; &lt;meta charset="UTF-8"&gt; &lt;title&gt;lesson5&lt;/title&gt; &lt;style&gt; .div1,.div2,.div3&#123; width: 500px; height: 150px; border: 1px solid black; margin: 20px; &#125; div p&#123; width: 100px; height: 100px; background-color: skyblue; float: left; margin: 5px; &#125; div .teshu&#123; background-color: orange; &#125; &lt;/style&gt;&lt;/head&gt;&lt;body&gt;&lt;div class="div1"&gt; &lt;p class="teshu"&gt;teshu&lt;/p&gt; &lt;p&gt;&lt;/p&gt; &lt;p&gt;&lt;span&gt;span&lt;/span&gt;&lt;/p&gt;&lt;/div&gt;&lt;div class="div2"&gt; &lt;p&gt;&lt;/p&gt; &lt;p class="teshu"&gt;teshu&lt;/p&gt; &lt;p class="teshu"&gt;teshu&lt;/p&gt;&lt;/div&gt;&lt;div class="div3"&gt; &lt;p&gt;&lt;/p&gt; &lt;p class="teshu"&gt;teshu&lt;/p&gt; &lt;p&gt;&lt;/p&gt;&lt;/div&gt;&lt;script src="js/jquery-1.12.3.min.js"&gt;&lt;/script&gt;&lt;script&gt; $('.teshu').click(function () &#123; alert($(this).index()); &#125;);&lt;/script&gt;&lt;/body&gt;&lt;/html&gt; 对应jquery中的对应实际上指的是eq()函数，他的作用是获取集合中指定序号的节点 需要说明的一点就是序号是从0开始。 语法：$(‘selector’).eq(index) 123456789101112131415161718192021222324252627282930313233343536373839404142434445&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head lang="en"&gt; &lt;meta charset="UTF-8"&gt; &lt;title&gt;lesson5&lt;/title&gt; &lt;style&gt; .div1,.div2,.div3&#123; width: 500px; height: 150px; border: 1px solid black; margin: 20px; &#125; div p&#123; width: 100px; height: 100px; background-color: skyblue; float: left; margin: 5px; &#125; div .teshu&#123; background-color: orange; &#125; &lt;/style&gt;&lt;/head&gt;&lt;body&gt;&lt;div class="div1"&gt; &lt;p class="teshu"&gt;teshu&lt;/p&gt; &lt;p&gt;&lt;/p&gt; &lt;p&gt;&lt;span&gt;span&lt;/span&gt;&lt;/p&gt;&lt;/div&gt;&lt;div class="div2"&gt; &lt;p&gt;&lt;/p&gt; &lt;p class="teshu"&gt;teshu&lt;/p&gt; &lt;p class="teshu"&gt;teshu&lt;/p&gt;&lt;/div&gt;&lt;div class="div3"&gt; &lt;p&gt;&lt;/p&gt; &lt;p class="teshu"&gt;teshu&lt;/p&gt; &lt;p&gt;&lt;/p&gt;&lt;/div&gt;&lt;script src="js/jquery-1.12.3.min.js"&gt;&lt;/script&gt;&lt;script&gt; $('.div1 p').click(function () &#123; $('.div2 p').eq($(this).index()).css('background-color','red'); &#125;);&lt;/script&gt;&lt;/body&gt;&lt;/html&gt; 选项卡例子1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head lang="en"&gt; &lt;meta charset="UTF-8"&gt; &lt;title&gt;lesson5&lt;/title&gt; &lt;style&gt; *&#123; margin: 0; padding: 0; &#125; .xxk&#123; width: 300px; height: 150px; border: 1px solid black; margin: 100px auto; &#125; .xxk .up&#123; width: 100%; height: 50px; border: 1px solid black; &#125; .xxk .up ul&#123; list-style: none; &#125; .up ul li&#123; float:left; width: 100px; height: 100%; text-align: center; line-height: 50px; &#125; /*鼠标移动到哪里改变颜色*/ /*.up ul li:hover&#123;*/ /*background-color: orange;*/ /*&#125;*/ .up ul .show&#123; background-color:orange; &#125; .xxk .down&#123; width: 100%; height: 100px; &#125; .xxk .down ul&#123; list-style: none; &#125; .down ul li&#123; width: 300px; height: 100px; display: none; &#125; /*选择器的有限级 id&gt;class&gt;标签*/ .down ul .show-li&#123; display: block; &#125; &lt;/style&gt;&lt;/head&gt;&lt;body&gt;&lt;div class="xxk"&gt; &lt;div class="up"&gt; &lt;ul&gt; &lt;li class="show"&gt;&lt;a href="#"&gt;首页&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="#"&gt;视频&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="#"&gt;游戏&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;div class="down"&gt; &lt;ul&gt; &lt;li class="show-li"&gt;首页首页首页&lt;/li&gt; &lt;li&gt;视频视频视频&lt;/li&gt; &lt;li&gt;游戏游戏游戏&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;/div&gt;&lt;script src="js/jquery-1.12.3.min.js"&gt;&lt;/script&gt;&lt;script&gt; $('.up li').click(function () &#123; //控制 标题 $(this).addClass('show').siblings().removeClass('show'); // 控制内容部分 $('.down li').eq($(this).index()).addClass('show-li').siblings().removeClass('show-li'); &#125;);&lt;/script&gt;&lt;/body&gt;&lt;/html&gt; each()方法作用：遍历每个节点，然后执行里面的回调函数。 注意：回调函数中如果存在$(this)，那么它指的是【遍历中当前这一次的这个节点】。 语法：$(‘selector’).each(func) 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head lang="en"&gt; &lt;meta charset="UTF-8"&gt; &lt;title&gt;lesson5&lt;/title&gt; &lt;style&gt; .div1,.div2,.div3&#123; width: 500px; height: 150px; border: 1px solid black; margin: 20px; &#125; div p&#123; width: 100px; height: 100px; background-color: skyblue; float: left; margin: 5px; &#125; div .teshu&#123; background-color: orange; &#125; &lt;/style&gt;&lt;/head&gt;&lt;body&gt;&lt;div class="div1"&gt; &lt;p class="teshu"&gt;teshu&lt;/p&gt; &lt;p&gt;&lt;/p&gt; &lt;p&gt;&lt;span&gt;span&lt;/span&gt;&lt;/p&gt;&lt;/div&gt;&lt;div class="div2"&gt; &lt;p&gt;&lt;/p&gt; &lt;p class="teshu"&gt;teshu&lt;/p&gt; &lt;p class="teshu"&gt;teshu&lt;/p&gt;&lt;/div&gt;&lt;div class="div3"&gt; &lt;p&gt;&lt;/p&gt; &lt;p class="teshu"&gt;teshu&lt;/p&gt; &lt;p&gt;&lt;/p&gt;&lt;/div&gt;&lt;script src="js/jquery-1.12.3.min.js"&gt;&lt;/script&gt;&lt;script&gt; &lt;!--谁访问就谁的子元素修改--&gt; // $('div').each(function (index) &#123; // $(this).children().eq(1).css('background-color','red'); // console.log(index); // &#125;); // 单击遍历 // $('div').each(function (index) &#123; // $(this).children().eq(1).click(function () &#123; // console.log(index); // &#125;); // &#125;) $('div').each(function (index,ele) &#123; $(this).children().eq(1).css('background-color','red'); console.log(index); console.log(ele); &#125;);&lt;/script&gt;&lt;/body&gt;&lt;/html&gt; 补充：delay()函数表示延迟：** $().delay(600).animate(); $().delay(600).fadeOut(); $().delay(600).show(400); //均表示动画延迟600ms执行 360特效例子：1234567891011121314151617181920212223242526272829303132333435363738&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head lang="en"&gt; &lt;meta charset="UTF-8"&gt; &lt;title&gt;lesson5&lt;/title&gt; &lt;style&gt; .cont&#123; width: 1200px; height: 300px; border: 1px solid black; position: relative; &#125; .cont img&#123; position: absolute; display: none; &#125; img:nth-child(2)&#123; left: 100px; top: 50px; &#125; img:nth-child(3)&#123; left: 300px; top: 50px; &#125; img:nth-child(4)&#123; left: 500px; top: 50px; &#125; img:nth-child(5)&#123; left: 800px; top: 50px; &#125; &lt;/style&gt;&lt;/head&gt;&lt;body&gt;&lt;div class="cont"&gt; &lt;img src="img/360_1.png" alt=""/&gt; &lt;img src="img/360_2.png" alt=""/&gt; &lt;img src="img/360_3.png" alt=""/&gt; &lt;img src="img/360_4.png" alt=""/&gt; &lt;img src="img/360_5.png" alt=""/&gt;&lt;/div&gt;&lt;script src="js/jquery-1.12.3.min.js"&gt;&lt;/script&gt;&lt;script&gt; $('img').each(function (index) &#123; // $(this).fadeIn(100).fadeOut(100).fadeIn(100).fadeOut(100).fadeIn(100); $(this).delay(500*index).fadeIn(100).fadeOut(100).fadeIn(100).fadeOut(100).fadeIn(100); &#125;);&lt;/script&gt;&lt;/body&gt;&lt;/html&gt; 表格列变色例子12345678910111213141516171819202122232425262728293031323334&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head lang="en"&gt; &lt;meta charset="UTF-8"&gt; &lt;title&gt;lesson5&lt;/title&gt; &lt;style&gt; &lt;/style&gt;&lt;/head&gt;&lt;body&gt;&lt;table align="center" width="500" border="1" cellspacing="0"&gt; &lt;tr&gt;&lt;td&gt;111&lt;/td&gt;&lt;td&gt;111&lt;/td&gt;&lt;td&gt;111&lt;/td&gt;&lt;td&gt;111&lt;/td&gt;&lt;td&gt;111&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;111&lt;/td&gt;&lt;td&gt;111&lt;/td&gt;&lt;td&gt;111&lt;/td&gt;&lt;td&gt;111&lt;/td&gt;&lt;td&gt;111&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;111&lt;/td&gt;&lt;td&gt;111&lt;/td&gt;&lt;td&gt;111&lt;/td&gt;&lt;td&gt;111&lt;/td&gt;&lt;td&gt;111&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;111&lt;/td&gt;&lt;td&gt;111&lt;/td&gt;&lt;td&gt;111&lt;/td&gt;&lt;td&gt;111&lt;/td&gt;&lt;td&gt;111&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;111&lt;/td&gt;&lt;td&gt;111&lt;/td&gt;&lt;td&gt;111&lt;/td&gt;&lt;td&gt;111&lt;/td&gt;&lt;td&gt;111&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;111&lt;/td&gt;&lt;td&gt;111&lt;/td&gt;&lt;td&gt;111&lt;/td&gt;&lt;td&gt;111&lt;/td&gt;&lt;td&gt;111&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;111&lt;/td&gt;&lt;td&gt;111&lt;/td&gt;&lt;td&gt;111&lt;/td&gt;&lt;td&gt;111&lt;/td&gt;&lt;td&gt;111&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/table&gt;&lt;script src="js/jquery-1.12.3.min.js"&gt;&lt;/script&gt;&lt;script&gt; &lt;!-- 让表格的列变色 --&gt; $('tr').each(function () &#123; $(this).children().eq(2).css('background-color','red'); &#125;);&lt;/script&gt;&lt;/body&gt;&lt;/html&gt; 节点操作内部插入append()、appendTo()、prepend()、prependTo()方法作用：这四个方法都用来在某个节点内部插入新内容 语法： A.append(B); //向【A节点内部现有内容之后】追加【B节点】 B.appendTo(A); //将【B节点】追加到【A节点内部现有内容之后】 A.prepend(B); //向【A节点内部现有内容之前】追加【B节点】 B.prependTo(A); //将【B节点】追加到【A节点内部的现有内容之前】 说明：四个方法所表达的含义大致相同，只不过在语法上略有出入。其中AB均为节点。 123456789101112131415161718192021222324252627282930313233343536373839404142434445&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head lang="en"&gt; &lt;meta charset="UTF-8"&gt; &lt;title&gt;lesson6&lt;/title&gt; &lt;style&gt; .div1,.div2,.div3&#123; width: 500px; height: 150px; border: 1px solid black; margin: 20px; &#125; div p&#123; width: 100px; height: 100px; background-color: skyblue; float: left; margin: 5px; &#125; div .teshu&#123; background-color: orange; &#125; &lt;/style&gt;&lt;/head&gt;&lt;body&gt;&lt;div class="div1"&gt; &lt;p class="teshu"&gt;teshu&lt;/p&gt; &lt;p&gt;&lt;/p&gt; &lt;p&gt;&lt;span&gt;span&lt;/span&gt;&lt;/p&gt;&lt;/div&gt;&lt;div class="div2"&gt; &lt;p&gt;&lt;/p&gt; &lt;p class="teshu"&gt;teshu&lt;/p&gt; &lt;p class="teshu"&gt;teshu&lt;/p&gt;&lt;/div&gt;&lt;div class="div3"&gt; &lt;p&gt;&lt;/p&gt; &lt;p class="teshu"&gt;teshu&lt;/p&gt; &lt;p&gt;&lt;/p&gt;&lt;/div&gt;&lt;script src="js/jquery-1.12.3.min.js"&gt;&lt;/script&gt;&lt;script&gt; $('div').click(function () &#123; $(this).append($('&lt;p&gt;新添加的&lt;/p&gt;')); &#125;); &lt;/script&gt;&lt;/body&gt;&lt;/html&gt; 外部插入after()、before()、insertAfter()、insertBefore()作用：相比于前面的四个方法的作用，这四个方法可以认为是给当前节点添加兄弟 语法： A.after(B);//在【A节点之后】添加【同级节点B】 A.before(B);//在【A节点之前】添加【同级节点B】 A.insertAfter(B);//把【A节点】添加到【B节点之后】 A.insertBefore(B); //将【A节点】添加到【B节点之前】 说明：四个方法所表达的含义大致相同，只不过在语法上略有出入。其中AB均为节点。 123456789101112131415161718192021222324252627282930313233343536373839404142434445&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head lang="en"&gt; &lt;meta charset="UTF-8"&gt; &lt;title&gt;lesson6&lt;/title&gt; &lt;style&gt; .div1,.div2,.div3&#123; width: 500px; height: 150px; border: 1px solid black; margin: 20px; &#125; div p&#123; width: 100px; height: 100px; background-color: skyblue; float: left; margin: 5px; &#125; div .teshu&#123; background-color: orange; &#125; &lt;/style&gt;&lt;/head&gt;&lt;body&gt;&lt;div class="div1"&gt; &lt;p class="teshu"&gt;teshu&lt;/p&gt; &lt;p&gt;&lt;/p&gt; &lt;p&gt;&lt;span&gt;span&lt;/span&gt;&lt;/p&gt;&lt;/div&gt;&lt;div class="div2"&gt; &lt;p&gt;&lt;/p&gt; &lt;p class="teshu"&gt;teshu&lt;/p&gt; &lt;p class="teshu"&gt;teshu&lt;/p&gt;&lt;/div&gt;&lt;div class="div3"&gt; &lt;p&gt;&lt;/p&gt; &lt;p class="teshu"&gt;teshu&lt;/p&gt; &lt;p&gt;&lt;/p&gt;&lt;/div&gt;&lt;script src="js/jquery-1.12.3.min.js"&gt;&lt;/script&gt;&lt;script&gt; $('div').click(function () &#123; $(this).after($('&lt;h2&gt;这是新添加的兄弟节点&lt;/h2&gt;')) &#125;);&lt;/script&gt;&lt;/body&gt;&lt;/html&gt; 改变节点位置对于jq来说操作的实际上要么是节点，要么是节点组。那么有一条原则在操作节点的时候我们就必须遵守： 通过搜索获得的节点在页面中只能同一时刻出现在一个位置 这就是frank总结的‘节点守恒定律’。 1234567891011121314假设存在HTML结构： &lt;div id="box1"&gt; &lt;p class="xiaoming"&gt;我是小明&lt;/p&gt; &lt;/div&gt; &lt;div id="box2"&gt; &lt;/div&gt;现在执行命令： $("#box2").append($(".xiaoming"));则HTML页面将变为： &lt;div id="box1"&gt; &lt;/div&gt; &lt;div id="box2"&gt; &lt;p class="xiaoming"&gt;我是小明&lt;/p&gt; &lt;/div&gt; 特别需要说明的是，在jq中并没有提供所谓的change之类的方法。因此改变节点的位置还是需要通过append这种方法来实现。 包裹wrap()作用：给自己增加一个父类(开发中基本没啥用) 语法：A.warp(B) 删除节点empty()、remove()作用：empty()表示删除指定节点中的内容，而remove()则表示移除自己 语法： A.empty() 等价于 A.html(‘’); A.remove(); 12345678910111213141516171819202122232425262728293031323334353637383940414243444546&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head lang="en"&gt; &lt;meta charset="UTF-8"&gt; &lt;title&gt;lesson6&lt;/title&gt; &lt;style&gt; .div1,.div2,.div3&#123; width: 500px; height: 150px; border: 1px solid black; margin: 20px; &#125; div p&#123; width: 100px; height: 100px; background-color: skyblue; float: left; margin: 5px; &#125; div .teshu&#123; background-color: orange; &#125; &lt;/style&gt;&lt;/head&gt;&lt;body&gt;&lt;div class="div1"&gt; &lt;p class="teshu"&gt;teshu&lt;/p&gt; &lt;p&gt;&lt;/p&gt; &lt;p&gt;&lt;span&gt;span&lt;/span&gt;&lt;/p&gt;&lt;/div&gt;&lt;div class="div2"&gt; &lt;p&gt;&lt;/p&gt; &lt;p class="teshu"&gt;teshu&lt;/p&gt; &lt;p class="teshu"&gt;teshu&lt;/p&gt;&lt;/div&gt;&lt;div class="div3"&gt; &lt;p&gt;&lt;/p&gt; &lt;p class="teshu"&gt;teshu&lt;/p&gt; &lt;p&gt;&lt;/p&gt;&lt;/div&gt;&lt;script src="js/jquery-1.12.3.min.js"&gt;&lt;/script&gt;&lt;script&gt; $('div').click(function () &#123; //$(this).empty(); $(this).remove(); &#125;);&lt;/script&gt;&lt;/body&gt;&lt;/html&gt; 克隆clone()作用：相当于js中的cloneNode操作，即克隆的节点在页面中没有自己的位置。需要通过append等操作才能够追加到页面当中。 语法：A.append(B.clone) 123456789101112131415161718192021222324252627282930313233343536373839404142434445&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head lang="en"&gt; &lt;meta charset="UTF-8"&gt; &lt;title&gt;lesson6&lt;/title&gt; &lt;style&gt; .div1,.div2,.div3&#123; width: 500px; height: 150px; border: 1px solid black; margin: 20px; &#125; div p&#123; width: 100px; height: 100px; background-color: skyblue; float: left; margin: 5px; &#125; div .teshu&#123; background-color: orange; &#125; &lt;/style&gt;&lt;/head&gt;&lt;body&gt;&lt;div class="div1"&gt; &lt;p class="teshu"&gt;teshu&lt;/p&gt; &lt;p&gt;&lt;/p&gt; &lt;p&gt;&lt;span&gt;span&lt;/span&gt;&lt;/p&gt;&lt;/div&gt;&lt;div class="div2"&gt; &lt;p&gt;&lt;/p&gt; &lt;p class="teshu"&gt;teshu&lt;/p&gt; &lt;p class="teshu"&gt;teshu&lt;/p&gt;&lt;/div&gt;&lt;div class="div3"&gt; &lt;p&gt;&lt;/p&gt; &lt;p class="teshu"&gt;teshu&lt;/p&gt; &lt;p&gt;&lt;/p&gt;&lt;/div&gt;&lt;script src="js/jquery-1.12.3.min.js"&gt;&lt;/script&gt;&lt;script&gt; $('div').click(function () &#123; $('body').append($(this).clone()); &#125;);&lt;/script&gt;&lt;/body&gt;&lt;/html&gt; 节点移动例子1234567891011121314151617181920212223242526272829303132333435363738394041&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head lang="en"&gt; &lt;meta charset="UTF-8"&gt; &lt;title&gt;lesson6&lt;/title&gt; &lt;style&gt; .leftDiv,.rightDiv&#123; width: 150px; height: 400px; float: left; border: 1px solid black; &#125; .menu&#123; float: left; margin: 0 30px; &#125; &lt;/style&gt;&lt;/head&gt;&lt;body&gt;&lt;div class="leftDiv"&gt; &lt;p&gt;&lt;span&gt;大狮子&lt;/span&gt;&lt;input type="checkbox"/&gt;&lt;/p&gt; &lt;p&gt;&lt;span&gt;大老虎&lt;/span&gt;&lt;input type="checkbox"/&gt;&lt;/p&gt; &lt;p&gt;&lt;span&gt;梅花鹿&lt;/span&gt;&lt;input type="checkbox"/&gt;&lt;/p&gt; &lt;p&gt;&lt;span&gt;小松鼠&lt;/span&gt;&lt;input type="checkbox"/&gt;&lt;/p&gt; &lt;p&gt;&lt;span&gt;大象&lt;/span&gt;&lt;input type="checkbox"/&gt;&lt;/p&gt; &lt;p&gt;&lt;span&gt;黄河&lt;/span&gt;&lt;input type="checkbox"/&gt;&lt;/p&gt; &lt;p&gt;&lt;span&gt;长江&lt;/span&gt;&lt;input type="checkbox"/&gt;&lt;/p&gt; &lt;p&gt;&lt;span&gt;高山&lt;/span&gt;&lt;input type="checkbox"/&gt;&lt;/p&gt; &lt;p&gt;&lt;span&gt;水杯&lt;/span&gt;&lt;input type="checkbox"/&gt;&lt;/p&gt; &lt;p&gt;&lt;span&gt;麦克风&lt;/span&gt;&lt;input type="checkbox"/&gt;&lt;/p&gt;&lt;/div&gt;&lt;div class="menu"&gt; &lt;button&gt; ----&gt; &lt;/button&gt; &lt;button&gt; &lt;---- &lt;/button&gt;&lt;/div&gt;&lt;div class="rightDiv"&gt;&lt;/div&gt;&lt;script src="js/jquery-1.12.3.min.js"&gt;&lt;/script&gt;&lt;script&gt; $('.menu button').eq(0).click(function () &#123; $('.rightDiv').append($('.leftDiv input:checked').parent()); $('.rightDiv input:checked').attr('checked',false); &#125;);&lt;/script&gt;&lt;/body&gt;&lt;/html&gt;]]></content>
      <categories>
        <category>jQuery</category>
      </categories>
      <tags>
        <tag>前端框架</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[搭建 vue 框架的使用环境]]></title>
    <url>%2F2018%2F06%2F03%2F%E6%90%AD%E5%BB%BA%20vue%20%E6%A1%86%E6%9E%B6%E7%9A%84%E4%BD%BF%E7%94%A8%E7%8E%AF%E5%A2%83%2F</url>
    <content type="text"><![CDATA[搭建 vue 框架的使用环境安装 nodejsnode 附带了 npm 指令。 查询是否安装了 nodejs ,输入以下命令： 1node -v 这里我们是需要 npm 命令才安装的 nodejs 。 安装 webpackwebpack：自动化。对于需要反复重复的任务，例如压缩（minification）、编译、单元测试、linting等，自动化工具可以减轻你的劳动，简化你的工作。 webpack 可以通过 https://webpack.js.org/ 中的 DOCUMENTATION 来学习它。 安装 淘宝 NPM 镜像注意：我们不使用 npm ，因为 npm 在国外速度非常慢，所以我们使用 cnpm (淘宝的镜像代替)。 1$ npm install -g cnpm --registry=https://registry.npm.taobao.org 命令安装 淘宝 NPM 镜像 查看淘宝 NPM 镜像是否安装成功 1cnpm 说明安装成功。 安装 webpack1cnpm install -g webpack 测试 webpack 是否安装成功 1webpack 全局安装 vue-cli 这是 vue 提供的一种创建项目架构的方式，使用命令就可以生成。 安装 vue-cli 使用命令： 1cnpm install --global vue-cli 安装会出现此结果。 判断是否安装成功 1vue 说明安装成功。 创建项目创建一个基于 webpack 模板的新项目 1vue init webpack my-project 初始化项目生成 package.json 文件 1cnpm install 创建完成可以在我们的项目目录下看到 package.json 文件： 完整的项目结构 运行项目1cnpm run dev 访问：http://localhost:8080]]></content>
      <categories>
        <category>vue</category>
      </categories>
      <tags>
        <tag>前端框架</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[常见方法]]></title>
    <url>%2F2018%2F06%2F03%2F%E5%B8%B8%E8%A7%81%E6%96%B9%E6%B3%95%2F</url>
    <content type="text"><![CDATA[常见方法 css()方法 show()和hide()方法 slideDown()和slideUp()方法 fadeIn()和fadeOut()方法 addClass()和removeClass()方法 attr()方法 html()方法 css()方法(1).jq 对象.css(‘属性名’,’属性值’); 1234567891011121314151617181920212223242526272829303132333435&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head lang="en"&gt; &lt;meta charset="UTF-8"&gt; &lt;title&gt;lesson4&lt;/title&gt; &lt;style&gt; .div1&#123; width: 100px; height: 100px; background-color: orange; &#125; .div2&#123; width: 100px; height: 100px; background-color: skyblue; &#125; &lt;/style&gt;&lt;/head&gt;&lt;body&gt;&lt;div class="div1"&gt;div1&lt;/div&gt;&lt;br/&gt;&lt;div class="div2"&gt;div2&lt;/div&gt;&lt;script src="js/jquery-1.12.3.min.js"&gt;&lt;/script&gt;&lt;!--&lt;script src="http://apps.bdimg.com/libs/jquery/1.7.2/jquery.min.js"&gt;&lt;/script&gt;--&gt;&lt;script&gt; /** * 1.css() * 描述：本方法提供给 jq 对象，用来读写 jq 对象的 css 属性。相当于 js 中的 style 属性 * 语法： * (1).jq 对象.css('属性名','属性值'); * 说明：第二个参数为可选的参数，如果不写代表读取当前属性的值 * 注意：使用 css() 方法读写属性的时候，css 属性不需要改写 */ $('.div1').click(function () &#123; $(this).css('background-color','red'); &#125;); $('.div2').click(function () &#123; console.log($(this).css('background-color')); &#125;);&lt;/script&gt;&lt;/body&gt;&lt;/html&gt; 1234567891011121314151617181920212223242526272829303132333435363738&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head lang="en"&gt; &lt;meta charset="UTF-8"&gt; &lt;title&gt;lesson4&lt;/title&gt; &lt;style&gt; .div1&#123; width: 100px; height: 100px; background-color: orange; &#125; .div2&#123; width: 100px; height: 100px; background-color: skyblue; &#125; &lt;/style&gt;&lt;/head&gt;&lt;body&gt;&lt;div class="div1"&gt;div1&lt;/div&gt;&lt;br/&gt;&lt;div class="div2"&gt;div2&lt;/div&gt;&lt;script src="js/jquery-1.12.3.min.js"&gt;&lt;/script&gt;&lt;!--&lt;script src="http://apps.bdimg.com/libs/jquery/1.7.2/jquery.min.js"&gt;&lt;/script&gt;--&gt;&lt;script&gt; /** * 1.css() * 描述：本方法提供给 jq 对象，用来读写 jq 对象的 css 属性。相当于 js 中的 style 属性 * 语法： * (1).jq 对象.css('属性名','属性值'); * 说明：第二个参数为可选的参数，如果不写代表读取当前属性的值 * 注意：使用 css() 方法读写属性的时候，css 属性不需要改写 */ $('.div1').click(function () &#123; $(this).css('background-color','red'); // 宽度变长 $(this).css('width','300px'); &#125;); $('.div2').click(function () &#123; // 可以拿到属性样式 console.log($(this).css('background-color')); &#125;);&lt;/script&gt;&lt;/body&gt;&lt;/html&gt; （2）JSON样式参数就是设置样式 123456789101112131415161718192021222324252627282930313233343536373839&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head lang="en"&gt; &lt;meta charset="UTF-8"&gt; &lt;title&gt;lesson4&lt;/title&gt; &lt;style&gt; .div1&#123; width: 100px; height: 100px; background-color: orange; &#125; .div2&#123; width: 100px; height: 100px; background-color: skyblue; &#125; &lt;/style&gt;&lt;/head&gt;&lt;body&gt;&lt;div class="div1"&gt;div1&lt;/div&gt;&lt;br/&gt;&lt;div class="div2"&gt;div2&lt;/div&gt;&lt;script src="js/jquery-1.12.3.min.js"&gt;&lt;/script&gt;&lt;!--&lt;script src="http://apps.bdimg.com/libs/jquery/1.7.2/jquery.min.js"&gt;&lt;/script&gt;--&gt;&lt;script&gt; /** * 1.css() * 描述：本方法提供给 jq 对象，用来读写 jq 对象的 css 属性。相当于 js 中的 style 属性 * 语法： * (2).jq对象.css(&#123;'css属性1':'属性值1','css属性2':'属性值2',...&#125;) * 说明：参数是 JSON 格式的时候，css() 方法允许一次性修改多个样式。 * 注意：JSON 参数理论上属性可以不写引号，但是如果 css 属性存在 - 等特殊字符，则必须添加。 */ $('.div1').click(function () &#123; $(this).css(&#123; // 注意：样式名称要加上 单引号引上 'background-color':'red', 'width':'300px' &#125;); &#125;); $('.div2').click(function () &#123; console.log($(this).css('background-color')); &#125;);&lt;/script&gt;&lt;/body&gt;&lt;/html&gt; 直接在原值上累加12345678910111213141516171819202122232425262728293031323334&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head lang="en"&gt; &lt;meta charset="UTF-8"&gt; &lt;title&gt;lesson4&lt;/title&gt; &lt;style&gt; .div1&#123; width: 100px; height: 100px; background-color: orange; &#125; .div2&#123; width: 100px; height: 100px; background-color: skyblue; &#125; &lt;/style&gt;&lt;/head&gt;&lt;body&gt;&lt;div class="div1"&gt;div1&lt;/div&gt;&lt;br/&gt;&lt;div class="div2"&gt;div2&lt;/div&gt;&lt;script src="js/jquery-1.12.3.min.js"&gt;&lt;/script&gt;&lt;!--&lt;script src="http://apps.bdimg.com/libs/jquery/1.7.2/jquery.min.js"&gt;&lt;/script&gt;--&gt;&lt;script&gt; // 第一种写法 // $('.div1').click(function () &#123; // var oldWidth = parseInt($(this).css('width')); // var newWidth = oldWidth+50+'px'; // $(this).css('width',newWidth); // &#125;); // 第二种写法 $('.div1').click(function () &#123; $(this).css('width','+=50px'); &#125;);&lt;/script&gt;&lt;/body&gt;&lt;/html&gt; show() &amp;&amp; hide()1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head lang="en"&gt; &lt;meta charset="UTF-8"&gt; &lt;title&gt;lesson4&lt;/title&gt; &lt;style&gt; .div1&#123; width: 200px; height: 200px; background-color: orange; &#125; &lt;/style&gt;&lt;/head&gt;&lt;body&gt;&lt;button&gt;show()&lt;/button&gt;&lt;button&gt;hide()&lt;/button&gt;&lt;button&gt;show(5000)&lt;/button&gt;&lt;button&gt;hide(5000)&lt;/button&gt;&lt;div class="div1"&gt;&lt;/div&gt;&lt;!--&lt;script src="js/jquery-1.12.3.min.js"&gt;&lt;/script&gt;--&gt;&lt;script src="http://apps.bdimg.com/libs/jquery/1.7.2/jquery.min.js"&gt;&lt;/script&gt;&lt;script&gt; /** * 1. 常见方法 show()&amp;hide() * 描述：show() 和 hide() 方法表示设置 jq 对象的显示和隐藏 * 语法：jq对象.show(animationTime); / jq对象.hide(animationTime); * 说明：show() / hide() 方法实际上允许添加单位为 ms 的动画时间. * 如果添加参数，则动画是宽高渐变，透明度渐变。 */ // eq():通过下标获取 $('button').eq(0).click(function () &#123; $('.div1').show(); &#125;); $('button').eq(1).click(function () &#123; $('.div1').hide(); &#125;); $('button').eq(2).click(function () &#123; $('.div1').show(5000); &#125;); $('button').eq(3).click(function () &#123; $('.div1').hide(5000); &#125;);&lt;/script&gt;&lt;/body&gt;&lt;/html&gt; 案例：某网站品牌 方法1：使用 1.7.2/jquery.min.js 版本来实现 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head lang="en"&gt; &lt;meta charset="UTF-8"&gt; &lt;title&gt;lesson4&lt;/title&gt; &lt;style&gt; .cont&#123; border:1px solid black; width: 600px; height: 350px; margin: 0 auto; &#125; .cont ul&#123; list-style: none; padding: 0px; margin: 0px; &#125; .cont ul li&#123; display: inline-block; width: 193px; height: 50px; line-height:30px; text-align: center; &#125; .cont div&#123; border: 1px solid #333; width: 50%; height: 35px; text-align: center; line-height: 35px; margin: 10px auto; cursor: pointer; &#125; &lt;/style&gt;&lt;/head&gt;&lt;body&gt;&lt;!--完成的主要功能选中的留下--&gt;&lt;div class="cont"&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="#"&gt;佳能&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="#"&gt;富士康&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="#"&gt;索尼&lt;/a&gt;&lt;/li&gt; &lt;li class="hd"&gt;&lt;a href="#"&gt;松下&lt;/a&gt;&lt;/li&gt; &lt;li class="hd"&gt;&lt;a href="#"&gt;奥林巴斯&lt;/a&gt;&lt;/li&gt; &lt;li class="hd"&gt;&lt;a href="#"&gt;苹果&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="#"&gt;三星&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="#"&gt;htc&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="#"&gt;水杯&lt;/a&gt;&lt;/li&gt; &lt;li class="hd"&gt;&lt;a href="#"&gt;鼠标&lt;/a&gt;&lt;/li&gt; &lt;li class="hd"&gt;&lt;a href="#"&gt;工牌&lt;/a&gt;&lt;/li&gt; &lt;li class="hd"&gt;&lt;a href="#"&gt;键盘&lt;/a&gt;&lt;/li&gt; &lt;li class="hd"&gt;&lt;a href="#"&gt;头盔&lt;/a&gt;&lt;/li&gt; &lt;li class="hd"&gt;&lt;a href="#"&gt;手套&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;div&gt;显示精简品牌&lt;/div&gt;&lt;/div&gt;&lt;script src="http://apps.bdimg.com/libs/jquery/1.7.2/jquery.min.js"&gt;&lt;/script&gt;&lt;script&gt; // 点击选中消失 var $hideItems = $('.cont .hd'); // 单击按钮事件 // 切换方法（允许你写多个方法） // 此方法在 1.12 中被废弃了 $('.cont div').toggle(function () &#123; // 第一次点击按钮品牌消失 $hideItems.hide(); $(this).html('显示全部品牌'); &#125;,function () &#123; // 再次点击按钮品牌出现 $hideItems.show(); $(this).html('显示精简品牌'); &#125;);&lt;/script&gt;&lt;/body&gt;&lt;/html&gt; slideDown() 和 slideUp() 方法下拉显示和上滑隐藏 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head lang="en"&gt; &lt;meta charset="UTF-8"&gt; &lt;title&gt;lesson5&lt;/title&gt; &lt;style&gt; .div1&#123; width: 300px; height: 300px; background-color: orange; &#125; &lt;/style&gt;&lt;/head&gt;&lt;body&gt;&lt;button&gt;slideUp()&lt;/button&gt;&lt;button&gt;slideDown()&lt;/button&gt;&lt;button&gt;slideUp(5000)&lt;/button&gt;&lt;button&gt;slideDown(5000)&lt;/button&gt;&lt;br/&gt;&lt;br/&gt;&lt;div class="div1"&gt;&lt;/div&gt;&lt;script src="js/jquery-1.12.3.min.js"&gt;&lt;/script&gt;&lt;script&gt; /** * 1. slideUp() 和 slideDown() * 描述：本方法用来设置 jq 对象上拉收起和下滑显示 * 语法：jq对象.slideUp(动画事件ms)/jq对象.slideDown(动画时间ms) * 注意：slideUp 和 slideDown 方法存在隐形的属性，叫“边界” * 边界是一个可以设置的属性，通过定位属性 top 或 bottom 设置 */ $('button').eq(0).click(function () &#123; $('.div1').slideUp(); &#125;); $('button').eq(1).click(function () &#123; $('.div1').slideDown(); &#125;); $('button').eq(2).click(function () &#123; $('.div1').slideUp(5000); &#125;); $('button').eq(3).click(function () &#123; $('.div1').slideDown(5000); &#125;);&lt;/script&gt;&lt;/body&gt;&lt;/html&gt; slideUp 和 slideDown 方法存在隐形的属性，叫“边界” 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head lang="en"&gt; &lt;meta charset="UTF-8"&gt; &lt;title&gt;lesson5&lt;/title&gt; &lt;style&gt; .div1&#123; width: 300px; height: 300px; background-color: orange; position: absolute; /*控制距离页面低部多远开始伸缩*/ bottom: 150px; &#125; &lt;/style&gt;&lt;/head&gt;&lt;body&gt;&lt;button&gt;slideUp()&lt;/button&gt;&lt;button&gt;slideDown()&lt;/button&gt;&lt;button&gt;slideUp(5000)&lt;/button&gt;&lt;button&gt;slideDown(5000)&lt;/button&gt;&lt;div class="div1"&gt;&lt;/div&gt;&lt;script src="js/jquery-1.12.3.min.js"&gt;&lt;/script&gt;&lt;script&gt; /** * 1. slideUp() 和 slideDown() * 描述：本方法用来设置 jq 对象上拉收起和下滑显示 * 语法：jq对象.slideUp(动画事件ms)/jq对象.slideDown(动画时间ms) * 注意：slideUp 和 slideDown 方法存在隐形的属性，叫“边界” * 边界是一个可以设置的属性，通过定位属性 top 或 bottom 设置（只有在上下没有在左右） */ $('button').eq(0).click(function () &#123; $('.div1').slideUp(); &#125;); $('button').eq(1).click(function () &#123; $('.div1').slideDown(); &#125;); $('button').eq(2).click(function () &#123; $('.div1').slideUp(5000); &#125;); $('button').eq(3).click(function () &#123; $('.div1').slideDown(5000); &#125;);&lt;/script&gt;&lt;/body&gt;&lt;/html&gt; 常用的例子： 1234567891011121314151617181920212223242526272829303132333435363738394041424344&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head lang="en"&gt; &lt;meta charset="UTF-8"&gt; &lt;title&gt;lesson5&lt;/title&gt; &lt;style&gt; .xiaomi&#123; width: 120px;height: 300px;border: 1px solid black; position: absolute;margin:100px auto;margin: 100px 50%; &#125; .xiaomi .innerChuangkou&#123; width: 100%; height: 200px; position: absolute; bottom: 0; background-color: rgba(10,10,10,0.5); /*一开进来不可见*/ display: none; &#125; &lt;/style&gt;&lt;/head&gt;&lt;body&gt;&lt;div class="xiaomi"&gt; 正常的商品正常的商品 &lt;div class="innerChuangkou"&gt;&lt;/div&gt;&lt;/div&gt;&lt;script src="js/jquery-1.12.3.min.js"&gt;&lt;/script&gt;&lt;script&gt; // 进来出现，出去消失 var $innerChuangkou = $('.innerChuangkou'); $('.xiaomi').mouseenter(function () &#123; $innerChuangkou.stop().slideDown(); &#125;).mouseleave(function () &#123; $innerChuangkou.stop().slideUp(); &#125;);&lt;/script&gt;&lt;/body&gt;&lt;/html&gt; fadeIn()和fadeOut()方法淡入、淡出 123456789101112131415161718192021222324252627282930313233343536&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head lang="en"&gt; &lt;meta charset="UTF-8"&gt; &lt;title&gt;lesson5&lt;/title&gt; &lt;style&gt; .div1&#123; width: 200px;height: 200px; background-color: orange; &#125; &lt;/style&gt;&lt;/head&gt;&lt;body&gt;&lt;button&gt;fadeIn()&lt;/button&gt;&lt;button&gt;fadeOut()&lt;/button&gt;&lt;button&gt;fadeIn(5000)&lt;/button&gt;&lt;button&gt;fadeOut(5000)&lt;/button&gt;&lt;div class="div1"&gt;&lt;/div&gt;&lt;script src="js/jquery-1.12.3.min.js"&gt;&lt;/script&gt;&lt;script&gt; $('button').eq(0).click(function () &#123; $('.div1').fadeIn(); &#125;); $('button').eq(1).click(function () &#123; $('.div1').fadeOut(); &#125;); $('button').eq(2).click(function () &#123; $('.div1').fadeIn(5000); &#125;); $('button').eq(3).click(function () &#123; $('.div1').fadeOut(5000); &#125;);&lt;/script&gt;&lt;/body&gt;&lt;/html&gt; 需要注意的是： fadeIn()的起点是display:none; 而不是opacity:0; fadeOut()的终点是display:none;也不是opacity:0; 也就是说一个元素如果想淡入，一定要给这个元素加上display:none;属性，而不要给他加上opacity:0;的属性。 同样的fadeIn()和fadeOut()的函数括号里面可以加数字，表示动画的时间。fadeIn(4000);就是用4000毫秒进行淡入。 addClass()和removeClass()方法追加类和移除类。add就是添加，remove就是移除。 123456789101112131415161718192021222324252627282930&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head lang="en"&gt; &lt;meta charset="UTF-8"&gt; &lt;title&gt;lesson5&lt;/title&gt; &lt;style&gt; .div1&#123; width: 200px;height: 200px;border: 1px solid black; &#125; .red&#123; background-color: red; &#125; &lt;/style&gt;&lt;/head&gt;&lt;body&gt;&lt;button&gt;添加red类&lt;/button&gt;&lt;button&gt;删除red类&lt;/button&gt;&lt;div class="div1"&gt;&lt;/div&gt;&lt;script src="js/jquery-1.12.3.min.js"&gt;&lt;/script&gt;&lt;script&gt; $('button').eq(0).click(function() &#123; $('.div1').addClass('red'); &#125;); $('button').eq(1).click(function () &#123; $('.div1').removeClass('red'); &#125;);&lt;/script&gt;&lt;/body&gt;&lt;/html&gt; attr()方法attr是英语attribute属性的简写，用来更改HTML元素的属性。之前我们在原生JS中学习过，一个元素可以”有什么属性就点什么”。 例如：document.getElementById(“tutu”).src = “2.jpg”; 实际上就是更改这个HTML元素的属性。现在在jQuery中就是用attr()方法,来更改HTML元素的属性。 语法：$(“div”).attr(“key”,”value”); 例如：$(“img”).mouseenter(function(){$(this).attr(“src”,”images/longlong.jpg”);}); 鼠标进入的时候，换图，就是换图的src属性attr()也可以读属性，当只有一个参数的时候，表示读取这个东西的属性。 例如：var s = $(“img”).attr(“src”); 123456789101112131415161718192021222324252627282930&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head lang="en"&gt; &lt;meta charset="UTF-8"&gt; &lt;title&gt;lesson5&lt;/title&gt; &lt;style&gt; .div1&#123; width: 200px;height: 200px;border: 1px solid black; &#125; .red&#123; background-color: red; &#125; &lt;/style&gt;&lt;/head&gt;&lt;body&gt;&lt;button&gt;添加red类&lt;/button&gt;&lt;button&gt;删除red类&lt;/button&gt;&lt;div class="div1"&gt;&lt;/div&gt;&lt;script src="js/jquery-1.12.3.min.js"&gt;&lt;/script&gt;&lt;script&gt; $('button').eq(0).click(function() &#123; $('.div1').attr('class','red'); &#125;); $('button').eq(1).click(function () &#123; $('.div1').removeClass('red'); &#125;);&lt;/script&gt;&lt;/body&gt;&lt;/html&gt; 单击（覆盖属性） 交叉淡入淡出轮播图1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head lang="en"&gt; &lt;meta charset="UTF-8"&gt; &lt;title&gt;lesson6&lt;/title&gt; &lt;style&gt; *&#123;margin: 0;padding: 0;&#125; .lunbotu&#123; width: 560px; height: 300px;margin: 50px auto; border: 1px solid black; position: relative; &#125; .lunbotu .imgUL&#123; /*去掉li标签的点*/ list-style: none; &#125; .lunbotu .imgUL li&#123; position: absolute; /*初始话的时候为不可见*/ display: none; &#125; .lunbotu .imgUL .selected&#123; display: block; &#125; .rightBtn,.leftBtn&#123; width: 30px; height: 60px; background-color: rgba(10,150,10,0.5); position: absolute; font-size: 35px; /*top代表*/ top: 40%; color: white; text-align: center; line-height: 60px; &#125; .lunbotu .rightBtn&#123; right: 0; &#125; &lt;/style&gt;&lt;/head&gt;&lt;body&gt;&lt;div class="lunbotu"&gt; &lt;ul class="imgUL"&gt; &lt;li class="selected"&gt;&lt;a href="#"&gt;&lt;img src="img/0.jpg" alt=""/&gt;&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="#"&gt;&lt;img src="img/1.jpg" alt=""/&gt;&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="#"&gt;&lt;img src="img/2.jpg" alt=""/&gt;&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="#"&gt;&lt;img src="img/3.jpg" alt=""/&gt;&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="#"&gt;&lt;img src="img/4.jpg" alt=""/&gt;&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;div class="leftBtn"&gt; &lt; &lt;/div&gt; &lt;div class="rightBtn"&gt; &gt; &lt;/div&gt;&lt;/div&gt;&lt;script src="js/jquery-1.12.3.min.js"&gt;&lt;/script&gt;&lt;script&gt; var $lis = $('.lunbotu li'); // index 是当前正在加载的图片的序号，默认从 0 开始加载 var index = 0; $('.rightBtn').click(function () &#123; // 防止动画积累 if($lis.is(':animated'))&#123; return; &#125; //旧的怎么样 $lis.eq(index).fadeOut(); index++; //设置边界 if(index == 5)&#123; index = 0; &#125; //新的怎么样 $lis.eq(index).fadeIn(); &#125;); $('.leftBtn').click(function () &#123; // 防止动画积累 if($lis.is(':animated'))&#123; return; &#125; $lis.eq(index).fadeOut(); index--; if(index == -1)&#123; index = 4; &#125; $lis.eq(index).fadeIn(); &#125;);&lt;/script&gt;&lt;/body&gt;&lt;/html&gt; 交叉淡入淡出轮播图锚点风格1123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head lang="en"&gt; &lt;meta charset="UTF-8"&gt; &lt;title&gt;lesson6&lt;/title&gt; &lt;style&gt; *&#123;margin: 0;padding: 0;&#125; .lunbotu&#123; width: 560px; height: 300px;margin: 50px auto; border: 1px solid black; position: relative; &#125; .lunbotu .imgUL&#123; /*去掉li标签的点*/ list-style: none; &#125; .lunbotu .imgUL li&#123; position: absolute; /*初始话的时候为不可见*/ display: none; &#125; .lunbotu .imgUL .selected&#123; display: block; &#125; .rightBtn,.leftBtn&#123; width: 30px; height: 60px; background-color: rgba(10,150,10,0.5); position: absolute; font-size: 35px; /*top代表*/ top: 40%; color: white; text-align: center; line-height: 60px; &#125; .lunbotu .rightBtn&#123; right: 0; &#125; .lunbotu .maodainUL&#123; list-style: none; position: absolute; /*下面的距离为10px*/ bottom: 10px; /*左右为 35%*/ left: 35%; &#125; .lunbotu .maodainUL li&#123; width: 25px; height: 25px; border-radius: 50%; text-align: center; line-height: 25px; display: inline-block; background-color: orange; &#125; .maodainUL li a&#123; /*去掉下划线*/ text-decoration: none; color: white; font-size: 20px; &#125; .lunbotu .maodainUL .selected-li&#123; background-color: skyblue; &#125; &lt;/style&gt;&lt;/head&gt;&lt;body&gt;&lt;div class="lunbotu"&gt; &lt;ul class="imgUL"&gt; &lt;li class="selected"&gt;&lt;a href="#"&gt;&lt;img src="img/0.jpg" alt=""/&gt;&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="#"&gt;&lt;img src="img/1.jpg" alt=""/&gt;&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="#"&gt;&lt;img src="img/2.jpg" alt=""/&gt;&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="#"&gt;&lt;img src="img/3.jpg" alt=""/&gt;&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="#"&gt;&lt;img src="img/4.jpg" alt=""/&gt;&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;div class="leftBtn"&gt; &lt; &lt;/div&gt; &lt;div class="rightBtn"&gt; &gt; &lt;/div&gt; &lt;ul class="maodainUL"&gt; &lt;li class="selected-li"&gt;&lt;a href="#"&gt;0&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="#"&gt;1&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="#"&gt;2&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="#"&gt;3&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="#"&gt;4&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt;&lt;/div&gt;&lt;script src="js/jquery-1.12.3.min.js"&gt;&lt;/script&gt;&lt;script&gt; // 获取图片的li var $lis = $('.imgUL li'); // 获取锚点的li var $maodians = $('.maodainUL li') // index 是当前正在加载的图片的序号，默认从 0 开始加载 var index = 0; $('.rightBtn').click(function () &#123; // 防止动画积累 if($lis.is(':animated'))&#123; return; &#125; //旧的怎么样 $lis.eq(index).fadeOut(); $maodians.eq(index).removeClass('selected-li'); index++; //设置边界 if(index == 5)&#123; index = 0; &#125; //新的怎么样 $lis.eq(index).fadeIn(); $maodians.eq(index).addClass('selected-li') &#125;); $('.leftBtn').click(function () &#123; // 防止动画积累 if($lis.is(':animated'))&#123; return; &#125; $lis.eq(index).fadeOut(); $maodians.eq(index).removeClass('selected-li') index--; if(index == -1)&#123; index = 4; &#125; $lis.eq(index).fadeIn(); $maodians.eq(index).addClass('selected-li'); &#125;); // 锚点的点击事件 $maodians.click(function () &#123; // 防止动画积累 if($lis.is(':animated'))&#123; return; &#125; // 旧的怎么样 $lis.eq(index).fadeOut(); $maodians.eq(index).removeClass('selected-li'); // 设置 index 为点击的 li 对应的 index // 获取当前元素在兄弟中的序号：jq对象.index() index = $(this).index(); // 新的怎么样 $lis.eq(index).fadeIn(); $maodians.eq(index).addClass('selected-li'); &#125;);&lt;/script&gt;&lt;/body&gt;&lt;/html&gt; 风格2123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt; &lt;meta http-equiv="Content-Type" content="text/html;charset=UTF-8"&gt; &lt;title&gt;三位置法轮播图&lt;/title&gt; &lt;style type="text/css"&gt; *&#123; margin: 0; padding: 0; &#125; .carousel&#123; width: 560px; height: 300px; margin: 50px auto; position: relative; border: 10px solid #ccc; &#125; .carousel .btns span&#123; position: absolute; width: 40px; height: 40px; top: 50%; margin-top: -20px; background-color: orange; cursor: pointer; &#125; .carousel .btns span.leftBtn&#123; left: 10px; &#125; .carousel .btns span.rightBtn&#123; right:10px; &#125; .carousel ul&#123; list-style: none; &#125; .carousel ul li&#123; position: absolute; left: 560px; top: 0; &#125; .carousel ul li.first&#123; left: 0; &#125; &lt;/style&gt;&lt;/head&gt;&lt;body&gt; &lt;div class="carousel"&gt; &lt;ul&gt; &lt;li class="first"&gt;&lt;a href="#"&gt;&lt;img src="img/0.jpg" alt="" /&gt;&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="#"&gt;&lt;img src="img/1.jpg" alt="" /&gt;&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="#"&gt;&lt;img src="img/2.jpg" alt="" /&gt;&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="#"&gt;&lt;img src="img/3.jpg" alt="" /&gt;&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="#"&gt;&lt;img src="img/4.jpg" alt="" /&gt;&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;div class="btns"&gt; &lt;span class="leftBtn"&gt;&lt;/span&gt; &lt;span class="rightBtn"&gt;&lt;/span&gt; &lt;/div&gt; &lt;/div&gt; &lt;script src="js/jquery-1.12.3.min.js"&gt;&lt;/script&gt; &lt;script&gt; //信号量 var idx = 0; //右按钮的事件监听 $(".rightBtn").click(function()&#123; //改变之前，老图往-560移动 $("li").eq(idx).animate(&#123;"left":-560&#125;,600); //信号量改变 idx++; if(idx &gt; 4)&#123; idx = 0; &#125; //信号量改变之后，新图先瞬间移动到560，然后往0移动 $("li").eq(idx).css("left",560).animate(&#123;"left":0&#125;,600); &#125;); //左按钮的事件监听 $(".leftBtn").click(function()&#123; //改变之前，老图往560移动 $("li").eq(idx).animate(&#123;"left":560&#125;,600); //信号量改变 idx--; if(idx &lt; 0)&#123; idx = 4; &#125; //信号量改变之后，新图先瞬间移动到-560，然后往0移动 $("li").eq(idx).css("left",-560).animate(&#123;"left":0&#125;,600); &#125;); &lt;/script&gt;&lt;/body&gt;&lt;/html&gt; 风格3123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt; &lt;meta http-equiv="Content-Type" content="text/html;charset=UTF-8"&gt; &lt;title&gt;火车法轮播图&lt;/title&gt; &lt;style type="text/css"&gt; *&#123; margin: 0; padding: 0; &#125; .carousel&#123; width: 560px; height: 300px; margin: 50px auto; position: relative; border: 10px solid #ccc; /*ovh*/ &#125; .carousel .btns span&#123; position: absolute; width: 40px; height: 40px; top: 50%; margin-top: -20px; background-color: orange; cursor: pointer; &#125; .carousel .btns span.leftBtn&#123; left: 10px; &#125; .carousel .btns span.rightBtn&#123; right:10px; &#125; .carousel ul&#123; list-style: none; /*布局的难点，一定要记住ul要宽一点，否则li不能并排*/ width: 8000px; position: absolute; top: 0; left: 0; &#125; .carousel ul li&#123; float: left; width: 560px; &#125; &lt;/style&gt;&lt;/head&gt;&lt;body&gt; &lt;div class="carousel"&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="#"&gt;&lt;img src="img/0.jpg" alt="" /&gt;&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="#"&gt;&lt;img src="img/1.jpg" alt="" /&gt;&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="#"&gt;&lt;img src="img/2.jpg" alt="" /&gt;&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="#"&gt;&lt;img src="img/3.jpg" alt="" /&gt;&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="#"&gt;&lt;img src="img/4.jpg" alt="" /&gt;&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;div class="btns"&gt; &lt;span class="leftBtn"&gt;&lt;/span&gt; &lt;span class="rightBtn"&gt;&lt;/span&gt; &lt;/div&gt; &lt;/div&gt; &lt;script src="js/jquery-1.12.3.min.js"&gt;&lt;/script&gt; &lt;script&gt; //得到ul var $ul = $(".carousel ul"); //克隆第一张图片，追加到图片列表末尾 //clone()就是克隆，就是复制一下，然后apeendTo就是把克隆的这张图片追加到ul末尾。 $(".carousel ul li:first").clone().appendTo(".carousel ul"); //信号量，表示当前是第几张图片 var idx = 0; //点击右按钮，做的事情 $(".rightBtn").click(function()&#123; //防止流氓 if($ul.is(":animated"))&#123; return; &#125; //信号量增加 idx++; //我们允许信号量临时等于5一下。 //先拉动，后瞬移 $ul.animate(&#123;"left":-560 * idx&#125;,600,function()&#123; //如果idx大于了4，那么就瞬间移动回来： if(idx &gt; 4)&#123; idx = 0; $(this).css("left",0); &#125; &#125;); &#125;); $(".leftBtn").click(function()&#123; //防止流氓 if($ul.is(":animated"))&#123; return; &#125; //信号量变化 idx--; //先瞬移再拉，如果信号量小于了0，那么信号量为4。 if(idx == 0)&#123; idx = 4; //往下标为5的，就是猫腻那张图片瞬移 $ul.css("left",-560 * 5); &#125; $ul.animate(&#123;"left":-560 * idx&#125; , 600); &#125;); &lt;/script&gt;&lt;/body&gt;&lt;/html&gt; animate() 方法自定义动画。 最简单的形态&amp;不能够变化的属性123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head lang="en"&gt; &lt;meta charset="UTF-8"&gt; &lt;title&gt;lesson1&lt;/title&gt; &lt;style&gt; .div1&#123; width: 100px; height: 100px; background-color: orange; &#125; .div2&#123; width: 100px; height: 100px; background-color: skyblue; border: 1px solid black; &#125; &lt;/style&gt;&lt;/head&gt;&lt;body&gt;&lt;div class="div1"&gt;&lt;/div&gt;&lt;br/&gt;&lt;div class="div2"&gt;&lt;/div&gt;&lt;script src="js/jquery-1.12.3.min.js"&gt;&lt;/script&gt;&lt;script src="js/jquery.color.js"&gt;&lt;/script&gt;&lt;script&gt; var $div1 = $('.div1'); var $div2 = $('.div2'); $div1.click(function () &#123; $(this).animate(&#123;'width':'500px'&#125;,1500); &#125;); $div2.click(function () &#123; $(this).animate(&#123;'width':'500px','height':'200px','background-color':'red'&#125;,1500); &#125;); /** * animate() 方法 * 描述：jq提供了一个用来自定义动画的方法 * 语法：jq.animate（动画结束状态JSON，动画时间 ms，动画线性运动，回掉函数function）; * 说明： * 【1】前面两个参数为必要参数，后面两个参数为可选参数 * 【2】第一个参数格是 json 格式，哪怕只有一个属性 * 【3】 * 【4】 * 局限：jquery 框架中提供的 animate() 方法只能够修改可以量化的属性，对于颜色&amp;display 等无法量化的属性则不能够生效 * 如果想要颜色的改变，则需要引入 jquery.color.js 插件 */&lt;/script&gt;&lt;/body&gt;&lt;/html&gt; 动画顺序jq中动画的执行遵循两大原则： 同步原则：同一个元素如果存在多个animate命令，则按照添加顺序执行。 异步原则：不同元素如果存在多个animate命令，则他们同时执行。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head lang="en"&gt; &lt;meta charset="UTF-8"&gt; &lt;title&gt;lesson1&lt;/title&gt; &lt;style&gt; .div1&#123; width: 100px; height: 100px; background-color: orange; &#125; .div2&#123; width: 100px; height: 100px; background-color: skyblue; border: 1px solid black; &#125; &lt;/style&gt;&lt;/head&gt;&lt;body&gt;&lt;div class="div1"&gt;&lt;/div&gt;&lt;br/&gt;&lt;div class="div2"&gt;&lt;/div&gt;&lt;script src="js/jquery-1.12.3.min.js"&gt;&lt;/script&gt;&lt;script src="js/jquery.color.js"&gt;&lt;/script&gt;&lt;script&gt; var $div1 = $('.div1'); var $div2 = $('.div2'); // 同步执行 // $div1.click(function () &#123; // $(this).animate(&#123;'width':'500px'&#125;,1500) // .animate(&#123;'height':300&#125;,1500); // &#125;); $div1.animate(&#123;'width':'500px'&#125;,1500).animate(&#123;'height':300&#125;,1500); $div2.animate(&#123;'width':'300px'&#125;,3000); /** * animate() 方法 * 描述：jq提供了一个用来自定义动画的方法 * 语法：jq.animate（动画结束状态JSON，动画时间 ms，动画线性运动，回掉函数function）; * 说明： * 【1】前面两个参数为必要参数，后面两个参数为可选参数 * 【2】第一个参数格是 json 格式，哪怕只有一个属性 * 局限：jquery 框架中提供的 animate() 方法只能够修改可以量化的属性，对于颜色&amp;display 等无法量化的属性则不能够生效 * 如果想要颜色的改变，则需要引入 jquery.color.js 插件 * 动画顺序 * 原则： * 同步原则:同一个元素如果存在多个 animate 命令，则按照添加顺序执行 * 异步原则:不同元素如果存在多个 animate 命令，则他们同时执行 */&lt;/script&gt;&lt;/body&gt;&lt;/html&gt; 回调函数123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head lang="en"&gt; &lt;meta charset="UTF-8"&gt; &lt;title&gt;lesson1&lt;/title&gt; &lt;style&gt; .div1&#123; width: 100px; height: 100px; background-color: orange; &#125; .div2&#123; width: 100px; height: 100px; background-color: skyblue; border: 1px solid black; &#125; &lt;/style&gt;&lt;/head&gt;&lt;body&gt;&lt;div class="div1"&gt;&lt;/div&gt;&lt;br/&gt;&lt;div class="div2"&gt;&lt;/div&gt;&lt;script src="js/jquery-1.12.3.min.js"&gt;&lt;/script&gt;&lt;script src="js/jquery.color.js"&gt;&lt;/script&gt;&lt;script&gt; var $div1 = $('.div1'); var $div2 = $('.div2'); // 同步执行 // $div1.click(function () &#123; // $(this).animate(&#123;'width':'500px'&#125;,1500) // .animate(&#123;'height':300&#125;,1500); // &#125;); $div1.animate(&#123;'width':'500px'&#125;,1500).animate(&#123;'height':300&#125;,1500,function () &#123; $(this).css('background-color','red'); &#125;); $div2.animate(&#123;'width':'300px'&#125;,3000,function () &#123; $(this).css('background-color','red'); &#125;); /** * animate() 方法 * 描述：jq提供了一个用来自定义动画的方法 * 语法：jq.animate（动画结束状态JSON，动画时间 ms，动画线性运动，回掉函数function）; * 说明： * 【1】前面两个参数为必要参数，后面两个参数为可选参数 * 【2】第一个参数格是 json 格式，哪怕只有一个属性 * 【3】第三个参数如果写出 'linear' ,则表示线性运动，否则为加快在再减速 * 【4】回调函数 * 局限：jquery 框架中提供的 animate() 方法只能够修改可以量化的属性，对于颜色&amp;display 等无法量化的属性则不能够生效 * 如果想要颜色的改变，则需要引入 jquery.color.js 插件 * 动画顺序 * 原则： * 同步原则:同一个元素如果存在多个 animate 命令，则按照添加顺序执行 * 异步原则:不同元素如果存在多个 animate 命令，则他们同时执行 */&lt;/script&gt;&lt;/body&gt;&lt;/html&gt; 匀速运动1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head lang="en"&gt; &lt;meta charset="UTF-8"&gt; &lt;title&gt;lesson1&lt;/title&gt; &lt;style&gt; .div1&#123; width: 100px; height: 100px; background-color: orange; position: absolute; &#125; .div2&#123; width: 100px; height: 100px; background-color: skyblue; border: 1px solid black; position: absolute; top: 150px; &#125; &lt;/style&gt;&lt;/head&gt;&lt;body&gt;&lt;div class="div1"&gt;&lt;/div&gt;&lt;br/&gt;&lt;div class="div2"&gt;&lt;/div&gt;&lt;script src="js/jquery-1.12.3.min.js"&gt;&lt;/script&gt;&lt;script src="js/jquery.color.js"&gt;&lt;/script&gt;&lt;script&gt; var $div1 = $('.div1'); var $div2 = $('.div2'); $div1.animate(&#123;'left':'500px'&#125;,2500,function () &#123; $(this).css('background-color','red'); &#125;); $div2.animate(&#123;'left':'500px'&#125;,2500,'linear',function () &#123; $(this).css('background-color','red'); &#125;); /** * animate() 方法 * 描述：jq提供了一个用来自定义动画的方法 * 语法：jq.animate（动画结束状态JSON，动画时间 ms，动画线性运动，回掉函数function）; * 说明： * 【1】前面两个参数为必要参数，后面两个参数为可选参数 * 【2】第一个参数格是 json 格式，哪怕只有一个属性 * 【3】第三个参数如果写出 'linear' ,则表示线性运动，否则为加快在再减速 * 【4】回调函数 * 局限：jquery 框架中提供的 animate() 方法只能够修改可以量化的属性，对于颜色&amp;display 等无法量化的属性则不能够生效 * 如果想要颜色的改变，则需要引入 jquery.color.js 插件 * 动画顺序 * 原则： * 同步原则:同一个元素如果存在多个 animate 命令，则按照添加顺序执行 * 异步原则:不同元素如果存在多个 animate 命令，则他们同时执行 */&lt;/script&gt;&lt;/body&gt;&lt;/html&gt; stop()方法1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head lang="en"&gt; &lt;meta charset="UTF-8"&gt; &lt;title&gt;lesson2&lt;/title&gt; &lt;style&gt; .div1&#123; width: 100px;height: 100px;background-color: orange;position: absolute; &#125; &lt;/style&gt;&lt;/head&gt;&lt;body&gt;&lt;button&gt;stop()&lt;/button&gt;&lt;button&gt;stop(false,false)&lt;/button&gt;&lt;button&gt;stop(false,true)&lt;/button&gt;&lt;button&gt;stop(true,false)&lt;/button&gt;&lt;button&gt;stop(true,true)&lt;/button&gt;&lt;div class="div1"&gt;&lt;/div&gt;&lt;script src="js/jquery-1.12.3.min.js"&gt;&lt;/script&gt;&lt;script&gt; $div1 = $('.div1'); $btns = $('button'); $btns.eq(0).click(function () &#123; $div1.stop(); &#125;); $btns.eq(1).click(function () &#123; $div1.stop(false,false); &#125;); $btns.eq(2).click(function () &#123; $div1.stop(false,true); &#125;); $btns.eq(3).click(function () &#123; $div1.stop(false,true); &#125;); $btns.eq(4).click(function () &#123; $div1.stop(true,true); &#125;); $div1.animate(&#123;'left':800&#125;,5000) .animate(&#123;'top':400&#125;,5000) .animate(&#123;'left':50&#125;,5000) .animate(&#123;'top':50&#125;,5000); /** * stop() 方法 * 描述：stop() 方法的作用用于停止jq对象的动画 * 语法：jq对象.stop(clearAllAnimation,gotoEnd) * 说明： * 【1】stop 方法的两个语法都是布尔值类型，并且都是可选参数，默认为 false * 【2】第一个参数代表是否清空jq对象动画队列中所有未完成的动画。false表示不清空所有未完成的动画， * 只清空当前正在执行的动画 * 【3】第二个参数代表是否立即完成当前动画（去到动画的终点）。false表示不去终点位置，而是停留在清除瞬间所在的这个位置。 */&lt;/script&gt;&lt;/body&gt;&lt;/html&gt; 动画积累的防止animate()方法中的防止动画积累的策略就是如下的实现方式。 策略1：立即结束当前动画，执行新的命令：$().stop(true).animate(); 策略2：如果当前正在运动，那么不接收新的命令：if($(“div”).is(“:animated”)){ return;} 新的动画命令 百叶窗例子 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head lang="en"&gt; &lt;meta charset="UTF-8"&gt; &lt;title&gt;lesson2&lt;/title&gt; &lt;style&gt; *&#123; margin: 0; padding: 0; &#125; .byc&#123; width: 800px; height: 300px; position: relative; margin: 80px auto; /*把多余的隐藏起来*/ overflow: hidden; &#125; .byc ul&#123; list-style: none; &#125; .byc ul li&#123; /*绝对定位*/ position: absolute; &#125; .li1&#123; left:160px; &#125; .li2&#123; left:320px; &#125; .li3&#123; left:480px; &#125; .li4&#123; left:640px; &#125; .byc ul li .cover&#123; position: absolute; width: 100%; height: 100%; background-color: rgba(0,0,0,0.5); &#125; &lt;/style&gt;&lt;/head&gt;&lt;body&gt;&lt;div class="byc"&gt; &lt;ul&gt; &lt;li class="li0"&gt; &lt;div class="cover"&gt;&lt;/div&gt; &lt;a href="#"&gt;&lt;img src="img/0.jpg" alt=""/&gt;&lt;/a&gt; &lt;/li&gt; &lt;li class="li1"&gt; &lt;div class="cover"&gt;&lt;/div&gt; &lt;a href="#"&gt;&lt;img src="img/1.jpg" alt=""/&gt;&lt;/a&gt; &lt;/li&gt; &lt;li class="li2"&gt; &lt;div class="cover"&gt;&lt;/div&gt; &lt;a href="#"&gt;&lt;img src="img/2.jpg" alt=""/&gt;&lt;/a&gt; &lt;/li&gt; &lt;li class="li3"&gt; &lt;div class="cover"&gt;&lt;/div&gt; &lt;a href="#"&gt;&lt;img src="img/3.jpg" alt=""/&gt;&lt;/a&gt; &lt;/li&gt; &lt;li class="li4"&gt; &lt;div class="cover"&gt;&lt;/div&gt; &lt;a href="#"&gt;&lt;img src="img/4.jpg" alt=""/&gt;&lt;/a&gt; &lt;/li&gt; &lt;/ul&gt;&lt;/div&gt;&lt;script src="js/jquery-1.12.3.min.js"&gt;&lt;/script&gt;&lt;script&gt; var $lis = $('.byc li'); $lis.mouseenter(function () &#123; $(this).find('.cover').stop(true).fadeOut(); &#125;).mouseleave(function () &#123; $(this).find('.cover').stop(true).fadeIn(); $lis.stop(true); $lis.eq(1).animate(&#123;'left':160&#125;,500); $lis.eq(2).animate(&#123;'left':320&#125;,500); $lis.eq(3).animate(&#123;'left':480&#125;,500); $lis.eq(4).animate(&#123;'left':640&#125;,500); &#125;); $lis.eq(0).mouseenter(function () &#123; $lis.stop(true); $lis.eq(1).animate(&#123;'left':560&#125;,500); $lis.eq(2).animate(&#123;'left':620&#125;,500); $lis.eq(3).animate(&#123;'left':680&#125;,500); $lis.eq(4).animate(&#123;'left':740&#125;,500); &#125;); $lis.eq(1).mouseenter(function () &#123; $lis.stop(true); $lis.eq(1).animate(&#123;'left':60&#125;,500); $lis.eq(2).animate(&#123;'left':620&#125;,500); $lis.eq(3).animate(&#123;'left':680&#125;,500); $lis.eq(4).animate(&#123;'left':740&#125;,500); &#125;); $lis.eq(2).mouseenter(function () &#123; $lis.stop(true); $lis.eq(1).animate(&#123;'left':60&#125;,500); $lis.eq(2).animate(&#123;'left':120&#125;,500); $lis.eq(3).animate(&#123;'left':680&#125;,500); $lis.eq(4).animate(&#123;'left':740&#125;,500); &#125;); $lis.eq(3).mouseenter(function () &#123; $lis.stop(true); $lis.eq(1).animate(&#123;'left':60&#125;,500); $lis.eq(2).animate(&#123;'left':120&#125;,500); $lis.eq(3).animate(&#123;'left':180&#125;,500); $lis.eq(4).animate(&#123;'left':740&#125;,500); &#125;); $lis.eq(4).mouseenter(function () &#123; $lis.stop(true); $lis.eq(1).animate(&#123;'left':60&#125;,500); $lis.eq(2).animate(&#123;'left':120&#125;,500); $lis.eq(3).animate(&#123;'left':180&#125;,500); $lis.eq(4).animate(&#123;'left':240&#125;,500); &#125;);&lt;/script&gt;&lt;/body&gt;&lt;/html&gt;]]></content>
      <categories>
        <category>jQuery</category>
      </categories>
      <tags>
        <tag>前端框架</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python 常用的内置（BIF）函数]]></title>
    <url>%2F2018%2F06%2F03%2F%E5%B8%B8%E7%94%A8%E7%9A%84%E5%86%85%E7%BD%AE%E5%87%BD%E6%95%B0%2F</url>
    <content type="text"><![CDATA[Python 常用的内置函数如果你遇到一个需求，且你认为这个需求很普遍，先想想有没有什么内置函数可以使用（BIF）。另外要记住：Python 3 包含 70 多个 BIF ，所以有大量现成的功能等着你来发现。 list()这是一个工厂函数，创建一个新的空列表。 1234import os;aTuole = (123,'xyz','zare','abc');aList = list(aTuole);print("列表元素：",aList); range()range() BIF 迭代固定次数。 可以提供你需要的控制来迭代指定的次数，而且可以用来生成一个从 0 直到（但不包含）某个数的数字列表。 一下是这个 BIF 的用法： 1234import os;# num 是目标标识符，会琢个赋值为 "range()" 生成的各个数字for num in range(4); print(num); F5 运行程序 enumerate()创建成对数据的一个编码列表，从 0 开始 先来做一个对比: 法1: 使用 range() 和 len() 来实现 123import os;aTuole = ('xyz','zare','abc'); print(i,aTuole[i]); 法2:使用enumerate () 来实现 1234import os;aTuole = ('xyz','zare','abc');for index,text in enumerate(aTuole): print(index,text); enumerate会将数组或列表组成一个索引序列。使我们再获取索引和索引内容的时候更加方便。 int()int()函数的作用是将一个数字或base类型的字符串转换成整数。 函数原型 int(x, base=10)，base缺省值为10，也就是说不指定base的值时，函数将x按十进制处理。 注意： x 可以是数字或字符串，但是base被赋值后 x 只能是字符串 x 作为字符串时必须是 base 类型，也就是说 x 变成数字时必须能用 base 进制表示 【1】 x 是数字的情况： 123int(2.345) # 2int(2e2) # 200int(23, 2) # 出错，base 被赋值后函数只接收字符串 【2】x 是字符串的情况： 12int('23', 16) # 35int('HI', 16) # 出错，HI不是个16进制数 【3】 base 可取值范围是 2~36，囊括了所有的英文字母(不区分大小写)，十六进制中F表示15，那么G将在二十进制中表示16，依此类推….Z在三十六进制中表示35 12int('FZ', 16) # 出错，FZ不能用十六进制表示int('FZ', 36) # 575 【4】字符串 0x 可以出现在十六进制中，视作十六进制的符号，同理 0b 可以出现在二进制中，除此之外视作数字 0 和字母 x 123int('0x10', 16) # 16，0x是十六进制的符号int('0x10', 17) # 出错，'0x10'中的 x 被视作英文字母 xint('0x10', 36) # 42804，36进制包含字母 x id()id(object)函数是返回对象object在其生命周期内位于内存中的地址，id函数的参数类型是一个对象。 注意： 我们需要明确一点就是在Python中一切皆对象，变量中存放的是对象的引用。这个确实有点难以理解，“一切皆对象”？对，在Python中确实是这样，包括我们之前经常用到的字符串常量，整型常量都是对象。 1234567import os;print(id(5));print( id('python'));x=2print(id(x));y='hello'print(id(y)); 这段代码的运行结果: 1234567import os;x=2print(id(2));print(id(x)); y='hello'print(id('hello')); print(id(y)); 运行结果: 结果说明:对于这个语句id(2)没有报错，就可以知道2在这里是一个对象。id(x)和id(2)的值是一样的，id(y)和id(‘hello’)的值也是一样的。 12345678x=2;print(id(x));y=2;print(id(y));s='hello';print(id(s));t=s;print(id(t)); 运行结果: 结果说明:id(x)和id(y)的结果是相同的，id(s)和id(t)的结果也是相同的。这说明x和y指向的是同一对象，而t和s也是指向的同一对象。x=2这句让变量x指向了int类型的对象2，而y=2这句执行时，并不重新为2分配空间，而是让y直接指向了已经存在的int类型的对象2.这个很好理解，因为本身只是想给y赋一个值2，而在内存中已经存在了这样一个int类型对象2，所以就直接让y指向了已经存在的对象。这样一来不仅能达到目的，还能节约内存空间。t=s这句变量互相赋值，也相当于是让t指向了已经存在的字符串类型的对象’hello’。 看这幅图就理解了： 1234567891011121314x=2;print(id(2)); print(id(x)); x=3;print(id(3)); print(id(x)); L=[1,2,3];M=L;print(id(L));print(id(M)); print(id(L[2])); L[0]=2;print(id(L)); print(M); 运行结果: 结果分析:两次的id(x)的值不同，这个可能让人有点难以理解。注意，在Python中，单一元素的对象是不允许更改的，比如整型数据、字符串、浮点数等。x=3这句的执行过程并不是先获取x原来指向的对象的地址，再把内存中的值更改为3，而是新申请一段内存来存储对象3，再让x去指向对象3，所以两次id(x)的值不同。然而为何改变了L中的某个子元素的值后，id(L)的值没有发生改变？在Python中，复杂元素的对象是允许更改的，比如列表、字典、元组等。Python中变量存储的是对象的引用，对于列表，其id()值返回的是列表第一个子元素L[0]的存储地址。就像上面的例子，L=[1,2,3]，这里的L有三个子元素L[0]，L[1]，L[2]，L[0]、L[1]、L[2]分别指向对象1、2、3，id(L)值和对象3的存储地址相同. 看下面这个图就明白了: 因为L和M指向的是同一对象，所以在更改了L中子元素的值后，M也相应改变了，但是id(L)值并没有改变，因为这句L[0]=2只是让L[0]重新指向了对象2，而L[0]本身的存储地址并没有发生改变，所以id(L)的值没有改变（ id(L)的值实际等于L[0]本身的存储地址）。 next()next()函数返回迭代器的下一个元素 1234567it = iter([10, 20, 30, 40])while True: try: x = next(it) print(x); # 或者 x = it.next() except StopIteration: break 运行结果:]]></content>
      <categories>
        <category>常用的内置（BIF）函数</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基础面试题]]></title>
    <url>%2F2018%2F06%2F03%2F%E5%9F%BA%E7%A1%80%E9%9D%A2%E8%AF%95%E9%A2%98%2F</url>
    <content type="text"><![CDATA[基础面试题引言：以下各方面知识点的面试题，是为了将要出来工作的小师妹和小师弟而精心整理的。希望对你们都帮助。这些面试题都是很基础的，希望你们能够好好利用起来。有问题，或者不对的地方欢迎给我留言哈！ forward 和redirect的区别forward是服务器请求资源，服务器直接访问目标地址的URL，把那个URL的响应内容读取过来，然后把这些内容再发给浏览器，浏览器根本不知道服务器发送的内容是从哪儿来的，所以它的地址栏中还是原来的地址。 redirect就是服务端根据逻辑,发送一个状态码,告诉浏览器重新去请求那个地址，一般来说浏览器会用刚才请求的所有参数重新请求，所以session,request参数都可以获取。 int 和 Integer 有什么区别Java 提供两种不同的类型：引用类型和原始类型（或内置类型）。Int 是 java 的原始数据类型，Integer 是 java为int提供的封装类。Java为每个原始类型提供了封装类。原始类型封装类，booleanBoolean,charCharacter,byteByte,shortShort,intInteger,longLong,floatFloat,doubleDouble引用类型和原始类型的行为完全不同，并且它们具有不同的语义。引用类型和原始类型具有不同的特征和用法，它们包括：大小和速度问题，这种类型以哪种类型的数据结构存储，当引用类型和原始类型用作某个类的实例数据时所指定的缺省值。对象引用实例变量的缺省值为 null，而原始类型实例变量的缺省值与它们的类型有关 error和exception有什么区别error 表示恢复不是不可能但很困难的情况下的一种严重问题。比如说内存溢出。不可能指望程序能处理这样的情况。exception 表示一种设计或实现问题。也就是说，它表示如果程序运行正常，从不会发生的情况。 最常见到的runtime exception1234567891011121314ArithmeticException, ArrayStoreException, BufferOverflowException, BufferUnderflowException, CannotRedoException, CannotUndoException, ClassCastException, CMMException, ConcurrentModificationException, DOMException, EmptyStackException, IllegalArgumentException, IllegalMonitorStateException, IllegalPathStateException, IllegalStateException, ImagingOpException, IndexOutOfBoundsException, MissingResourceException, NegativeArraySizeException, NoSuchElementException, NullPointerException, ProfileDataException, ProviderException, RasterFormatException, SecurityException, SystemException, UndeclaredThrowableException, UnmodifiableSetException, UnsupportedOperationException Overload和Override区别，Overloaded方法可以改变返回值的类型吗方法的重写Overriding和重载Overloading是Java多态性的不同表现。重写Overriding是父类与子类之间多态性的一种表现，重载Overloading是一个类中多态性的一种表现。如果在子类中定义某方法与其父类有相同的名称和参数，我们说该方法被重写 (Overriding)。子类的对象使用这个方法时，将调用子类中的定义，对它而言，父类中的定义如同被”屏蔽”了。如果在一个类中定义了多个同名的方法，它们或有不同的参数个数或有不同的参数类型，则称为方法的重载(Overloading)。Overloaded的方法是可以改变返回值的类型。 OOP是什么OOP面向对象编程，针对业务处理过程的实体及其属性和行为进行抽象封装，以获得更加清晰高效的逻辑单元划分。 java中有哪些集合，主要方法有哪些主要有LinkedList，ArrayList，Vector等。下面是详细：Collection├List│├LinkedList│├ArrayList│└Vector│ └Stack└SetMap├Hashtable├HashMap└WeakHashMap最常用的集合类是 List 和 Map。 List 的具体实现包括 ArrayList 和 Vector，它们是可变大小的列表，比较适合构建、存储和操作任何类型对象的元素列表。 List 适用于按数值索引访问元素的情形。 Map 提供了一个更通用的元素存储方法。 Map 集合类用于存储元素对（称作“键”和“值”）其中每个键映射到一个值。 List、Map、Set接口，存取元素时各自特点List 以特定次序来持有元素，可有重复元素。Set 无法拥有重复元素,内部排序。Map 保存key-value值，value可多值。 List的遍历： List接口有size()和get()方法，用这两个方法可以实现对List的遍历。size()方法得到List中的元素个数。get()方法取得某个位置上的元素 HashMap与HashTable的区别1、HashMap 是非线程安全的，HashTable 是线程安全的。 2、HashMap 的键和值都允许有 null 值存在，而 HashTable 则不行。 3、因为线程安全的问题，HashMap 效率比 HashTable 的要高。HashMap 的实现机制：维护一个每个元素是一个链表的数组，而且链表中的每个节点是一个 Entry[] 键值对的数据结构。实现了 数组+链表 的特性，查找快，插入删除也快。对于每个 key , 他对应的数组索引下标是 int i = hash(key.hashcode)&amp;(len-1);每个新加入的节点放在链表首，然后该新加入的节点指向原链表首 Hashcode的作用Java中的集合有两类，一类是List，再有一类是Set。前者集合内的元素是有序的，元素可以重复；后者元素无序，但元素不可重复。 equals方法可用于保证元素不重复，但如果每增加一个元素就检查一次，若集合中现在已经有1000个元素，那么第1001个元素加入集合时，就要调用1000次equals方法。这显然会大大降低效率。?于是，Java采用了哈希表的原理。 哈希算法也称为散列算法，是将数据依特定算法直接指定到一个地址上。 这样一来，当集合要添加新的元素时，先调用这个元素的HashCode方法，就一下子能定位到它应该放置的物理位置上。 （1）如果这个位置上没有元素，它就可以直接存储在这个位置上，不用再进行任何比较了。 （2）如果这个位置上已经有元素了，就调用它的equals方法与新元素进行比较，相同的话就不存了。 （3）不相同的话，也就是发生了Hash key相同导致冲突的情况，那么就在这个Hash key的地方产生一个链表，将所有产生相同HashCode的对象放到这个单链表上去，串在一起（很少出现）。 这样一来实际调用equals方法的次数就大大降低了，几乎只需要一两次。 如何理解HashCode的作用： 从Object角度看，JVM每new一个Object，它都会将这个Object丢到一个Hash表中去，这样的话，下次做Object的比较或者取这个对象的时候（读取过程），它会根据对象的HashCode再从Hash表中取这个对象。这样做的目的是提高取对象的效率。若HashCode相同再去调用equal。 HashMap，ConcurrentHashMap与LinkedHashMap的区别ConcurrentHashMap是使用了锁分段技术技术来保证线程安全的，锁分段技术：首先将数据分成一段一段的存储，然后给每一段数据配一把锁，当一个线程占用锁访问其中一个段数据的时候，其他段的数据也能被其他线程访问ConcurrentHashMap 是在每个段（segment）中线程安全的LinkedHashMap维护一个双链表，可以将里面的数据按写入的顺序读出 ConcurrentHashMap应用场景1：ConcurrentHashMap 的应用场景是高并发，但是并不能保证线程安全，而同步的 HashMap 和 HashMap 的是锁住整个容器，而加锁之后 ConcurrentHashMap 不需要锁住整个容器，只需要锁住对应的 Segment 就好了，所以可以保证高并发同步访问，提升了效率。2：可以多线程写。ConcurrentHashMap把HashMap分成若干个Segmenet1.get时，不加锁，先定位到segment然后在找到头结点进行读取操作。而value是volatile变量，所以可以保证在竞争条件时保证读取最新的值，如果读到的value是null，则可能正在修改，那么久调用ReadValueUnderLock函数，加锁保证读到的数据是正确的。 2.Put时会加锁，一律添加到hash链的头部。 3.Remove时也会加锁，由于next是final类型不可改变，所以必须把删除的节点之前的节点都复制一遍。 4.ConcurrentHashMap允许多个修改操作并发进行，其关键在于使用了锁分离技术。它使用了多个锁来控制对Hash表的不同Segment进行的修改。ConcurrentHashMap的应用场景是高并发，但是并不能保证线程安全，而同步的HashMap和HashTable的是锁住整个容器，而加锁之后ConcurrentHashMap不需要锁住整个容器，只需要锁住对应的segment就好了，所以可以保证高并发同步访问，提升了效率。 HashMap的hashcode的作用hashCode的存在主要是用于查找的快捷性，如Hashtable，HashMap等，hashCode是用来在散列存储结构中确定对象的存储地址的。 如果两个对象相同，就是适用于equals(java.lang.Object) 方法，那么这两个对象的hashCode一定要相同。 如果对象的equals方法被重写，那么对象的hashCode也尽量重写，并且产生hashCode使用的对象，一定要和equals方法中使用的一致，否则就会违反上面提到的第2点。 两个对象的hashCode相同，并不一定表示两个对象就相同，也就是不一定适用于equals(java.lang.Object) 方法，只能够说明这两个对象在散列存储结构中，如Hashtable，他们“存放在同一个篮子里”。 什么时候需要重写？ 一般的地方不需要重载hashCode，只有当类需要放在HashTable、HashMap、HashSet等等hash结构的集合时才会重载hashCode，那么为什么要重载hashCode呢？ 要比较两个类的内容属性值，是否相同时候，根据hashCode 重写规则，重写类的 指定字段的hashCode()，equals()方法。 Vector和ArrayList的区别 首先看这两类都实现List接口，而List接口一共有三个实现类，分别是 ArrayList、Vector 和 LinkedList 。List 用于存放多个元素，能够维护元素的次序，并且允许元素的重复。3个具体实现类的相关区别如下： 1.ArrayList是最常用的List实现类，内部是通过数组实现的，它允许对元素进行快速随机访问。数组的缺点是每个元素之间不能有间隔，当数组大小不满足时需要增加存储能力，就要将已经有数组的数据复制到新的存储空间中。当从 ArrayList 的中间位置插入或者删除元素时，需要对数组进行复制、移动、代价比较高。因此，它适合随机查找和遍历，不适合插入和删除。2.Vector与ArrayList一样，也是通过数组实现的，不同的是它支持线程的同步，即某一时刻只有一个线程能够写Vector，避免多线程同时写而引起的不一致性，但实现同步需要很高的花费，因此，访问它比访问ArrayList慢。3.LinkedList是用链表结构存储数据的，很适合数据的动态插入和删除，随机访问和遍历速度比较慢。另外，他还提供了List接口中没有定义的方法，专门用于操作表头和表尾元素，可以当作堆栈、队列和双向队列使用。 ArrayList 与 LinkedList 的区别最明显的区别是 ArrrayList 底层的数据结构是数组，支持随机访问，而 LinkedList 的底层数据结构是链表，不支持随机访问。使用下标访问一个元素，ArrayList 的时间复杂度是 O(1)，而 LinkedList 是 O(n)。1.LinkedList内部存储的是Node，不仅要维护数据域，还要维护prev和next，如果LinkedList中的结点特别多，则LinkedList比ArrayList更占内存。插入删除操作效率：2.LinkedList在做插入和删除操作时，插入或删除头部或尾部时是高效的，操作越靠近中间位置的元素时，需要遍历查找，速度相对慢一些，如果在数据量较大时，每次插入或删除时遍历查找比较费时。所以LinkedList插入与删除，慢在遍历查找，快在只需要更改相关结点的引用地址。ArrayList在做插入和删除操作时，插入或删除尾部时也一样是高效的，操作其他位置，则需要批量移动元素，所以ArrayList插入与删除，快在遍历查找，慢在需要批量移动元素。3.循环遍历效率：由于ArrayList实现了RandomAccess随机访问接口，所以使用for(int i = 0; i &lt; size; i++)遍历会比使用Iterator迭代器来遍历快而由于LinkedList未实现RandomAccess接口，所以推荐使用Iterator迭代器来遍历数据。因此，如果我们需要频繁在列表的中部改变插入或删除元素时，建议使用LinkedList，否则，建议使用ArrayList，因为ArrayList遍历查找元素较快，并且只需存储元素的数据域，不需要额外记录其他数据的位置信息，可以节省内存空间。 Java 中的 LinkedList 是单向链表还是双向链表是双向链表。 String、StringBuffer、StringBuilder之间区别1.三者在执行速度方面的比较：StringBuilder &gt; StringBuffer &gt; String2.在线程方面：StringBuilder是线程非安全的;StringBuffer是线程安全的 3.对于三者的使用：如果要操作少量的数据用 = String；单线程操作字符串缓冲区 下操作大量数据 = StringBuilder；多线程操作字符串缓冲区 下操作大量数据 = StringBuffer； Object 的常用方有哪些clone()、equals()、hashCode()、notify()、notifyAll()、toString()、wait()、finalize() Java序列化的方式(1).Java原生以流的方法进行的序列化 (2).Json序列化 (3).FastJson序列化 (4).Protobuff序列化 传值和传引用的区别，Java是怎么样的，有没有传值引用定义： 传值：传递的是值的副本。方法中对副本的修改，不会影响到调用方 传引用：传递的是引用的副本，共用一个内存，会影响到调用方。此时，形参和实参指向同一个内存地址。对引用副本本身（对象地址）的修改，如设置为null，重新指向其他对象，不会影响到调用方。 总结： 1.基本类型（byte,short,int,long,double,float,char,boolean）为传值 2.对象类型（Object,数组，容器）为传引用 3.String、Integer、Double等immutable类型因为类的变量设为final属性，无法被修改，只能重新赋值或生成对象。当Integer作为方法参数传递时，对其赋值会导致原有的引用被指向了方法内的栈地址，失去原有的的地址指向，所以对赋值后的Integer做任何操作都不会影响原有值。 补充： 值传递和引用传递，属于函数调用时参数的求值策略(Evaluation Strategy)，这是对调用函数时，求值和传值的方式的描述，而非传递的内容的类型（内容指：是值类型还是引用类型，是值还是指针）。值类型/引用类型，是用于区分两种内存分配方式，值类型在调用栈上分配，引用类型在堆上分配。（不要问我引用类型里定义个值类型成员或反之会发生什么，这不在这个本文的讨论范畴内，而且你看完之后，你应该可以自己想明白）。一个描述内存分配方式，一个描述参数求值策略，两者之间无任何依赖或约束关系。 一个ArrayList在循环过程中删除，会不会出问题会，发报出并发修改异常Java.util.ConcurrentModificationException。 错误原因都是ArrayList集合中remove方法底层的源码中有一个fastRemove(index)方法，然后会有一个modCount++的操作，然后在ArratList内部的迭代器中有一个checkForComodification操作，也就是检查modCount是否改变，如果改变了，就抛出并发修改错误。同样的在For each增强for循环中，也是利用了ArrayList自身的Iterator迭代器，也是会出现这样的错误。 对于一般的for遍历，可能并没有删除要修改的数，可以采用倒序删除的写法改正这个错误。对于增强for循环中的遍历，会抛出并发修改异常，使用Iterator自己的remove方法。 要避免这种情况的出现，则在使用迭代器迭代时(显式或for each的隐式)不要使用ArrayList的remove，改用Iterator的remove即可。 @transactional注解在什么情况下会失效1.@Transactional 注解只能应用到 public 可见度的方法上。 如果应用在protected、private或者 package可见度的方法上，也不会报错，不过事务设置不会起作用。 2.默认情况下，Spring会对unchecked异常进行事务回滚；如果是checked异常则不回滚。辣么什么是checked异常，什么是unchecked异常。 3.只读事务：@Transactional(propagation=Propagation.NOT_SUPPORTED,readOnly=true)只读标志只在事务启动时应用，否则即使配置也会被忽略。启动事务会增加线程开销，数据库因共享读取而锁定(具体跟数据库类型和事务隔离级别有关)。通常情况下，仅是读取数据时，不必设置只读事务而增加额外的系统开销。 JVM的内存结构主要分为6个区域： 程序计数器：可看做是当前线程执行的字节码的行号指示器，字节码解释器就是通过改变这个计数器的值来获取下一条需要执行的字节码指令，完成分支、循环、跳转和异常处理等功能。 虚拟机栈：每创建一个线程时，JVM就会为这个线程创建一个对应的栈，所以栈是线程私有的。方法执行的时候还会创建一个栈帧在虚拟机栈上，用于存储局部变量表、操作数栈、动态链接、方法出口等信息。局部变量表所需的空间在编译期间就完成了分配。 本地方法栈：基本同虚拟机栈，只不过本地方法栈是为本地方法服务。 java堆：JVM管理的内存最大的部分，线程共享，用于存放对象实例。java堆可以处于物理上不连续的内存空间上。用-Xms表示堆起始内存大小，-Xmx表示堆最大内存大小。当堆内存大小大于-Xmx时抛出OutOfMemoryError异常。 方法区：存储已被JVM加载的类信息、常量、静态变量、即时编译器编译后的代码等数据。在Hotspot实现中，它位于java堆上。可称为永久代(PermGen)，使用-XX:MaxPermGen参数配置永久代最大内存。 运行时常量池：方法区的一部分。在class文件被JVM加载后，其常量池中的字面量和符号引用将被保存到这里。 补充了解：逃逸分析 常用的对象堆外存储技术需要基于逃逸分析(Escape Analysis)技术实现。其目标就是分析出对象的作用域。比如当一个对象定义在方法体内部时，它的受访范围就在方法体内，jvm会在栈帧中为其分配内存空间。但一旦被外部成员引用后，这个对象就发生了逃逸。 在JDK 6u23后，HotSpot就默认开启了逃逸分析，早期版本可用-XX:+DoEscapeAnalysis参数开启，-XX:+PrintEscapeAnalysis查看。 Mysql 的分页 SQL 语句select * from tablename limit m,n(n是指从第m+1条开始，取n条) Hibernate与MyBatis的异同相同点：Hibernate与MyBatis都可以是通过SessionFactoryBuider由XML配置文件生成SessionFactory，然后由SessionFactory 生成Session，最后由Session来开启执行事务和SQL语句。其中SessionFactoryBuider，SessionFactory，Session的生命周期都是差不多的。Hibernate和MyBatis都支持JDBC和JTA事务处理。Mybatis优势：MyBatis可以进行更为细致的SQL优化，可以减少查询字段。MyBatis容易掌握，而Hibernate门槛较高。Hibernate优势：Hibernate的DAO层开发比MyBatis简单，Mybatis需要维护SQL和结果映射。Hibernate对对象的维护和缓存要比MyBatis好，对增删改查的对象的维护要方便。Hibernate数据库移植性很好，MyBatis的数据库移植性不好，不同的数据库需要写不同SQL。Hibernate有更好的二级缓存机制，可以使用第三方缓存。MyBatis本身提供的缓存机制不佳。 Hibernate与MyBatis在sql优化方面异同Hibernate的查询会将表中的所有字段查询出来，这一点会有性能消耗。Hibernate也可以自己写SQL来指定需要查询的字段，但这样就破坏了Hibernate开发的简洁性。而Mybatis的SQL是手动编写的，所以可以按需求指定查询的字段。Hibernate HQL语句的调优需要将SQL打印出来，而Hibernate的SQL被很多人嫌弃因为太丑了。MyBatis的SQL是自己手动写的所以调整方便。但Hibernate具有自己的日志统计。Mybatis本身不带日志统计，使用Log4j进行日志记录。 Hibernate与MyBatis对象管理对比Hibernate 是完整的对象/关系映射解决方案，它提供了对象状态管理（state management）的功能，使开发者不再需要理会底层数据库系统的细节。也就是说，相对于常见的 JDBC/SQL 持久层方案中需要管理 SQL 语句，Hibernate采用了更自然的面向对象的视角来持久化 Java 应用中的数据。换句话说，使用 Hibernate 的开发者应该总是关注对象的状态（state），不必考虑 SQL 语句的执行。这部分细节已经由 Hibernate 掌管妥当，只有开发者在进行系统性能调优的时候才需要进行了解。而MyBatis在这一块没有文档说明，用户需要对对象自己进行详细的管理。 Jsp九大内置对象1.Request: request对象主要用于客户端请求处理2.Response: response对象提供了多个方法用来处理HTTP响应，可以调用response中的方法修改ContentType中的MIME类型以及实现页面的跳转等等，3.Page: page对象有点类似于Java编程中的this指针，就是指当前JSP页面本身。page是java.lang.Object类的对象。4.Session: session是与请求有关的会话期，它是java.servlet.http.HttpSession类的对象，用来表示和存储当前页面的请求信息。5.Application: application是javax.servlet.ServletContext类对象的一个实例，用于实现用户之间的数据共享6.Out:7.Exception: exception内置对象是用来处理页面出现的异常错误8.Config: config内置对象是ServletConfig类的一个实例。在Servlet初始化的时候，JSP引擎通过config向它传递信息。这种信息可以是属性名/值匹配的参数，也可以是通过ServletContext对象传递的服务器的有关信息。9.pageContext: pageContext对象是一个比较特殊的对象。它相当于页面中所有其他对象功能的最大集成者，即使用它可以访问到本页面中所有其他对象 Comparator 与 Comparable 有什么不同Comparable 接口用于定义对象的自然顺序，而 comparator 通常用于定义用户定制的顺序。Comparable 总是只有一个，但是可以有多个 comparator 来定义对象的顺序。 Collection 和 Collections的区别Collection是集合类的上级接口，继承与他的接口主要有Set 和List.Collections是针对集合类的一个帮助类，他提供一系列静态方法实现对各种集合的搜索、排序、线程安全化等操作。 String s = new String(“xyz”);创建了几个String Object两个对象，一个是“xyx”,一个是指向“xyx”的引用对象s。 线程同步的方法wait():使一个线程处于等待状态，并且释放所持有的对象的lock。sleep():使一个正在运行的线程处于睡眠状态，是一个静态方法，调用此方法要捕捉InterruptedException异常。notify():唤醒一个处于等待状态的线程，注意的是在调用此方法的时候，并不能确切的唤醒某一个等待状态的线程，而是由JVM确定唤醒哪个线程，而且不是按优先级。Allnotity():唤醒所有处入等待状态的线程，注意并不是给所有唤醒线程一个对象的锁，而是让它们竞争。 Volatile和Synchronized四个不同点 粒度不同，前者锁对象和类，后者针对变量 syn阻塞，volatile线程不阻塞 syn保证三大特性，volatile不保证原子性 syn编译器优化，volatile不优化volatile具备两种特性：保证此变量对所有线程的可见性，指一条线程修改了这个变量的值，新值对于其他线程来说是可见的，但并不是多线程安全的。禁止指令重排序优化。Volatile如何保证内存可见性:1.当写一个volatile变量时，JMM会把该线程对应的本地内存中的共享变量刷新到主内存。2.当读一个volatile变量时，JMM会把该线程对应的本地内存置为无效。线程接下来将从主内存中读取共享变量。同步：就是一个任务的完成需要依赖另外一个任务，只有等待被依赖的任务完成后，依赖任务才能完成。异步：不需要等待被依赖的任务完成，只是通知被依赖的任务要完成什么工作，只要自己任务完成了就算完成了，被依赖的任务是否完成会通知回来。（异步的特点就是通知）。打电话和发短信来比喻同步和异步操作。阻塞：CPU停下来等一个慢的操作完成以后，才会接着完成其他的工作。非阻塞：非阻塞就是在这个慢的执行时，CPU去做其他工作，等这个慢的完成后，CPU才会接着完成后续的操作。非阻塞会造成线程切换增加，增加CPU的使用时间能不能补偿系统的切换成本需要考虑。 SpringMVC运行原理 客户端请求提交到DispatcherServlet 由DispatcherServlet控制器查询HandlerMapping，找到并分发到指定的Controller中。 Controller调用业务逻辑处理后，返回ModelAndView DispatcherServlet查询一个或多个ViewResoler视图解析器，找到ModelAndView指定的视图 视图负责将结果显示到客户端 SpringMVC与Struts2区别与比较总结1、Struts2是类级别的拦截， 一个类对应一个request上下文，SpringMVC是方法级别的拦截，一个方法对应一个request上下文，而方法同时又跟一个url对应,所以说从架构本身上SpringMVC就容易实现restful url,而struts2的架构实现起来要费劲，因为Struts2中Action的一个方法可以对应一个url，而其类属性却被所有方法共享，这也就无法用注解或其他方式标识其所属方法了。 2、由上边原因，SpringMVC的方法之间基本上独立的，独享request response数据，请求数据通过参数获取，处理结果通过ModelMap交回给框架，方法之间不共享变量，而Struts2搞的就比较乱，虽然方法之间也是独立的，但其所有Action变量是共享的，这不会影响程序运行，却给我们编码 读程序时带来麻烦，每次来了请求就创建一个Action，一个Action对象对应一个request上下文。3、由于Struts2需要针对每个request进行封装，把request，session等servlet生命周期的变量封装成一个一个Map，供给每个Action使用，并保证线程安全，所以在原则上，是比较耗费内存的。 4、 拦截器实现机制上，Struts2有以自己的interceptor机制，SpringMVC用的是独立的AOP方式，这样导致Struts2的配置文件量还是比SpringMVC大。 5、SpringMVC的入口是servlet，而Struts2是filter（这里要指出，filter和servlet是不同的。以前认为filter是servlet的一种特殊），这就导致了二者的机制不同，这里就牵涉到servlet和filter的区别了。 6、SpringMVC集成了Ajax，使用非常方便，只需一个注解@ResponseBody就可以实现，然后直接返回响应文本即可，而Struts2拦截器集成了Ajax，在Action中处理时一般必须安装插件或者自己写代码集成进去，使用起来也相对不方便。 7、SpringMVC验证支持JSR303，处理起来相对更加灵活方便，而Struts2验证比较繁琐，感觉太烦乱。 8、Spring MVC和Spring是无缝的。从这个项目的管理和安全上也比Struts2高（当然Struts2也可以通过不同的目录结构和相关配置做到SpringMVC一样的效果，但是需要xml配置的地方不少）。 9、 设计思想上，Struts2更加符合OOP的编程思想， SpringMVC就比较谨慎，在servlet上扩展。 10、SpringMVC开发效率和性能高于Struts2。11、SpringMVC可以认为已经100%零配置。 简单总结springMVC和struts2的区别 springmvc的入口是一个servlet即前端控制器，而struts2入口是一个filter过虑器。 springmvc是基于方法开发(一个url对应一个方法)，请求参数传递到方法的形参，可以设计为单例或多例(建议单例)，struts2是基于类开发，传递参数是通过类的属性，只能设计为多例。 Struts采用值栈存储请求和响应的数据，通过OGNL存取数据， springmvc通过参数解析器是将request请求内容解析，并给方法形参赋值，将数据和视图封装成ModelAndView对象，最后又将ModelAndView中的模型数据通过reques域传输到页面。Jsp视图解析器默认使用jstl。 SpringMvc怎么和AJAX相互调用的通过Jackson框架就可以把Java里面的对象直接转化成Js可以识别的Json对象具体步骤如下1.加入Jackson.jar2.在配置文件中配置json的映射3.在接受Ajax方法里面可以直接返回Object,List等,但方法前面要加上@ResponseBody注解 Spring有哪些优点1.轻量级：Spring在大小和透明性方面绝对属于轻量级的，基础版本的Spring框架大约只有2MB。2.控制反转(IOC)：Spring使用控制反转技术实现了松耦合。依赖被注入到对象，而不是创建或寻找依赖对象。3.面向切面编程(AOP)： Spring支持面向切面编程，同时把应用的业务逻辑与系统的服务分离开来。4.容器：Spring包含并管理应用程序对象的配置及生命周期。5.MVC框架：Spring的web框架是一个设计优良的web MVC框架，很好的取代了一些web框架。6.事务管理：Spring对下至本地业务上至全局业务(JAT)提供了统一的事务管理接口。7.异常处理：Spring提供一个方便的API将特定技术的异常(由JDBC, Hibernate, 或JDO抛出)转化为一致的、Unchecked异常。 spring 主要使用了哪些 ，IOC和AOP实现原理是什么spring主要功能有IOC，AOP，MVC等，IOC实现原理：先反射生成实例，然后调用时主动注入。AOP原理：主要使用java动态代理。 解释AOP模块AOP(Aspect Oriented Programming) 面向切面编程，是目前软件开发中的一个热点，是Spring框架内容，利用AOP可以对业务逻辑的各个部分隔离，从而使的业务逻辑各部分的耦合性降低，提高程序的可重用性，踢开开发效率，主要功能：日志记录，性能统计，安全控制，事务处理，异常处理等。 AOP实现原理是java动态代理，但是jdk的动态代理必须实现接口，所以spring的aop是用cglib这个库实现的，cglis使用里asm这个直接操纵字节码的框架，所以可以做到不使用接口的情况下实现动态代理。 AOP与OOP的区别OOP面向对象编程，针对业务处理过程的实体及其属性和行为进行抽象封装，以获得更加清晰高效的逻辑单元划分。而AOP则是针对业务处理过程中的切面进行提取，它所面对的是处理过程的某个步骤或阶段，以获得逻辑过程的中各部分之间低耦合的隔离效果。这两种设计思想在目标上有着本质的差异。 举例： 对于“雇员”这样一个业务实体进行封装，自然是OOP的任务，我们可以建立一个“Employee”类，并将“雇员”相关的属性和行为封装其中。而用AOP 设计思想对“雇员”进行封装则无从谈起。 同样，对于“权限检查”这一动作片段进行划分，则是AOP的目标领域。 OOP面向名次领域，AOP面向动词领域。 总之AOP可以通过预编译方式和运行期动态代理实现在不修改源码的情况下，给程序动态同意添加功能的一项技术。 IoC容器是什么其优点Spring IOC负责创建对象、管理对象(通过依赖注入)、整合对象、配置对象以及管理这些对象的生命周期。优点:IOC或依赖注入减少了应用程序的代码量。它使得应用程序的测试很简单，因为在单元测试中不再需要单例或JNDI查找机制。简单的实现以及较少的干扰机制使得松耦合得以实现。IOC容器支持勤性单例及延迟加载服务。 Spring 的依赖注入方式有哪一些Spring 的依赖注入可以有两种方式来完成:setter 方法注入和构造方法注入。构造器依赖注入：构造器依赖注入在容器触发构造器的时候完成，该构造器有一系列的参数，每个参数代表注入的对象。Setter方法依赖注入：首先容器会触发一个无参构造函数或无参静态工厂方法实例化对象，之后容器调用bean中的setter方法完成Setter方法依赖注入。 Spring支持的事务管理类型Spring支持如下两种方式的事务管理：编程式事务管理：这意味着你可以通过编程的方式管理事务，这种方式带来了很大的灵活性，但很难维护。声明式事务管理：这种方式意味着你可以将事务管理和业务代码分离。你只需要通过注解或者XML配置管理事务。 ThreadLocal(线程变量副本)Synchronized实现内存共享，ThreadLocal为每个线程维护一个本地变量。 采用空间换时间，它用于线程间的数据隔离，为每一个使用该变量的线程提供一个副本，每个线程都可以独立地改变自己的副本，而不会和其他线程的副本冲突。 ThreadLocal类中维护一个Map，用于存储每一个线程的变量副本，Map中元素的键为线程对象，而值为对应线程的变量副本。 ThreadLocal在Spring中发挥着巨大的作用，在管理Request作用域中的Bean、事务管理、任务调度、AOP等模块都出现了它的身影。 Spring中绝大部分Bean都可以声明成Singleton作用域，采用ThreadLocal进行封装，因此有状态的Bean就能够以singleton的方式在多线程中正常工作了。 throw 和 throws 的区别throw 用于抛出 java.lang.Throwable 类的一个实例化对象，意思是说你可以通过关键字 throw 抛出一个 Error 或者 一个Exception，如： throw new IllegalArgumentException(“size must be multiple of 2″) 而throws 的作用是作为方法声明和签名的一部分，方法被抛出相应的异常以便调用者能处理。Java 中，任何未处理的受检查异常强制在 throws 子句中声明。 final关键字的作用final class 表示此类不允许有子类。final virable 表示一个常量。final method 表示一个方法不能被重写 static关键字有哪些作用static 修饰变量、修饰方法;静态块;静态内部类;静态导包; String是最基本的数据类型吗基本数据类型包括byte、int、char、long、float、double、boolean和short。java.lang.String类是final类型的，因此不可以继承这个类、不能修改这个类。为了提高效率节省空间，我们应该用StringBuffer类。 synchronized和java.util.concurrent.locks.Lock的异同主要相同点:Lock 能完成 synchronized 所实现的所有功能.主要不同点:Lock 有比 synchronized 更精确的线程语义和更好的性能(在相同点中回答此点也行)synchronized 会自动释放锁. 而 Lock 一定要求程序员手工释放.并且必须在 finally 从句中释放,如果没有答出在 finally 中释放不得分.就如 Connection 没有在 finally 中关闭一样.连最基本的资源释放都做不好,还谈什么多线程编程. spring的事务有几种它的隔离级别和传播行为声明式事务和编程式事务隔离级别： DEFAULT使用数据库默认的隔离级别 READ_UNCOMMITTED会出现脏读，不可重复读和幻影读问题 READ_COMMITTED会出现重复读和幻影读 REPEATABLE_READ会出现幻影读 SERIALIZABLE最安全，但是代价最大，性能影响极其严重和传播行： REQUIRED存在事务就融入该事务，不存在就创建事务 SUPPORTS存在事务就融入事务，不存在则不创建事务 MANDATORY存在事务则融入该事务，不存在，抛异常 REQUIRES_NEW总是创建新事务 NOT_SUPPORTED存在事务则挂起，一直执行非事务操作 NEVER总是执行非事务，如果当前存在事务则抛异常 NESTED嵌入式事务 sleep() 和 wait() 有什么区别最大区别是等待时wait会释放锁，而sleep会一直持有锁，wait通常用于线程时交互，sleep通常被用于暂停执行。 sleep是线程类（Thread）的方法，导致此线程暂停执行指定时间，给执行机会给其他线程，但是监控状态依然保持，到时后会自动恢复。调用sleep不会释放对象锁。wait是Object类的方法，对此对象调用wait方法导致本线程放弃对象锁，进入等待此对象的等待锁定池，只有针对此对象发出notify方法（或notifyAll）后本线程才进入对象锁定池准备获得对象锁进入运行状态。 多线程和同步有几种实现方法多线程有两种实现方法，分别是继承Thread类与实现Runnable接口同步的实现方面有两种，分别是synchronized,wait与notify 启动一个线程是用run()还是start()启动一个线程是调用start()方法，使线程所代表的虚拟处理机处于可运行状态，这意味着它可以由JVM调度并执行。这并不意味着线程就会立即运行。run()方法可以产生必须退出的标志来停止一个线程。 final,finally,finalize的区别final—修饰符（关键字）如果一个类被声明为final，意味着它不能再派生出新的子类，不能作为父类被继承。因此一个类不能既被声明为 abstract的，又被声明为final的。将变量或方法声明为final，可以保证它们在使用中不被改变。被声明为final的变量必须在声明时给定初值，而在以后的引用中只能读取，不可修改。被声明为final的方法也同样只能使用，不能重载。 finally—再异常处理时提供 finally 块来执行任何清除操作。如果抛出一个异常，那么相匹配的 catch 子句就会执行，然后控制就会进入 finally 块（如果有的话）。 finalize—方法名。Java 技术允许使用 finalize() 方法在垃圾收集器将对象从内存中清除出去之前做必要的清理工作。这个方法是由垃圾收集器在确定这个对象没有被引用时对这个对象调用的。它是在 Object 类中定义的，因此所有的类都继承了它。子类覆盖 finalize() 方法以整理系统资源或者执行其他清理工作。finalize() 方法是在垃圾收集器删除对象之前对这个对象调用的。 abstract class和interface有什么区别抽象类与接口的区别：1.接口可以多重继承 ，抽象类不可以2.接口定义方法，不给实现；而抽象类可以实现部分方法3.接口中基本数据类型的数据成员，都默认为static和final，抽象类则不是如果事先知道某种东西会成为基础类，那么第一个选择就是把它变成一个接口。只有在必须使用方法定义或者成员变量的时候，才应考虑采用抽象类。 Set里的元素不能重复，用==还是equals ()判断Set里的元素是不能重复的，那么用iterator()方法来区分重复与否。equals()是判读两个Set是否相等。equals()和==方法决定引用值是否指向同一对象equals()在类中被覆盖，为的是当两个分离的对象的内容和类型相配的话，返回真值。 struts 框架是如何体现MVC模式struts 框架为开发者提供了MVC 的3个逻辑组成部分，主要由ActionServlet、Action和strust-config.xml配置文件组成控制层，由ActionForm 来承担模型层的功能，而struts 下的视图由JSP来完成。处理请求：由ActionServlet接收请求，然后根据 struts-config.xml 中的配置，类判断由于哪个Action来处理请求和由哪个ActionForm来保存数据，在通过Action的返回值来判断应该由哪个JSP来负责页面的展示，最后由 JSP 来完成结果响应。 Hibernate 的实体存在哪几种状态Hibernate 中的实体在它的生命周期里面，存在 3 中状态。瞬时：new语句创建的实体类对象是就是瞬时状态，它一般没有id。持久：存放在 Session 中的实体对象就属于持久状态，一般通过 save() 或 saveOrUpdate()等等，方法转换而来。托管：实体中Session中脱离出来的时候，它的状态就属于托管状态了，尽管它具有 id 值，但已经不存在Session 中了，即使 实体中的数据发生变化也不能同步到数据库中。通过 close()、evict()等方法转化而来。 Hibernate 的get()和load()的区别Hibernate 对于 load() 方法该方法认为数据一定存在于数据，可以放心的代理来延迟加载，如果在使用过程中发现了问题，只能抛出异常，而get()方法可以不存在。 为什么wait和notify方法要在同步块中调用主要是因为Java API强制要求这样做，如果你不这么做，你的代码会抛出IllegalMonitorStateException异常。还有一个原因是为了避免wait和notify之间产生竞态条件。 线程有几种状态在Java当中，线程通常都有五种状态，创建、就绪、运行、阻塞和死亡。 第一是创建状态。在生成线程对象，并没有调用该对象的start方法，这是线程处于创建状态； 第二是就绪状态。当调用了线程对象的start方法之后，该线程就进入了就绪状态，但是此时线程调度程序还没有把该线程设置为当前线程，此时处于就绪状态。在线程运行之后，从等待或者睡眠中回来之后，也会处于就绪状态 第三是运行状态。线程调度程序将处于就绪状态的线程设置为当前线程，此时线程就进入了运行状态，开始运行run函数当中的代码。 第四是阻塞状态。线程正在运行的时候，被暂停，通常是为了等待某个时间的发生（比如说某项资源就绪）之后再继续运行。sleep,suspend等方法都可以导致线程阻塞。 第五是死亡状态。如果一个线程的run方法执行结束，该线程就会死亡。对于已经死亡的线程，无法再使用start方法令其进入就绪状态。 有A、B、C线程，A线程输出A, B线程输出B, C线程输出C要求, 同时启动线程, 按顺序输出ABC主要通过join方法来实现顺序输出ABC。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970package thread; public class TestThread1 &#123; public static void main(String[] args) &#123; // 线程A final Thread a = new Thread(new Runnable() &#123; @Override public void run() &#123; System.out.println("A"); &#125; &#125;); // 线程B final Thread b = new Thread(new Runnable() &#123; @Override public void run() &#123; try &#123; // 执行b线程之前，加入a线程,让a线程执行 a.join(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println("B"); &#125; &#125;); // 线程C final Thread c = new Thread(new Runnable() &#123; @Override public void run() &#123; try &#123; // 执行c线程之前，加入b线程,让b线程执行 b.join(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println("C"); &#125; &#125;); // 线程D Thread d = new Thread(new Runnable() &#123; @Override public void run() &#123; try &#123; // 执行d线程之前，加入c线程,让c线程执行 c.join(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println("D"); &#125; &#125;); // 启动四个线程 a.start(); b.start(); c.start(); d.start(); &#125; &#125; 什么是ThreadLocal变量ThreadLocal是Java里一种特殊的变量。每个线程都有一个ThreadLocal就是每个线程都拥有了自己独立的一个变量，竞争条件被彻底消除了。它是为创建代价高昂的对象获取线程安全的好方法，比如你可以用ThreadLocal让SimpleDateFormat变成线程安全的，因为那个类创建代价高昂且每次调用都需要创建不同的实例所以不值得在局部范围使用它，如果为每个线程提供一个自己独有的变量拷贝，将大大提高效率。首先，通过复用减少了代价高昂的对象的创建个数。其次，你在没有使用高代价的同步或者不变性的情况下获得了线程安全。线程局部变量的另一个不错的例子是ThreadLocalRandom类，它在多线程环境中减少了创建代价高昂的Random对象的个数。 从使用场景的角度出发来介绍对ReentrantLock的使用，相对来说容易理解一些。 可重入锁的用处及实现原理场景1：如果已加锁，则不再重复加锁 a、忽略重复加锁。b、用在界面交互时点击执行较长时间请求操作时，防止多次点击导致后台重复执行（忽略重复触发）。 以上两种情况多用于进行非重要任务防止重复执行，（如：清除无用临时文件，检查某些资源的可用性，数据备份操作等） 1234567if (lock.tryLock()) &#123; //如果已经被lock，则立即返回false不会等待，达到忽略操作的效果 try &#123; //操作 &#125; finally &#123; lock.unlock(); &#125;&#125; 场景2：如果发现该操作已经在执行，则尝试等待一段时间，等待超时则不执行（尝试等待执行） 这种其实属于场景2的改进，等待获得锁的操作有一个时间的限制，如果超时则放弃执行。用来防止由于资源处理不当长时间占用导致死锁情况（大家都在等待资源，导致线程队列溢出）。 1234567891011try &#123; if (lock.tryLock(5, TimeUnit.SECONDS)) &#123; //如果已经被lock，尝试等待5s，看是否可以获得锁，如果5s后仍然无法获得锁则返回false继续执行 try &#123; //操作 &#125; finally &#123; lock.unlock(); &#125; &#125;&#125; catch (InterruptedException e) &#123; e.printStackTrace(); //当前线程被中断时(interrupt)，会抛InterruptedException &#125; 场景3：如果发现该操作已经加锁，则等待一个一个加锁（同步执行，类似synchronized） 这种比较常见大家也都在用，主要是防止资源使用冲突，保证同一时间内只有一个操作可以使用该资源。但与synchronized的明显区别是性能优势（伴随jvm的优化这个差距在减小）。同时Lock有更灵活的锁定方式，公平锁与不公平锁，而synchronized永远是公平的。 这种情况主要用于对资源的争抢（如：文件操作，同步消息发送，有状态的操作等） ReentrantLock默认情况下为不公平锁 12private ReentrantLock lock = new ReentrantLock(); //参数默认false，不公平锁private ReentrantLock lock = new ReentrantLock(true); //公平锁 123456try &#123; lock.lock(); //如果被其它资源锁定，会在此等待锁释放，达到暂停的效果 //操作&#125; finally &#123; lock.unlock();&#125; 不公平锁与公平锁的区别： 公平情况下，操作会排一个队按顺序执行，来保证执行顺序。（会消耗更多的时间来排队）不公平情况下，是无序状态允许插队，jvm会自动计算如何处理更快速来调度插队。（如果不关心顺序，这个速度会更快） 场景4：可中断锁 synchronized与Lock在默认情况下是不会响应中断(interrupt)操作，会继续执行完。lockInterruptibly()提供了可中断锁来解决此问题。（场景3的另一种改进，没有超时，只能等待中断或执行完毕） 这种情况主要用于取消某些操作对资源的占用。如：（取消正在同步运行的操作，来防止不正常操作长时间占用造成的阻塞） 12345678try &#123; lock.lockInterruptibly(); //操作&#125; catch (InterruptedException e) &#123; e.printStackTrace();&#125; finally &#123; lock.unlock();&#125; 可重入概念:若一个程序或子程序可以“安全的被并行执行(Parallel computing)”，则称其为可重入（reentrant或re-entrant）的。即当该子程序正在运行时，可以再次进入并执行它（并行执行时，个别的执行结果，都符合设计时的预期）。可重入概念是在单线程操作系统的时代提出的。 如何避免死锁Java多线程中的死锁死锁是指两个或两个以上的进程在执行过程中，因争夺资源而造成的一种互相等待的现象，若无外力作用，它们都将无法推进下去。这是一个严重的问题，因为死锁会让你的程序挂起无法完成任务，死锁的发生必须满足以下四个条件：互斥条件：一个资源每次只能被一个进程使用。请求与保持条件：一个进程因请求资源而阻塞时，对已获得的资源保持不放。不剥夺条件：进程已获得的资源，在末使用完之前，不能强行剥夺。循环等待条件：若干进程之间形成一种头尾相接的循环等待资源关系。避免死锁最简单的方法就是阻止循环等待条件，将系统中所有的资源设置标志位、排序，规定所有的进程申请资源必须以一定的顺序（升序或降序）做操作来避免死锁。 Java中活锁和死锁有什么区别这是上题的扩展，活锁和死锁类似，不同之处在于处于活锁的线程或进程的状态是不断改变的，活锁可以认为是一种特殊的饥饿。一个现实的活锁例子是两个人在狭小的走廊碰到，两个人都试着避让对方好让彼此通过，但是因为避让的方向都一样导致最后谁都不能通过走廊。简单的说就是，活锁和死锁的主要区别是前者进程的状态可以改变但是却不能继续执行。 怎么检测一个线程是否拥有锁我一直不知道我们竟然可以检测一个线程是否拥有锁，直到我参加了一次电话面试。在java.lang.Thread中有一个方法叫holdsLock()，它返回true如果当且仅当当前线程拥有某个具体对象的锁。 Maven有哪些优点优点如下：简化了项目依赖管理：易于上手，对于新手可能一个”mvn clean package”命令就可能满足他的工作便于与持续集成工具（jenkins）整合便于项目升级，无论是项目本身升级还是项目使用的依赖升级。有助于多模块项目的开发，一个模块开发好后，发布到仓库，依赖该模块时可以直接从仓库更新，而不用自己去编译。maven有很多插件，便于功能扩展，比如生产站点，自动发布版本等 Maven常见的依赖范围有哪些1.compile:编译依赖，默认的依赖方式，在编译（编译项目和编译测试用例），运行测试用例，运行（项目实际运行）三个阶段都有效，典型地有spring-core等jar。2.test:测试依赖，只在编译测试用例和运行测试用例有效，典型地有JUnit。provided:对于编译和测试有效，不会打包进发布包中，典型的例子为servlet-api,一般的web工程运行时都使用容器的servlet-api。3.runtime:只在运行测试用例和实际运行时有效，典型地是jdbc驱动jar包。4.system: 不从maven仓库获取该jar,而是通过systemPath指定该jar的路径。5.import: 用于一个dependencyManagement对另一个dependencyManagement的继承。 使用“Mvn Clean Package”进行项目打包,其过程执行了哪些动作在这个命令中我们调用了maven的clean周期的clean阶段绑定的插件任务，以及default周期的package阶段绑定的插件任务默认执行的任务有（maven的术语叫goal, 也有人翻译成目标，我这里用任务啦）： maven-clean-plugin:clean-&gt;maven-resources-plugin:resources-&gt;maven-compile-plugin:compile-&gt;mavne-resources-plugin:testResources-&gt;maven-compile-plugin:testCompile-&gt;maven-jar-plugin:jar Maven 多模块如何聚合配置一个打包类型为pom的聚合模块，然后在该pom中使用元素声明要聚合的模块 缓存框架memcache和redis的区别？项目中，怎么去选择？ehcache,memcache和redis等。 区别： Redis和Memcache都是将数据存放在内存中，都是内存数据库。不过memcache还可用于缓存其他东西，例如图片、视频等等。 Redis不仅仅支持简单的k/v类型的数据，同时还提供list，set，hash等数据结构的存储。 虚拟内存–Redis当物理内存用完时，可以将一些很久没用到的value 交换到磁盘。 过期策略–memcache在set时就指定，例如set key1 0 0 8,即永不过期。Redis可以通 过例如expire 设定，例如expire name 10。 分布式–设定memcache集群，利用magent做一主多从;redis可以做一主多从。都 可以一主一从。 存储数据安全–memcache挂掉后，数据没了；redis可以定期保存到磁盘（持久化）。 灾难恢复–memcache挂掉后，数据不可恢复; redis数据丢失后可以通过aof恢复。 Redis支持数据的备份，即master-slave模式的数据备份。 java的原子类，实现原理是什么采用硬件提供原子操作指令实现的，即CAS。每次调用都会先判断预期的值是否符合，才进行写操作，保证数据安全。 数据库性能优化有哪些方法使用explain进行优化，查看sql是否充分使用索引。避免使用in,用exist替代，字段值尽可能使用更小的值，任何对列的操作都将导致表扫描，它包括数据库函数、计算表达式等等，查询时要尽可能将操作移至等号右边。使用连接查询(join)代替子查询。 在表的多列字段上建立一个索引，但只有在查询这些字段的第一个字段时，索引才会被使用。 HTTP请求方法get和post有什么区别 Post传输数据时，不需要在URL中显示出来，而Get方法要在URL中显示。 Post传输的数据量大，可以达到2M，而Get方法由于受到URL长度限制,只能传递大约1024字节. Post就是为了将数据传送到服务器段,Get就是为了从服务器段取得数据.而Get之所以也能传送数据,只是用来设计告诉服务器,你到底需要什么样的数据.Post的信息作为http请求的内容，而Get是在Http头部传输的。 其他 HTTP 请求方法 HEAD 与 GET 相同，但只返回 HTTP 报头，不返回文档主体。 PUT上传指定的 URI 表示。 DELETE 删除指定资源。 OPTIONS 返回服务器支持的 HTTP 方法 CONNECT 把请求连接转换到透明的 TCP/IP 通道。 linux命令，查看某个线程，整个机器负载和文件内容快速查找的命令查看线程：ps -ef|greptomcat 查看负载：top 文件内容查找：vi /aa test.txt 或者先打开文件，再查找: vi test.txt /aa JVM内存的模型，垃圾回收的机制，如何对JVM进行调优由栈和堆组成，栈是运行时单位，堆内存则分为年轻代、年老代、持久代等，年轻代中的对象经过几次的回收，仍然存在则被移到年老代；持久代主要是保存class,method,filed等对象。 sun回收机制：主要对年轻代和年老代中的存活对象进行回收，分为以下： 年轻代串行（Serial Copying）、年轻代并行（ParNew）、年老代串行（SerialMSC），年老代并行（Parallel Mark Sweep），年老代并发（Concurrent Mark-Sweep GC，即CMS）等等,目前CMS回收算法使用最广泛。 JVM调优主要是对堆内容和回收算法进行配置，需要对jdk产生的回收日志进行观察，同时通过工具（Jconsole，jProfile，VisualVM）对堆内存不断分析，这些优化是一个过程，需要不断地进行观察和维护。 高并发时，又如何保证性能和数据正确如果是单机内完成这些操作，那使用数据库的事务，即可轻松实现。 分布式事务如何实现分布式事务可以采用分布式锁进行实现，目前zookeeper就提供此锁；分布式锁需要牺牲一定性能去实现，若业务支付最终一致性，那此方法是最佳方案。如在京东下订单，过一会才会告诉你订单审核通过，而不是马上响应订单结果。 抽象类和接口的区别，项目中如何使用它们 相同点： 两者都是抽象类，都不能实例化。 interface实现类及abstractclass的子类都必须要实现已经声明的抽象方法。 不同点： interface需要实现，要用implements，而abstractclass需要继承，要用extends。 一个类可以实现多个interface，但一个类只能继承一个abstractclass。 interface强调特定功能的实现，而abstractclass强调所属关系。 尽管interface实现类及abstrctclass的子类都必须要实现相应的抽象方法，但实现的形式不同。interface中的每一个方法都是抽象方法，都只是声明的 (declaration, 没有方法体)，实现类必须要实现。而abstractclass的子类可以有选择地实现。 使用： abstract：在既需要统一的接口，又需要实例变量或缺省的方法的情况下，使用abstract; interface：类与类之前需要特定的接口进行协调，而不在乎其如何实现。 作为能够实现特定功能的标识存在，也可以是什么接口方法都没有的纯粹标识。需要将一组类视为单一的类，而调用者只通过接口来与这组类发生联系。需要实现特定的多项功能，而这些功能之间可能完全没有任何联系。 TCP通讯有几次握手，有使用过哪些socket框架​ 3次握手，客户端–&gt;服务端，服务端–&gt;客户端，客户端–&gt;服务端，当这些过程完成之后，才真正建立起通信。java中比较有名的socket框架有：mina,netty,都是韩国小棒子写的。 反射的作用与原理反射概念：Java 反射是可以让我们在运行时，通过一个类的Class对象来获取它获取类的方法、属性、父类、接口等类的内部信息的机制。这种动态获取信息以及动态调用对象的方法的功能称为JAVA的反射。 作用：在任意一个方法里， 1.如果我知道一个类的名称/或者它的一个实例对象， 我就能把这个类的所有方法和变量的信息找出来(方法名，变量名，方法，修饰符，类型，方法参数等等所有信息) 2.如果我还明确知道这个类里某个变量的名称，我还能得到这个变量当前的值。 3.当然，如果我明确知道这个类里的某个方法名+参数个数类型，我还能通过传递参数来运行那个类里的那个方法。 反射机制主要提供了以下功能： 在运行时判断任意一个对象所属的类。 在运行时构造任意一个类的对象。 在运行时判断任意一个类所具有的成员变量和方法。 在运行时调用任意一个对象的方法。 生成动态代理。 反射的原理：JAVA语言编译之后会生成一个.class文件，反射就是通过字节码文件找到某一个类、类中的方法以及属性等。 反射的实现API：反射的实现主要借助以下四个类： 1234Class：类的对象Constructor：类的构造方法Field：类中的属性对象Method：类中的方法对象 java反射机制，反射生成类，可否访问私有变量即动态生成java的实例，可以。 Java反射机制是一个非常强大的功能，在很多的项目比如Spring，Mybatis都都可以看到反射的身影。通过反射机制，我们可以在运行期间获取对象的类型信息。利用这一点我们可以实现工厂模式和代理模式等设计模式，同时也可以解决java泛型擦除等令人苦恼的问题。 获取一个对象对应的反射类，在Java中有三种方法可以获取一个对象的反射类， 通过getClass()方法 通过Class.forName()方法 使用类.class 通过类加载器实现，getClassLoader() RPC是什么，有使用过哪些RPC框架​ 远程进程调用，本地机器调用远程的服务，在项目规模大到一定程度，需要使用RPC相关框架进行服务化部署。如：hessian 、webservice等 jquery如何绑定页面某元素的点击事件​ $(“#btn”).click(function(){ …. }) volatile实现原理volatile如何保证可见性和禁止指令重排序的： 观察加入volatile关键字和没有加入volatile关键字时所生成的汇编代码发现，加入volatile关键字时，会多出一个lock前缀指令，lock前缀指令实际上相当于一个 内存屏障（也成内存栅栏），内存屏障会提供3个功能： 它 确保指令重排序时不会把其后面的指令排到内存屏障之前的位置，也不会把前面的指令排到内存屏障的后面；即在执行到内存屏障这句指令时，在它前面的操作已经全部完成； 它会 强制将对缓存的修改操作立即写入主存； 如果是写操作，它会导致其他CPU中对应的缓存行无效。 session 与 cookie 区别 cookie数据存放在客户的浏览器上，session数据放在服务器上。 cookie不是很安全，别人可以分析存放在本地的COOKIE并进行COOKIE欺骗考虑到安全应当使用session。 session会在一定时间内保存在服务器上。当访问增多，会比较占用你服务器的性能考虑到减轻服务器性能方面，应当使用COOKIE。 单个cookie保存的数据不能超过4K，很多浏览器都限制一个站点最多保存20个cookie。 所以个人建议： 将登陆信息等重要信息存放为SESSION 其他信息如果需要保留，可以放在COOKIE中 session 分布式处理第一种：粘性session 粘性Session是指将用户锁定到某一个服务器上，比如上面说的例子，用户第一次请求时，负载均衡器将用户的请求转发到了A服务器上，如果负载均衡器设置了粘性Session的话，那么用户以后的每次请求都会转发到A服务器上，相当于把用户和A服务器粘到了一块，这就是粘性Session机制。 第二种：服务器session复制 原理：任何一个服务器上的session发生改变（增删改），该节点会把这个 session的所有内容序列化，然后广播给所有其它节点，不管其他服务器需不需要session，以此来保证Session同步。 第三种：session共享机制 使用分布式缓存方案比如memcached、Redis，但是要求Memcached或Redis必须是集群。 原理：不同的 tomcat指定访问不同的主memcached。多个Memcached之间信息是同步的，能主从备份和高可用。用户访问时首先在tomcat中创建session，然后将session复制一份放到它对应的memcahed上 第四种：session持久化到数据库 原理：就不用多说了吧，拿出一个数据库，专门用来存储session信息。保证session的持久化。 优点：服务器出现问题，session不会丢失 缺点：如果网站的访问量很大，把session存储到数据库中，会对数据库造成很大压力，还需要增加额外的开销维护数据库。 第五种terracotta实现session复制 原理：就不用多说了吧，拿出一个数据库，专门用来存储session信息。保证session的持久化。 优点：服务器出现问题，session不会丢失 缺点：如果网站的访问量很大，把session存储到数据库中，会对数据库造成很大压力，还需要增加额外的开销维护数据库。 说说自定义注解的场景及实现跟踪代码的依赖性，实现代替配置文件的功能。比较常见的是Spring等框架中的基于注解配置。 还可以生成文档常见的@See@param@return等。如@override放在方法签名，如果这个方法 并不是覆盖了超类方法，则编译时就能检查出。 使用@interface自定义注解时，自动继承了java.lang.annotation.Annotation接口，由编译程序自动完成其他细节，在定义注解时，不能继承其他注解或接口。 HashSet 和 HashMap 区别 HashSet： HashSet实现了Set接口，它不允许集合中出现重复元素。当我们提到HashSet时，第一件事就是在将对象存储在 HashSet之前，要确保重写hashCode（）方法和equals（）方法，这样才能比较对象的值是否相等，确保集合中没有储存相同的对象。如果不重写上述两个方法，那么将使用下面方法默认实现： public boolean add(Object obj)方法用在Set添加元素时，如果元素值重复时返回 “false”，如果添加成功则返回”true” HashMap： HashMap实现了Map接口，Map接口对键值对进行映射。Map中不允许出现重复的键（Key）。Map接口有两个基本的实现TreeMap和HashMap。TreeMap保存了对象的排列次序，而HashMap不能。HashMap可以有空的键值对（Key（null）-Value（null））HashMap是非线程安全的（非Synchronize），要想实现线程安全，那么需要调用collections类的静态方法synchronizeMap（）实现。 public Object put(Object Key,Object value)方法用来将元素添加到map中。 总结： HashMap 实现了 Map 接口；存储键值对；调用put（）向map中添加元素；HashMap使用键（Key）计算Hashcode；HashMap相对于HashSet较快，因为它是使用唯一的键获取对象。 HashSet 实现了 Set 接口；仅存储对象；调用add（）方法向Set中添加元素；HashSet使用成员对象来计算hashcode值，对于两个对象来说hashcode可能相同，所以equals()方法用来判断对象的相等性，如果两个对象不同的话，那么返回false；HashSet较HashMap来说比较慢。 HashMap 的工作原理及代码实现 HashMap是基于哈希表的Map接口的非同步实现，允许使用null值和null键，但不保证映射的顺序。 底层使用数组实现，数组中每一项是个单向链表，即数组和链表的结合体；当链表长度大于一定阈值时，链表转换为红黑树，这样减少链表查询时间。 HashMap在底层将key-value当成一个整体进行处理，这个整体就是一个Node对象。HashMap底层采用一个Node[]数组来保存所有的key-value对，当需要存储一个Node对象时，会根据key的hash算法来决定其在数组中的存储位置，在根据equals方法决定其在该数组位置上的链表中的存储位置；当需要取出一个Node时，也会根据key的hash算法找到其在数组中的存储位置，再根据equals方法从该位置上的链表中取出该Node。 HashMap进行数组扩容需要重新计算扩容后每个元素在数组中的位置，很耗性能 采用了Fail-Fast机制，通过一个modCount值记录修改次数，对HashMap内容的修改都将增加这个值。迭代器初始化过程中会将这个值赋给迭代器的expectedModCount，在迭代过程中，判断modCount跟expectedModCount是否相等，如果不相等就表示已经有其他线程修改了Map，马上抛出异常 ConcurrentHashMap 的工作原理及代码实现 ConcurrentHashMap允许多个修改操作并发进行，其关键在于使用了锁分离技术。 它使用了多个锁来控制对hash表的不同段进行的修改，每个段其实就是一个小的hashtable，它们有自己的锁。只要多个并发发生在不同的段上，它们就可以并发进行。 ConcurrentHashMap在底层将key-value当成一个整体进行处理，这个整体就是一个Entry对象。Hashtable底层采用一个Entry[]数组来保存所有的key-value对，当需要存储一个Entry对象时，会根据key的hash算法来决定其在数组中的存储位置，在根据equals方法决定其在该数组位置上的链表中的存储位置；当需要取出一个Entry时，也会根据key的hash算法找到其在数组中的存储位置，再根据equals方法从该位置上的链表中取出该Entry。 与HashMap不同的是，ConcurrentHashMap使用多个子Hash表，也就是段(Segment) ConcurrentHashMap完全允许多个读操作并发进行，读操作并不需要加锁。如果使用传统的技术，如HashMap中的实现，如果允许可以在hash链的中间添加或删除元素，读操作不加锁将得到不一致的数据。ConcurrentHashMap实现技术是保证HashEntry几乎是不可变的。 ConcurrentHashMap能完全替代HashTable吗HashTable虽然性能上不如ConcurrentHashMap，但并不能完全被取代，两者的迭代器的一致性不同的，HashTable的迭代器是强一致性的，而ConcurrentHashMap是弱一致的。ConcurrentHashMap的get，clear，iterator 都是弱一致性的。 Doug Lea 也将这个判断留给用户自己决定是否使用ConcurrentHashMap。 什么是强一致性和弱一致性get方法是弱一致的，是什么含义？可能你期望往ConcurrentHashMap底层数据结构中加入一个元素后，立马能对get可见，但ConcurrentHashMap并不能如你所愿。换句话说，put操作将一个元素加入到底层数据结构后，get可能在某段时间内还看不到这个元素，若不考虑内存模型，单从代码逻辑上来看，却是应该可以看得到的。 下面将结合代码和java内存模型相关内容来分析下put/get方法。put方法我们只需关注Segment#put，get方法只需关注Segment#get，在继续之前，先要说明一下Segment里有两个volatile变量：count和table；HashEntry里有一个volatile变量：value。 总结：ConcurrentHashMap的弱一致性主要是为了提升效率，是一致性与效率之间的一种权衡。要成为强一致性，就得到处使用锁，甚至是全局锁，这就与Hashtable和同步的HashMap一样了。 ThreadLocal 原理分析ThreadLocal 为解决多线程程序的并发问题提供了一种新的思路。使用这个工具类可以很简洁地编写出优美的多线程程序。当使用 ThreadLocal 维护变量时，ThreadLocal 为每个使用该变量的线程提供独立的变量副本，所以每一个线程都可以独立地改变自己的副本，而不会影响其它线程所对应的副本。 每个线程中都保有一个 ThreadLocalMap 的成员变量，ThreadLocalMap 内部采用 WeakReference 数组保存，数组的key即为 ThreadLocal 内部的Hash值。 创建线程的方式及实现Java使用Thread类代表线程，所有的线程对象都必须是Thread类或其子类的实例。Java可以用三种方式来创建线程，如下所示： 继承Thread类创建线程 实现Runnable接口创建线程 使用Callable和Future创建线程 继承Thread类创建线程 通过继承Thread类来创建并启动多线程的一般步骤如下 1】d定义Thread类的子类，并重写该类的run()方法，该方法的方法体就是线程需要完成的任务，run()方法也称为线程执行体。 2】创建Thread子类的实例，也就是创建了线程对象 3】启动线程，即调用线程的start()方法 代码实例 1234567891011public class MyThread extends Thread&#123;//继承Thread类 public void run()&#123; //重写run方法 &#125;&#125;public class Main &#123; public static void main(String[] args)&#123; new MyThread().start();//创建并启动线程 &#125;&#125; 实现Runnable接口创建线程 通过实现Runnable接口创建并启动线程一般步骤如下： 1】定义Runnable接口的实现类，一样要重写run()方法，这个run（）方法和Thread中的run()方法一样是线程的执行体 2】创建Runnable实现类的实例，并用这个实例作为Thread的target来创建Thread对象，这个Thread对象才是真正的线程对象 3】第三部依然是通过调用线程对象的start()方法来启动线程 代码实例： 123456789101112131415public class MyThread2 implements Runnable &#123;//实现Runnable接口 public void run()&#123; //重写run方法 &#125;&#125;public class Main &#123; public static void main(String[] args)&#123; //创建并启动线程 MyThread2 myThread=new MyThread2(); Thread thread=new Thread(myThread); thread().start(); //或者 new Thread(new MyThread2()).start(); &#125;&#125; 使用Callable和Future创建线程 和Runnable接口不一样，Callable接口提供了一个call（）方法作为线程执行体，call()方法比run()方法**功能要强大。 》call()方法可以有返回值 》call()方法可以声明抛出异常 Java5提供了Future接口来代表Callable接口里call()方法的返回值，并且为Future接口提供了一个实现类FutureTask，这个实现类既实现了Future接口，还实现了Runnable接口，因此可以作为Thread类的target。在Future接口里定义了几个公共方法来控制它关联的Callable任务。 >boolean cancel(boolean mayInterruptIfRunning)：视图取消该Future里面关联的Callable任务 >V get()：返回Callable里call（）方法的返回值，调用这个方法会导致程序阻塞，必须等到子线程结束后才会得到返回值 >V get(long timeout,TimeUnit unit)：返回Callable里call（）方法的返回值，最多阻塞timeout时间，经过指定时间没有返回抛出TimeoutException >boolean isDone()：若Callable任务完成，返回True >boolean isCancelled()：如果在Callable任务正常完成前被取消，返回True 介绍了相关的概念之后，创建并启动有返回值的线程的步骤如下： 1】创建Callable接口的实现类，并实现call()方法，然后创建该实现类的实例（从java8开始可以直接使用Lambda表达式创建Callable对象）。 2】使用FutureTask类来包装Callable对象，该FutureTask对象封装了Callable对象的call()方法的返回值 3】使用FutureTask对象作为Thread对象的target创建并启动线程（因为FutureTask实现了Runnable接口） 4】调用FutureTask对象的get()方法来获得子线程执行结束后的返回值 代码实例： 123456789101112131415161718192021222324252627282930313233public class Main &#123; public static void main(String[] args)&#123; MyThread3 th=new MyThread3(); //使用Lambda表达式创建Callable对象 //使用FutureTask类来包装Callable对象 FutureTask&lt;Integer&gt; future=new FutureTask&lt;Integer&gt;( (Callable&lt;Integer&gt;)()-&gt;&#123; return 5; &#125; ); new Thread(task,"有返回值的线程").start();//实质上还是以Callable对象来创建并启动线程 try&#123; System.out.println("子线程的返回值："+future.get()); //get()方法会阻塞，直到子线程执行结束才返回 &#125;catch(Exception e)&#123; ex.printStackTrace(); &#125; &#125;&#125; ————————————–三种创建线程方法对比————————————– 实现Runnable和实现Callable接口的方式基本相同，不过是后者执行call()方法有返回值，后者线程执行体run()方法无返回值，因此可以把这两种方式归为一种这种方式与继承Thread类的方法之间的差别如下： 1、线程只是实现Runnable或实现Callable接口，还可以继承其他类。 2、这种方式下，多个线程可以共享一个target对象，非常适合多线程处理同一份资源的情形。 3、但是编程稍微复杂，如果需要访问当前线程，必须调用Thread.currentThread()方法。 4、继承Thread类的线程类不能再继承其他父类（Java单继承决定）。 注：一般推荐采用实现接口的方式来创建多线程 sleep() 、join（）、yield（）有什么区别sleep():方法导致了程序暂停执行指定的时间，让出cpu给其他线程，但是他的监控状态依然保持者，当指定的时间到了又会自动恢复运行状态，但不会释放“锁标志”，不推荐使用。 wait():在其他线程调用对象的notify或notifyAll方法前，导致当前线程等待。线程会释放掉它所占有的“锁标志”，从而使别的线程有机会抢占该锁。 yield():暂停当前正在执行的线程对象。yield()只是使当前线程重新回到可执行状态，所以执行yield()的线程有可能在进入到可执行状态后马上又被执行。yield()只能使同优先级或更高优先级的线程有执行的机会。 join():等待调用join方法的线程结束，再继续执行。 sleep是针对于thread对象，wait是针对于Object对象。 ConcurrentHashMap如何保证线程安全JDK 1.7及以前： ConcurrentHashMap允许多个修改操作并发进行，其关键在于使用了锁分离技术。它使用了多个锁来控制对hash表的不同部分进行的修改。ConcurrentHashMap内部使用段(Segment)来表示这些不同的部分，每个段其实就是一个小的hash table，它们有自己的锁。只要多个修改操作发生在不同的段上，它们就可以并发进行。 JDK 1.8： Segment虽保留，但已经简化属性，仅仅是为了兼容旧版本。 插入时使用CAS算法：unsafe.compareAndSwapInt(this, valueOffset, expect, update)。 CAS(Compare And Swap)意思是如果valueOffset位置包含的值与expect值相同，则更新valueOffset位置的值为update，并返回true，否则不更新，返回false。插入时不允许key或value为null 与Java8的HashMap有相通之处，底层依然由“数组”+链表+红黑树； 底层结构存放的是TreeBin对象，而不是TreeNode对象； CAS作为知名无锁算法，那ConcurrentHashMap就没用锁了么？当然不是，当hash值与链表的头结点相同还是会synchronized上锁，锁链表。 new与newInstance()的区别 new是一个关键字，它是调用new指令创建一个对象，然后调用构造方法来初始化这个对象，可以使用带参数的构造器 newInstance()是Class的一个方法，在这个过程中，是先取了这个类的不带参数的构造器Constructor，然后调用构造器的newInstance方法来创建对象。 Class.newInstance不能带参数，如果要带参数需要取得对应的构造器，然后调用该构造器的Constructor.newInstance(Object … initargs)方法 JDK中用到的设计模式 装饰模式：java.io 单例模式：Runtime类 简单工厂模式：Integer.valueOf方法 享元模式：String常量池、Integer.valueOf(int i)、Character.valueOf(char c) 迭代器模式：Iterator 职责链模式：ClassLoader的双亲委派模型 解释器模式：正则表达式java.util.regex.Pattern hashCode() &amp;&amp; equals()hashcode() 返回该对象的哈希码值，支持该方法是为哈希表提供一些优点，例如，java.util.Hashtable 提供的哈希表。 在 Java 应用程序执行期间，在同一对象上多次调用 hashCode 方法时，必须一致地返回相同的整数，前提是对象上 equals 比较中所用的信息没有被修改（equals默认返回对象地址是否相等）。如果根据 equals(Object)方法，两个对象是相等的，那么在两个对象中的每个对象上调用 hashCode 方法都必须生成相同的整数结果。 以下情况不是必需的：如果根据 equals(java.lang.Object) 方法，两个对象不相等，那么在两个对象中的任一对象上调用 hashCode 方法必定会生成不同的整数结果。但是，程序员应该知道，为不相等的对象生成不同整数结果可以提高哈希表的性能。 实际上，由 Object 类定义的 hashCode 方法确实会针对不同的对象返回不同的整数。（这一般是通过将该对象的内部地址转换成一个整数来实现的，但是 JavaTM 编程语言不需要这种实现技巧I。） hashCode的存在主要是用于查找的快捷性，如 Hashtable，HashMap等，hashCode 是用来在散列存储结构中确定对象的存储地址的； 如果两个对象相同，就是适用于 equals(java.lang.Object) 方法，那么这两个对象的 hashCode 一定要相同； 如果对象的 equals 方法被重写，那么对象的 hashCode 也尽量重写，并且产生 hashCode使用的对象，一定要和 equals 方法中使用的一致，否则就会违反上面提到的第2点； 两个对象的hashCode相同，并不一定表示两个对象就相同，也就是不一定适用于equals(java.lang.Object) 方法，只能够说明这两个对象在散列存储结构中，如Hashtable，他们“存放在同一个篮子里”。 Object类的finalize方法的实现原理Object 类提供的实现不Finalize方法和垃圾回收器将派生的类型不标记Object终止除非它们将覆盖Finalize方法。 如果类型未重写Finalize方法，则垃圾回收器会将类型的每个实例的条目添加到调用终止队列中的内部结构。 终止队列中包含垃圾回收器才能回收其内存之前，必须运行其终止代码托管堆中的所有对象的条目。 然后，垃圾回收器调用Finalize在以下情况下自动的方法︰ 垃圾回收器发现，一个对象不可访问，除非您通过调用从终止豁免已对象后 GC.SuppressFinalize 方法。 在关闭应用程序域中，除非该对象是免于终止的对象。 在关闭期间，终止甚至仍是可访问的对象。 Finalize将自动调用一次在给定实例中，除非的对象重新注册通过使用一种机制，如GC.ReRegisterForFinalize和GC.SuppressFinalize尚未随后调用方法。 Finalize操作具有以下限制︰ 终结器执行时的确切时间不确定。 若要确保确定性释放资源，对你的类的实例实现Close方法，或者提供IDisposable.Dispose实现。 两个对象的终结器不保证任何特定顺序运行即使另一个对象引用。 也就是说，如果对象 A 具有对对象 B 的引用，并且二者的终结器，对象 B 可能已经被终结的对象 A 终结器启动时。 终结器运行的线程未指定。 Finalize方法可能无法运行完成，或可能根本不运行下列异常情况下︰ 如果另一个终结器会无限期阻止 （进入无限循环，尝试获取的锁，它可以永远不会获取，等等）。 运行时尝试运行终结器来完成，因为其他终结器可能不会调用终结器块如果无限期。 如果不提供机会清理的运行时，进程将终止。 在这种情况下，运行时的第一个通知的进程是终止的一个 DLL_PROCESS_DETACH 通知。 运行时将继续完成在关闭过程的对象，仅当可终结对象数目继续减少。 如果Finalize或的重写Finalize引发异常，并且运行时不承载的应用程序将替代默认策略，运行时终止进程，且无活动try/finally执行块或终结器。如果终结器无法释放或销毁资源，则此行为确保处理完整性。 实施者注意事项 应重写Finalize类使用非托管的资源，如文件句柄或数据库必须在垃圾回收期间放弃使用它们的托管的对象时释放的连接。 Lock是Java 5以后引入的新的API，和关键字synchronized相比主要相同点：Lock 能完成synchronized所实现的所有功能；主要不同点：Lock有比synchronized更精确的线程语义和更好的性能，而且不强制性的要求一定要获得锁。synchronized会自动释放锁，而Lock一定要求程序员手工释放，并且最好在finally 块中释放（这是释放外部资源的最好的地方）。 CAS 乐观锁CAS是通过unsafe类的compareAndSwap方法实现的；方法参数作用，第一个参数是要修改的对象，第二个参数是对象中要修改变量的偏移量，第三个参数是修改之前的值，第四个参数是预想修改后的值；CAS指令有缺点，存在ABA问题。 ABA 问题就是一个变量V，如果变量V初次读取的时候是A，并且在准备赋值的时候检查到它仍然是A，那能说明它的值没有被其他线程修改过了吗？如果在这段期间它的值曾经被改成了B，然后又改回A，那CAS操作就会误认为它从来没有被修改过。解决：针对这种情况，java并发包中提供了一个带有标记的原子引类”AtomicStampedReference”，它可以通过控制变量值的版本来保证CAS的正确性。 CAS有什么缺陷，该如何解决CAS虽然很高效的解决原子操作，但是CAS仍然存在三大问题。ABA问题，循环时间长开销大和只能保证一个共享变量的原子操作 ABA问题。因为CAS需要在操作值的时候检查下值有没有发生变化，如果没有发生变化则更新，但是如果一个值原来是A，变成了B，又变成了A，那么使用CAS进行检查时会发现它的值没有发生变化，但是实际上却变化了。ABA问题的解决思路就是使用版本号。在变量前面追加上版本号，每次变量更新的时候把版本号加一，那么A－B－A 就会变成1A-2B－3A。从Java1.5开始JDK的atomic包里提供了一个类AtomicStampedReference来解决ABA问题。这个类的compareAndSet方法作用是首先检查当前引用是否等于预期引用，并且当前标志是否等于预期标志，如果全部相等，则以原子方式将该引用和该标志的值设置为给定的更新值 循环时间长开销大。自旋CAS如果长时间不成功，会给CPU带来非常大的执行开销。如果JVM能支持处理器提供的pause指令那么效率会有一定的提升，pause指令有两个作用，第一它可以延迟流水线执行指令（de-pipeline）,使CPU不会消耗过多的执行资源，延迟的时间取决于具体实现的版本，在一些处理器上延迟时间是零。第二它可以避免在退出循环的时候因内存顺序冲突（memory order violation）而引起CPU流水线被清空（CPU pipeline flush），从而提高CPU的执行效率。 只能保证一个共享变量的原子操作。当对一个共享变量执行操作时，我们可以使用循环CAS的方式来保证原子操作，但是对多个共享变量操作时，循环CAS就无法保证操作的原子性，这个时候就可以用锁，或者有一个取巧的办法，就是把多个共享变量合并成一个共享变量来操作。比如有两个共享变量i＝2,j=a，合并一下ij=2a，然后用CAS来操作ij。从Java1.5开始JDK提供了AtomicReference类来保证引用对象之间的原子性，你可以把多个变量放在一个对象里来进行CAS操作。 乐观锁的业务场景及实现方式1.我们经常会在访问数据库的时候用到锁，怎么实现乐观锁和悲观锁呢？以Hibernate为例，可以通过为记录添加版本或时间戳字段来实现乐观锁。可以用session.Lock()锁定对象来实现悲观锁（本质上就是执行了SELECT * FROM t FOR UPDATE语句）。 2.如果把乐观锁看作是关于冲突检测的，那么悲观锁就是关于冲突避免的。在实际应用的源代码控制系统中， 这两种策略都可以被使用，但是现在大多数源代码开发者更倾向于使用乐观锁策略。（有一种很有道理的说法：乐观锁并不是真正的锁定，但是这种叫法很方便并且广泛流传，以至于不容忽略。） 在乐观锁和悲观锁之间进行选择的标准是：冲突的频率与严重性。如果冲突很少，或者冲突的后果不会很严重，那么通常情况下应该选择乐观锁，因为它能得到更好的并发性，而且更容易实现。但是，如果冲突的结果对于用户来说痛苦的，那么就需要使用悲观策略。 访问修饰符public,private,protected,以及不写时的区别 修饰符 当前类 同 包 子 类 其他包 public √ √ √ √ protected √ √ √ × default √ √ × × private √ × × × 类的成员不写访问修饰时默认为default。默认对于同一个包中的其他类相当于公开（public），对于不是同一个包中的其他类相当于私有（private）。受保护（protected）对子类相当于公开，对不是同一包中的没有父子关系的类相当于私有。Java中，外部类的修饰符只能是public或默认，类的成员（包括内部类）的修饰符可以是以上四种。 String 是不是最基本的数据类型不是。Java中的基本数据类型只有8个：byte、short、int、long、float、double、char、boolean；除了基本类型（primitive type），剩下的都是引用类型（reference type），Java 5以后引入的枚举类型也算是一种比较特殊的引用类型。 short s1 = 1; s1 = s1 + 1;有错吗?short s1 = 1; s1 += 1;有错吗对于short s1 = 1; s1 = s1 + 1;由于1是int类型，因此s1+1运算结果也是int 型，需要强制转换类型才能赋值给short型可修改为s1 =(short)(s1 + 1)。而short s1 = 1; s1 += 1;可以正确编译，因为s1+= 1;相当于s1 = (short)(s1 + 1);其中有隐含的强制类型转换。 是否可以继承String类String 类是final类，不可以被继承。 补充：继承String本身就是一个错误的行为，对String类型最好的重用方式是关联关系（Has-A）和依赖关系（Use-A）而不是继承关系（Is-A） JVM加载class文件的原理机制JVM中类的装载是由类加载器（ClassLoader）和它的子类来实现的，Java中的类加载器是一个重要的Java运行时系统组件，它负责在运行时查找和装入类文件中的类。由于Java的跨平台性，经过编译的Java源程序并不是一个可执行程序，而是一个或多个类文件。当Java程序需要使用某个类时，JVM会确保这个类已经被加载、连接（验证、准备和解析）和初始化。类的加载是指把类的.class文件中的数据读入到内存中，通常是创建一个字节数组读入.class文件，然后产生与所加载类对应的Class对象。加载完成后，Class对象还不完整，所以此时的类还不可用。当类被加载后就进入连接阶段，这一阶段包括验证、准备（为静态变量分配内存并设置默认的初始值）和解析（将符号引用替换为直接引用）三个步骤。最后JVM对类进行初始化，包括：1)如果类存在直接的父类并且这个类还没有被初始化，那么就先初始化父类；2)如果类中存在初始化语句，就依次执行这些初始化语句。类的加载是由类加载器完成的，类加载器包括：根加载器（BootStrap）、扩展加载器（Extension）、系统加载器（System）和用户自定义类加载器（java.lang.ClassLoader的子类）。从Java 2（JDK 1.2）开始，类加载过程采取了父亲委托机制（PDM）。PDM更好的保证了Java平台的安全性，在该机制中，JVM自带的Bootstrap是根加载器，其他的加载器都有且仅有一个父类加载器。类的加载首先请求父类加载器加载，父类加载器无能为力时才由其子类加载器自行加载。JVM不会向Java程序提供对Bootstrap的引用。下面是关于几个类加载器的说明： Bootstrap：一般用本地代码实现，负责加载JVM基础核心类库（rt.jar）； Extension：从java.ext.dirs系统属性所指定的目录中加载类库，它的父加载器是Bootstrap； System：又叫应用类加载器，其父类是Extension。它是应用最广泛的类加载器。它从环境变量classpath或者系统属性java.class.path所指定的目录中记载类，是用户自定义加载器的默认父加载器。 静态嵌套类(Static Nested Class)和内部类（Inner Class）的不同Static Nested Class是被声明为静态（static）的内部类，它可以不依赖于外部类实例被实例化。而通常的内部类需要在外部类实例化后才能实例化，其语法看起来挺诡异的，如下所示。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475/** * 扑克类（一副扑克） * @author 骆昊 * */public class Poker &#123; private static String[] suites = &#123;"黑桃", "红桃", "草花", "方块"&#125;; private static int[] faces = &#123;1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13&#125;; private Card[] cards; /** * 构造器 * */ public Poker() &#123; cards = new Card[52]; for(int i = 0; i &lt; suites.length; i++) &#123; for(int j = 0; j &lt; faces.length; j++) &#123; cards[i * 13 + j] = new Card(suites[i], faces[j]); &#125; &#125; &#125; /** * 洗牌 （随机乱序） * */ public void shuffle() &#123; for(int i = 0, len = cards.length; i &lt; len; i++) &#123; int index = (int) (Math.random() * len); Card temp = cards[index]; cards[index] = cards[i]; cards[i] = temp; &#125; &#125; /** * 发牌 * @param index 发牌的位置 * */ public Card deal(int index) &#123; return cards[index]; &#125; /** * 卡片类（一张扑克） * [内部类] * @author 骆昊 * */ public class Card &#123; private String suite; // 花色 private int face; // 点数 public Card(String suite, int face) &#123; this.suite = suite; this.face = face; &#125; @Override public String toString() &#123; String faceStr = ""; switch(face) &#123; case 1: faceStr = "A"; break; case 11: faceStr = "J"; break; case 12: faceStr = "Q"; break; case 13: faceStr = "K"; break; default: faceStr = String.valueOf(face); &#125; return suite + faceStr; &#125; &#125;&#125; 测试代码： 1234567891011121314class PokerTest &#123; public static void main(String[] args) &#123; Poker poker = new Poker(); poker.shuffle(); // 洗牌 Poker.Card c1 = poker.deal(0); // 发第一张牌 // 对于非静态内部类Card // 只有通过其外部类Poker对象才能创建Card对象 Poker.Card c2 = poker.new Card("红心", 1); // 自己创建一张牌 System.out.println(c1); // 洗牌后的第一张 System.out.println(c2); // 打印: 红心A &#125;&#125; GC是什么？为什么要有GC？GC是垃圾收集的意思，内存处理是编程人员容易出现问题的地方，忘记或者错误的内存回收会导致程序或系统的不稳定甚至崩溃，Java提供的GC功能可以自动监测对象是否超过作用域从而达到自动回收内存的目的，Java语言没有提供释放已分配内存的显示操作方法。Java程序员不用担心内存管理，因为垃圾收集器会自动进行管理。要请求垃圾收集，可以调用下面的方法之一：System.gc() 或Runtime.getRuntime().gc() ，但JVM可以屏蔽掉显示的垃圾回收调用。垃圾回收可以有效的防止内存泄露，有效的使用可以使用的内存。垃圾回收器通常是作为一个单独的低优先级的线程运行，不可预知的情况下对内存堆中已经死亡的或者长时间没有使用的对象进行清除和回收，程序员不能实时的调用垃圾回收器对某个对象或所有对象进行垃圾回收。在Java诞生初期，垃圾回收是Java最大的亮点之一，因为服务器端的编程需要有效的防止内存泄露问题，然而时过境迁，如今Java的垃圾回收机制已经成为被诟病的东西。移动智能终端用户通常觉得iOS的系统比Android系统有更好的用户体验，其中一个深层次的原因就在于Android系统中垃圾回收的不可预知性。 补充：垃圾回收机制有很多种，包括：分代复制垃圾回收、标记垃圾回收、增量垃圾回收等方式。标准的Java进程既有栈又有堆。栈保存了原始型局部变量，堆保存了要创建的对象。Java平台对堆内存回收和再利用的基本算法被称为标记和清除，但是Java对其进行了改进，采用“分代式垃圾收集”。这种方法会跟Java对象的生命周期将堆内存划分为不同的区域，在垃圾收集过程中，可能会将对象移动到不同区域： 伊甸园（Eden）：这是对象最初诞生的区域，并且对大多数对象来说，这里是它们唯一存在过的区域。 幸存者乐园（Survivor）：从伊甸园幸存下来的对象会被挪到这里。 终身颐养园（Tenured）：这是足够老的幸存对象的归宿。年轻代收集（Minor-GC）过程是不会触及这个地方的。当年轻代收集不能把对象放进终身颐养园时，就会触发一次完全收集（Major-GC），这里可能还会牵扯到压缩，以便为大对象腾出足够的空间。 String s = new String(“xyz”);创建了几个字符串对象两个对象，一个是静态区的”xyz”，一个是用new创建在堆上的对象。 如何实现字符串的反转及替换方法很多，可以自己写实现也可以使用String或StringBuffer/StringBuilder中的方法。有一道很常见的面试题是用递归实现字符串反转，代码如下所示： public static String reverse(String originStr) {​ if(originStr == null || originStr.length() &lt;= 1)​ return originStr;​ return reverse(originStr.substring(1)) + originStr.charAt(0);​ } List、Set、Map是否继承自Collection接口List、Set 是，Map 不是。Map是键值对映射容器，与List和Set有明显的区别，而Set存储的零散的元素且不允许有重复元素（数学中的集合也是如此），List是线性结构的容器，适用于按数值索引访问元素的情形。 List、Map、Set三个接口存取元素时，各有什么特点List以特定索引来存取元素，可以有重复元素。Set不能存放重复元素（用对象的equals()方法来区分元素是否重复）。Map保存键值对（key-value pair）映射，映射关系可以是一对一或多对一。Set和Map容器都有基于哈希存储和排序树的两种实现版本，基于哈希存储的版本理论存取时间复杂度为O(1)，而基于排序树版本的实现在插入或删除元素时会按照元素或元素的键（key）构成排序树从而达到排序和去重的效果。 当一个线程进入一个对象的synchronized方法A之后，其它线程是否可进入此对象的synchronized方法B不能。其它线程只能访问该对象的非同步方法，同步方法则不能进入。因为非静态方法上的synchronized修饰符要求执行方法时要获得对象的锁，如果已经进入A方法说明对象锁已经被取走，那么试图进入B方法的线程就只能在等锁池（注意不是等待池哦）中等待对象的锁。 线程同步以及线程调度相关的方法 wait()：使一个线程处于等待（阻塞）状态，并且释放所持有的对象的锁； sleep()：使一个正在运行的线程处于睡眠状态，是一个静态方法，调用此方法要处理InterruptedException异常； notify()：唤醒一个处于等待状态的线程，当然在调用此方法的时候，并不能确切的唤醒某一个等待状态的线程，而是由JVM确定唤醒哪个线程，而且与优先级无关； notityAll()：唤醒所有处于等待状态的线程，该方法并不是将对象的锁给所有线程，而是让它们竞争，只有获得锁的线程才能进入就绪状态； 启动一个线程是调用run()还是start()方法启动一个线程是调用start()方法，使线程所代表的虚拟处理机处于可运行状态，这意味着它可以由JVM 调度并执行，这并不意味着线程就会立即运行。run()方法是线程启动后要进行回调（callback）的方法。 Java中有几种类型的流字节流和字符流。字节流继承于InputStream、OutputStream，字符流继承于Reader、Writer。在java.io 包中还有许多其他的流，主要是为了提高性能和使用方便。关于Java的I/O需要注意的有两点：一是两种对称性（输入和输出的对称性，字节和字符的对称性）；二是两种设计模式（适配器模式和装潢模式）。另外Java中的流不同于C#的是它只有一个维度一个方向。 在项目中哪些地方用到了XMLXML的主要作用有两个方面：数据交换和信息配置。在做数据交换时，XML将数据用标签组装成起来，然后压缩打包加密后通过网络传送给接收者，接收解密与解压缩后再从XML文件中还原相关信息进行处理，XML曾经是异构系统间交换数据的事实标准，但此项功能几乎已经被JSON（JavaScript Object Notation）取而代之。当然，目前很多软件仍然使用XML来存储配置信息，我们在很多项目中通常也会将作为配置信息的硬代码写在XML文件中，Java的很多框架也是这么做的，而且这些框架都选择了dom4j作为处理XML的工具，因为Sun公司的官方API实在不怎么好用。 vector、ArrayList、LinkedList 的区别是什么vector是同步的，arraylist和linkedlist不是同步的。底层方面，vector与arraylist都是基于object[]array实现的，但考虑vector线程安全，所以arraylist效率上回比vector较快。元素随机访问上，vector与arraylist是基本相同的，时间复杂度是O(1)，linkedlist的随机访问元素的复杂度为O(n)。但在插入删除数据上，linkedlist则比arraylist要快很多。linkedlist比arraylist更占内存，因为linkedlist每个节点上还要存储对前后两个节点的引用。 NIO和传统的IO有什么区别IO是面向流的，NIO是面向块（缓冲区）的。 IO面向流的操作一次一个字节地处理数据。一个输入流产生一个字节的数据，一个输出流消费一个字节的数据。，导致了数据的读取和写入效率不佳。 NIO面向块的操作在一步中产生或者消费一个数据块。按块处理数据比按(流式的)字节处理数据要快得多，同时数据读取到一个它稍后处理的缓冲区，需要时可在缓冲区中前后移动。这就增加了处理过程中的灵活性。通俗来说，NIO采取了“预读”的方式，当你读取某一部分数据时，他就会猜测你下一步可能会读取的数据而预先缓冲下来。 IO是阻塞的，NIO是非阻塞的 对于传统的IO，当一个线程调用read() 或 write()时，该线程被阻塞，直到有一些数据被读取，或数据完全写入。该线程在此期间不能再干任何事情了。 而对于NIO，使用一个线程发送读取数据请求，没有得到响应之前，线程是空闲的，此时线程可以去执行别的任务，而不是像IO中那样只能等待响应完成。 NIO和IO适用场景 NIO是为弥补传统IO的不足而诞生的，但是尺有所短寸有所长，NIO也有缺点，因为NIO是面向缓冲区的操作，每一次的数据处理都是对缓冲区进行的，那么就会有一个问题，在数据处理之前必须要判断缓冲区的数据是否完整或者已经读取完毕，如果没有，假设数据只读取了一部分，那么对不完整的数据处理没有任何意义。所以每次数据处理之前都要检测缓冲区数据。 那么NIO和IO各适用的场景是什么呢？ 如果需要管理同时打开的成千上万个连接，这些连接每次只是发送少量的数据，例如聊天服务器，这时候用NIO处理数据可能是个很好的选择。 而如果只有少量的连接，而这些连接每次要发送大量的数据，这时候传统的IO更合适。使用哪种处理数据，需要在数据的响应等待时间和检查缓冲区数据的时间上作比较来权衡选择。 通俗解释，最后，对于NIO和传统IO 有一个网友讲的生动的例子： 以前的流总是堵塞的，一个线程只要对它进行操作，其它操作就会被堵塞，也就相当于水管没有阀门，你伸手接水的时候，不管水到了没有，你就都只能耗在接水（流）上。 nio的Channel的加入，相当于增加了水龙头（有阀门），虽然一个时刻也只能接一个水管的水，但依赖轮换策略，在水量不大的时候，各个水管里流出来的水，都可以得到妥 善接纳，这个关键之处就是增加了一个接水工，也就是Selector，他负责协调，也就是看哪根水管有水了的话，在当前水管的水接到一定程度的时候，就切换一下：临时关上当 前水龙头，试着打开另一个水龙头（看看有没有水）。 当其他人需要用水的时候，不是直接去接水，而是事前提了一个水桶给接水工，这个水桶就是Buffer。也就是，其他人虽然也可能要等，但不会在现场等，而是回家等，可以做 其它事去，水接满了，接水工会通知他们。 这其实也是非常接近当前社会分工细化的现实，也是统分利用现有资源达到并发效果的一种很经济的手段，而不是动不动就来个并行处理，虽然那样是最简单的，但也是最浪费资源的方式 如何通过反射创建对象 方法1：通过类对象调用newInstance()方法，例如：String.class.newInstance() 方法2：通过类对象的getConstructor()或getDeclaredConstructor()方法获得构造器（Constructor）对象并调用其newInstance()方法创建对象，例如：String.class.getConstructor(String.class).newInstance(“Hello”); Statement与PreparedStatement的区别,什么是SQL注入，如何防止SQL注入使用PreparedStatement可以提升代码的可读性和可维护性，可以尽最大可能提高性能。因为Statement每次执行一个SQL命令都会对其编译，但PreparedStatement则只编译一次。PreparedStatement就类似于流水线生产。另一方面PreparedStatement可以极大提高安全性：它对传递过来的参数进行了强制参数类型转换，确保插入或查询数据时，与底层数据库格式匹配。SQL注入：就是通过将sql命令插入到web表单递交或输入域名或页面请求的查询字符串，最终达到欺骗服务器执行恶意SQL命令。如sql命令：select id from test where name=’1’ or 1=1; drop table test,但用PreparedStatement就可以避免这种问题。 用Java写一个单例类饿汉式单例: public class Singleton {​ private Singleton(){}​ private static Singleton instance = new Singleton();​ public static Singleton getInstance(){​ return instance;​ }} 懒汉式单例: public class Singleton {​ private static Singleton instance = null;​ private Singleton() {}​ public static synchronized Singleton getInstance(){​ if (instance == null) instance ＝ new Singleton();​ return instance;​ }} 用Java写一个冒泡排序冒泡排序几乎是个程序员都写得出来，但是面试的时候如何写一个逼格高的冒泡排序却不是每个人都能做到 12345678910111213141516171819202122import java.util.Comparator;/** * 排序器接口(策略模式: 将算法封装到具有共同接口的独立的类中使得它们可以相互替换) * @author骆昊 * */public interface Sorter &#123; /** * 排序 * @param list 待排序的数组 */ public &lt;T extends Comparable&lt;T&gt;&gt; void sort(T[] list); /** * 排序 * @param list 待排序的数组 * @param comp 比较两个对象的比较器 */ public &lt;T&gt; void sort(T[] list, Comparator&lt;T&gt; comp);&#125; 123456789101112131415161718192021222324252627282930313233343536373839404142import java.util.Comparator;/** * 冒泡排序 * * @author骆昊 * */public class BubbleSorter implements Sorter &#123; @Override public &lt;T extends Comparable&lt;T&gt;&gt; void sort(T[] list) &#123; boolean swapped = true; for (int i = 1, len = list.length; i &lt; len &amp;&amp; swapped; ++i) &#123; swapped = false; for (int j = 0; j &lt; len - i; ++j) &#123; if (list[j].compareTo(list[j + 1]) &gt; 0) &#123; T temp = list[j]; list[j] = list[j + 1]; list[j + 1] = temp; swapped = true; &#125; &#125; &#125; &#125; @Override public &lt;T&gt; void sort(T[] list, Comparator&lt;T&gt; comp) &#123; boolean swapped = true; for (int i = 1, len = list.length; i &lt; len &amp;&amp; swapped; ++i) &#123; swapped = false; for (int j = 0; j &lt; len - i; ++j) &#123; if (comp.compare(list[j], list[j + 1]) &gt; 0) &#123; T temp = list[j]; list[j] = list[j + 1]; list[j + 1] = temp; swapped = true; &#125; &#125; &#125; &#125;&#125; char型变量中能不能存贮一个中文汉字char型变量是用来存储Unicode编码的字符的，unicode编码字符集中包含了汉字，所以，char型变量中当然可以存储汉字啦。不过，如果某个特殊的汉字没有被包含在unicode编码字符集中，那么，这个char型变量中就不能存储这个特殊汉字。补充说明：unicode编码占用两个字节，所以，char类型的变量也是占用两个字节。 用最有效率的方法算出2乘以8等于几2&lt;&lt; 3，(左移三位)因为将一个数左移n位，就相当于乘以了2的n次方，那么，一个数乘以8只要将其左移3位即可，而位运算cpu直接支持的，效率最高，所以，2乘以8等於几的最效率的方法是2&lt;&lt; 3。 静态变量和实例变量的区别在语法定义上的区别：静态变量前要加 static 关键字，而实例变量前则不加。 在程序运行时的区别：实例变量属于某个对象的属性，必须创建了实例对象，其中的实例变量才会被分配空间，才能使用这个实例变量。静态变量不属于某个实例对象，而是属于类，所以也称为类变量，只要程序加载了类的字节码，不用创建任何实例对象，静态变量就会被分配空间，静态变量就可以被使用了。总之，实例变量必须创建对象后才可以通过这个对象来使用，静态变量则可以直接使用类名来引用。 例如，对于下面的程序，无论创建多少个实例对象，永远都只分配了一个staticVar变量，并且每创建一个实例对象，这个staticVar就会加1；但是，每创建一个实例对象，就会分配一个instanceVar，即可能分配多个instanceVar，并且每个instanceVar的值都只自加了1次。 1234567891011121314151617public class VariantTest&#123; publicstatic int staticVar = 0; publicint instanceVar = 0; publicVariantTest()&#123; staticVar++; instanceVar++; System.out.println(staticVar +instanceVar); &#125;&#125; switch语句能否作用在 byte 、 long 和String 上在switch（e）中，e只能是一个整数表达式或者枚举常量（更大字体），整数表达式可以是int基本类型或Integer包装类型，由于byte,short,char都可以隐含转换为int，所以，这些类型以及这些类型的包装类型也是可以的。显然，long和String类型都不符合switch的语法规定，并且不能被隐式转换成int类型，所以，它们不能作用于swtich语句中。 switch语句能否作用在String上说错了，Java1.7之后已经支持这种写法了！ 如何跳出当前的多重嵌套循环在Java中，要想跳出多重循环，可以在外面的循环语句前定义一个标号，然后在里层循环体的代码中使用带有标号的break语句，即可跳出外层循环。 例如： 123456for(int i=0;i&lt;10;i++)&#123; for(intj=0;j&lt;10;j++)&#123; System.out.println(“i=” + i + “,j=” + j); if(j == 5) break ok; &#125;&#125; 另外，我个人通常并不使用标号这种方式，而是让外层的循环条件表达式的结果可以受到里层循环体代码的控制，例如，要在二维数组中查找到某个数字。 123456789101112131415161718192021int arr[][] =&#123;&#123;1,2,3&#125;,&#123;4,5,6,7&#125;,&#123;9&#125;&#125;;boolean found = false;for(int i=0;i&lt;arr.length&amp;&amp;!found;i++) &#123; for(intj=0;j&lt;arr[i].length;j++)&#123; System.out.println(“i=” + i + “,j=” + j); if(arr[i][j] ==5) &#123; found =true; break; &#125; &#125;&#125; 说说&amp;和&amp;&amp;的区别 &amp;和&amp;&amp;都可以用作逻辑与的运算符，表示逻辑与（and），当运算符两边的表达式的结果都为true时，整个运算结果才为true，否则，只要有一方为false，则结果为false。 ​ &amp;&amp;还具有短路的功能，即如果第一个表达式为false，则不再计算第二个表达式，例如，对于if(str!= null&amp;&amp; !str.equals(s))表达式，当str为null时，后面的表达式不会执行，所以不会出现NullPointerException如果将&amp;&amp;改为&amp;，则会抛出NullPointerException异常。If(x==33 &amp;++y&gt;0) y会增长，If(x==33 &amp;&amp; ++y&gt;0)不会增长 ​ &amp;还可以用作位运算符，当&amp;操作符两边的表达式不是boolean类型时，&amp;表示按位与操作，我们通常使用0x0f来与一个整数进行&amp;运算，来获取该整数的最低4个bit位，例如，0x31 &amp; 0x0f的结果为0x01。 一个”.java”源文件中是否可以包括多个类（不是内部类）有什么限制 可以有多个类，但只能有一个public的类，并且public的类名必须与文件名相一致。 可以从一个static方法内部发出对非static方法的调用吗不可以。因为非static方法是要与对象关联在一起的，必须创建一个对象后，才可以在该对象上进行方法调用，而static方法调用时不需要创建对象，可以直接调用。也就是说，当一个static方法被调用时，可能还没有创建任何实例对象，如果从一个static方法中发出对非static方法的调用，那个非static方法是关联到哪个对象上的呢？这个逻辑无法成立，所以，一个static方法内部发出对非static方法的调用。 Hibernate中怎样实现类之间的关系 类与类之间的关系主要体现在表与表之间的关系进行操作，它们都是对对象进行操作，我们在程序中把所有的表与类都映射在一起，它们通过配置文件中的many-to-one、one-to-many、many-to-many进行操作。 Hibernate中的update()和saveOrUpdate()的区别saveOrUpdate()： ​ 1、如果对象已经在本session中持久化了，不做任何事 ​ 2、如果另一个与本session关联的对象拥有相同的持久化标识(identifier)，抛出一个异常 ​ 3、如果对象没有持久化标识(identifier)属性，对其调用save() ​ 4、如果对象的持久标识(identifier)表明其是一个新实例化的对象，对其调用save() ​ 5、如果对象是附带版本信息的（通过或 ）并且版本属性的值表明其是一个新实例化的 对象，调用save()。否则update() 这个对象。 update() ：是将一个游离状态的实体对象直接更新。 Hibernate的缓存机制 一级缓存：内部缓存存在Hibernate中，属于应用事物级缓存。 二级缓存：应用级缓存、 分布式缓存。使用场景：数据不会被第三方修改、数据大小在可接受范围、数据更新频率低、同一数据被系统频繁使用、非关键数据 引入第三方缓存（如ehcache等）。 如何优化Hibernate1.使用双向一对多关联，不使用单向一对多 2.灵活使用单向一对多关联 3.不用一对一，用多对一取代 4.配置对象缓存，不使用集合缓存 5.一对多集合使用Bag,多对多集合使用Set 6.继承类使用显式多态 7.表字段要少，表关联不要怕多，有二级缓存撑腰 hibernate的延迟加载和openSessionInView延迟加载要在session范围内，用到的时候再加载； opensessioninview是在web层写了一个filter来打开和关闭session，这样就表示在一次request过程中session一直开着，保证了延迟加载在session中的这个前提。 Mysql 优化1.如果明确知道只有一条结果返回，limit1能够提高效率 2.把计算放在业务层而不是数据库层，除了节省数据的 CPU ,还有意想不到的查询缓存优化效果。 3.强制类型转换会全表扫描 4.在属性上进行计算不能命中索引 5.使用 ENUM 而不是字符串 6.数据分区度不大的字段不宜使用索引 7.负向查询和前导模糊查询不能使用索引 8.用TRUNCATE替代DELETE 9.删除重复记录 10.用Where子句替换HAVING子句 11.用EXISTS替代IN、用NOT EXISTS替代NOT IN 12.用索引提高效率 13.用EXISTS替换DISTINCT 14.用&gt;=替代&gt; 15.用IN来替换OR Mysql 的交集、差集、并集只有并集没有交集差集的关键字。 1.并集 1234-- UNION 不包含重复数据-- UNION ALL 包含重复数据SELECT NAME FROM a UNIONSELECT NAME FROM b; 2.差集 找出在a表中存在的id 但是在b表中不存在的id 1234567-- 利用 unionSELECT ID FROM (-- 并集SELECT DISTINCT a.id AS ID FROM a UNION ALLSELECT DISTINCT B.ID AS ID FROM b)TEMP GROUP BY ID HAVING COUNT(ID) = 1; 12-- 子查询 not inSELECT id FROM a WHERE id NOT IN (SELECT id FROM b); 12-- 子查询 not existsSELECT id FROM a WHERE NOT EXISTS (SELECT id FROM b WHERE a.id = b.id); 12-- 左连接判断右表IS NULLSELECT a.id FROM a LEFT JOIN b ON a.id = b.id WHERE b.id IS NULL ORDER BY a.id 3.交集 INTERSECT 123456SELECT ID FROM (-- 并集 SELECT DISTINCT a.id AS ID FROM a UNION ALLSELECT DISTINCT B.ID AS ID FROM b)TEMP GROUP BY ID HAVING COUNT(ID) != 1; Java内存模型是什么 Java内存模型规定和指引Java程序在不同的内存架构、CPU和操作系统间有确定性地行为。它在多线程的情况下尤其重要。Java内存模型对一个线程所做的变动能被其它线程可见提供了保证，它们之间是先行发生关系。这个关系定义了一些规则让程序员在并发编程时思路更清晰。比如，先行发生关系确保了： ​ 线程内的代码能够按先后顺序执行，这被称为程序次序规则。 ​ 对于同一个锁，一个解锁操作一定要发生在时间上后发生的另一个锁定操作之前，也叫做管程锁定规则。 ​ 前一个对volatile的写操作在后一个volatile的读操作之前，也叫volatile变量规则。 ​ 一个线程内的任何操作必需在这个线程的start()调用之后，也叫作线程启动规则。 ​ 一个线程的所有操作都会在线程终止之前，线程终止规则。 ​ 一个对象的终结操作必需在这个对象构造完成之后，也叫对象终结规则。 可传递性 Thread接口和Runnable接口的区别 可以避免由于Java的单继承特性而带来的局限. 使用Runnable实现多线程可以达到资源共享目的。 Runnable接口和Callable接口的区别Runnable应该是比较熟悉的接口，它只有一个run()函数，用于将耗时操作写在其中，该函数没有返回值，不能将结果返回给客户程序。然后使用某个线程去执行runnable即可实现多线程，Thread类在调用start()函数后就是执行的是Runnable的run()函数。Runnable的声明如下 : 123public interface Runnable &#123; public abstract void run(); &#125; Callable与Runnable的功能大致相似，Callable中有一个call()函数，但是call()函数有返回值。 Callable的声明如下 : 123public interface Callable&lt;V&gt; &#123; V call() throws Exception; &#125; 可以看到，这是一个泛型接口，call()函数返回的类型就是客户程序传递进来的V类型。不同之处：1.Callable可以返回一个类型V，而Runnable不可以；2.Callable能够抛出checked exception,而Runnable不可以；3.Runnable是自从java1.1就有了，而Callable是1.5之后才加上去的；4.Callable和Runnable都可以应用于executors。而Thread类只支持Runnable；Callable与executors联合在一起，在任务完成时可立刻获得一个更新了的Future；而Runable却要自己处理。 5.加入线程池运行，Runnable使用ExecutorService的execute方法，Callable使用submit方法。 线程池的实现原理先启动若干数量的线程，并让这些线程都处于睡眠状态，当客户端有一个新请求时，就会唤醒线程池中的某一个睡眠线程，让它来处理客户端的这个请求，当处理完这个请求后，线程又处于睡眠状态。 节约大量的的系统资源，使得更多的CPU时间和内存用来处理实际的商业应用，而不是频繁的线程创建与销毁。 线程池的几种方式Java通过Executors提供四种线程池，分别为： newCachedThreadPool创建一个可缓存线程池，如果线程池长度超过处理需要，可灵活回收空闲线程，若无可回收，则新建线程。 newFixedThreadPool 创建一个定长线程池，可控制线程最大并发数，超出的线程会在队列中等待。 newScheduledThreadPool 创建一个定长线程池，支持定时及周期性任务执行。 newSingleThreadExecutor 创建一个单线程化的线程池，它只会用唯一的工作线程来执行任务，保证所有任务按照指定顺序(FIFO, LIFO, 优先级)执行。newCachedThreadPool创建一个可缓存线程池，如果线程池长度超过处理需要，可灵活回收空闲线程，若无可回收，则新建线程。 newFixedThreadPool 创建一个定长线程池，可控制线程最大并发数，超出的线程会在队列中等待。 newScheduledThreadPool 创建一个定长线程池，支持定时及周期性任务执行。 newSingleThreadExecutor 创建一个单线程化的线程池，它只会用唯一的工作线程来执行任务，保证所有任务按照指定顺序(FIFO, LIFO, 优先级)执行。 (1).newCacheThreadPool 创建一个可缓存线程池，如果线程池长度超过处理需要，可灵活回收空闲线程，若无可回收，则新建线程。示例代码如下： 123456789101112131415161718ExecutorService cachedThreadPool = Executors.newCachedThreadPool(); for (int i = 0; i &lt; 10; i++) &#123; final int index = i; try &#123; Thread.sleep(index * 1000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125;cachedThreadPool.execute(new Runnable() &#123;@Overridepublic void run() &#123; System.out.println(index);&#125;&#125;);&#125; 线程池为无限大，当执行第二个任务时第一个任务已经完成，会复用执行第一个任务的线程，而不用每次新建线程。 (2). newFixedThreadPool： 创建一个定长线程池，可控制线程最大并发数，超出的线程会在队列中等待。示例代码如下： 123456789101112131415161718ExecutorService fixedThreadPool = Executors.newFixedThreadPool(3); for (int i = 0; i &lt; 10; i++) &#123; final int index = i; fixedThreadPool.execute(new Runnable() &#123;@Overridepublic void run() &#123;try &#123; System.out.println(index); Thread.sleep(2000);&#125; catch (InterruptedException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125;&#125;&#125;);&#125; 因为线程池大小为3，每个任务输出index后sleep 2秒，所以每两秒打印3个数字。 定长线程池的大小最好根据系统资源进行设置。如Runtime.getRuntime().availableProcessors()。可参考PreloadDataCache。 (3).newScheduledThreadPool: 创建一个定长线程池，支持定时及周期性任务执行。延迟执行示例代码如下： 12345678ScheduledExecutorService scheduledThreadPool = Executors.newScheduledThreadPool(5); scheduledThreadPool.schedule(new Runnable() &#123;@Overridepublic void run() &#123; System.out.println("delay 3 seconds");&#125;&#125;, 3, TimeUnit.SECONDS); 表示延迟3秒执行。 定期执行示例代码如下： 1234567scheduledThreadPool.scheduleAtFixedRate(new Runnable() &#123;@Overridepublic void run() &#123; System.out.println("delay 1 seconds, and excute every 3 seconds");&#125;&#125;, 1, 3, TimeUnit.SECONDS); 表示延迟1秒后每3秒执行一次。 ScheduledExecutorService比Timer更安全，功能更强大 (4).newSingleThreadExecutor: 创建一个单线程化的线程池，它只会用唯一的工作线程来执行任务，保证所有任务按照指定顺序(FIFO, LIFO, 优先级)执行。示例代码如下： 1234567891011121314151617ExecutorService singleThreadExecutor = Executors.newSingleThreadExecutor();for (int i = 0; i &lt; 10; i++) &#123;final int index = i;singleThreadExecutor.execute(new Runnable() &#123;@Overridepublic void run() &#123; try &#123; System.out.println(index); Thread.sleep(2000);&#125; catch (InterruptedException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125;&#125; &#125;);&#125; 结果依次输出，相当于顺序执行各个任务。 现行大多数GUI程序都是单线程的。Android中单线程可用于数据库操作，文件操作，应用批量安装，应用批量删除等不适合并发但可能IO阻塞性及影响UI线程响应的操作。 线程池的作用线程池作用就是限制系统中执行线程的数量。根 据系统的环境情况，可以自动或手动设置线程数量，达到运行的最佳效果；少了浪费了系统资源，多了造成系统拥挤效率不高。用线程池控制线程数量，其他线程排 队等候。一个任务执行完毕，再从队列的中取最前面的任务开始执行。若队列中没有等待进程，线程池的这一资源处于等待。当一个新任务需要运行时，如果线程池 中有等待的工作线程，就可以开始运行了；否则进入等待队列。 为什么要使用线程池1.减少了创建和销毁线程的次数，每个工作线程都可以被重复利用，可执行多个任务。 2.可以根据系统的承受能力，调整线程池中工作线线程的数目，防止因为消耗过多的内存，而把服务器累趴下(每个线程需要大约1MB内存，线程开的越多，消耗的内存也就越大，最后死机)。 Java里面线程池的顶级接口是Executor，但是严格意义上讲Executor并不是一个线程池，而只是一个执行线程的工具。真正的线程池接口是ExecutorService。 比较重要的几个类： ExecutorService： 真正的线程池接口。 ScheduledExecutorService： 能和Timer/TimerTask类似，解决那些需要任务重复执行的问题。 ThreadPoolExecutor： ExecutorService的默认实现。 ScheduledThreadPoolExecutor： 继承ThreadPoolExecutor的ScheduledExecutorService接口实现，周期性任务调度的类实现。 要配置一个线程池是比较复杂的，尤其是对于线程池的原理不是很清楚的情况下，很有可能配置的线程池不是较优的，因此在Executors类里面提供了一些静态工厂，生成一些常用的线程池。 newSingleThreadExecutor 创建一个单线程的线程池。这个线程池只有一个线程在工作，也就是相当于单线程串行执行所有任务。如果这个唯一的线程因为异常结束，那么会有一个新的线程来替代它。此线程池保证所有任务的执行顺序按照任务的提交顺序执行。 newFixedThreadPool 创建固定大小的线程池。每次提交一个任务就创建一个线程，直到线程达到线程池的最大大小。线程池的大小一旦达到最大值就会保持不变，如果某个线程因为执行异常而结束，那么线程池会补充一个新线程。 newCachedThreadPool 创建一个可缓存的线程池。如果线程池的大小超过了处理任务所需要的线程，那么就会回收部分空闲（60秒不执行任务）的线程，当任务数增加时，此线程池又可以智能的添加新线程来处理任务。此线程池不会对线程池大小做限制，线程池大小完全依赖于操作系统（或者说JVM）能够创建的最大线程大小。 newScheduledThreadPool 创建一个大小无限的线程池。此线程池支持定时以及周期性执行任务的需求。 synchronized关键字的用法，优缺点用法： 指定对象加锁:对给定的对象进行加锁,进入同步代码块要获得给定对象的锁 直接作用于实例方法:相当于对当前实例加锁,进入同步代码块要获得当前实例的锁(这要求创建Thread的时候,要用同一个Runnable的实例才可以) 直接作用于静态方法:相当于给当前类加锁,进入同步代码块前要获得当前类的锁 优缺点： 使用synchronized，当多个线程尝试获取锁时，未获取到锁的线程会不断的尝试获取锁，而不会发生中断，这样会造成性能消耗。而ReentranLock的lockInterruptibly()可以优先相应中断。举例：两个线程A，B，A获得了锁（A.lockInterruptibly（）），B在请求锁的时候发生阻塞，如果调用 B.interrupt()，会中断B的阻塞。 多线程的作用（1）发挥多核CPU的优势，提高CPU的利用率（2）防止阻塞，提高效率 什么是线程安全当多个线程访问某一个类（对象或方法）时，这个对象始终都能表现出正确的行为，那么这个类（对象或方法）就是线程安全的。 线程安全级别（1）不可变（2）绝对线程安全（3）相对线程安全（4）线程非安全 如何在两个线程之间共享数据线程之间数据共享，其实可以理解为线程之间的通信，可以用wait/notify/notifyAll 进行等待和唤醒。 Java中提供了对象的那些级别引用在Java中提供了对象的4个级别引用, 分别是强引用、软引用、弱引用以及虚引用。这四个类型的引用中, 只有强类型的引用是包内可见的, 其他级别的引用都是Public, 可以直接被应用程序开发者使用的。 强引用 我们在Java中创建的对象引用, 一般都是强类型引用. 这同样意味着当该对象如果有引用存在的情况下, 同时在JVM整个环境中该对象是路径可达的, 那么该对象永远不会被垃圾回收机制回收的。 例如我们创建一个对象Object object = new Object(); 该object就是一个强引用. 当强引用的对象占用内存过多, 而又没有释放的时候, 就会出现OOM问题; 软引用 软引用使用的时候需要通过一个软引用对象来进行声明, 软引用对象比强引用稍微弱一点, 通过软引用的对象当出现内存不足的情况的时候, 垃圾回收机制会将该类型的对象进行回收 弱引用 弱引用使用的时候需要通过一个弱引用对象进行声明。弱引用可以维持对对象的引用, 但是一旦垃圾回收线程工作的时候, 发现一个对象只有弱引用保持的时候, 那么就会对该对象进行垃圾回收.引用类型是一种比软引用弱的易用类型, 在系统GC时, 只要发现一个对象只有弱引用时不管系统堆空间是否足够, 都会将对象回收。但是由于垃圾回收器的线程通常优先级很低, 因此并不一定能够很快发现只有弱引用持有的对象, 在这种情况下, 弱引用对象可以存在很长的时间。一旦弱引用对象被垃圾回收器回收, 那么有一种机制能够保证该弱引用添加到一个注册引用的队列中。 虚引用 虚引用与其他的引用都不相同, 虚引用并不会决定引用对象的生命周期, 所以虚引用又成为”幽灵引用”。如果一个对象只持有虚引用, 那么该对象就跟没有任何引用一样, 在任何时候都有可能被垃圾回收器回收. 所以没办法在程序中调用它的任何相关函数, 因为存在太多的不确定性。而虚引用主要是用来提供当对象被GC的时候的通知机制. 通过该通知机制我们可以做一些资源回收等方面的工作. 因为对象只存在虚引用完全没有意义, 即在程序中声明一个对象的虚引用完全没有意义, 所以虚引用一定要与Reference队列一起使用, 才能起到对象回收通知机制. Finalizer对象什么时候会在引用队列中对于Java而言： 调用时机：当垃圾回收器要宣告一个对象死亡时，至少要经过两次标记过程：如果对象在进行可达性分析后发现没有和GC Roots相连接的引用链，就会被第一次标记，并且判断是否执行finalizer( )方法，如果对象覆盖finalizer( )方法且未被虚拟机调用过，那么这个对象会被放置在F-Queue队列中，并在稍后由一个虚拟机自动建立的低优先级的Finalizer线程区执行触发finalizer( )方法，但不承诺等待其运行结束。 finalization的目的：对象逃脱死亡的最后一次机会。（只要重新与引用链上的任何一个对象建立关联即可。）但是不建议使用，运行代价高昂，不确定性大，且无法保证各个对象的调用顺序。可用try-finally或其他替代。 CountDownLatch 原理实现原理：计数器的值由构造函数传入，并用它初始化AQS的state值。当线程调用await方法时会检查state的值是否为0，如果是就直接返回（即不会阻塞）；如果不是，将表示该节点的线程入列，然后将自身阻塞。当其它线程调用countDown方法会将计数器减1，然后判断计数器的值是否为0，当它为0时，会唤醒队列中的第一个节点，由于CountDownLatch使用了AQS的共享模式，所以第一个节点被唤醒后又会唤醒第二个节点，以此类推，使得所有因await方法阻塞的线程都能被唤醒而继续执行。 从源代码和实现原理中可以看出一个CountDownLatch对象，只能使用一次，不能重复使用。 CyclicBarrier 原理实现原理：在CyclicBarrier的内部定义了一个Lock对象，每当一个线程调用CyclicBarrier的await方法时，将剩余拦截的线程数减1，然后判断剩余拦截数是否为0，如果不是，进入Lock对象的条件队列等待。如果是，执行barrierAction对象的Runnable方法，然后将锁的条件队列中的所有线程放入锁等待队列中，这些线程会依次的获取锁、释放锁，接着先从await方法返回，再从CyclicBarrier的await方法中返回。 Semaphore 原理信号量主要用于两个目的，一个是用于多个共享资源的互斥使用，另一个用于并发线程数的控制。 Semaphore的流程的一些特性： • 管理一系列许可证，即state共享资源值；• 每acquire一次则state就减1一次，直到许可证数量小于0则阻塞等待；• 释放许可的时候要保证唤醒后继结点，以此来保证线程释放他们所持有的信号量；• 是Synchronized的升级版，因为Synchronized是只有一个许可，而Semaphore就像开了挂一样，可以有多个许可； Exchanger 原理作用：Exchanger类用于两个线程之间交换数据。 换句话说Exchanger提供的是一个交换服务，允许原子性的交换两个（多个）对象，但同时只有一对才会成功。先看一个简单的实例模型。 在上面的模型中，我们假定一个空的栈（Stack），栈顶（Top）当然是没有元素的。同时我们假定一个数据结构Node，包含一个要交换的元素E和一个要填充的“洞”Node。这时线程T1携带节点node1进入栈（cas_push)，当然这是CAS操作，这样栈顶就不为空了。线程T2携带节点node2进入栈，发现栈里面已经有元素了node1，同时发现node1的hold（Node）为空，于是将自己（node2）填充到node1的hold中（cas_fill）。然后将元素node1从栈中弹出（cas_take）。这样线程T1就得到了node1.hold.item也就是node2的元素e2，线程T2就得到了node1.item也就是e1，从而达到了交换的目的。 算法描述就是下图展示的内容。 JDK 5就是采用类似的思想实现的Exchanger。JDK 6以后为了支持多线程多对象同时Exchanger了就进行了改造（为了支持更好的并发），采用ConcurrentHashMap的思想，将Stack分割成很多的片段（或者说插槽Slot），线程Id（Thread.getId()）hash相同的落在同一个Slot上，这样在默认32个Slot上就有很好的吞吐量。当然会根据机器CPU内核的数量有一定的优化，有兴趣的可以去了解下Exchanger的源码。 CountDownLatch 与 CyclicBarrier 区别 CountDownLatch：一个或者多个线程，等待其他多个线程完成某件事情之后才能执行； CyclicBarrier：多个线程互相等待，直到到达同一个同步点，再继续一起执行。 对于CountDownLatch来说，重点是“一个线程（多个线程）等待”，而其他的N个线程在完成“某件事情”之后，可以终止，也可以等待。而对于CyclicBarrier，重点是多个线程，在任意一个线程没有完成，所有的线程都必须等待。 CountDownLatch是计数器，线程完成一个记录一个，只不过计数不是递增而是递减，而CyclicBarrier更像是一个阀门，需要所有线程都到达，阀门才能打开，然后继续执行。 线程池中的coreNum和maxNum有什么不同在不同的业务场景中，线程池参数如何设置线程的生命周期 上图是一个线程的生命周期状态流转图，很清楚的描绘了一个线程从创建到终止的过程。 这些状态的枚举值都定义在java.lang.Thread.State下 12345678public enum State &#123; NEW, RUNNABLE, BLOCKED, WAITING, TIMED_WAITING, TERMINATED;&#125; NEW：毫无疑问表示的是刚创建的线程，还没有开始启动。 RUNNABLE: 表示线程已经触发start()方式调用，线程正式启动，线程处于运行中状态。 BLOCKED：表示线程阻塞，等待获取锁，如碰到synchronized、lock等关键字等占用临界区的情况，一旦获取到锁就进行RUNNABLE状态继续运行。 WAITING：表示线程处于无限制等待状态，等待一个特殊的事件来重新唤醒，如通过wait()方法进行等待的线程等待一个notify()或者notifyAll()方法，通过join()方法进行等待的线程等待目标线程运行结束而唤醒，一旦通过相关事件唤醒线程，线程就进入了RUNNABLE状态继续运行。 TIMED_WAITING：表示线程进入了一个有时限的等待，如sleep(3000)，等待3秒后线程重新进行RUNNABLE状态继续运行。 TERMINATED：表示线程执行完毕后，进行终止状态。 需要注意的是，一旦线程通过start方法启动后就再也不能回到初始NEW状态，线程终止后也不能再回到RUNNABLE状态。 synchronize 实现原理synchronized可以保证方法或者代码块在运行时，同一时刻只有一个方法可以进入到临界区，同时它还可以保证共享变量的内存可见性 Java中每一个对象都可以作为锁，这是synchronized实现同步的基础： 普通同步方法，锁是当前实例对象 静态同步方法，锁是当前类的class对象 同步方法块，锁是括号里面的对象 Lock接口有哪些实现类，使用场景是什么Lock接口有三个实现类，一个是ReentrantLock,另两个是ReentrantReadWriteLock类中的两个静态内部类ReadLock和WriteLock。与互斥锁定相比，读-写锁定允许对共享数据进行更高级别的并发访问。虽然一次只有一个线程（writer 线程）可以修改共享数据，但在许多情况下，任何数量的线程可以同时读取共享数据（reader 线程）。从理论上讲，与互斥锁定相比，使用读-写锁定所允许的并发性增强将带来更大的性能提高。在实践中，只有在多处理器上并且只在访问模式适用于共享数据时，才能完全实现并发性增强。——例如，某个最初用数据填充并且之后不经常对其进行修改的 collection，因为经常对其进行搜索（比如搜索某种目录），所以这样的 collection 是使用读-写锁定的理想候选者。 ReentrantLock 的原理原理：可重入锁的原理是在锁内部维护了一个线程标示，标示该锁目前被那个线程占用，然后关联一个计数器，一开始计数器值为0，说明该锁没有被任何线程占用，当一个线程获取了该锁，计数器会变成1，其他线程在获取该锁时候发现锁的所有者不是自己所以被阻塞，但是当获取该锁的线程再次获取锁时候发现锁拥有者是自己会把计数器值+1， 当释放锁后计数器会-1，当计数器为0时候，锁里面的线程标示重置为null,这时候阻塞的线程会获取被唤醒来获取该锁. ReentrantLock 类实现了Lock ，它拥有与synchronized 相同的并发性和内存语义，但是添加了类似锁投票、定时锁等候和可中断锁等候的一些特性。 此外，它还提供了在激烈争用情况下更佳的性能。（换句话说，当许多线程都想访问共享资源时，JVM 可以花更少的时候来调度线程，把更多时间用在执行线程上。） ReentrantLock扩展的功能1.实现可轮询的锁请求： –在内部锁中，死锁是致命的——唯一的恢复方法是重新启动程序，唯一的预防方法是在构建程序时不要出错。而可轮询的锁获取模式具有更完善的错误 恢复机制，可以规避死锁的发生。 –如果你不能获得所有需要的锁，那么使用可轮询的获取方式使你能够重新拿到控制权，它会释放你已经获得的这些锁，然后再重新尝试。 可轮询的锁获取模式，由tryLock()方法实现。此方法仅在调用时锁为空闲状态才获取该锁。如果锁可用，则获取锁，并立即返回值true。 如果锁不可用，则此方法将立即返回值false。此方法的典型使用语句如下： 12345678910111213141516171819Lock lock = ...; if (lock.tryLock()) &#123; try &#123; // manipulate protected state &#125; finally &#123; lock.unlock(); &#125; &#125; else &#123; // perform alternative actions &#125; 2.实现可定时的锁请求 –当使用内部锁时，一旦开始请求，锁就不能停止了，所以内部锁给实现具有时限的活动带来了风险。为了解决这一问题，可以使用定时锁。 当具有时限的活动调用了阻塞方法，定时锁能够在时间预算内设定相应的超时。如果活动在期待的时间内没能获得结果，定时锁能使程序提前返回。 可定时的锁获取模式，由tryLock(long, TimeUnit)方法实现。 3.实现可中断的锁获取请求 –可中断的锁获取操作允许在可取消的活动中使用。lockInterruptibly()方法能够使你获得锁的时候响应中断。 ReentrantLock不好与需要注意的地方–lock 必须在 finally 块中释放。否则，如果受保护的代码将抛出异常，锁就有可能永远得不到释放！这一点区别看起来可能没什么，但是实际上， 它极为重要。忘记在 finally 块中释放锁，可能会在程序中留下一个定时炸弹，当有一天炸弹爆炸时，您要花费很大力气才有找到源头在哪。 而使用同步，JVM 将确保锁会获得自动释放. –当 JVM 用 synchronized 管理锁定请求和释放时，JVM 在生成线程转储时能够包括锁定信息。这些对调试非常有价值，因为它们能标识死锁或者 其他异常行为的来源。 Lock 类只是普通的类，JVM 不知道具体哪个线程拥有 Lock 对象。 synchronized 和java.util.concurrent.locks.Lock的异同主要相同点：Lock能完成synchronized所实现的所有功能 主要不同点：Lock有比synchronized更精确的线程语义和更好的性能。synchronized会自动释放锁，而Lock一定要求程序员手工释放，并且必须在finally从句中释放。Lock还有更强大的功能，例如，它的tryLock方法可以非阻塞方式去拿锁。 介绍下栈和队列栈和队列都是动态集合，且在其上进行DELETE操作所移除的元素是预先设定的。在栈（stack）中，被删除的是最近插入的元素：栈实现的是一种后进先出（last-in, first-out, LIFO)策略。类似地，在队列（queue）中，被删去的总是在集合中存在时间最长的那个元素：队列实现的是一种先进先出（first-in, first-out, FIFO)策略。 synchronized、Lock、ReentrantLock、ReadWriteLockLock和synchronized有以下几点不同： Lock是一个接口，而synchronized是Java中的关键字，synchronized是内置的语言实现； synchronized在发生异常时，会自动释放线程占有的锁，因此不会导致死锁现象发生；而Lock在发生异常时，如果没有主动通过unLock()去释放锁，则很可能造成死锁现象，因此使用Lock时需要在finally块中释放锁； Lock可以让等待锁的线程响应中断，而synchronized却不行，使用synchronized时，等待的线程会一直等待下去，不能够响应中断； 通过Lock可以知道有没有成功获取锁，而synchronized却无法办到。 Lock可以提高多个线程进行读操作的效率。在性能上来说，如果竞争资源不激烈，两者的性能是差不多的，而当竞争资源非常激烈时（即有大量线程同时竞争），此时Lock的性能要远远优于synchronized。所以说，在具体使用时要根据适当情况选择。 ReentrantReadWriteLock相比ReentrantLock的最大区别是： ReentrantReadWriteLock的读锁是共享锁，任何线程都可以获取，而写锁是独占锁。ReentrantLock不论读写，是独占锁。 介绍下CAS(无锁技术)CAS: java.util.concurrent包中借助CAS实现了区别于synchronouse同步锁的一种乐观锁。 CAS应用：CAS有3个操作数，内存值V，旧的预期值A，要修改的新值B。当且仅当预期值A和内存值V相同时，将内存值V修改为B，否则什么都不做。 CAS原理:CAS通过调用JNI的代码实现的。JNI:Java Native Interface为JAVA本地调用，允许java调用其他语言。 CAS缺点: CAS虽然很高效的解决原子操作，但是CAS仍然存在三大问题。ABA问题，循环时间长开销大和只能保证一个共享变量的原子操作 ThreadPoolExecutor的内部工作原理 如果线程池大小poolSize小于corePoolSize，则创建新线程执行任务。 如果线程池大小poolSize大于corePoolSize，且等待队列未满，则进入等待队列。 如果线程池大小poolSize大于corePoolSize且小于maximumPoolSize，且等待队列已满，则创建新线程执行任务。 如果线程池大小poolSize大于corePoolSize且大于maximumPoolSize，且等待队列已满，则调用拒绝策略来处理该任务。 线程池里的每个线程执行完任务后不会立刻退出，而是会去检查下等待队列里是否还有线程任务需要执行，如果在keepAliveTime里等不到新的任务了，那么线程就会退出。 ThreadPoolExecutor线程池中拒绝策略： AbortPolicy：为java线程池默认的阻塞策略，不执行此任务，而且会直接抛出一个执行时异常，切记TreadPoolExecutor.execute需要try catch，否则程序会直接退出。 DiscardPolicy：直接抛弃，任务不执行，空方法 DiscardOldestPolicy：从队列里面抛弃head的一个任务，并再次execute 此任务（task） CallerRunsPolicy：在调用execute的线程里面执行此command，会阻塞入口。 用户自定义拒绝策略：实现RejectdExecutionHandler，并自己定义策略模式。 分布式环境下，怎么保证线程安全有哪些类加载器能不能自己写一个类叫java.lang.String可以，但在应用的时候，需要用自己的类加载器去加载，否则，系统的类加载器永远只是去加载jre.jar包中的那个java.lang.String。由于在tomcat的web应用程序中，都是由webapp自己的类加载器先自己加载WEB-INF/classess目录中的类，然后才委托上级的类加载器加载，如果我们在tomcat的web应用程序中写一个java.lang.String，这时候Servlet程序加载的就是我们自己写的java.lang.String，但是这么干就会出很多潜在的问题，原来所有用了java.lang.String类的都将出现问题。 虽然java提供了endorsed技术，可以覆盖jdk中的某些类，具体做法是….。但是，能够被覆盖的类是有限制范围，反正不包括java.lang这样的包中的类。 例如，运行下面的程序： 1234567891011package java.lang;public class String &#123;public static void main(String[] args) &#123;System.out.println("string");&#125;&#125; 报告的错误如下： java.lang.NoSuchMethodError: main Exception in thread “main” 这是因为加载了jre自带的java.lang.String，而该类中没有main方法。 介绍下B树、二叉树B树： B树（英语：B-tree）是一种自平衡的树)，能够保持数据有序。这种数据结构能够让查找数据、顺序访问、插入数据及删除的动作，都在对数时间内完成。B树，概括来说是一个一般化的二叉查找树（binary search tree），可以拥有多于2个子节点。与自平衡二叉查找树不同，B树为系统大块数据的读写操作做了优化。B树减少定位记录时所经历的中间过程，从而加快存取速度。B树这种数据结构可以用来描述外部存储。这种数据结构常被应用在数据库和文件系统的实现上。 概括关键词：自平衡，可以拥有多于2个子节点，适用于数据库和文件系统。 二叉树： 二叉树是一种特殊的有序树：每个节点至多有两个分支（子节点），分支具有左右次序，不能颠倒。两种特殊的二叉树：完全二叉树：除最后一层外，若其余层都是满的，并且最后一层或者是满的，或者是在右边缺少连续若干节点（注意是右边，而不能是左边缺少）。满二叉树：每一层都是满的（除了最后一层，这里的最后一层是指叶节点）。 红黑树： 红黑树（英语：Red–black tree）是一种自平衡二叉查找树。它的操作有着良好的最坏情况运行时间，并且在实践中是高效的：它可以在O(log n)时间内做查找，插入和删除，这里的n是树中元素的数目。 红黑树的性质 节点是红色或黑色。 根是黑色。 所有叶子都是黑色（叶子是NIL节点）。 每个红色节点必须有两个黑色的子节点。（从每个叶子到根的所有路径上不能有两个连续的红色节点。） 从任一节点到其每个叶子的所有简单路径)都包含相同数目的黑色节点。 分布式锁的实现针对分布式锁的实现，目前比较常用的有以下几种方案： (1).基于数据库实现分布式锁 (2).基于缓存（redis，memcached，tair）实现分布式锁 (3).基于Zookeeper实现分布式锁 实现方式 优点 缺点 数据库实现 直接借助数据库，容易理解。 1. 会有各种各样的问题，在解决问题的过程中会使整个方案变得越来越复杂。 2.操作数据库需要一定的开销，性能问题需要考虑。 使用数据库的行级锁并不一定靠谱，尤其是当我们的锁表并不大的时候。 缓存实现 性能好，实现起来较为方便。 1.通过超时时间来控制锁的失效时间并不是十分的靠谱。 Zookeeper实现 有效的解决单点问题，不可重入问题，非阻塞问题以及锁无法释放的问题。实现起来较为简单。 1.性能上不如使用缓存实现分布式锁。 需要对ZK的原理有所了解。 三种方法比较： 上面几种方式，哪种方式都无法做到完美。就像CAP一样，在复杂性、可靠性、性能等方面无法同时满足，所以，根据不同的应用场景选择最适合自己的才是王道。 从理解的难易程度角度（从低到高） 数据库 &gt; 缓存 &gt; Zookeeper 从实现的复杂程度角度（从低到高） Zookeeper &gt;= 缓存 &gt; 数据库 从性能角度（从高到低） 缓存 &gt; Zookeeper &gt;= 数据库 从可靠性角度（从高到低） Zookeeper &gt; 缓存 &gt; 数据库 分布式session存储解决方案1. Session Stick Session Stick 方案即将客户端的每次请求都转发至同一台服务器，这就需要负载均衡器能够根据每次请求的会话标识（SessionId）来进行请求转发，如下图所示。 这种方案实现比较简单，对于Web服务器来说和单机的情况一样。但是可能会带来如下问题： 如果有一台服务器宕机或者重启，那么这台机器上的会话数据会全部丢失。 会话标识是应用层信息，那么负载均衡要将同一个会话的请求都保存到同一个Web服务器上的话，就需要进行应用层（第7层）的解析，这个开销比第4层大。 负载均衡器将变成一个有状态的节点，要将会话保存到具体Web服务器的映射。和无状态节点相比，内存消耗更大，容灾方面也会更麻烦。 2. Session Replication Session Replication 的方案则不对负载均衡器做更改，而是在Web服务器之间增加了会话数据同步的功能，各个服务器之间通过同步保证不同Web服务器之间的Session数据的一致性，如下图所示。 Session Replication 方案对负载均衡器不再有要求，但是同样会带来以下问题： 同步Session数据会造成额外的网络带宽的开销，只要Session数据有变化，就需要将新产生的Session数据同步到其他服务器上，服务器数量越多，同步带来的网络带宽开销也就越大。 每台Web服务器都需要保存全部的Session数据，如果整个集群的Session数量太多的话，则对于每台机器用于保存Session数据的占用会很严重。 3. Session 数据集中存储 Session 数据集中存储方案则是将集群中的所有Session集中存储起来，Web服务器本身则并不存储Session数据，不同的Web服务器从同样的地方来获取Session，如下图所示。 相对于Session Replication方案，此方案的Session数据将不保存在本机，并且Web服务器之间也没有了Session数据的复制，但是该方案存在的问题在于： 读写Session数据引入了网络操作，这相对于本机的数据读取来说，问题就在于存在时延和不稳定性，但是通信发生在内网，则问题不大。 如果集中存储Session的机器或集群出现问题，则会影响应用。 4. Cookie Based Cookie Based 方案是将Session数据放在Cookie里，访问Web服务器的时候，再由Web服务器生成对应的Session数据，如下图所示。 但是Cookie Based 方案依然存在不足： Cookie长度的限制。这会导致Session长度的限制。 安全性。Seesion数据本来是服务端数据，却被保存在了客户端，即使可以加密，但是依然存在不安全性。 带宽消耗。这里不是指内部Web服务器之间的宽带消耗，而是数据中心的整体外部带宽的消耗。 性能影响。每次HTTP请求和响应都带有Seesion数据，对Web服务器来说，在同样的处理情况下，响应的结果输出越少，支持的并发就会越高。 总结前面四个方案都是可行的，但是对于大型网站来说，Session Sticky和Session数据集中存储是比较好的方案。 常用的linux命令mkdir: 用于新建一个新目录 pwd: 显示当前工作目录 rmdir: 删除给定的目录。 rm: 会删除给定的文件 mv: 命令对文件或文件夹进行移动，如果文件或文件夹存在于当前工作目录，还可以对文件或文件夹进行重命名。 cat: 用于在标准输出（监控器或屏幕）上查看文件内容 tail: 默认在标准输出上显示给定文件的最后10行内容，可以使用tail -n N 指定在标准输出上显示文件的最后N行内容。 less: 按页或按窗口打印文件内容。在查看包含大量文本数据的大文件时是非常有用和高效的。你可以使用Ctrl+F向前翻页，Ctrl+B向后翻页。 grep: 在给定的文件中搜寻指定的字符串。grep -i “” 在搜寻时会忽略字符串的大小写，而grep -r “” 则会在当前工作目录的文件中递归搜寻指定的字符串。 find: 这个命令会在给定位置搜寻与条件匹配的文件。你可以使用find -name 的-name选项来进行区分大小写的搜寻，find -iname 来进行不区分大小写的搜寻。 tar: 命令能创建、查看和提取tar压缩文件。tar -cvf 是创建对应压缩文件，tar -tvf 来查看对应压缩文件，tar -xvf 来提取对应压缩文件。 gzip: 命令创建和提取gzip压缩文件，还可以用gzip -d 来提取压缩文件。 unzip: 对gzip文档进行解压。在解压之前，可以使用unzip -l 命令查看文件内容。 whatis: 会用单行来描述给定的命令，就是解释当前命令。 exit: 用于结束当前的终端会话。 who: 能列出当前登录的用户名。 su: 用于切换不同的用户。即使没有使用密码，超级用户也能切换到其它用户。 uname: 会显示出关于系统的重要信息，如内核名称、主机名、内核版本、处理机类型等等，使用uname -a可以查看所有信息。 df: 查看文件系统中磁盘的使用情况–硬盘已用和可用的存储空间以及其它存储设备。你可以使用df -h将结果以人类可读的方式显示。 ps: 显示系统的运行进程。 top: 命令会默认按照CPU的占用情况，显示占用量较大的进程,可以使用top -u 查看某个用户的CPU使用排名情况。 文章地址：https://www.jianshu.com/p/0056d671ea6d https://juejin.im/post/5a9f5ce86fb9a028de443ed9]]></content>
      <categories>
        <category>基础面试题</category>
      </categories>
      <tags>
        <tag>基础面试题</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基础线程面试题]]></title>
    <url>%2F2018%2F06%2F03%2F%E5%9F%BA%E7%A1%80%E7%BA%BF%E7%A8%8B%E9%9D%A2%E8%AF%95%E9%A2%98%2F</url>
    <content type="text"><![CDATA[基础线程面试题摘要： 不管你是新程序员还是老手，你一定在面试中遇到过有关线程的问题。Java语言一个重要的特点就是内置了对并发的支持，让Java大受企业和程序员的欢迎。大多数待遇丰厚的Java开发职位都要求开发者精通多线程技术并且有丰富的Java程序开发、调试、优化经验，所以线程相关的问题在面试中经常会被提到。 在典型的Java面试中， 面试官会从线程的基本概念问起, 如：为什么你需要使用线程， 如何创建线程，用什么方式创建线程比较好（比如：继承thread类还是调用Runnable接口），然后逐渐问到并发问题像在Java并发编程的过程中遇到了什么挑战，Java内存模型，JDK1.5引入了哪些更高阶的并发工具，并发编程常用的设计模式，经典多线程问题如生产者消费者，哲学家就餐，读写器或者简单的有界缓冲区问题。仅仅知道线程的基本概念是远远不够的， 你必须知道如何处理死锁，竞态条件，内存冲突和线程安全等并发问题。掌握了这些技巧，你就可以轻松应对多线程和并发面试了。 许多Java程序员在面试前才会去看面试题，这很正常。因为收集面试题和练习很花时间，所以我从许多面试者那里收集了Java多线程和并发相关的50个热门问题。我只收集了比较新的面试题且没有提供全部答案。想必聪明的你对这些问题早就心中有数了， 如果遇到不懂的问题，你可以用Google找到答案。若你实在找不到答案，可以在文章的评论中向我求助。你也可以在这找到一些答案Java线程问答Top 12。 什么是线程 线程是操作系统能够进行运算调度的最小单位，它被包含在进程之中，是进程中的实际运作单位。程序员可以通过它进行多处理器编程，你可以使用多线程对运算密集型任务提速。比如，如果一个线程完成一个任务要100毫秒，那么用十个线程完成改任务只需10毫秒。Java在语言层面对多线程提供了卓越的支持，它也是一个很好的卖点。欲了解更多详细信息请点击这里。 线程和进程有什么区别 线程是进程的子集，一个进程可以有很多线程，每条线程并行执行不同的任务。不同的进程使用不同的内存空间，而所有的线程共享一片相同的内存空间。别把它和栈内存搞混，每个线程都拥有单独的栈内存用来存储本地数据。更多详细信息请点击这里。 如何在Java中实现线程 在语言层面有两种方式。java.lang.Thread 类的实例就是一个线程但是它需要调用java.lang.Runnable接口来执行，由于线程类本身就是调用的Runnable接口所以你可以继承java.lang.Thread 类或者直接调用Runnable接口来重写run()方法实现线程。更多详细信息请点击这里. 用Runnable还是Thread 这个问题是上题的后续，大家都知道我们可以通过继承Thread类或者调用Runnable接口来实现线程，问题是，那个方法更好呢？什么情况下使用它？这个问题很容易回答，如果你知道Java不支持类的多重继承，但允许你调用多个接口。所以如果你要继承其他类，当然是调用Runnable接口好了。更多详细信息请点击这里。 Thread 类中的start() 和 run() 方法有什么区别 这个问题经常被问到，但还是能从此区分出面试者对Java线程模型的理解程度。start()方法被用来启动新创建的线程，而且start()内部调用了run()方法，这和直接调用run()方法的效果不一样。当你调用run()方法的时候，只会是在原来的线程中调用，没有新的线程启动，start()方法才会启动新线程。更多讨论请点击这里 Runnable和Callable有什么不同 Runnable和Callable都代表那些要在不同的线程中执行的任务。Runnable从JDK1.0开始就有了，Callable是在JDK1.5增加的。它们的主要区别是Callable的 call() 方法可以返回值和抛出异常，而Runnable的run()方法没有这些功能。Callable可以返回装载有计算结果的Future对象。我的博客有更详细的说明。 CyclicBarrier 和 CountDownLatch有什么不同 CyclicBarrier 和 CountDownLatch 都可以用来让一组线程等待其它线程。与 CyclicBarrier 不同的是，CountdownLatch 不能重新使用。点此查看更多信息和示例代码。 Java内存模型是什么 Java内存模型规定和指引Java程序在不同的内存架构、CPU和操作系统间有确定性地行为。它在多线程的情况下尤其重要。Java内存模型对一个线程所做的变动能被其它线程可见提供了保证，它们之间是先行发生关系。这个关系定义了一些规则让程序员在并发编程时思路更清晰。比如，先行发生关系确保了： 线程内的代码能够按先后顺序执行，这被称为程序次序规则。对于同一个锁，一个解锁操作一定要发生在时间上后发生的另一个锁定操作之前，也叫做管程锁定规则。前一个对volatile的写操作在后一个volatile的读操作之前，也叫volatile变量规则。一个线程内的任何操作必需在这个线程的start()调用之后，也叫作线程启动规则。一个线程的所有操作都会在线程终止之前，线程终止规则。一个对象的终结操作必需在这个对象构造完成之后，也叫对象终结规则。可传递性 我强烈建议大家阅读《Java并发编程实践》第十六章来加深对Java内存模型的理解。 volatile 变量是什么 volatile是一个特殊的修饰符，只有成员变量才能使用它。在Java并发程序缺少同步类的情况下，多线程对成员变量的操作对其它线程是透明的。volatile变量可以保证下一个读取操作会在前一个写操作之后发生，就是上一题的volatile变量规则。点击这里查看更多volatile的相关内容。 什么是线程安全，Vector是一个线程安全类吗 如果你的代码所在的进程中有多个线程在同时运行，而这些线程可能会同时运行这段代码。如果每次运行结果和单线程运行的结果是一样的，而且其他的变量的值也和预期的是一样的，就是线程安全的。一个线程安全的计数器类的同一个实例对象在被多个线程使用的情况下也不会出现计算失误。很显然你可以将集合类分成两组，线程安全和非线程安全的。Vector 是用同步方法来实现线程安全的, 而和它相似的ArrayList不是线程安全的。 什么是竞态条件，举个例子说明 竞态条件会导致程序在并发情况下出现一些bugs。多线程对一些资源的竞争的时候就会产生竞态条件，如果首先要执行的程序竞争失败排到后面执行了，那么整个程序就会出现一些不确定的bugs。这种bugs很难发现而且会重复出现，因为线程间的随机竞争。一个例子就是无序处理，详见答案。 如何停止一个线程 Java提供了很丰富的API但没有为停止线程提供API。JDK 1.0本来有一些像stop(), suspend() 和 resume()的控制方法但是由于潜在的死锁威胁因此在后续的JDK版本中他们被弃用了，之后Java API的设计者就没有提供一个兼容且线程安全的方法来停止一个线程。当run() 或者 call() 方法执行完的时候线程会自动结束,如果要手动结束一个线程，你可以用volatile 布尔变量来退出run()方法的循环或者是取消任务来中断线程。点击这里查看示例代码。 一个线程运行时发生异常会怎样？ 这是我在一次面试中遇到的一个很刁钻的Java面试题, 简单的说，如果异常没有被捕获该线程将会停止执行。Thread.UncaughtExceptionHandler是用于处理未捕获异常造成线程突然中断情况的一个内嵌接口。当一个未捕获异常将造成线程中断的时候JVM会使用Thread.getUncaughtExceptionHandler()来查询线程的UncaughtExceptionHandler并将线程和异常作为参数传递给handler的uncaughtException()方法进行处理。 如何在两个线程间共享数据 你可以通过共享对象来实现这个目的，或者是使用像阻塞队列这样并发的数据结构。这篇教程《Java线程间通信》(涉及到在两个线程间共享对象)用wait和notify方法实现了生产者消费者模型。 notify 和 notifyAll有什么区别？ 这又是一个刁钻的问题，因为多线程可以等待单监控锁，Java API 的设计人员提供了一些方法当等待条件改变的时候通知它们，但是这些方法没有完全实现。notify()方法不能唤醒某个具体的线程，所以只有一个线程在等待的时候它才有用武之地。而notifyAll()唤醒所有线程并允许他们争夺锁确保了至少有一个线程能继续运行。我的博客有更详细的资料和示例代码。 为什么wait, notify 和 notifyAll这些方法不在thread类里面 这是个设计相关的问题，它考察的是面试者对现有系统和一些普遍存在但看起来不合理的事物的看法。回答这些问题的时候，你要说明为什么把这些方法放在Object类里是有意义的，还有不把它放在Thread类里的原因。一个很明显的原因是JAVA提供的锁是对象级的而不是线程级的，每个对象都有锁，通过线程获得。如果线程需要等待某些锁那么调用对象中的wait()方法就有意义了。如果wait()方法定义在Thread类中，线程正在等待的是哪个锁就不明显了。简单的说，由于wait，notify和notifyAll都是锁级别的操作，所以把他们定义在Object类中因为锁属于对象。你也可以查看这篇文章了解更多。 什么是ThreadLocal变量 ThreadLocal是Java里一种特殊的变量。每个线程都有一个ThreadLocal就是每个线程都拥有了自己独立的一个变量，竞争条件被彻底消除了。它是为创建代价高昂的对象获取线程安全的好方法，比如你可以用ThreadLocal让SimpleDateFormat变成线程安全的，因为那个类创建代价高昂且每次调用都需要创建不同的实例所以不值得在局部范围使用它，如果为每个线程提供一个自己独有的变量拷贝，将大大提高效率。首先，通过复用减少了代价高昂的对象的创建个数。其次，你在没有使用高代价的同步或者不变性的情况下获得了线程安全。线程局部变量的另一个不错的例子是ThreadLocalRandom类，它在多线程环境中减少了创建代价高昂的Random对象的个数。查看答案了解更多。 什么是FutureTask 在Java并发程序中FutureTask表示一个可以取消的异步运算。它有启动和取消运算、查询运算是否完成和取回运算结果等方法。只有当运算完成的时候结果才能取回，如果运算尚未完成get方法将会阻塞。一个FutureTask对象可以对调用了Callable和Runnable的对象进行包装，由于FutureTask也是调用了Runnable接口所以它可以提交给Executor来执行。 interrupted 和 isInterruptedd方法的区别 interrupted() 和 isInterrupted()的主要区别是前者会将中断状态清除而后者不会。Java多线程的中断机制是用内部标识来实现的，调用Thread.interrupt()来中断一个线程就会设置中断标识为true。当中断线程调用静态方法Thread.interrupted()来检查中断状态时，中断状态会被清零。而非静态方法isInterrupted()用来查询其它线程的中断状态且不会改变中断状态标识。简单的说就是任何抛出InterruptedException异常的方法都会将中断状态清零。无论如何，一个线程的中断状态有有可能被其它线程调用中断来改变。 为什么wait和notify方法要在同步块中调用 主要是因为Java API强制要求这样做，如果你不这么做，你的代码会抛出IllegalMonitorStateException异常。还有一个原因是为了避免wait和notify之间产生竞态条件。 为什么你应该在循环中检查等待条件 处于等待状态的线程可能会收到错误警报和伪唤醒，如果不在循环中检查等待条件，程序就会在没有满足结束条件的情况下退出。因此，当一个等待线程醒来时，不能认为它原来的等待状态仍然是有效的，在notify()方法调用之后和等待线程醒来之前这段时间它可能会改变。这就是在循环中使用wait()方法效果更好的原因，你可以在Eclipse中创建模板调用wait和notify试一试。如果你想了解更多关于这个问题的内容，我推荐你阅读《Effective Java》这本书中的线程和同步章节。 同步集合与并发集合有什么区别 同步集合与并发集合都为多线程和并发提供了合适的线程安全的集合，不过并发集合的可扩展性更高。在Java1.5之前程序员们只有同步集合来用且在多线程并发的时候会导致争用，阻碍了系统的扩展性。Java5介绍了并发集合像ConcurrentHashMap，不仅提供线程安全还用锁分离和内部分区等现代技术提高了可扩展性。更多内容详见答案。 堆和栈有什么不同 为什么把这个问题归类在多线程和并发面试题里？因为栈是一块和线程紧密相关的内存区域。每个线程都有自己的栈内存，用于存储本地变量，方法参数和栈调用，一个线程中存储的变量对其它线程是不可见的。而堆是所有线程共享的一片公用内存区域。对象都在堆里创建，为了提升效率线程会从堆中弄一个缓存到自己的栈，如果多个线程使用该变量就可能引发问题，这时volatile 变量就可以发挥作用了，它要求线程从主存中读取变量的值。 更多内容详见答案。 什么是线程池，为什么要使用它 创建线程要花费昂贵的资源和时间，如果任务来了才创建线程那么响应时间会变长，而且一个进程能创建的线程数有限。为了避免这些问题，在程序启动的时候就创建若干线程来响应处理，它们被称为线程池，里面的线程叫工作线程。从JDK1.5开始，Java API提供了Executor框架让你可以创建不同的线程池。比如单线程池，每次处理一个任务；数目固定的线程池或者是缓存线程池（一个适合很多生存期短的任务的程序的可扩展线程池）。更多内容详见这篇文章。 如何写代码来解决生产者消费者问题？ 在现实中你解决的许多线程问题都属于生产者消费者模型，就是一个线程生产任务供其它线程进行消费，你必须知道怎么进行线程间通信来解决这个问题。比较低级的办法是用wait和notify来解决这个问题，比较赞的办法是用Semaphore 或者 BlockingQueue来实现生产者消费者模型，这篇教程有实现它。 如何避免死锁http://www.cnblogs.com&amp;iframeId=iframe_0.21145907562000632&quot; frameborder=”0” scrolling=”no” height=”20”&gt; Java多线程中的死锁 死锁是指两个或两个以上的进程在执行过程中，因争夺资源而造成的一种互相等待的现象，若无外力作用，它们都将无法推进下去。这是一个严重的问题，因为死锁会让你的程序挂起无法完成任务，死锁的发生必须满足以下四个条件： 互斥条件：一个资源每次只能被一个进程使用。请求与保持条件：一个进程因请求资源而阻塞时，对已获得的资源保持不放。不剥夺条件：进程已获得的资源，在末使用完之前，不能强行剥夺。循环等待条件：若干进程之间形成一种头尾相接的循环等待资源关系。避免死锁最简单的方法就是阻止循环等待条件，将系统中所有的资源设置标志位、排序，规定所有的进程申请资源必须以一定的顺序（升序或降序）做操作来避免死锁。这篇教程有代码示例和避免死锁的讨论细节。 活锁和死锁有什么区别 这是上题的扩展，活锁和死锁类似，不同之处在于处于活锁的线程或进程的状态是不断改变的，活锁可以认为是一种特殊的饥饿。一个现实的活锁例子是两个人在狭小的走廊碰到，两个人都试着避让对方好让彼此通过，但是因为避让的方向都一样导致最后谁都不能通过走廊。简单的说就是，活锁和死锁的主要区别是前者进程的状态可以改变但是却不能继续执行。 怎么检测一个线程是否拥有锁 我一直不知道我们竟然可以检测一个线程是否拥有锁，直到我参加了一次电话面试。在java.lang.Thread中有一个方法叫holdsLock()，它返回true如果当且仅当当前线程拥有某个具体对象的锁。你可以查看这篇文章了解更多。 如何获取线程堆栈 对于不同的操作系统，有多种方法来获得Java进程的线程堆栈。当你获取线程堆栈时，JVM会把所有线程的状态存到日志文件或者输出到控制台。在Windows你可以使用Ctrl + Break组合键来获取线程堆栈，Linux下用kill -3命令。你也可以用jstack这个工具来获取，它对线程id进行操作，你可以用jps这个工具找到id。 JVM中哪个参数是用来控制线程的栈堆栈小的 这个问题很简单， -Xss参数用来控制线程的堆栈大小。你可以查看JVM配置列表来了解这个参数的更多信息。 synchronized 和 ReentrantLock 有什么不同？ Java在过去很长一段时间只能通过synchronized关键字来实现互斥，它有一些缺点。比如你不能扩展锁之外的方法或者块边界，尝试获取锁时不能中途取消等。Java 5 通过Lock接口提供了更复杂的控制来解决这些问题。 ReentrantLock 类实现了 Lock，它拥有与 synchronized 相同的并发性和内存语义且它还具有可扩展性。你可以查看这篇文章了解更多 有三个线程T1，T2，T3，怎么确保它们按顺序执行？ 在多线程中有多种方法让线程按特定顺序执行，你可以用线程类的join()方法在一个线程中启动另一个线程，另外一个线程完成该线程继续执行。为了确保三个线程的顺序你应该先启动最后一个(T3调用T2，T2调用T1)，这样T1就会先完成而T3最后完成。你可以查看这篇文章了解更多。 Thread类中的yield方法有什么作用 Yield方法可以暂停当前正在执行的线程对象，让其它有相同优先级的线程执行。它是一个静态方法而且只保证当前线程放弃CPU占用而不能保证使其它线程一定能占用CPU，执行yield()的线程有可能在进入到暂停状态后马上又被执行。点击这里查看更多yield方法的相关内容。 ConcurrentHashMap的并发度是什么 ConcurrentHashMap把实际map划分成若干部分来实现它的可扩展性和线程安全。这种划分是使用并发度获得的，它是ConcurrentHashMap类构造函数的一个可选参数，默认值为16，这样在多线程情况下就能避免争用。欲了解更多并发度和内部大小调整请阅读我的文章How ConcurrentHashMap works in Java。 Semaphore是什么 Java中的Semaphore是一种新的同步类，它是一个计数信号。从概念上讲，从概念上讲，信号量维护了一个许可集合。如有必要，在许可可用前会阻塞每一个 acquire()，然后再获取该许可。每个 release()添加一个许可，从而可能释放一个正在阻塞的获取者。但是，不使用实际的许可对象，Semaphore只对可用许可的号码进行计数，并采取相应的行动。信号量常常用于多线程的代码中，比如数据库连接池。更多详细信息请点击这里。 如果你提交任务时，线程池队列已满。会时发会生什么 这个问题问得很狡猾，许多程序员会认为该任务会阻塞直到线程池队列有空位。事实上如果一个任务不能被调度执行那么ThreadPoolExecutor’s submit()方法将会抛出一个RejectedExecutionException异常。 线程池中submit() 和 execute()方法有什么区别 两个方法都可以向线程池提交任务，execute()方法的返回类型是void，它定义在Executor接口中, 而submit()方法可以返回持有计算结果的Future对象，它定义在ExecutorService接口中，它扩展了Executor接口，其它线程池类像ThreadPoolExecutor和ScheduledThreadPoolExecutor都有这些方法。更多详细信息请点击这里。 什么是阻塞式方法 阻塞式方法是指程序会一直等待该方法完成期间不做其他事情，ServerSocket的accept()方法就是一直等待客户端连接。这里的阻塞是指调用结果返回之前，当前线程会被挂起，直到得到结果之后才会返回。此外，还有异步和非阻塞式方法在任务完成前就返回。更多详细信息请点击这里。 Swing是线程安全的吗为什么 你可以很肯定的给出回答，Swing不是线程安全的，但是你应该解释这么回答的原因即便面试官没有问你为什么。当我们说swing不是线程安全的常常提到它的组件，这些组件不能在多线程中进行修改，所有对GUI组件的更新都要在AWT线程中完成，而Swing提供了同步和异步两种回调方法来进行更新。点击这里查看更多swing和线程安全的相关内容。 invokeAndWait 和 invokeLater区别 这两个方法是Swing API 提供给Java开发者用来从当前线程而不是事件派发线程更新GUI组件用的。InvokeAndWait()同步更新GUI组件，比如一个进度条，一旦进度更新了，进度条也要做出相应改变。如果进度被多个线程跟踪，那么就调用invokeAndWait()方法请求事件派发线程对组件进行相应更新。而invokeLater()方法是异步调用更新组件的。更多详细信息请点击这里。 Swing API中那些方法是线程安全的 这个问题又提到了swing和线程安全，虽然组件不是线程安全的但是有一些方法是可以被多线程安全调用的，比如repaint(), revalidate()。 JTextComponent的setText()方法和JTextArea的insert() 和 append() 方法也是线程安全的。 如何创建Immutable对象 这个问题看起来和多线程没什么关系， 但不变性有助于简化已经很复杂的并发程序。Immutable对象可以在没有同步的情况下共享，降低了对该对象进行并发访问时的同步化开销。可是Java没有@Immutable这个注解符，要创建不可变类，要实现下面几个步骤：通过构造方法初始化所有成员、对变量不要提供setter方法、将所有的成员声明为私有的，这样就不允许直接访问这些成员、在getter方法中，不要直接返回对象本身，而是克隆对象，并返回对象的拷贝。我的文章how to make an object Immutable in Java有详细的教程，看完你可以充满自信。 ReadWriteLock是什么 一般而言，读写锁是用来提升并发程序性能的锁分离技术的成果。Java中的ReadWriteLock是Java 5 中新增的一个接口，一个ReadWriteLock维护一对关联的锁，一个用于只读操作一个用于写。在没有写线程的情况下一个读锁可能会同时被多个读线程持有。写锁是独占的，你可以使用JDK中的ReentrantReadWriteLock来实现这个规则，它最多支持65535个写锁和65535个读锁。 多线程中的忙循环是什么 忙循环就是程序员用循环让一个线程等待，不像传统方法wait(), sleep() 或 yield() 它们都放弃了CPU控制，而忙循环不会放弃CPU，它就是在运行一个空循环。这么做的目的是为了保留CPU缓存，在多核系统中，一个等待线程醒来的时候可能会在另一个内核运行，这样会重建缓存。为了避免重建缓存和减少等待重建的时间就可以使用它了。你可以查看这篇文章获得更多信息。 volatile 变量和 atomic 变量有什么不同 这是个有趣的问题。首先，volatile 变量和 atomic 变量看起来很像，但功能却不一样。Volatile变量可以确保先行关系，即写操作会发生在后续的读操作之前, 但它并不能保证原子性。例如用volatile修饰count变量那么 count++ 操作就不是原子性的。而AtomicInteger类提供的atomic方法可以让这种操作具有原子性如getAndIncrement()方法会原子性的进行增量操作把当前值加一，其它数据类型和引用变量也可以进行相似操作。 如果同步块内的线程抛出异常会发生什么 这个问题坑了很多Java程序员，若你能想到锁是否释放这条线索来回答还有点希望答对。无论你的同步块是正常还是异常退出的，里面的线程都会释放锁，所以对比锁接口我更喜欢同步块，因为它不用我花费精力去释放锁，该功能可以在finally block里释放锁实现。 单例模式的双检锁是什么 这个问题在Java面试中经常被问到，但是面试官对回答此问题的满意度仅为50%。一半的人写不出双检锁还有一半的人说不出它的隐患和Java1.5是如何对它修正的。它其实是一个用来创建线程安全的单例的老方法，当单例实例第一次被创建时它试图用单个锁进行性能优化，但是由于太过于复杂在JDK1.4中它是失败的，我个人也不喜欢它。无论如何，即便你也不喜欢它但是还是要了解一下，因为它经常被问到。你可以查看how double checked locking on Singleton works这篇文章获得更多信息。 如何创建线程安全的Singleton 这是上面那个问题的后续，如果你不喜欢双检锁而面试官问了创建Singleton类的替代方法，你可以利用JVM的类加载和静态变量初始化特征来创建Singleton实例，或者是利用枚举类型来创建Singleton，我很喜欢用这种方法。你可以查看这篇文章获得更多信息。 写出3条你遵循的多线程最佳实践 这种问题我最喜欢了，我相信你在写并发代码来提升性能的时候也会遵循某些最佳实践。以下三条最佳实践我觉得大多数Java程序员都应该遵循： 给你的线程起个有意义的名字。 这样可以方便找bug或追踪。OrderProcessor, QuoteProcessor or TradeProcessor 这种名字比 Thread-1. Thread-2 and Thread-3 好多了，给线程起一个和它要完成的任务相关的名字，所有的主要框架甚至JDK都遵循这个最佳实践。避免锁定和缩小同步的范围 锁花费的代价高昂且上下文切换更耗费时间空间，试试最低限度的使用同步和锁，缩小临界区。因此相对于同步方法我更喜欢同步块，它给我拥有对锁的绝对控制权。多用同步类少用wait 和 notify 首先，CountDownLatch, Semaphore, CyclicBarrier 和 Exchanger 这些同步类简化了编码操作，而用wait和notify很难实现对复杂控制流的控制。其次，这些类是由最好的企业编写和维护在后续的JDK中它们还会不断优化和完善，使用这些更高等级的同步工具你的程序可以不费吹灰之力获得优化。多用并发集合少用同步集合 这是另外一个容易遵循且受益巨大的最佳实践，并发集合比同步集合的可扩展性更好，所以在并发编程时使用并发集合效果更好。如果下一次你需要用到map，你应该首先想到用ConcurrentHashMap。我的文章Java并发集合有更详细的说明。 如何强制启动一个线程 这个问题就像是如何强制进行Java垃圾回收，目前还没有觉得方法，虽然你可以使用System.gc()来进行垃圾回收，但是不保证能成功。在Java里面没有办法强制启动一个线程，它是被线程调度器控制着且Java没有公布相关的API。 fork join框架是什么 fork join框架是JDK7中出现的一款高效的工具，Java开发人员可以通过它充分利用现代服务器上的多处理器。它是专门为了那些可以递归划分成许多子模块设计的，目的是将所有可用的处理能力用来提升程序的性能。fork join框架一个巨大的优势是它使用了工作窃取算法，可以完成更多任务的工作线程可以从其它线程中窃取任务来执行。你可以查看这篇文章获得更多信息。 多线程中调用wait() 和 sleep()方法有什么不同 Java程序中wait 和 sleep都会造成某种形式的暂停，它们可以满足不同的需要。wait()方法用于线程间通信，如果等待条件为真且其它线程被唤醒时它会释放锁，而sleep()方法仅仅释放CPU资源或者让当前线程停止执行一段时间，但不会释放锁。]]></content>
      <categories>
        <category>基础面试题</category>
      </categories>
      <tags>
        <tag>基础面试题</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[函数]]></title>
    <url>%2F2018%2F06%2F03%2F%E5%87%BD%E6%95%B0%2F</url>
    <content type="text"><![CDATA[函数2.1 size()方法和length属性 2.2 jQuery全面支持css2.1的选择器 2.3 jQuery全面支持css3的选择器 2.4 jQuery自己发明的伪类 2.5 ()函数和jQuery函数等价 2.6 ()函数得到的是jQuery对象 2.7 关于引号 123456789101112131415161718192021222324252627282930313233&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head lang="en"&gt; &lt;meta charset="UTF-8"&gt; &lt;title&gt;lesson2&lt;/title&gt; &lt;style&gt; .div1&#123; width: 100px; height: 100px; background-color: orange; &#125; .div2&#123; width: 100px; height: 100px; background-color: skyblue; &#125; &lt;/style&gt;&lt;/head&gt;&lt;body&gt;&lt;div class="div1"&gt;&lt;/div&gt;&lt;br/&gt;&lt;div class="div2"&gt;&lt;/div&gt;&lt;script src="js/jquery-1.12.3.min.js"&gt;&lt;/script&gt;&lt;!--&lt;script src="http://apps.bdimg.com/libs/jquery/1.7.2/jquery.min.js"&gt;&lt;/script&gt;--&gt;&lt;script&gt; /** * $() 函数 * 1. jq 是批量处理的，jq函数的结果是一个 jq 对象 * 说明： * a.在创建变量保存 jq 对象的时候，约定习惯使用 $ 做前缀，表示对象为 jq 对象而不是 js 对象 * b.jq 对象和 js 对象不是同一种类型，因此两者所拥有的方法不能混合使用 */ var $div = $('div'); console.log($div); console.log(typeof $div)&lt;/script&gt;&lt;/body&gt;&lt;/html&gt; size()方法和length属性12345678910111213141516171819202122232425262728293031323334&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head lang="en"&gt; &lt;meta charset="UTF-8"&gt; &lt;title&gt;lesson2&lt;/title&gt; &lt;style&gt; .div1&#123; width: 100px; height: 100px; background-color: orange; &#125; .div2&#123; width: 100px; height: 100px; background-color: skyblue; &#125; &lt;/style&gt;&lt;/head&gt;&lt;body&gt;&lt;div class="div1"&gt;&lt;/div&gt;&lt;br/&gt;&lt;div class="div2"&gt;&lt;/div&gt;&lt;div class="div3"&gt;&lt;/div&gt;&lt;script src="js/jquery-1.12.3.min.js"&gt;&lt;/script&gt;&lt;!--&lt;script src="http://apps.bdimg.com/libs/jquery/1.7.2/jquery.min.js"&gt;&lt;/script&gt;--&gt;&lt;script&gt; /** * $() 函数 * 1. jq 是批量处理的，jq函数的结果是一个 jq 对象 * 说明： * a.在创建变量保存 jq 对象的时候，约定习惯使用 $ 做前缀，表示对象为 jq 对象而不是 js 对象 * b.jq 对象和 js 对象不是同一种类型，因此两者所拥有的方法不能混合使用 * c.jq 中提供了 size() 方法和 length 属性，用来获取 jq 对象中包含的页面元素个数 */ var $div = $('div'); console.log($div.size()); console.log($div.length)&lt;/script&gt;&lt;/body&gt;&lt;/html&gt; 全面支持css2.1和css3的选择器1234567891011121314151617181920212223242526272829303132333435&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head lang="en"&gt; &lt;meta charset="UTF-8"&gt; &lt;title&gt;lesson2&lt;/title&gt; &lt;style&gt; .div1&#123; width: 100px; height: 100px; background-color: orange; &#125; .div2&#123; width: 100px; height: 100px; background-color: skyblue; &#125; &lt;/style&gt;&lt;/head&gt;&lt;body&gt;&lt;div class="div1"&gt;div1&lt;/div&gt;&lt;div class="div2"&gt;div2&lt;/div&gt;&lt;script src="js/jquery-1.12.3.min.js"&gt;&lt;/script&gt;&lt;!--&lt;script src="http://apps.bdimg.com/libs/jquery/1.7.2/jquery.min.js"&gt;&lt;/script&gt;--&gt;&lt;script&gt; /** * $() 函数 * 2. $ 函数在对页面元素的选择上具有非常灵活的写法 * 说明： * a.$() 函数全面支持 css2.1 之前的所有选择器写法 * b.$() 函数全面支持 css3 中的所有选择器写法 * c.$() 函数在选中页面元素的时候不存在兼容性的问题 */ // 空格和+都被称为关系选择器 // 空格表示：包含关系 // + 表示：相连关系，表示读取 div1 选择器后面紧挨着它的div2 选择器 // 下面的例子：是证明选择器的写法是支持的 var $div2 = $('.div1+.div2'); console.log($div2);&lt;/script&gt;&lt;/body&gt;&lt;/html&gt; 自己发明的伪类jQuery自己发明的【伪类】 其实这里我个人觉得更精准的说法应该是jq自己发明的【筛选器】。 因为他们的作用是能够从指定元素集合中【筛选】出想要的元素。 jq中自创的筛选器有七种： 选择器:first 选中指定元素集合中的第一个元素 选择器:last 选中指定元素集合中的最后一个元素 选择器:eq(n) 选中指定元素集合中从0开始，第n个元素 选择器:lt(n) 选中指定元素集合中从0开始，第n个元素之前的所有元素 选择器:gt(n) 选中指定元素集合中从0开始，第n个元素之后的所有元素 选择器:odd 选中指定元素集合中从0开始，所有奇数序号的元素 选择器:even 选中指定元素集合中从0开始，所有偶数序号的元素 12345678910111213141516171819202122232425262728293031&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head lang="en"&gt; &lt;meta charset="UTF-8"&gt; &lt;title&gt;lesson2&lt;/title&gt; &lt;style&gt; .div1&#123; width: 50px; height: 50px; background-color: orange; margin: 10px; &#125; &lt;/style&gt;&lt;/head&gt;&lt;body&gt;&lt;div class="div1"&gt;div0&lt;/div&gt;&lt;div class="div1"&gt;div1&lt;/div&gt;&lt;div class="div1"&gt;div2&lt;/div&gt;&lt;div class="div1"&gt;div3&lt;/div&gt;&lt;div class="div1"&gt;div4&lt;/div&gt;&lt;div class="div1"&gt;div5&lt;/div&gt;&lt;div class="div1"&gt;div6&lt;/div&gt;&lt;div class="div1"&gt;div7&lt;/div&gt;&lt;script src="js/jquery-1.12.3.min.js"&gt;&lt;/script&gt;&lt;!--&lt;script src="http://apps.bdimg.com/libs/jquery/1.7.2/jquery.min.js"&gt;&lt;/script&gt;--&gt;&lt;script&gt; // 选中所有div var $div = $('div'); console.log($div); $('div').css('background-color','red');&lt;/script&gt;&lt;/body&gt;&lt;/html&gt; 选择器:first1234567891011121314151617181920212223242526272829&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head lang="en"&gt; &lt;meta charset="UTF-8"&gt; &lt;title&gt;lesson2&lt;/title&gt; &lt;style&gt; .div1&#123; width: 50px; height: 50px; background-color: orange; margin: 10px; &#125; &lt;/style&gt;&lt;/head&gt;&lt;body&gt;&lt;div class="div1"&gt;div0&lt;/div&gt;&lt;div class="div1"&gt;div1&lt;/div&gt;&lt;div class="div1"&gt;div2&lt;/div&gt;&lt;div class="div1"&gt;div3&lt;/div&gt;&lt;div class="div1"&gt;div4&lt;/div&gt;&lt;div class="div1"&gt;div5&lt;/div&gt;&lt;div class="div1"&gt;div6&lt;/div&gt;&lt;div class="div1"&gt;div7&lt;/div&gt;&lt;script src="js/jquery-1.12.3.min.js"&gt;&lt;/script&gt;&lt;!--&lt;script src="http://apps.bdimg.com/libs/jquery/1.7.2/jquery.min.js"&gt;&lt;/script&gt;--&gt;&lt;script&gt; // 选择器:first 选中指定元素集合中的第一个元素 $('div:first').css('background-color','red');&lt;/script&gt;&lt;/body&gt;&lt;/html&gt; 选择器:last12345678910111213141516171819202122232425262728&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head lang="en"&gt; &lt;meta charset="UTF-8"&gt; &lt;title&gt;lesson2&lt;/title&gt; &lt;style&gt; .div1&#123; width: 50px; height: 50px; background-color: orange; margin: 10px; &#125; &lt;/style&gt;&lt;/head&gt;&lt;body&gt;&lt;div class="div1"&gt;div0&lt;/div&gt;&lt;div class="div1"&gt;div1&lt;/div&gt;&lt;div class="div1"&gt;div2&lt;/div&gt;&lt;div class="div1"&gt;div3&lt;/div&gt;&lt;div class="div1"&gt;div4&lt;/div&gt;&lt;div class="div1"&gt;div5&lt;/div&gt;&lt;div class="div1"&gt;div6&lt;/div&gt;&lt;div class="div1"&gt;div7&lt;/div&gt;&lt;script src="js/jquery-1.12.3.min.js"&gt;&lt;/script&gt;&lt;!--&lt;script src="http://apps.bdimg.com/libs/jquery/1.7.2/jquery.min.js"&gt;&lt;/script&gt;--&gt;&lt;script&gt; // 选择器:last 选中指定元素集合中的最后一个元素 $('div:last').css('background-color','red');&lt;/script&gt;&lt;/body&gt;&lt;/html&gt; 选择器:eq(n)123456789101112131415161718192021222324252627282930&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head lang="en"&gt; &lt;meta charset="UTF-8"&gt; &lt;title&gt;lesson2&lt;/title&gt; &lt;style&gt; .div1&#123; width: 50px; height: 50px; background-color: orange; margin: 10px; &#125; &lt;/style&gt;&lt;/head&gt;&lt;body&gt;&lt;div class="div1"&gt;div0&lt;/div&gt;&lt;div class="div1"&gt;div1&lt;/div&gt;&lt;div class="div1"&gt;div2&lt;/div&gt;&lt;div class="div1"&gt;div3&lt;/div&gt;&lt;div class="div1"&gt;div4&lt;/div&gt;&lt;div class="div1"&gt;div5&lt;/div&gt;&lt;div class="div1"&gt;div6&lt;/div&gt;&lt;div class="div1"&gt;div7&lt;/div&gt;&lt;script src="js/jquery-1.12.3.min.js"&gt;&lt;/script&gt;&lt;!--&lt;script src="http://apps.bdimg.com/libs/jquery/1.7.2/jquery.min.js"&gt;&lt;/script&gt;--&gt;&lt;script&gt; // 下标为 3 的元素 $('div:eq(3)').css('background-color','red'); // 另一种写法 //$('div').eq(3).css('background-color','red');&lt;/script&gt;&lt;/body&gt;&lt;/html&gt; 选择器:lt(n)1234567891011121314151617181920212223242526272829&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head lang="en"&gt; &lt;meta charset="UTF-8"&gt; &lt;title&gt;lesson2&lt;/title&gt; &lt;style&gt; .div1&#123; width: 50px; height: 50px; background-color: orange; margin: 10px; &#125; &lt;/style&gt;&lt;/head&gt;&lt;body&gt;&lt;div class="div1"&gt;div0&lt;/div&gt;&lt;div class="div1"&gt;div1&lt;/div&gt;&lt;div class="div1"&gt;div2&lt;/div&gt;&lt;div class="div1"&gt;div3&lt;/div&gt;&lt;div class="div1"&gt;div4&lt;/div&gt;&lt;div class="div1"&gt;div5&lt;/div&gt;&lt;div class="div1"&gt;div6&lt;/div&gt;&lt;div class="div1"&gt;div7&lt;/div&gt;&lt;script src="js/jquery-1.12.3.min.js"&gt;&lt;/script&gt;&lt;!--&lt;script src="http://apps.bdimg.com/libs/jquery/1.7.2/jquery.min.js"&gt;&lt;/script&gt;--&gt;&lt;script&gt; // 小于下标为 4 的元素 $('div:lt(4)').css('background-color','red');&lt;/script&gt;&lt;/body&gt;&lt;/html&gt; 选择器:gt(n)1234567891011121314151617181920212223242526272829&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head lang="en"&gt; &lt;meta charset="UTF-8"&gt; &lt;title&gt;lesson2&lt;/title&gt; &lt;style&gt; .div1&#123; width: 50px; height: 50px; background-color: orange; margin: 10px; &#125; &lt;/style&gt;&lt;/head&gt;&lt;body&gt;&lt;div class="div1"&gt;div0&lt;/div&gt;&lt;div class="div1"&gt;div1&lt;/div&gt;&lt;div class="div1"&gt;div2&lt;/div&gt;&lt;div class="div1"&gt;div3&lt;/div&gt;&lt;div class="div1"&gt;div4&lt;/div&gt;&lt;div class="div1"&gt;div5&lt;/div&gt;&lt;div class="div1"&gt;div6&lt;/div&gt;&lt;div class="div1"&gt;div7&lt;/div&gt;&lt;script src="js/jquery-1.12.3.min.js"&gt;&lt;/script&gt;&lt;!--&lt;script src="http://apps.bdimg.com/libs/jquery/1.7.2/jquery.min.js"&gt;&lt;/script&gt;--&gt;&lt;script&gt; // 大于下标为 4 的元素 $('div:gt(4)').css('background-color','red');&lt;/script&gt;&lt;/body&gt;&lt;/html&gt; 选择器:odd1234567891011121314151617181920212223242526272829&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head lang="en"&gt; &lt;meta charset="UTF-8"&gt; &lt;title&gt;lesson2&lt;/title&gt; &lt;style&gt; .div1&#123; width: 50px; height: 50px; background-color: orange; margin: 10px; &#125; &lt;/style&gt;&lt;/head&gt;&lt;body&gt;&lt;div class="div1"&gt;div0&lt;/div&gt;&lt;div class="div1"&gt;div1&lt;/div&gt;&lt;div class="div1"&gt;div2&lt;/div&gt;&lt;div class="div1"&gt;div3&lt;/div&gt;&lt;div class="div1"&gt;div4&lt;/div&gt;&lt;div class="div1"&gt;div5&lt;/div&gt;&lt;div class="div1"&gt;div6&lt;/div&gt;&lt;div class="div1"&gt;div7&lt;/div&gt;&lt;script src="js/jquery-1.12.3.min.js"&gt;&lt;/script&gt;&lt;!--&lt;script src="http://apps.bdimg.com/libs/jquery/1.7.2/jquery.min.js"&gt;&lt;/script&gt;--&gt;&lt;script&gt; // 下标为奇数的触发 $('div:odd').css('background-color','red');&lt;/script&gt;&lt;/body&gt;&lt;/html&gt; 选择器:even1234567891011121314151617181920212223242526272829&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head lang="en"&gt; &lt;meta charset="UTF-8"&gt; &lt;title&gt;lesson2&lt;/title&gt; &lt;style&gt; .div1&#123; width: 50px; height: 50px; background-color: orange; margin: 10px; &#125; &lt;/style&gt;&lt;/head&gt;&lt;body&gt;&lt;div class="div1"&gt;div0&lt;/div&gt;&lt;div class="div1"&gt;div1&lt;/div&gt;&lt;div class="div1"&gt;div2&lt;/div&gt;&lt;div class="div1"&gt;div3&lt;/div&gt;&lt;div class="div1"&gt;div4&lt;/div&gt;&lt;div class="div1"&gt;div5&lt;/div&gt;&lt;div class="div1"&gt;div6&lt;/div&gt;&lt;div class="div1"&gt;div7&lt;/div&gt;&lt;script src="js/jquery-1.12.3.min.js"&gt;&lt;/script&gt;&lt;!--&lt;script src="http://apps.bdimg.com/libs/jquery/1.7.2/jquery.min.js"&gt;&lt;/script&gt;--&gt;&lt;script&gt; // 下标为偶数的触发 $('div:even').css('background-color','red');&lt;/script&gt;&lt;/body&gt;&lt;/html&gt; 实例：表格奇数行颜色加深 12345678910111213141516171819202122232425262728&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head lang="en"&gt; &lt;meta charset="UTF-8"&gt; &lt;title&gt;lesson2&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;table border="1" cellapacing="0" align="center" width="600px"&gt; &lt;tr&gt;&lt;td&gt;111&lt;/td&gt;&lt;td&gt;111&lt;/td&gt;&lt;td&gt;111&lt;/td&gt;&lt;td&gt;111&lt;/td&gt;&lt;td&gt;111&lt;/td&gt;&lt;td&gt;111&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;111&lt;/td&gt;&lt;td&gt;111&lt;/td&gt;&lt;td&gt;111&lt;/td&gt;&lt;td&gt;111&lt;/td&gt;&lt;td&gt;111&lt;/td&gt;&lt;td&gt;111&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;111&lt;/td&gt;&lt;td&gt;111&lt;/td&gt;&lt;td&gt;111&lt;/td&gt;&lt;td&gt;111&lt;/td&gt;&lt;td&gt;111&lt;/td&gt;&lt;td&gt;111&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;111&lt;/td&gt;&lt;td&gt;111&lt;/td&gt;&lt;td&gt;111&lt;/td&gt;&lt;td&gt;111&lt;/td&gt;&lt;td&gt;111&lt;/td&gt;&lt;td&gt;111&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;111&lt;/td&gt;&lt;td&gt;111&lt;/td&gt;&lt;td&gt;111&lt;/td&gt;&lt;td&gt;111&lt;/td&gt;&lt;td&gt;111&lt;/td&gt;&lt;td&gt;111&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;111&lt;/td&gt;&lt;td&gt;111&lt;/td&gt;&lt;td&gt;111&lt;/td&gt;&lt;td&gt;111&lt;/td&gt;&lt;td&gt;111&lt;/td&gt;&lt;td&gt;111&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;script src="js/jquery-1.12.3.min.js"&gt;&lt;/script&gt;&lt;!--&lt;script src="http://apps.bdimg.com/libs/jquery/1.7.2/jquery.min.js"&gt;&lt;/script&gt;--&gt;&lt;script&gt; // jq 批量的，所以可以一次选择多个 // 下标为奇数的触发 $('tr:odd').css('background-color','darkgray');&lt;/script&gt;&lt;/body&gt;&lt;/html&gt; $()函数和jQuery函数等价1234567891011121314151617181920212223242526272829303132&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head lang="en"&gt; &lt;meta charset="UTF-8"&gt; &lt;title&gt;lesson2&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;table border="1" cellapacing="0" align="center" width="600px"&gt; &lt;tr&gt;&lt;td&gt;111&lt;/td&gt;&lt;td&gt;111&lt;/td&gt;&lt;td&gt;111&lt;/td&gt;&lt;td&gt;111&lt;/td&gt;&lt;td&gt;111&lt;/td&gt;&lt;td&gt;111&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;111&lt;/td&gt;&lt;td&gt;111&lt;/td&gt;&lt;td&gt;111&lt;/td&gt;&lt;td&gt;111&lt;/td&gt;&lt;td&gt;111&lt;/td&gt;&lt;td&gt;111&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;111&lt;/td&gt;&lt;td&gt;111&lt;/td&gt;&lt;td&gt;111&lt;/td&gt;&lt;td&gt;111&lt;/td&gt;&lt;td&gt;111&lt;/td&gt;&lt;td&gt;111&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;111&lt;/td&gt;&lt;td&gt;111&lt;/td&gt;&lt;td&gt;111&lt;/td&gt;&lt;td&gt;111&lt;/td&gt;&lt;td&gt;111&lt;/td&gt;&lt;td&gt;111&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;111&lt;/td&gt;&lt;td&gt;111&lt;/td&gt;&lt;td&gt;111&lt;/td&gt;&lt;td&gt;111&lt;/td&gt;&lt;td&gt;111&lt;/td&gt;&lt;td&gt;111&lt;/td&gt;&lt;/tr&gt; &lt;tr&gt;&lt;td&gt;111&lt;/td&gt;&lt;td&gt;111&lt;/td&gt;&lt;td&gt;111&lt;/td&gt;&lt;td&gt;111&lt;/td&gt;&lt;td&gt;111&lt;/td&gt;&lt;td&gt;111&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;script src="js/jquery-1.12.3.min.js"&gt;&lt;/script&gt;&lt;!--&lt;script src="http://apps.bdimg.com/libs/jquery/1.7.2/jquery.min.js"&gt;&lt;/script&gt;--&gt;&lt;script&gt; /** * $() 和 jQuery() 的关系 * 说明：在 jq 框架中 $() 完全等价于 jQuery() 函数，两者作用相同，只是写法不同而已。 * 原因： * a.简操作 * b.避免冲突 */ jQuery('tr:odd').css('background-color','darkgray');&lt;/script&gt;&lt;/body&gt;&lt;/html&gt; $ ()函数得到的是jQuery对象jq 方法返回值 12345678910111213141516171819202122232425&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head lang="en"&gt; &lt;meta charset="UTF-8"&gt; &lt;title&gt;lesson2&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;script src="js/jquery-1.12.3.min.js"&gt;&lt;/script&gt;&lt;!--&lt;script src="http://apps.bdimg.com/libs/jquery/1.7.2/jquery.min.js"&gt;&lt;/script&gt;--&gt;&lt;script&gt; /** * $() 和 jQuery() 的关系 * $() 函数的执行结果会返回一个 jq 对象 * 说明：jq 对象是一个集合对象，和 js 中的 "对象" 不是一个东西 */ // js中规定函数都会有一个执行结果，或者说是返回值。 // 如果函数没有返回值，会默认设置为 undefined function func()&#123;&#125; var result = func(); console.log(result);&lt;/script&gt;&lt;/body&gt;&lt;/html&gt; jq对象仅能够调用jq中设定的属性和方法，对于原生js的属性和方法都无法调用 jq对象可以在必要的时候转换为js原生对象。 jq对象可以通过【jq对象[n]】方式转换为js原生对象 var p = $(“p”)[0]; jq对象可以通过【.get(n)】方式转换为js原生对象 var p = $(“p”).get(0); 123456789101112131415161718192021&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head lang="en"&gt; &lt;meta charset="UTF-8"&gt; &lt;title&gt;lesson2&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;span&gt;123&lt;/span&gt;&lt;script src="js/jquery-1.12.3.min.js"&gt;&lt;/script&gt;&lt;!--&lt;script src="http://apps.bdimg.com/libs/jquery/1.7.2/jquery.min.js"&gt;&lt;/script&gt;--&gt;&lt;script&gt; /** * $() 和 jQuery() 的关系 * 说明：jq 对象因为和 js 对象不同，因此 js 的方法对 jq 无效 反之。。 */ console.log($('span'));&lt;/script&gt;&lt;/body&gt;&lt;/html&gt; jq 转换为 js 1234567891011121314151617181920212223&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head lang="en"&gt; &lt;meta charset="UTF-8"&gt; &lt;title&gt;lesson2&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;span&gt;123&lt;/span&gt;&lt;script src="js/jquery-1.12.3.min.js"&gt;&lt;/script&gt;&lt;!--&lt;script src="http://apps.bdimg.com/libs/jquery/1.7.2/jquery.min.js"&gt;&lt;/script&gt;--&gt;&lt;script&gt; /** * 补充： * 可以通过 jq 对象后【添加中括号】或调用【get(n)】方法，将 jq 对象装换为 js 对象。 */ // 方式1： var span = $('span')[0]; // 方式 2： var span = $('span').get(0); console.log(span);&lt;/script&gt;&lt;/body&gt;&lt;/html&gt; 关于引号前面提到过$()在选择元素的时候括号中先写引号，但是存在获取某些对象的时候不需要加引号的特例： $(window) $(document) $(this)]]></content>
      <categories>
        <category>jQuery</category>
      </categories>
      <tags>
        <tag>前端框架</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[事件监听]]></title>
    <url>%2F2018%2F06%2F03%2F%E4%BA%8B%E4%BB%B6%E7%9B%91%E5%90%AC%2F</url>
    <content type="text"><![CDATA[事件监听JQuery中的常用事件jQuery中事件的名字，一律没有on。 .click() 鼠标单击触发事件，参数可选（data，function） .dblclick() 双击触发，同上 .mousedown()/up() 鼠标按下/弹起触发事件 .mousemove() 鼠标移动事件 .mouseover()/out() 鼠标移入/移出触发事件 .mouseenter()/leave() 鼠标进入/离开触发事件* .hover(func1,func2) 鼠标移入调用func1函数，移出调用func2函数 .focusin() 鼠标聚焦到该元素时触发事件 .focusout() 鼠标失去焦点时触发事件 . focus()/.blur() 鼠标聚焦/失去焦点触发事件（不支持冒泡） .change() 表单元素发生改变时触发事件 .select() 文本元素被选中时触发事件 .submit() 表单提交动作触发* .keydown()/up() 键盘按键按下/弹起触发 .on() 多事件的绑定 .off() 移除事件的绑定 .trigger(“event”) 触发事件event调用 .triggerHandler() 触发事件，不会冒泡，不会触发默认事件 jQuery添加事件监听12345678910111213141516171819202122232425262728293031&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head lang="en"&gt; &lt;meta charset="UTF-8"&gt; &lt;title&gt;lesson3&lt;/title&gt; &lt;style&gt; .div1&#123; width: 100px; height: 100px; background-color: orange; &#125; .div2&#123; width: 100px; height: 100px; background-color: skyblue; &#125; &lt;/style&gt;&lt;/head&gt;&lt;body&gt;&lt;div class="div1"&gt;&lt;/div&gt;&lt;br/&gt;&lt;div class="div2"&gt;&lt;/div&gt;&lt;script src="js/jquery-1.12.3.min.js"&gt;&lt;/script&gt;&lt;!--&lt;script src="http://apps.bdimg.com/libs/jquery/1.7.2/jquery.min.js"&gt;&lt;/script&gt;--&gt;&lt;script&gt; /** * 监听事件 * 语法：jq 对象.事件名（回调函数） * 特征：因为jq对象是集合对象，因此添加事件监听也是批量操作的。 */ $('div').click(function () &#123; $(this).css('background-color','red'); &#125;);&lt;/script&gt;&lt;/body&gt;&lt;/html&gt; 注意：jq中对于事件的绑定还允许链式声明，不必重复获取jq对象。 且链式声明时除最后一个绑定函数末尾加分号表示绑定结束外，其余函数后均不必写任何内容。 12$("div").mouseenter(function()&#123;$(this).css("background-color","red");&#125;).mouseleave(function()&#123;$(this).css("background-color","orange");&#125;); 例子： 123456789101112131415161718192021222324252627282930313233&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head lang="en"&gt; &lt;meta charset="UTF-8"&gt; &lt;title&gt;lesson3&lt;/title&gt; &lt;style&gt; .div1&#123; width: 100px; height: 100px; background-color: orange; &#125; .div2&#123; width: 100px; height: 100px; background-color: skyblue; &#125; &lt;/style&gt;&lt;/head&gt;&lt;body&gt;&lt;div class="div1"&gt;&lt;/div&gt;&lt;br/&gt;&lt;div class="div2"&gt;&lt;/div&gt;&lt;script src="js/jquery-1.12.3.min.js"&gt;&lt;/script&gt;&lt;!--&lt;script src="http://apps.bdimg.com/libs/jquery/1.7.2/jquery.min.js"&gt;&lt;/script&gt;--&gt;&lt;script&gt; // $('div').dblclick(function () &#123; // $(this).css('background-color','red'); // &#125;).click(function () &#123; // $(this).css('background-color','green'); // &#125;); var $div = $('div'); $div.dblclick(function () &#123; $(this).css('background-color','red'); &#125;).click(function () &#123; $(this).css('background-color','green'); &#125;);&lt;/script&gt;&lt;/body&gt;&lt;/html&gt; 事件监听的特点：123456789101112131415161718192021222324252627282930313233343536&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head lang="en"&gt; &lt;meta charset="UTF-8"&gt; &lt;title&gt;lesson3&lt;/title&gt; &lt;style&gt; .div1&#123; width: 100px; height: 100px; background-color: orange; &#125; .div2&#123; width: 100px; height: 100px; background-color: skyblue; &#125; &lt;/style&gt;&lt;/head&gt;&lt;body&gt;&lt;div class="div1"&gt;&lt;/div&gt;&lt;br/&gt;&lt;div class="div2"&gt;&lt;/div&gt;&lt;script src="js/jquery-1.12.3.min.js"&gt;&lt;/script&gt;&lt;!--&lt;script src="http://apps.bdimg.com/libs/jquery/1.7.2/jquery.min.js"&gt;&lt;/script&gt;--&gt;&lt;script&gt; /** * 事件监听的特点： * jq中添加事件监听不会产生覆盖，并且执行顺序按照添加顺序执行。 */ $('div').click(function () &#123; console.log('这是第一个回调函数！'); &#125;).click(function () &#123; console.log('这是第二个回调函数！'); &#125;); $('div').click(function () &#123; console.log('这是第三个回调函数！'); &#125;);&lt;/script&gt;&lt;/body&gt;&lt;/html&gt; 添加事件监听的方法为jq对象添加事件监听除了本身的方法之外，jq还提供了很多添加事件监听的方法 (1)通过on方法来为jq对象添加事件监听、通过off方法来为jq对象取消事件监听。123456789101112131415161718192021222324252627282930313233343536373839404142434445&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head lang="en"&gt; &lt;meta charset="UTF-8"&gt; &lt;title&gt;lesson3&lt;/title&gt; &lt;style&gt; .div1&#123; width: 100px; height: 100px; background-color: orange; &#125; .div2&#123; width: 100px; height: 100px; background-color: skyblue; &#125; &lt;/style&gt;&lt;/head&gt;&lt;body&gt;&lt;div class="div1"&gt;&lt;/div&gt;&lt;br/&gt;&lt;div class="div2"&gt;&lt;/div&gt;&lt;script src="js/jquery-1.12.3.min.js"&gt;&lt;/script&gt;&lt;!--&lt;script src="http://apps.bdimg.com/libs/jquery/1.7.2/jquery.min.js"&gt;&lt;/script&gt;--&gt;&lt;script&gt; /** * 事件监听的其他添加方式 * (1)on/off方法 * 描述：on方法用来给jq对象添加事件监听 * off方法用来将jq对象上的事件监听取消 * 语法：jq对象.on('事件名',回调函数,c,d); * jq对象.off('事件名',b,c); */ $('div').on('click',function () &#123; console.log('这是第一个回调函数！'); &#125;).on('click',function () &#123; console.log('这是第二个回调函数！'); &#125;).on('dblclick',function () &#123; console.log('这是双击事件！'); &#125;); $('div').on('click',function () &#123; console.log('这是第三个回调函数！'); &#125;); // 取消掉单击事件 $('div').off('click');&lt;/script&gt;&lt;/body&gt;&lt;/html&gt; (2)通过bind方法来给jq对象添加事件监听bind方法的好处的就是能够给一个jq对象添加多个事件监听。事件名用空格隔开。 123456789101112131415161718192021222324252627282930313233&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head lang="en"&gt; &lt;meta charset="UTF-8"&gt; &lt;title&gt;lesson3&lt;/title&gt; &lt;style&gt; .div1&#123; width: 100px; height: 100px; background-color: orange; &#125; .div2&#123; width: 100px; height: 100px; background-color: skyblue; &#125; &lt;/style&gt;&lt;/head&gt;&lt;body&gt;&lt;div class="div1"&gt;&lt;/div&gt;&lt;br/&gt;&lt;div class="div2"&gt;&lt;/div&gt;&lt;script src="js/jquery-1.12.3.min.js"&gt;&lt;/script&gt;&lt;!--&lt;script src="http://apps.bdimg.com/libs/jquery/1.7.2/jquery.min.js"&gt;&lt;/script&gt;--&gt;&lt;script&gt; /** * 事件监听的其他添加方式 * bind()方法 * 语法： * 将多个事件绑定为同一个事件监听，每一个事件触发都会执行这个唯一的回调函数 * jq对象.bind('事件名1 事件名2 ...', 回调函数); */ $('.div1').bind('mousedown mouseup',function () &#123; console.log('状态改变啦！'); &#125;);&lt;/script&gt;&lt;/body&gt;&lt;/html&gt; bind还可以采用JSON形式的参数来给jq对象添加事件监听。 12345678910111213141516171819202122232425262728293031323334353637383940&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head lang="en"&gt; &lt;meta charset="UTF-8"&gt; &lt;title&gt;lesson3&lt;/title&gt; &lt;style&gt; .div1&#123; width: 100px; height: 100px; background-color: orange; &#125; .div2&#123; width: 100px; height: 100px; background-color: skyblue; &#125; &lt;/style&gt;&lt;/head&gt;&lt;body&gt;&lt;div class="div1"&gt;&lt;/div&gt;&lt;br/&gt;&lt;div class="div2"&gt;&lt;/div&gt;&lt;script src="js/jquery-1.12.3.min.js"&gt;&lt;/script&gt;&lt;!--&lt;script src="http://apps.bdimg.com/libs/jquery/1.7.2/jquery.min.js"&gt;&lt;/script&gt;--&gt;&lt;script&gt; /** * 事件监听的其他添加方式 * bind()方法 * 语法： * 将多个事件绑定为同一个事件监听，每一个事件触发都会执行这个唯一的回调函数 * jq对象.bind('事件名1 事件名2 ...', 回调函数); * jq对象.bind(&#123; * 事件名1：回调函数1, * 事件名2：回调函数2, * ... * &#125;); */ $('.div1').bind(&#123; mousedown:function () &#123;console.log('鼠标按下')&#125;, mouseup:function () &#123;console.log('鼠标抬起')&#125; &#125;);&lt;/script&gt;&lt;/body&gt;&lt;/html&gt; (3)通过one方法来给jq对象添加事件监听。但是需要注意通过one方法添加的事件监听是’一次性的’，只能执行一次。 1234567891011121314151617181920212223242526272829303132&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head lang="en"&gt; &lt;meta charset="UTF-8"&gt; &lt;title&gt;lesson3&lt;/title&gt; &lt;style&gt; .div1&#123; width: 100px; height: 100px; background-color: orange; &#125; .div2&#123; width: 100px; height: 100px; background-color: skyblue; &#125; &lt;/style&gt;&lt;/head&gt;&lt;body&gt;&lt;div class="div1"&gt;&lt;/div&gt;&lt;br/&gt;&lt;div class="div2"&gt;&lt;/div&gt;&lt;script src="js/jquery-1.12.3.min.js"&gt;&lt;/script&gt;&lt;!--&lt;script src="http://apps.bdimg.com/libs/jquery/1.7.2/jquery.min.js"&gt;&lt;/script&gt;--&gt;&lt;script&gt; /** * 事件监听的其他添加方式 * 通过one方法来给jq对象添加事件监听 * 但是需要注意通过one方法添加的事件监听是'一次性的'，只能执行一次。 */ $('.div1').one('click',function () &#123; console.log('这是 one 添加的事件监听的回调函数！'); &#125;);&lt;/script&gt;&lt;/body&gt;&lt;/html&gt;]]></content>
      <categories>
        <category>jQuery</category>
      </categories>
      <tags>
        <tag>前端框架</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[乐观锁与悲观锁的应用]]></title>
    <url>%2F2018%2F06%2F03%2F%E4%B9%90%E8%A7%82%E9%94%81%E4%B8%8E%E6%82%B2%E8%A7%82%E9%94%81%E7%9A%84%E5%AE%9E%E9%99%85%E5%BA%94%E7%94%A8%2F</url>
    <content type="text"><![CDATA[乐观锁与悲观锁的应用概念悲观锁(Pessimistic Lock)每次去拿数据的时候都认为别人会修改，所以每次在拿数据的时候都会上锁，这样别人想拿这个数据就会block直到它拿到锁。传统的关系型数据库里边就用到了很多这种锁机制，比如行锁，表锁等，读锁，写锁等，都是在做操作之前先上锁。 悲观并发控制主要用于数据争用激烈的环境，以及发生并发冲突时使用锁保护数据的成本要低于回滚事务的成本的环境中。 乐观锁(Optimistic Lock)每次去拿数据的时候都认为别人不会修改，所以不会上锁，但是在更新的时候会判断一下在此期间别人有没有去更新这个数据，可以使用版本号等机制。乐观锁适用于多读的应用类型，这样可以提高吞吐量，像数据库如果提供类似于write_condition机制的其实都是提供的乐观锁。 实例假设一个业务场景：数据库中有一条数据，需要获取到当前的值，在当前值的基础上+10，然后再更新回去。如果此时有两个线程同时并发处理，第一个线程拿到数据是10，+10=20更新回去。第二个线程原本是要在第一个线程的基础上再+20=40,结果由于并发访问取到更新前的数据为10，+20=30。 这就是典型的存在中间状态，导致数据不正确。来看以下的例子： 并发所带来的问题和上文提到的类似，这里有一张price表，表结构如下： 1234567CREATE TABLE `price` ( `id` int(11) NOT NULL AUTO_INCREMENT COMMENT &apos;主键&apos;, `total` decimal(12,2) DEFAULT &apos;0.00&apos; COMMENT &apos;总值&apos;, `front` decimal(12,2) DEFAULT &apos;0.00&apos; COMMENT &apos;消费前&apos;, `end` decimal(12,2) DEFAULT &apos;0.00&apos; COMMENT &apos;消费后&apos;, PRIMARY KEY (`id`)) ENGINE=InnoDB AUTO_INCREMENT=1268 DEFAULT CHARSET=utf8 我这里写了一个单测：就一个主线程，循环100次，每次把 front 的值减去10，再写入一次流水记录，正常情况是写入的每条记录都会每次减去10。 1234567891011121314151617181920212223242526272829303132333435 /** * 单线程消费 */ @Test public void singleCounsumerTest1()&#123;// 使用 Mybaits /* for (int i=0 ;i&lt;100 ;i++)&#123; Price price = priceMapper.selectByPrimaryKey(1); int ron = 10 ; price.setFront(price.getFront().subtract(new BigDecimal(ron))); price.setEnd(price.getEnd().add(new BigDecimal(ron))); price.setTotal(price.getFront().add(price.getEnd())); priceMapper.updateByPrimaryKey(price) ; price.setId(null); priceMapper.insertSelective(price) ; &#125;*/ // 使用 hibernate for (int i=0 ;i&lt;100 ;i++)&#123; Price price = priceDao.selectByPrimaryKey("1"); int ron = 10 ; price.setFront(price.getFront().subtract(new BigDecimal(ron))); price.setEnd(price.getEnd().add(new BigDecimal(ron))); price.setTotal(price.getFront().add(price.getEnd())); priceDao.updateByPrimaryKey(price) ; // price.setId(null); priceDao.insertSelective(price) ; &#125; &#125; 初始化数据库中的值： 执行结果如下： 但是如果是多线程的情况下会是如何呢： 我这里新建了一个 PriceController 123456789101112131415161718192021222324252627282930313233343536373839/** * @Author shenwenfang * @Date 2018/3/8 10:50 * @Description: 线程池 无锁 */@RestController@RequestMapping(value="threadPrice")public class ThreadPoolConfigController &#123; @Autowired PriceDao priceDao; @Autowired ThreadPoolConfig config; @RequestMapping(value = "/threadPrice.do") @ApiOperation(value = "锁的应用",httpMethod = "POST") public void threadPrice()&#123; try &#123; for(int i= 0;i &lt;10;i++)&#123; Thread t = new Thread(new Runnable() &#123; public void run() &#123; Price price = priceDao.selectByPrimaryKey("1"); int ron = 10; price.setFront(price.getFront().subtract(new BigDecimal(ron))); price.setEnd(price.getEnd().add(new BigDecimal(ron))); priceDao.updateByPrimaryKey(price); priceDao.insertSelective(price); &#125; &#125;); config.submit(t); &#125; &#125;catch (Exception e)&#123; e.printStackTrace(); &#125; &#125;&#125; 其中为了节省资源使用了一个线程池: 123456789101112131415161718192021222324import java.util.concurrent.TimeUnit;/** * @Author shenwenfang * @Date 2018/3/8 10:40 * @Description: 为了节省资源使用一个线程池 */@Servicepublic class ThreadPoolConfig &#123; private static final int MAX_SIZE = 10; private static final int CORE_SIZE = 5; private static final int SECOND = 1000; private ThreadPoolExecutor executor; public ThreadPoolConfig()&#123; executor = new ThreadPoolExecutor(CORE_SIZE,MAX_SIZE,SECOND,TimeUnit.MICROSECONDS, new LinkedBlockingQueue&lt;Runnable&gt;()); &#125; public void submit(Thread thread)&#123; executor.submit(thread); &#125;&#125; 关于线程池的使用今后会仔细探讨。这里就简单理解为有10个线程并发去处理上面单线程的逻辑，来看看结果怎么样： 会看到明显的数据错误，导致错误的原因自然就是有线程读取到了中间状态进行了错误的更新。 进而有了以下两种解决方案：悲观锁和乐观锁。 悲观锁简单理解下悲观锁：当一个事务锁定了一些数据之后，只有当当前锁提交了事务，释放了锁，其他事务才能获得锁并执行操作。 使用方式如下：首先要关闭 MySQL 的自动提交：set autocommit = 0; 123456bigen --开启事务select id, total, front, end from price where id=1 for update insert into price values(?,?,?,?,?)commit --提交事务 这里使用select for update的方式利用数据库开启了悲观锁，锁定了id=1的这条数据(注意:这里除非是使用了索引会启用行级锁，不然是会使用表锁，将整张表都锁住。)。之后使用commit提交事务并释放锁，这样下一个线程过来拿到的就是正确的数据。 悲观锁一般是用于并发不是很高，并且不允许脏读等情况。但是对数据库资源消耗较大。 优点与不足 悲观并发控制实际上是“先取锁再访问”的保守策略，为数据处理的安全提供了保证。但是在效率方面，处理加锁的机制会让数据库产生额外的开销，还有增加产生死锁的机会；另外，在只读型事务处理中由于不会产生冲突，也没必要使用锁，这样做只能增加系统负载；还有会降低了并行性，一个事务如果锁定了某行数据，其他事务就必须等待该事务处理完才可以处理那行数 乐观锁那么有没有性能好，支持的并发也更多的方式呢？ 那就是乐观锁。 乐观锁是首先假设数据冲突很少，只有在数据提交修改的时候才进行校验，如果冲突了则不会进行更新。 通常的实现方式增加一个version字段，为每一条数据加上版本。每次更新的时候version+1，并且更新时候带上版本号。实现方式如下： 新建了一张price_version表： 12345678CREATE TABLE `price_version` ( `id` int(11) NOT NULL AUTO_INCREMENT COMMENT &apos;主键&apos;, `total` decimal(12,2) DEFAULT &apos;0.00&apos; COMMENT &apos;总值&apos;, `front` decimal(12,2) DEFAULT &apos;0.00&apos; COMMENT &apos;消费前&apos;, `end` decimal(12,2) DEFAULT &apos;0.00&apos; COMMENT &apos;消费后&apos;, `version` int(11) DEFAULT &apos;0&apos; COMMENT &apos;并发版本控制&apos;, PRIMARY KEY (`id`)) ENGINE=InnoDB AUTO_INCREMENT=1268 DEFAULT CHARSET=utf8 更新数据的SQL： 12345678&lt;!-- Mubatis --&gt;&lt;update id=&quot;updateByVersion&quot; parameterType=&quot;com.crossoverJie.pojo.PriceVersion&quot;&gt; UPDATE price_version SET front = #&#123;front,jdbcType=DECIMAL&#125;, version= version + 1 WHERE id = #&#123;id,jdbcType=INTEGER&#125; AND version = #&#123;version,jdbcType=INTEGER&#125; &lt;/update&gt; 123456789101112@Transactionalpublic int updateByPrimaryKey(PriceVersion priceVersion)&#123; String sql = "UPDATE price_version\n" + "SET front = :front, version= version + 1\n" + "WHERE id = :id AND version = :version"; Map&lt;String,Object&gt; param = new HashMap&lt;String,Object&gt;(); param.put("front",priceVersion.getFront()); param.put("id",priceVersion.getId()); param.put("version",priceVersion.getVersion()); int count = getCurrentSession().createSQLQuery(sql).setProperties(param).executeUpdate(); return count;&#125; 调用方式： 12345678910111213141516171819202122232425/** * 线程池，乐观锁 * @param redisContentReq * @return */@RequestMapping(value = "/threadPriceVersion.do")@ApiOperation(value = "乐观锁的应用",httpMethod = "POST")public void threadPriceVersion()&#123; for(int i = 0;i&lt;3;i++)&#123; Thread t = new Thread(new Runnable() &#123; public void run() &#123; PriceVersion priceVersion = goodStoreDao.vselectByPrimaryKey("1"); int ron = new Random().nextInt(20); System.out.println("本次消费="+ron); priceVersion.setFront(new BigDecimal(ron)); int count = goodStoreDao.vupdateByPrimaryKey(priceVersion); if(count == 0) System.out.println("更新失败！"); else System.out.println("更新成功！"); &#125; &#125;); config.submit(t); &#125;&#125; 处理逻辑：开了三个线程生成了20以内的随机数更新到 front 字段。 当调用该接口时日志如下： 可以看到线程1、4、5分别生成了15，2，11三个随机数。最后线程4、5都更新失败了，只有线程1更新成功了。 查看数据库： 发现也确实是更新的6。 乐观锁在实际应用相对较多，它可以提供更好的并发访问，并且数据库开销较少，但是有可能存在脏读的情况。 优点与不足 乐观并发控制相信事务之间的数据竞争(data race)的概率是比较小的，因此尽可能直接做下去，直到提交的时候才去锁定，所以不会产生任何锁和死锁。但如果直接简单这么做，还是有可能会遇到不可预期的结果，例如两个事务都读取了数据库的某一行，经过修改以后写回数据库，这时就遇到了问题。 补充1.我们经常会在访问数据库的时候用到锁，怎么实现乐观锁和悲观锁呢？以Hibernate为例，可以通过为记录添加版本或时间戳字段来实现乐观锁。可以用session.Lock()锁定对象来实现悲观锁（本质上就是执行了SELECT * FROM t FOR UPDATE语句）。 2.如果把乐观锁看作是关于冲突检测的，那么悲观锁就是关于冲突避免的。在实际应用的源代码控制系统中， 这两种策略都可以被使用，但是现在大多数源代码开发者更倾向于使用乐观锁策略。（有一种很有道理的说法：乐观锁并不是真正的锁定，但是这种叫法很方便并且广泛流传，以至于不容忽略。） 在乐观锁和悲观锁之间进行选择的标准是：冲突的频率与严重性。如果冲突很少，或者冲突的后果不会很严重，那么通常情况下应该选择乐观锁，因为它能得到更好的并发性，而且更容易实现。但是，如果冲突的结果对于用户来说痛苦的，那么就需要使用悲观策略。]]></content>
      <categories>
        <category>乐观锁与悲观锁</category>
      </categories>
      <tags>
        <tag>Java 高级应用</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ThreadLocal 源码解析和使用场景]]></title>
    <url>%2F2018%2F06%2F03%2FThreadLocal%20%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90%E5%92%8C%E4%BD%BF%E7%94%A8%E5%9C%BA%E6%99%AF%2F</url>
    <content type="text"><![CDATA[ThreadLocal 源码解析和使用场景ThreadLocal 主要用途ThreadLocal 是在 JDK 包里面提供的，它提供了线程本地变量，也就是如果你创建了一个 ThreadLocal 变量，那么访问这个变量的每个线程都会有这个变量的一个本地拷贝，多个线程操作这个变量的时候，实际是操作的自己本地内存里面的变量，从而避免了线程安全问题，创建一个ThreadLocal变量后每个线程会拷贝一个变量到自己本地内存，如下图： 从JAVA官方对 ThreadLocal 类的说明定义（定义在示例代码中）：ThreadLocal 类用来提供线程内部的局部变量。这种变量在多线程环境下访问（通过 get 和 set 方法访问）时能保证各个线程的变量相对独立于其他线程内的变量。ThreadLocal 实例通常来说都是 private static 类型的，用于关联线程和线程上下文。 我们可以得知 ThreadLocal 的作用是：ThreadLocal 的作用是提供线程内的局部变量，不同的线程之间不会相互干扰，这种变量在线程的生命周期内起作用，减少同一个线程内多个函数或组件之间一些公共变量的传递的复杂度。 上述可以概述为：ThreadLocal 提供线程内部的局部变量，在本线程内随时随地可取，隔离其他线程。 ThreadLocal使用实例使用例子来调试，看下ThreadLocal如何使用，从而加深理解。例子开启了两个线程，每个线程内部设置了本地变量的值，然后调用print函数打印当前本地变量的值，如果打印后调用了本地变量额remove方法则会删除本地内存中的该变量，代码如下： 1234567891011121314151617181920212223242526272829303132333435363738394041public class ThreadLocalTest &#123; //(1)打印函数 static void print(String str)&#123; //1.1 打印当前线程本地内存中localVariable变量的值 System.out.println(str + ":" +localVariable.get()); //1.2 清除当前线程本地内存中localVariable变量 //localVariable.remove(); &#125; //(2) 创建ThreadLocal变量 static ThreadLocal&lt;String&gt; localVariable = new ThreadLocal&lt;&gt;(); public static void main(String[] args) &#123; //(3) 创建线程one Thread threadOne = new Thread(new Runnable() &#123; public void run() &#123; //3.1 设置线程one中本地变量localVariable的值 localVariable.set("threadOne local variable"); //3.2 调用打印函数 print("threadOne"); //3.3打印本地变量值 System.out.println("threadOne remove after" + ":" +localVariable.get()); &#125; &#125;); //(4) 创建线程two Thread threadTwo = new Thread(new Runnable() &#123; public void run() &#123; //4.1 设置线程one中本地变量localVariable的值 localVariable.set("threadTwo local variable"); //4.2 调用打印函数 print("threadTwo"); //4.3打印本地变量值 System.out.println("threadTwo remove after" + ":" +localVariable.get()); &#125; &#125;); //(5)启动线程 threadOne.start(); threadTwo.start(); &#125; 运行结果： 1234threadOne:threadOne local variablethreadTwo:threadTwo local variablethreadOne remove after:threadOne local variablethreadTwo remove after:threadTwo local variable 代码（2）创建了一个ThreadLocal变量 代码（3）（4）分别创建了线程one和two 代码（5）启动了两个线程。 线程one中代码3.1通过set方法设置了localVariable的值，这个设置的其实是线程one本地内存中的一个拷贝，这个拷贝线程two是访问不了的。然后代码3.2调用了print函数，代码1.1通过get函数获取了当前线程（线程one）本地内存中localVariable的值。 线程two执行类似线程one 解开代码1.2的注释后，再次运行，运行结果为： 1234threadOne:threadOne local variablethreadOne remove after:nullthreadTwo:threadTwo local variablethreadTwo remove after:null ThreadLocal实现原理首先看下ThreadLocal相关的类的类图结构 如上类图可知 Thread 类中有一个 threadLocals 和 inheritableThreadLocals 都是 ThreadLocalMap 类型的变量，而 ThreadLocalMap 是一个定制化的 HashMap，默认每个线程中这个两个变量都为null。 只有当前线程第一次调用了 ThreadLocal 的set或者get方法时候才会进行创建。其实每个线程的本地变量不是存放到 ThreadLocal 实例里面的，而是存放到调用线程的 threadLocals 变量里面。 也就是说 ThreadLocal 类型的本地变量是存放到具体的线程内存空间的。 ThreadLocal 就是一个工具壳，它通过 set 方法把 value 值放入调用线程的 threadLocals 里面存放起来，当调用线程调用它的 get 方法时候再从当前线程的 threadLocals 变量里面拿出来使用。 如果调用线程一直不终止那么这个本地变量会一直存放到调用线程的 threadLocals 变量里面，所以当不需要使用本地变量时候可以通过调用ThreadLocal 变量的 remove 方法，从当前线程的 threadLocals 里面删除该本地变量。 另外 Thread 里面的threadLocals 为何设计为 map 结构那？很明显是因为每个线程里面可以关联多个 ThreadLocal 变量。 为什么使用了弱引用ThreadLocalMap 中的存储实体 Entry 使用 ThreadLocal 作为 key，但这个 Entry 是继承弱引用 WeakReference 的，为什么要这样设计，使用了弱引用 WeakReference 会造成内存泄露问题吗？ 首先，回答这个问题之前，我需要解释一下什么是强引用，什么是弱引用。 我们在正常情况下，普遍使用的是强引用： 123A a = new A();B b = new B(); 当 a = null;b = null; 时，一段时间后，JAVA垃圾回收机制GC会将 a 和 b 对应所分配的内存空间给回收。 但考虑这样一种情况： 12C c = new C(b);b = null; 当 b 被设置成 null 时，那么是否意味这一段时间后GC工作可以回收 b 所分配的内存空间呢？答案是否定的，因为即使 b 被设置成 null ，但 c 仍然持有对 b 的引用，而且还是强引用，所以GC不会回收 b 原先所分配的空间，既不能回收，又不能使用，这就造成了 内存泄露。 那么我们该如何处理呢？ 可以通过 c = null;，也可以使用弱引用 WeakReference w = new WeakReference(b); 。因为使用了弱引用 WeakReference，GC 是可以回收 b 原先所分配的空间的。 上述解释主要参考自：对ThreadLocal实现原理的一点思考 回到 ThreadLocal 的层面上，ThreadLocalMap 使用 ThreadLocal 的弱引用作为key，如果一个ThreadLocal 没有外部强引用来引用它，那么系统 GC 的时候，这个 ThreadLocal 势必会被回收，这样一来，ThreadLocalMap 中就会出现 key 为 null 的 Entry，就没有办法访问这些 key 为 null 的 Entry 的 value，如果当前线程再迟迟不结束的话，这些 key 为 null 的 Entry 的 value 就会一直存在一条强引用链：Thread Ref -&gt; Thread -&gt; ThreaLocalMap -&gt; Entry -&gt; value 永远无法回收，造成内存泄漏。 其实，ThreadLocalMap的设计中已经考虑到这种情况，也加上了一些防护措施：在 ThreadLocal 的get(), set(), remove() 的时候都会清除线程 ThreadLocalMap 里所有 key 为null 的 value。 但是这些被动的预防措施并不能保证不会内存泄漏： 使用static的ThreadLocal，延长了ThreadLocal的生命周期，可能导致的内存泄漏（参考ThreadLocal 内存泄露的实例分析）。 分配使用了ThreadLocal又不再调用get(),set(),remove()方法，那么就会导致内存泄漏。 从表面上看内存泄漏的根源在于使用了弱引用。网上的文章大多着重分析 ThreadLocal 使用了弱引用会导致内存泄漏，但是另一个问题也同样值得思考：为什么使用弱引用而不是强引用？ 我们先来看看官方文档的说法： 12To help deal with very large and long-lived usages, the hash table entries use WeakReferences for keys. 为了应对非常大和长时间的用途，哈希表使用弱引用的 key。 下面我们分两种情况讨论： key 使用强引用：引用的 ThreadLocal 的对象被回收了，但是 ThreadLocalMap 还持有 ThreadLocal 的强引用，如果没有手动删除，ThreadLocal 不会被回收，导致 Entry 内存泄漏。 key 使用弱引用：引用的 ThreadLocal 的对象被回收了，由于 ThreadLocalMap 持有 ThreadLocal 的弱引用，即使没有手动删除，ThreadLocal 也会被回收。value 在下一次 ThreadLocalMap 调用get() ,set(),remove()的时候会被清除。 比较两种情况，我们可以发现：由于 ThreadLocalMap 的生命周期跟 Thread 一样长，如果都没有手动删除对应 key ，都会导致内存泄漏，但是使用弱引用可以多一层保障：弱引用 ThreadLocal 不会内存泄漏，对应的 value 在下一次 ThreadLocalMap 调用 get() ,set(),remove()的时候会被清除。 因此， ThreadLocal 内存泄漏的根源是：由于 ThreadLocalMap 的生命周期跟 Thread 一样长，如果没有手动删除对应 key 就会导致内存泄漏，而不是因为弱引用。 综合上面的分析，我们可以理解ThreadLocal内存泄漏的前因后果，那么怎么避免内存泄漏呢？ 每次使用完ThreadLocal，都调用它的 remove() 方法，清除数据。 在使用线程池的情况下，没有及时清理 ThreadLocal，不仅是内存泄漏的问题，更严重的是可能导致业务逻辑出现问题。所以，使用 ThreadLocal 就跟加锁完要解锁一样，用完就清理。 上述解释主要参考自：深入分析 ThreadLocal 内存泄漏问题 常用操作的底层实现原理根据上面的例子，我们进行调试，看看一下几个常用操作的实现原理。 get() 方法1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192/** * 返回当前线程对应的ThreadLocal的初始值 * 此方法的第一次调用发生在，当线程通过&#123;@link #get&#125;方法访问此线程的ThreadLocal值时 * 除非线程先调用了 &#123;@link #set&#125;方法，在这种情况下， * &#123;@code initialValue&#125; 才不会被这个线程调用。 * 通常情况下，每个线程最多调用一次这个方法， * 但也可能再次调用，发生在调用&#123;@link #remove&#125;方法后， * 紧接着调用&#123;@link #get&#125;方法。 * * &lt;p&gt;这个方法仅仅简单的返回null &#123;@code null&#125;; * 如果程序员想ThreadLocal线程局部变量有一个除null以外的初始值， * 必须通过子类继承&#123;@code ThreadLocal&#125; 的方式去重写此方法 * 通常, 可以通过匿名内部类的方式实现 * * @return 当前ThreadLocal的初始值 */protected T initialValue() &#123; return null;&#125;/** * 创建一个ThreadLocal * @see #withInitial(java.util.function.Supplier) */public ThreadLocal() &#123;&#125;/** * 返回当前线程中保存ThreadLocal的值 * 如果当前线程没有此ThreadLocal变量， * 则它会通过调用&#123;@link #initialValue&#125; 方法进行初始化值 * * @return 返回当前线程对应此ThreadLocal的值 */public T get() &#123; // 获取当前线程对象 Thread t = Thread.currentThread(); // 获取此线程对象中维护的ThreadLocalMap对象 ThreadLocalMap map = getMap(t); // 如果此map存在 if (map != null) &#123; // 以当前的ThreadLocal 为 key，调用getEntry获取对应的存储实体e ThreadLocalMap.Entry e = map.getEntry(this); // 找到对应的存储实体 e if (e != null) &#123; @SuppressWarnings("unchecked") // 获取存储实体 e 对应的 value值 // 即为我们想要的当前线程对应此ThreadLocal的值 T result = (T)e.value; return result; &#125; &#125; // 如果map不存在，则证明此线程没有维护的ThreadLocalMap对象 // 调用setInitialValue进行初始化 return setInitialValue();&#125;/** * set的变样实现，用于初始化值initialValue， * 用于代替防止用户重写set()方法 * * @return the initial value 初始化后的值 */private T setInitialValue() &#123; // 调用initialValue获取初始化的值 T value = initialValue(); // 获取当前线程对象 Thread t = Thread.currentThread(); // 获取此线程对象中维护的ThreadLocalMap对象 ThreadLocalMap map = getMap(t); // 如果此map存在 if (map != null) // 存在则调用map.set设置此实体entry map.set(this, value); else // 1）当前线程Thread 不存在ThreadLocalMap对象 // 2）则调用createMap进行ThreadLocalMap对象的初始化 // 3）并将此实体entry作为第一个值存放至ThreadLocalMap中 createMap(t, value); // 返回设置的值value return value;&#125;/** * 获取当前线程Thread对应维护的ThreadLocalMap * * @param t the current thread 当前线程 * @return the map 对应维护的ThreadLocalMap */ThreadLocalMap getMap(Thread t) &#123; return t.threadLocals;&#125; 调用 get() 操作获取 ThreadLocal 中对应当前线程存储的值时，进行了如下操作： 1 ) 获取当前线程 Thread 对象，进而获取此线程对象中维护的 ThreadLocalMap 对象。 2 ) 判断当前的 ThreadLocalMap 是否存在： 如果存在，则以当前的ThreadLocal为 key，调用ThreadLocalMap中的getEntry方法获取对应的存储实体 e。找到对应的存储实体 e，获取存储实体 e 对应的 value 值，即为我们想要的当前线程对应此ThreadLocal的值，返回结果值。 如果不存在，则证明此线程没有维护的 ThreadLocalMap 对象，调用 setInitialValue 方法进行初始化。返回setInitialValue 初始化的值。 setInitialValue 方法的操作如下： 1 ) 调用initialValue获取初始化的值。 2 ) 获取当前线程Thread对象，进而获取此线程对象中维护的ThreadLocalMap对象。 3 ) 判断当前的ThreadLocalMap是否存在： 如果存在，则调用map.set 设置此实体entry。 如果不存在，则调用createMap 进行 ThreadLocalMap 对象的初始化，并将此实体 entry 作为第一个值存放至 ThreadLocalMap 中。 set() 方法123456789101112131415161718192021222324252627282930313233/** * 设置当前线程对应的ThreadLocal的值 * 大多数子类都不需要重写此方法， * 只需要重写 &#123;@link #initialValue&#125;方法代替设置当前线程对应的ThreadLocal的值 * * @param value 将要保存在当前线程对应的 ThreadLocal 的值 * */public void set(T value) &#123; // 获取当前线程对象 Thread t = Thread.currentThread(); // 获取此线程对象中维护的 ThreadLocalMap 对象 ThreadLocalMap map = getMap(t); // 如果此map存在 if (map != null) // 存在则调用map.set设置此实体entry map.set(this, value); else // 1）当前线程Thread 不存在ThreadLocalMap对象 // 2）则调用createMap进行ThreadLocalMap对象的初始化 // 3）并将此实体entry作为第一个值存放至ThreadLocalMap中 createMap(t, value);&#125;/** * 为当前线程Thread 创建对应维护的ThreadLocalMap. * * @param t the current thread 当前线程 * @param firstValue 第一个要存放的ThreadLocal变量值 */void createMap(Thread t, T firstValue) &#123; t.threadLocals = new ThreadLocalMap(this, firstValue);&#125; 调用 set(T value) 操作设置ThreadLocal中对应当前线程要存储的值时，进行了如下操作： 1 ) 获取当前线程 Thread 对象，进而获取此线程对象中维护的 ThreadLocalMap 对象。 2 ) 判断当前的 ThreadLocalMap 是否存在： 如果存在，则调用 map.set 设置此实体 entry。 如果不存在，则调用 createMap 进行 ThreadLocalMap 对象的初始化，并将此实体 entry 作为第一个值存放至 ThreadLocalMap 中。 remove() 方法123456789101112131415161718/** * 删除当前线程中保存的ThreadLocal对应的实体entry * 如果此ThreadLocal变量在当前线程中调用 &#123;@linkplain #get read&#125;方法 * 则会通过调用&#123;@link #initialValue&#125;进行再次初始化， * 除非此值value是通过当前线程内置调用 &#123;@linkplain #set set&#125;设置的 * 这可能会导致在当前线程中多次调用&#123;@code initialValue&#125;方法 * * @since 1.5 */ public void remove() &#123; // 获取当前线程对象中维护的ThreadLocalMap对象 ThreadLocalMap m = getMap(Thread.currentThread()); // 如果此map存在 if (m != null) // 存在则调用map.remove // 以当前ThreadLocal为key删除对应的实体entry m.remove(this); &#125; 调用 remove() 操作删除ThreadLocal中对应当前线程已存储的值时，进行了如下操作： 获取当前线程 Thread 对象，进而获取此线程对象中维护的 ThreadLocalMap 对象。 判断当前的 ThreadLocalMap 是否存在， 如果存在，则调用 map.remove ，以当前 ThreadLocal 为 key 删除对应的实体 entry。 ThreadLocalMap的内部底层实现对 ThreadLocal 的常用操作实际是对线程Thread中的ThreadLocalMap进行操作，核心是ThreadLocalMap这个哈希表，接着我们来谈谈ThreadLocalMap的内部底层实现。 源代码： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970/** * ThreadLocalMap 是一个定制的自定义 hashMap 哈希表，只适合用于维护 * 线程对应ThreadLocal的值. 此类的方法没有在ThreadLocal 类外部暴露， * 此类是私有的，允许在 Thread 类中以字段的形式声明 ， * 以助于处理存储量大，生命周期长的使用用途， * 此类定制的哈希表实体键值对使用弱引用WeakReferences 作为key， * 但是, 一旦引用不在被使用， * 只有当哈希表中的空间被耗尽时，对应不再使用的键值对实体才会确保被 移除回收。 */static class ThreadLocalMap &#123; /** * 实体entries在此hash map中是继承弱引用 WeakReference, * 使用ThreadLocal 作为 key 键. 请注意，当key为null（i.e. entry.get() * == null) 意味着此key不再被引用,此时实体entry 会从哈希表中删除。 */ static class Entry extends WeakReference&lt;ThreadLocal&lt;?&gt;&gt; &#123; /** 当前 ThreadLocal 对应储存的值value. */ Object value; Entry(ThreadLocal&lt;?&gt; k, Object v) &#123; super(k); value = v; &#125; &#125; /** * 初始容量大小 16 -- 必须是2的n次方. */ private static final int INITIAL_CAPACITY = 16; /** * 底层哈希表 table, 必要时需要进行扩容. * 底层哈希表 table.length 长度必须是2的n次方. */ private Entry[] table; /** * 实际存储键值对元素个数 entries. */ private int size = 0; /** * 下一次扩容时的阈值 */ private int threshold; // 默认为 0 /** * 设置触发扩容时的阈值 threshold * 阈值 threshold = 底层哈希表table的长度 len * 2 / 3 */ private void setThreshold(int len) &#123; threshold = len * 2 / 3; &#125; /** * 获取该位置i对应的下一个位置index */ private static int nextIndex(int i, int len) &#123; return ((i + 1 &lt; len) ? i + 1 : 0); &#125; /** * 获取该位置i对应的上一个位置index */ private static int prevIndex(int i, int len) &#123; return ((i - 1 &gt;= 0) ? i - 1 : len - 1); &#125;&#125; ThreadLocalMap 的底层实现是一个定制的自定义 HashMap 哈希表，核心组成元素有： 1 ) Entry[] table; ：底层哈希表 table, 必要时需要进行扩容，底层哈希表 table.length 长度必须是2的n次方。 2 ) int size;：实际存储键值对元素个数 entries 3 ) int threshold;：下一次扩容时的阈值，阈值 threshold = len 2 / 3 (底层哈希表table的长度)。当size &gt;= threshold时，遍历 table 并删除 key 为 null 的元素，如果删除后`size &gt;= threshold3/4`时，需要对table 进行扩容 其中 Entry[] table; 哈希表存储的核心元素是 Entry ，Entry 包含： 1 ) ThreadLocal&lt;?&gt; k；：当前存储的 ThreadLocal 实例对象 2 ) Object value;：当前 ThreadLocal 对应储存的值value 需要注意的是，此 Entry 继承了弱引用 WeakReference ，所以在使用 ThreadLocalMap 时，发现key == null，则意味着此 key ThreadLocal 不在被引用，需要将其从 ThreadLocalMap 哈希表中移除。 12345678910111213141516171819/** * 用于创建一个新的hash map包含 (firstKey, firstValue). * ThreadLocalMaps 构造方法是延迟加载的,所以我们只会在至少有一个 * 实体entry存放时，才初始化创建一次（仅初始化一次）。 */ThreadLocalMap(ThreadLocal&lt;?&gt; firstKey, Object firstValue) &#123; // 初始化 table 初始容量为 16 table = new Entry[INITIAL_CAPACITY]; // 计算当前entry的存储位置 // 存储位置计算等价于： // ThreadLocal 的 hash 值 threadLocalHashCode % 哈希表的长度 length int i = firstKey.threadLocalHashCode &amp; (INITIAL_CAPACITY - 1); // 存储当前的实体，key 为 : 当前ThreadLocal value：真正要存储的值 table[i] = new Entry(firstKey, firstValue); // 设置当前实际存储元素个数 size 为 1 size = 1; // 设置阈值，为初始化容量 16 的 2/3。 setThreshold(INITIAL_CAPACITY);&#125; ThreadLocalMap 的构造方法是延迟加载的，也就是说，只有当线程需要存储对应的 ThreadLocal 的值时，才初始化创建一次（仅初始化一次）。初始化步骤如下： 1） 初始化底层数组 table 的初始容量为 16。 2） 获取 ThreadLocal 中的 threadLocalHashCode ，通过threadLocalHashCode &amp; (INITIAL_CAPACITY - 1)，即ThreadLocal 的 hash 值 threadLocalHashCode % 哈希表的长度 length 的方式计算该实体的存储位置。 3） 存储当前的实体，key 为 : 当前ThreadLocal value：真正要存储的值 4）设置当前实际存储元素个数 size 为 1 5）设置阈值setThreshold(INITIAL_CAPACITY)，为初始化容量 16 的 2/3。 源代码： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119/** * 根据key 获取对应的实体 entry. 此方法快速适用于获取某一存在key的 * 实体 entry，否则，应该调用getEntryAfterMiss方法获取，这样做是为 * 了最大限制地提高直接命中的性能 * * @param key 当前thread local 对象 * @return the entry 对应key的 实体entry, 如果不存在，则返回null */private Entry getEntry(ThreadLocal&lt;?&gt; key) &#123; // 计算要获取的entry的存储位置 // 存储位置计算等价于： // ThreadLocal 的 hash 值 threadLocalHashCode % 哈希表 的长度 length int i = key.threadLocalHashCode &amp; (table.length - 1); // 获取到对应的实体 Entry Entry e = table[i]; // 存在对应实体并且对应key相等，即同一ThreadLocal if (e != null &amp;&amp; e.get() == key) // 返回对应的实体Entry return e; else // 不存在 或 key不一致，则通过调用getEntryAfterMiss继续查找 return getEntryAfterMiss(key, i, e);&#125;/** * 当根据key找不到对应的实体entry 时，调用此方法。 * 直接定位到对应的哈希表位置 * * @param key 当前thread local 对象 * @param i 此对象在哈希表 table中的存储位置 index * @param e the entry 实体对象 * @return the entry 对应key的 实体entry, 如果不存在，则返回null */private Entry getEntryAfterMiss(ThreadLocal&lt;?&gt; key, int i, Entry e) &#123; Entry[] tab = table; int len = tab.length; // 循环遍历当前位置的所有实体entry while (e != null) &#123; // 获取当前entry 的 key ThreadLocal ThreadLocal&lt;?&gt; k = e.get(); // 比较key是否一致，一致则返回 if (k == key) return e; // 找到对应的entry ，但其key 为 null，则证明引用已经不存在 // 这是因为Entry继承的是WeakReference，这是弱引用带来的坑 if (k == null) // 删除过期(stale)的entry expungeStaleEntry(i); else // key不一致 ，key也不为空，则遍历下一个位置，继续查找 i = nextIndex(i, len); // 获取下一个位置的实体 entry e = tab[i]; &#125; // 遍历完毕，找不到则返回null return null;&#125;/** * 删除对应位置的过期实体，并删除此位置后对应相关联位置key = null的实体 * * @param staleSlot 已知的key = null 的对应的位置索引 * @return 对应过期实体位置索引的下一个key = null的位置 * (所有的对应位置都会被检查) */private int expungeStaleEntry(int staleSlot) &#123; // 获取对应的底层哈希表 table Entry[] tab = table; // 获取哈希表长度 int len = tab.length; // 擦除这个位置上的脏数据 tab[staleSlot].value = null; tab[staleSlot] = null; size--; // 直到我们找到 Entry e = null，才执行rehash操作 // 就是遍历完该位置的所有关联位置的实体 Entry e; int i; // 查找该位置对应所有关联位置的过期实体，进行擦除操作 for (i = nextIndex(staleSlot, len); (e = tab[i]) != null; i = nextIndex(i, len)) &#123; ThreadLocal&lt;?&gt; k = e.get(); if (k == null) &#123; e.value = null; tab[i] = null; size--; &#125; else &#123; int h = k.threadLocalHashCode &amp; (len - 1); if (h != i) &#123; tab[i] = null; // 我们必须一直遍历直到最后 // 因为还可能存在多个过期的实体 while (tab[h] != null) h = nextIndex(h, len); tab[h] = e; &#125; &#125; &#125; return i;&#125;/** * 删除所有过期的实体 */private void expungeStaleEntries() &#123; Entry[] tab = table; int len = tab.length; for (int j = 0; j &lt; len; j++) &#123; Entry e = tab[j]; if (e != null &amp;&amp; e.get() == null) expungeStaleEntry(j); &#125;&#125; ThreadLocal 的 get() 操作实际是调用 ThreadLocalMap 的 getEntry(ThreadLocal&lt;?&gt; key) 方法,此方法快速适用于获取某一存在 key 的实体 entry，否则，应该调用getEntryAfterMiss(ThreadLocal&lt;?&gt; key, int i, Entry e)方法获取，这样做是为了最大限制地提高直接命中的性能，该方法进行了如下操作： 1 ) 计算要获取的 entry 的存储位置，存储位置计算等价于：ThreadLocal 的 hash 值 threadLocalHashCode % 哈希表的长度 length。 2 ) 根据计算的存储位置，获取到对应的实体 Entry。判断对应实体 Entry 是否存在 并且 key 是否相等： 存在对应实体 Entry 并且对应 key 相等，即同一 ThreadLocal ，返回对应的实体 Entry。 不存在对应实体 Entry 或者 key 不相等，则通过调用getEntryAfterMiss(ThreadLocal&lt;?&gt; key, int i, Entry e)方法继续查找。 getEntryAfterMiss(ThreadLocal&lt;?&gt; key, int i, Entry e)方法操作如下： 1 ) 获取底层哈希表数组 table，循环遍历对应要查找的实体 Entry 所关联的位置。 2 ) 获取当前遍历的 entry 的 key ThreadLocal ，比较 key 是否一致，一致则返回。 3 ) 如果 key 不一致 并且 key 为 null，则证明引用已经不存在，这是因为 Entry 继承的是 WeakReference，这是弱引用带来的坑。调用expungeStaleEntry(int staleSlot)方法删除过期的实体 Entry（此方法不单独解释，请查看示例代码，有详细注释说明）。 4 ) key不一致 ，key也不为空，则遍历下一个位置，继续查找。 5 ) 遍历完毕，仍然找不到则返回null。 ​ 源代码： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122/** * 设置对应ThreadLocal的值 * * @param key 当前thread local 对象 * @param value 要设置的值 */ private void set(ThreadLocal&lt;?&gt; key, Object value) &#123; // 我们不会像get()方法那样使用快速设置的方式， // 因为通常很少使用set()方法去创建新的实体 // 相对于替换一个已经存在的实体, 在这种情况下, // 快速设置方案会经常失败。 // 获取对应的底层哈希表 table Entry[] tab = table; // 获取哈希表长度 int len = tab.length; // 计算对应threalocal的存储位置 int i = key.threadLocalHashCode &amp; (len-1); // 循环遍历table对应该位置的实体，查找对应的threadLocal for (Entry e = tab[i];e != null;e = tab[i = nextIndex(i, len)]) &#123; // 获取当前位置的ThreadLocal ThreadLocal&lt;?&gt; k = e.get(); // 如果key threadLocal一致，则证明找到对应的threadLocal if (k == key) &#123; // 赋予新值 e.value = value; // 结束 return; &#125; // 如果当前位置的key threadLocal为null if (k == null) &#123; // 替换该位置key == null 的实体为当前要设置的实体 replaceStaleEntry(key, value, i); // 结束 return; &#125; &#125; // 当前位置的k ！= key &amp;&amp; k != null // 创建新的实体，并存放至当前位置i tab[i] = new Entry(key, value); // 实际存储键值对元素个数 + 1 int sz = ++size; // 由于弱引用带来了这个问题，所以先要清除无用数据，才能判断现在的size有没有达到阀值threshhold // 如果没有要清除的数据，存储元素个数仍然 大于 阈值 则扩容 if (!cleanSomeSlots(i, sz) &amp;&amp; sz &gt;= threshold) // 扩容 rehash(); &#125; /** * 当执行set操作时，获取对应的key threadLocal，并替换过期的实体 * 将这个value值存储在对应key threadLocal的实体中，无论是否已经存在体 * 对应的key threadLocal * * 有一个副作用, 此方法会删除该位置下和该位置nextIndex对应的所有过期的实体 * * @param key 当前thread local 对象 * @param value 当前thread local 对象对应存储的值 * @param staleSlot 第一次找到此过期的实体对应的位置索引index * . */ private void replaceStaleEntry(ThreadLocal&lt;?&gt; key, Object value, int staleSlot) &#123; // 获取对应的底层哈希表 table Entry[] tab = table; // 获取哈希表长度 int len = tab.length; Entry e; // 往前找，找到table中第一个过期的实体的下标 // 清理整个table是为了避免因为垃圾回收带来的连续增长哈希的危险 // 也就是说，哈希表没有清理干净，当GC到来的时候，后果很严重 // 记录要清除的位置的起始首位置 int slotToExpunge = staleSlot; // 从该位置开始，往前遍历查找第一个过期的实体的下标 for (int i = prevIndex(staleSlot, len); (e = tab[i]) != null; i = prevIndex(i, len)) if (e.get() == null) slotToExpunge = i; // 找到key一致的ThreadLocal或找到一个key为 null的 for (int i = nextIndex(staleSlot, len); (e = tab[i]) != null; i = nextIndex(i, len)) &#123; ThreadLocal&lt;?&gt; k = e.get(); // 如果我们找到了key，那么我们就需要把它跟新的过期数据交换来保持哈希表的顺序 // 那么剩下的过期Entry呢，就可以交给expungeStaleEntry方法来擦除掉 // 将新设置的实体放置在此过期的实体的位置上 if (k == key) &#123; // 替换，将要设置的值放在此过期的实体中 e.value = value; tab[i] = tab[staleSlot]; tab[staleSlot] = e; // 如果存在，则开始清除之前过期的实体 if (slotToExpunge == staleSlot) slotToExpunge = i; // 在这里开始清除过期数据 cleanSomeSlots(expungeStaleEntry(slotToExpunge), len); return; &#125; // / 如果我们没有在往后查找中找没有找到过期的实体， // 那么slotToExpunge就是第一个过期Entry的下标了 if (k == null &amp;&amp; slotToExpunge == staleSlot) slotToExpunge = i; &#125; // 最后key仍没有找到，则将要设置的新实体放置 // 在原过期的实体对应的位置上。 tab[staleSlot].value = null; tab[staleSlot] = new Entry(key, value); // 如果该位置对应的其他关联位置存在过期实体，则清除 if (slotToExpunge != staleSlot) cleanSomeSlots(expungeStaleEntry(slotToExpunge), len); &#125; 12345678910111213141516171819202122232425262728293031323334/** * 启发式的扫描查找一些过期的实体并清除， * 此方法会再添加新实体的时候被调用, * 或者过期的元素被清除时也会被调用. * 如果实在没有过期数据，那么这个算法的时间复杂度就是O(log n) * 如果有过期数据，那么这个算法的时间复杂度就是O(n) * * @param i 一个确定不是过期的实体的位置，从这个位置i开始扫描 * * @param n 扫描控制: 有&#123;@code log2(n)&#125; 单元会被扫描, * 除非找到了过期的实体, 在这种情况下 * 有&#123;@code log2(table.length)-1&#125; 的格外单元会被扫描. * 当调用插入时, 这个参数的值是存储实体的个数， * 但如果调用 replaceStaleEntry方法, 这个值是哈希表table的长度 * (注意: 所有的这些都可能或多或少的影响n的权重 * 但是这个版本简单，快速，而且似乎执行效率还可以） * * @return true 返回true，如果有任何过期的实体被删除。 */private boolean cleanSomeSlots(int i, int n) &#123; boolean removed = false; Entry[] tab = table; int len = tab.length; do &#123; i = nextIndex(i, len); Entry e = tab[i]; if (e != null &amp;&amp; e.get() == null) &#123; n = len; removed = true; i = expungeStaleEntry(i); &#125; &#125; while ( (n &gt;&gt;&gt;= 1) != 0); return removed;&#125; 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061/** * 哈希表扩容方法 * 首先扫描整个哈希表table，删除过期的实体 * 缩小哈希表table大小 或 扩大哈希表table大小，扩大的容量是加倍. */private void rehash() &#123; // 删除所有过期的实体 expungeStaleEntries(); // 使用较低的阈值threshold加倍以避免滞后 // 存储实体个数 大于等于 阈值的3/4则扩容 if (size &gt;= threshold - threshold / 4) resize();&#125;/** * 扩容方法，以2倍的大小进行扩容 * 扩容的思想跟HashMap很相似，都是把容量扩大两倍 * 不同之处还是因为WeakReference带来的 */private void resize() &#123; // 记录旧的哈希表 Entry[] oldTab = table; // 记录旧的哈希表长度 int oldLen = oldTab.length; // 新的哈希表长度为旧的哈希表长度的2倍 int newLen = oldLen * 2; // 创建新的哈希表 Entry[] newTab = new Entry[newLen]; int count = 0; // 逐一遍历旧的哈希表table的每个实体，重新分配至新的哈希表中 for (int j = 0; j &lt; oldLen; ++j) &#123; // 获取对应位置的实体 Entry e = oldTab[j]; // 如果实体不会null if (e != null) &#123; // 获取实体对应的ThreadLocal ThreadLocal&lt;?&gt; k = e.get(); // 如果该ThreadLocal 为 null if (k == null) &#123; // 则对应的值也要清除 // 就算是扩容，也不能忘了为擦除过期数据做准备 e.value = null; // Help the GC &#125; else &#123; // 如果不是过期实体，则根据新的长度重新计算存储位置 int h = k.threadLocalHashCode &amp; (newLen - 1); // 将该实体存储在对应ThreadLocal的最后一个位置 while (newTab[h] != null) h = nextIndex(h, newLen); newTab[h] = e; count++; &#125; &#125; &#125; // 重新分配位置完毕，则重新计算阈值Threshold setThreshold(newLen); // 记录实际存储元素个数 size = count; // 将新的哈希表赋值至底层table table = newTab;&#125; ThreadLocal 的set(T value)操作实际是调用 ThreadLocalMap 的set(ThreadLocal&lt;?&gt; key, Object value)方法，该方法进行了如下操作： 1 ) 获取对应的底层哈希表 table ，计算对应 threalocal 的存储位置。 2 ) 循环遍历 table 对应该位置的实体，查找对应的 threadLocal。 3 ) 获取当前位置的 threadLocal，如果 key threadLocal 一致，则证明找到对应的 threadLocal，将新值赋值给找到的当前实体 Entry 的 value 中，结束。 4 ) 如果当前位置的 key threadLocal 不一致，并且 key threadLocal 为 null，则调用replaceStaleEntry(ThreadLocal&lt;?&gt; key, Object value,int staleSlot)方法（此方法不单独解释，请查看示例代码，有详细注释说明），替换该位置key == null 的实体为当前要设置的实体，结束。 5 ) 如果当前位置的 key threadLocal 不一致，并且 key threadLocal 不为 null ，则创建新的实体，并存放至当前位置 i tab[i] = new Entry(key, value);，实际存储键值对元素个数size + 1，由于弱引用带来了这个问题，所以要调用cleanSomeSlots(int i, int n)方法清除无用数据（此方法不单独解释，请查看示例代码，有详细注释说明），才能判断现在的 size 有没有达到阀值 threshhold ，如果没有要清除的数据，存储元素个数仍然 大于 阈值 则调用 rehash 方法进行扩容（此方法不单独解释，请查看示例代码，有详细注释说明）。 1234567891011121314151617181920212223/** * 移除对应ThreadLocal的实体 */private void remove(ThreadLocal&lt;?&gt; key) &#123; // 获取对应的底层哈希表 table Entry[] tab = table; // 获取哈希表长度 int len = tab.length; // 计算对应threalocal的存储位置 int i = key.threadLocalHashCode &amp; (len-1); // 循环遍历table对应该位置的实体，查找对应的threadLocal for (Entry e = tab[i];e != null;e = tab[i = nextIndex(i, len)]) &#123; // 如果key threadLocal一致，则证明找到对应的threadLocal if (e.get() == key) &#123; // 执行清除操作 e.clear(); // 清除此位置的实体 expungeStaleEntry(i); // 结束 return; &#125; &#125;&#125; ThreadLocal 的remove()操作实际是调用 ThreadLocalMap 的remove(ThreadLocal&lt;?&gt; key)方法，该方法进行了如下操作： 1 ) 获取对应的底层哈希表 table，计算对应 threalocal 的存储位置。 2 ) 循环遍历 table 对应该位置的实体，查找对应的 threadLocal。 3 ) 获取当前位置的threadLocal，如果 key threadLocal 一致，则证明找到对应的 threadLocal，执行删除操作，删除此位置的实体，结束。 ThreadLocal 在现时有什么应用场景总的来说 ThreadLocal 主要是解决2种类型的问题： 解决并发问题：使用 ThreadLocal 代替 synchronized 来保证线程安全。同步机制采用了“以时间换空间”的方式，而ThreadLocal 采用了“以空间换时间”的方式。前者仅提供一份变量，让不同的线程排队访问，而后者为每一个线程都提供了一份变量，因此可以同时访问而互不影响。 解决数据存储问题：ThreadLocal 为变量在每个线程中都创建了一个副本，所以每个线程可以访问自己内部的副本变量，不同线程之间不会互相干扰。如一个Parameter对象的数据需要在多个模块中使用，如果采用参数传递的方式，显然会增加模块之间的耦合性。此时我们可以使用 ThreadLocal 解决。 应用场景： Spring 使用 ThreadLocal 解决线程安全问题 我们知道在一般情况下，只有无状态的 Bean 才可以在多线程环境下共享，在 Spring 中，绝大部分 Bean 都可以声明为 singleton 作用域。就是因为 Spring 对一些 Bean（如RequestContextHolder、TransactionSynchronizationManager、LocaleContextHolder等）中非线程安全状态采用ThreadLocal进行处理，让它们也成为线程安全的状态，因为有状态的 Bean 就可以在多线程中共享了。 一般的 Web 应用划分为展现层、服务层和持久层三个层次，在不同的层中编写对应的逻辑，下层通过接口向上层开放功能调用。在一般情况下，从接收请求到返回响应所经过的所有程序调用都同属于一个线程ThreadLocal 是解决线程安全问题一个很好的思路，它通过为每个线程提供一个独立的变量副本解决了变量并发访问的冲突问题。在很多情况下，ThreadLocal 比直接使用 synchronized 同步机制解决线程安全问题更简单，更方便，且结果程序拥有更高的并发性。 示例代码： 1234567891011121314public abstract class RequestContextHolder &#123;···· private static final boolean jsfPresent = ClassUtils.isPresent("javax.faces.context.FacesContext", RequestContextHolder.class.getClassLoader()); private static final ThreadLocal&lt;RequestAttributes&gt; requestAttributesHolder = new NamedThreadLocal&lt;RequestAttributes&gt;("Request attributes"); private static final ThreadLocal&lt;RequestAttributes&gt; inheritableRequestAttributesHolder = new NamedInheritableThreadLocal&lt;RequestAttributes&gt;("Request context");·····&#125; ThreadLocal 和 synchronized 的区别ThreadLocal 和 synchronized 关键字都用于处理多线程并发访问变量的问题，只是二者处理问题的角度和思路不同。 ThreadLocal是一个Java类,通过对当前线程中的局部变量的操作来解决不同线程的变量访问的冲突问题。所以，ThreadLocal提供了线程安全的共享对象机制，每个线程都拥有其副本。 Java中的synchronized是一个保留字，它依靠JVM的锁机制来实现临界区的函数或者变量的访问中的原子性。在同步机制中，通过对象的锁机制保证同一时间只有一个线程访问变量。此时，被用作“锁机制”的变量时多个线程共享的。 同步机制(synchronized关键字)采用了以“时间换空间”的方式，提供一份变量，让不同的线程排队访问。而ThreadLocal采用了“以空间换时间”的方式，为每一个线程都提供一份变量的副本，从而实现同时访问而互不影响。 总结： ThreadLocal提供线程内部的局部变量，在本线程内随时随地可取，隔离其他线程。 ThreadLocal的设计是：每个Thread维护一个ThreadLocalMap哈希表，这个哈希表的key是ThreadLocal实例本身，value才是真正要存储的值Object。 对ThreadLocal 的常用操作实际是对线程 Thread 中的 ThreadLocalMap 进行操作。 ThreadLocalMap 的底层实现是一个定制的自定义哈希表，ThreadLocalMap 的阈值threshold = 底层哈希表 table 的长度 len * 2 / 3，当实际存储元素个数 size 大于或等于 阈值 threshold 的 3/4 时size &gt;= threshold*3/4，则对底层哈希表数组 table 进行扩容操作。 ThreadLocalMap 中的哈希表 Entry[] table 存储的核心元素是 Entry ，存储的 key 是 ThreadLocal 实例对象，value 是ThreadLocal 对应储存的值value。需要注意的是，此Entry继承了弱引用 WeakReference，所以在使用ThreadLocalMap时，发现key == null，则意味着此key ThreadLocal不在被引用，需要将其从ThreadLocalMap哈希表中移除。 ThreadLocalMap使用ThreadLocal的弱引用作为key，如果一个ThreadLocal没有外部强引用来引用它，那么系统 GC 的时候，这个ThreadLocal势必会被回收。所以，在ThreadLocal的get(),set(),remove()的时候都会清除线程ThreadLocalMap里所有key为null的value。如果我们不主动调用上述操作，则会导致内存泄露。 为了安全地使用ThreadLocal，必须要像每次使用完锁就解锁一样，在每次使用完ThreadLocal后都要调用remove() 来清理无用的Entry。这在操作在使用线程池时尤为重要。 ThreadLocal 和synchronized的区别：同步机制(synchronized关键字)采用了以“时间换空间”的方式，提供一份变量，让不同的线程排队访问。而ThreadLocal采用了“以空间换时间”的方式，为每一个线程都提供一份变量的副本，从而实现同时访问而互不影响。 ThreadLocal主要是解决2种类型的问题：A. 解决并发问题：使用ThreadLocal代替同步机制解决并发问题。B. 解决数据存储问题：如一个Parameter对象的数据需要在多个模块中使用，如果采用参数传递的方式，显然会增加模块之间的耦合性。此时我们可以使用ThreadLocal解决。 每个线程内部都有一个名字为threadLocals的成员变量，该变量类型为HashMap，其中key为我们定义的ThreadLocal变量的this引用，value则为我们set时候的值，每个线程的本地变量是存到到线程自己的内存变量threadLocals里面的，如果当前线程一直不消失那么这些本地变量会一直存到，所以可能会造成内存溢出，所以使用完毕后要记得调用ThreadLocal的remove方法删除对应线程的threadLocals中的本地变量。如果子线程中想要使用父线程中的threadlocal变量该如何做那？敬请期待 Java中高并发编程必备基础之并发包源码剖析 一书出版 原文地址：https://juejin.im/entry/5a4c753a6fb9a0451a76c538]]></content>
      <categories>
        <category>ThreadLocal</category>
      </categories>
      <tags>
        <tag>Java 源码解读</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Sublime Text新建.vue模板并高亮]]></title>
    <url>%2F2018%2F06%2F03%2FSublime%20Text%E6%96%B0%E5%BB%BA.vue%E6%A8%A1%E6%9D%BF%E5%B9%B6%E9%AB%98%E4%BA%AE%2F</url>
    <content type="text"><![CDATA[Sublime Text新建.vue模板并高亮准备工作 下载安装新建文件模板插件 SublimeTmpl 下载安装vue语法高亮插件 Vue Syntax Highlight Sublime Text安装插件的方法有两种： 使用Sublime Text自带的安装库 Package Control 去安装点击菜单栏的 Preferences -&gt; Package Control 或使用快捷键 CTRL+SHIFT+P 打开终端窗口，输入Install选择Package Control: Install Package来安装 下载直接放入包目录 (Preferences / Browse Packages) 中文:(首选项 / 包浏览器) 文件夹里面 SublimeTmpl Vue Syntax Highlight 创建.vue模板并让语法高亮安装完Vue Syntax Highlight之后，你打开.vue格式的文件就已经可以高亮了，我们现在来设置用快捷键直接创建.vue格式的文件。 SublimeTmpl 默认只有6种语法模板： html ctrl+alt+h javascript ctrl+alt+j css ctrl+alt+c php ctrl+alt+p ruby ctrl+alt+r python ctrl+alt+shift+p 我们现在新增创建 vue 格式的模板 创建vue文件模板 直接打开插件包的文件夹 Preferences -&gt; Browse Packages ​ 首选项 -&gt; 浏览程序包 ​ ​ 包文件夹 打开存放模板的文件夹 templates，随便复制一项，改名为vue.tmpl ​ 创建vue.tmpl vue.tmpl内容改为你想要的模板 ​ vue.tmpl内容 修改新建菜单，增加新建vue选项 SublimeTmpl新建菜单默认是没有vue的，如图 ​ 新建 -&gt; New File (SublimeTmpl) 点击上图的 Menu 选项，或者打开 Preferences -&gt; Package Settings -&gt; SublimeTmpl -&gt; Settings - Menu，如图 ​ 打开菜单配置项 复制一项，然后粘贴修改为 vue 项，如图 ​ 新增vue项 保存修改，就会在新建菜单里面出现vue项，如图 ​ 出现vue项 点击上图vue新建项，就会出现之前设置的模板内容，只不过没有语法高亮，并且是纯文本格式，如图 ​ 新建vue文件 模板绑定vue语法高亮 打开 Preferences -&gt; Package Settings -&gt; SublimeTmpl -&gt; Settings - Default，如图 ​ 打开默认设置项 复制一项并修改为vue，路径如下 ​ 绑定vue语法 绑定语法关联文件路径请查看目录 Sublime Text3\Data\Cache，寻找vue高亮语法插件名，并打开，如图 ​ Sublime Text3\Data\Cache目录 ​ ​ Sublime Text3\Data\Cache\vue-syntax-highlight 再次菜单新建vue就语法高亮了，如图 ​ 新建vue文件 绑定新建vue文件快捷键 打开 Preferences -&gt; Package Settings -&gt; SublimeTmpl -&gt; Key Bindings - Default，如图 ​ 打开设置快捷键文件 复制一项，粘贴创建新建vue快捷键为 ctrl+alt+v，如图 ​ 创建快捷键 保存后，菜单新建里也有了，如图 ​ 新建文件菜单 试试，完美！ ​ 完美 最后Preferences -&gt; Package Settings -&gt; SublimeTmpl -&gt; Settings - Commands 文件好像是配置命令的，配置方法也跟上面相同，照猫画虎即可~ 最后的最后通过这种方法，其他的语言模板也可以自己去创建。]]></content>
      <categories>
        <category>Sublime Text</category>
      </categories>
      <tags>
        <tag>开发工具使用</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Sublime Text 3 搭建Python开发环境]]></title>
    <url>%2F2018%2F06%2F03%2FSublime%20Text%203%20%E6%90%AD%E5%BB%BAPython%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83%2F</url>
    <content type="text"><![CDATA[Sublime Text 3 搭建Python开发环境前言 Sublime Text：一款具有代码高亮、语法提示、自动完成且反应快速的编辑器软件，不仅具有华丽的界面，还支持插件扩展机制，用她来写代码，绝对是一种享受。相比于难于上手的 Vim ，浮肿沉重的 Eclipse ， VS ，即便体积轻巧迅速启动的 Editplus 、 Notepad++ ，在 Sublime Text 面前也略显失色，无疑这款性感无比的编辑器是 Coding 和 Writing 最佳的选择，没有之一。 Sublime Text 3 的功能实在是太强大了，搭配各种 package ，码代码、美如画。对于 Sublime Text 3 的介绍网上一大堆，博主就不再这里赘述了。本篇博文主要是记录一下博主如何在 Sublime Text 3 下优雅的编写、编译、运行 python 代码。 安装 我使用的版本是 Sublime Text Build 3143 ，大家自行下载后直接安装即可，安装完之后需要 License 来激活我们的软件。 ​ Sublime Text Build 3143的下载路径： ​ https://code.aliyun.com/shenwenfang106/SublimeTextBuild3143.git 直接将下面的 License 复制过去就好，亲测可用： 12345678910111213—– BEGIN LICENSE —– TwitterInc 200 User License EA7E-890007 1D77F72E 390CDD93 4DCBA022 FAF60790 61AA12C0 A37081C5 D0316412 4584D136 94D7F7D4 95BC8C1C 527DA828 560BB037 D1EDDD8C AE7B379F 50C9D69D B35179EF 2FE898C4 8E4277A8 555CE714 E1FB0E43 D5D52613 C3D12E98 BC49967F 7652EED2 9D2D2E61 67610860 6D338B72 5CF95C69 E36B85CC 84991F19 7575D828 470A92AB —— END LICENSE ——12345678910111213 配置Package Control 按 Ctrl+` 调出 console ，粘贴以下代码到底部命令行并回车： 1import urllib.request,os,hashlib; h = '6f4c264a24d933ce70df5dedcf1dcaee' + 'ebe013ee18cced0ef93d5f746d80ef60'; pf = 'Package Control.sublime-package'; ipp = sublime.installed_packages_path(); urllib.request.install_opener( urllib.request.build_opener( urllib.request.ProxyHandler()) ); by = urllib.request.urlopen( 'http://packagecontrol.io/' + pf.replace(' ', '%20')).read(); dh = hashlib.sha256(by).hexdigest(); print('Error validating download (got %s instead of %s), please try manual install' % (dh, h)) if dh != h else open(os.path.join( ipp, pf), 'wb' ).write(by)1 重启 Sublime Text 3。如果在 Perferences-&gt;package settings 中看到 package control 这一项，则安装成功。按下 Ctrl+Shift+P 调出命令面板输入 install 调出 Install Package 选项并回车，然后在列表中选中要安装的插件。 下面介绍几个比较实用的 package 。 SideBarEnhancements SideBarEnhancements 扩展了侧边栏中菜单选项的数量，从而提升你的工作效率。诸如 “New file” 和 “Duplicate” 这样的选项对于 ST3 来说实在是太重要了，而且仅凭 “Delete” 这一个功能就让这个插件值得下载。这个功能将你会在你删除文件的时候把它放入回收站。虽然这个功能乍一看没什么用，但是当你没有使用这样的功能而彻底删除了一个文件的时候，除非你用了版本管理软件，否则你将很难恢复这个文件。 Anaconda Anaconda 是一个终极 Python 插件。它为 ST3 增添了多项 IDE 类似的功能，例如： Autocompletion 自动完成，该选项默认开启，同时提供多种配置选项。 Code linting 使用支持 pep8 标准的 PyLint 或者 PyFlakes。 McCabe code complexity checker 让你可以在特定的文件中使用 McCabe complexity checker. Goto Definitions 能够在你的整个工程中查找并且显示任意一个变量，函数，或者类的定义。 Find Usage 能够快速的查找某个变量，函数或者类在某个特定文件中的什么地方被使用了。 Show Documentation： 能够显示一个函数或者类的说明性字符串(当然，是在定义了字符串的情况下) 但是，刚安装完之后，打开一个 python 文档，所有代码都会被白色细线框中，如图所示； ​ 强迫症的我看着好难受，决心要搞一搞这东西。后来发现在 Sublime &gt; Preferences &gt; Package Settings &gt; Anaconda &gt; Settings – Default 下修改 linting behaviour 选项即可，我这里改成了只有在保存的时候linting工作。 12345678/* Sets the linting behaviour for anaconda: "always" - Linting works always even while you are writing (in the background) "load-save" - Linting works in file load and save only "save-only" - Linting works in file save only*/"anaconda_linting_behaviour": "save-only", SublimeREPL 这可能是对程序员来说最有用的插件。SublimeREPL 允许你在 Sublime Text 中运行各种语言（NodeJS ，Python，Ruby， Scala 和 Haskell 等等）。 在 Sublime &gt; Tools &gt; SublimeREPL 下我们可以看到 SublimeREPL 支持运行的所有语言。 下面的代码是在 AppData\Roaming\Sublime Text 3\Packages\SublimeREPL\config\Python 下的 Default.sublime-commands 文件，从中我们可以看到 SublimeREPL 所支持的 python 的各种运行方式。 1234567891011121314151617181920212223242526272829303132333435363738[ &#123; "caption": "SublimeREPL: Python", "command": "run_existing_window_command", "args": &#123; "id": "repl_python", "file": "config/Python/Main.sublime-menu" &#125; &#125;, &#123; "caption": "SublimeREPL: Python - PDB current file", "command": "run_existing_window_command", "args": &#123; "id": "repl_python_pdb", "file": "config/Python/Main.sublime-menu" &#125; &#125;, &#123; "caption": "SublimeREPL: Python - RUN current file", "command": "run_existing_window_command", "args": &#123; "id": "repl_python_run", "file": "config/Python/Main.sublime-menu" &#125; &#125;, &#123; "command": "python_virtualenv_repl", "caption": "SublimeREPL: Python - virtualenv" &#125;, &#123; "caption": "SublimeREPL: Python - IPython", "command": "run_existing_window_command", "args": &#123; "id": "repl_python_ipython", "file": "config/Python/Main.sublime-menu" &#125; &#125;] 接下来配置快捷键，打开 Sublime &gt; Preferences &gt; Key Building ，在右侧栏（ User 部分）添加下面的代码。下面的代码用 F5 来执行当前 Python 脚本，用 F4 来实现切换至 Python 命令行窗口。 12345678910111213[ &#123;"keys":["f5"], "caption": "SublimeREPL: Python - RUN current file", "command": "run_existing_window_command", "args": &#123;"id": "repl_python_run", "file": "config/Python/Main.sublime-menu"&#125;&#125; , &#123;"keys":["f4"], "caption": "SublimeREPL: Python", "command": "run_existing_window_command", "args": &#123;"id": "repl_python", "file": "config/Python/Main.sublime-menu"&#125;&#125;] 【1】Shift+Ctrl+Alt+p 创建 python 文件 先保存再执行。 【2】F5 执行代码 （如果你没有设置快捷键就 win+b） 【3】 看 Python 命令行窗口 ​ 当然，如果你电脑里面安装了两个版本的 Python ，而你想指定使用某个版本，则需要修改下面的代码。下面的代码是在 AppData\Roaming\Sublime Text 3\Packages\SublimeREPL\config\Python 下的 Main.sublime-menu 文件，主要修改 “cmd” 后面跟着的 python 命令。比如我电脑里 python2.7 的执行程序命名是 python.exe ，而 python3.6 的执行程序命名为 python3.exe ，我想要使用 python3 ，所以把所有 “cmd” 后面跟着的命令都改为 “python3” 。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586[ &#123; "id": "tools", "children": [&#123; "caption": "SublimeREPL", "mnemonic": "R", "id": "SublimeREPL", "children": [ &#123;"caption": "Python", "id": "Python", "children":[ &#123;"command": "repl_open", "caption": "Python", "id": "repl_python", "mnemonic": "P", "args": &#123; "type": "subprocess", "encoding": "utf8", "cmd": ["python3", "-i", "-u"], "cwd": "$file_path", "syntax": "Packages/Python/Python.tmLanguage", "external_id": "python", "extend_env": &#123;"PYTHONIOENCODING": "utf-8"&#125; &#125; &#125;, &#123;"command": "python_virtualenv_repl", "id": "python_virtualenv_repl", "caption": "Python - virtualenv"&#125;, &#123;"command": "repl_open", "caption": "Python - PDB current file", "id": "repl_python_pdb", "mnemonic": "D", "args": &#123; "type": "subprocess", "encoding": "utf8", "cmd": ["python3", "-i", "-u", "-m", "pdb", "$file_basename"], "cwd": "$file_path", "syntax": "Packages/Python/Python.tmLanguage", "external_id": "python", "extend_env": &#123;"PYTHONIOENCODING": "utf-8"&#125; &#125; &#125;, &#123;"command": "repl_open", "caption": "Python - RUN current file", "id": "repl_python_run", "mnemonic": "R", "args": &#123; "type": "subprocess", "encoding": "utf8", "cmd": ["python3", "-u", "$file_basename"], "cwd": "$file_path", "syntax": "Packages/Python/Python.tmLanguage", "external_id": "python", "extend_env": &#123;"PYTHONIOENCODING": "utf-8"&#125; &#125; &#125;, &#123;"command": "repl_open", "caption": "Python - IPython", "id": "repl_python_ipython", "mnemonic": "I", "args": &#123; "type": "subprocess", "encoding": "utf8", "autocomplete_server": true, "cmd": &#123; "osx": ["python3", "-u", "$&#123;packages&#125;/SublimeREPL/config/Python/ipy_repl.py"], "linux": ["python3", "-u", "$&#123;packages&#125;/SublimeREPL/config/Python/ipy_repl.py"], "windows": ["python3", "-u", "$&#123;packages&#125;/SublimeREPL/config/Python/ipy_repl.py"] &#125;, "cwd": "$file_path", "syntax": "Packages/Python/Python.tmLanguage", "external_id": "python", "extend_env": &#123; "PYTHONIOENCODING": "utf-8", "SUBLIMEREPL_EDITOR": "$editor" &#125; &#125; &#125; ]&#125; ] &#125;] &#125;] 别忘了， Sublime Text 3 也有自己的 build 功能，即也支持 python 等语言的代码构建（ ctrl + b ）。同样的，我们如何添加不同的 python 版本到我们的构建系统呢？很简单，Sublime &gt; Tools &gt; Build System &gt; New Build System，分别添加如下代码之后，再分别保存为 python2.sublime-build 和 python3.sublime-build ，这样，当我们再次打开 Sublime &gt; Tools &gt; Build System 之后，就会发现我们新添加的 python2 和 python3 构建系统了。 12345&#123; "cmd": ["D:/Program Files/Python/Python27/python.exe", "-u", "$file"], "file_regex": "^[ ]*File \"(...*?)\", line([0-9]*)", "selector": "source.python"&#125; 12345&#123; "cmd": ["D:/Program Files/Python/Python36/python3.exe", "-u", "$file"], "file_regex": "^[ ]*File \"(...*?)\", line([0-9]*)", "selector": "source.python"&#125; SublimeTmpl 快速生成文件模板 ，SublimeTmpl能新建html、css、javascript、php、python、ruby六种类型的文件模板，所有的文件模板都在插件目录的templates文件夹里，可以自定义编辑文件模板。 SublimeTmpl默认的快捷键: 123456ctrl+alt+h htmlctrl+alt+j javascriptctrl+alt+c cssctrl+alt+p phpctrl+alt+r rubyctrl+alt+shift+p python 这里我想修改一下python模板，所以就需要进行如下操作：Sublime &gt; Preferences &gt; Package Settings &gt; SublimeTmpl &gt; Settings – User 添加如下代码。然后 ctrl+alt+shift+p 来新建一个模板试试看。 123456789&#123; "disable_keymap_actions": false, // "all"; "html,css" "date_format" : "%Y-%m-%d %H:%M:%S", "attr": &#123; "author": "WordZzzz", "email": "wordzzzz@foxmail.com", "link": "http://blog.csdn.net/u011475210" &#125; &#125; 快捷键也是可以更改的，全部在 Sublime &gt; Preferences &gt; Package Settings &gt; SublimeTmpl 的设置中。 如果想要新建其他类型的文件模板的话，先自定义文件模板方在templates文件夹里，再分别打开Default (Windows).sublime-keymap、Default.sublime-commands、Main.sublime-menu、SublimeTmpl.sublime-settings这四个文件照着里面的格式自定义想要新建的类型，这里就不详细介绍了，请各位自己折腾哈~ 快捷键 跳转到任意内容 (“cmd+p”) 用来快速查找和打开文件。你仅仅只需要工程中文件的一部分路径或者文件名你就可以很容易的打开这个文件。这在一个大型的 Django 工程中显得非常方便。 跳转到指定行 (“ctrl+g”) 让你在当前文件中跳转到指定行数。 跳转到标志 (“cmd+r”) 可以列出当前文件中所有的函数或者类，让你更方便查找。你可以通过输入关键字来查找你所需要的函数或者类。 跳转到行首 (cmd+left-arrow-key) 与 跳转到行尾 (cmd+right-arrow-key) 删除当前行(ctrl+shift+k) 多重编辑 是我迄今为止最喜欢的快捷键选定一个单词，点击 “cmd+d”来选择同样的单词，再次点击 “cmd+d”*继续选择下一个单词…或者 “cmd+单击”来指定多个你想要同时修改的地方。 块编辑 (option+left-mouse-click) 用于选择一整块的内容。通常在整理 CSV 文件的时候用于删除空白内容。 自定义命令 你可以很容易地使用 Python 来编辑你自己的自定义命令和快捷键组合。例如： 拷贝当前文件路径到剪贴板 – 链接 关闭除当前活动标签页以外的所有其他标签页 – 链接 通过文件选项打开你的 Package 文件夹(Sublime &gt; Preferences &gt; Browse Packages)，然后打开 User 文件夹，接下来将上述的 Python 文件添加到 “/Sublime Text 3/Packages/User” 文件夹中。 最后请在 Key Bindings – User file (Sublime Text &gt; Preferences &gt; Package Settings &gt; AdvancedNewFile &gt; Key Bindings – User) 文件中完成快捷键绑定。 1234567891011[ // Copy file name &#123; "keys": ["cmd+shift+c"], "command": "copy_path_to_clipboard" &#125;, // Close all other tabs &#123; "keys": ["cmd+alt+w"], "command": "close_tabs" &#125; 参看文章：http://blog.csdn.net/u011475210/article/details/78168341]]></content>
      <categories>
        <category>搭建Python开发环境</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python发布包到pypi]]></title>
    <url>%2F2018%2F06%2F03%2Fpython%E5%8F%91%E5%B8%83%E5%8C%85%E5%88%B0pypi%2F</url>
    <content type="text"><![CDATA[python发布包到pypipython更新太快了，甚至连这种发布上传机制都在不断的更新，这导致网上的一些关于python发布上传到pypi的教程都过时了，按着博文操作可能会失败。 pypi相关概念介绍关于pypi本身 pypi是专门用于存放第三方python包的地方，你可以在这里找别人分享的模块，也可以自己分享模块给别人。可以通过easy_install或者pip进行安装。pypi针对分享提供了两个平台，一个是测试发布平台，一个是正式发布平台，我们正式发布前可以先用测试发布平台发布，看是否正确，然后再采用正式发布平台． 关于python打包发布工具 python的打包安装工具也经历了很多次变化，由最早的distutils到setuptools到distribute又回到setuptools，后来还有disutils2以及distlib等，其中distutils是python标准库的一部分，它提出了采用setup.py机制安装和打包发布上传机制．setuptools(操作系统发布版本可能没有自带安装,需要自己额外安装)基于它扩展了很多功能，也是采用setup.py机制，针对安装额外提供了easy_install命令．distribute是setuptools的一个分之，后来又合并到setuptools了，所以姑且就把它看做是最新的setuptools吧！和我们打包最相关的貌似就是distutils或者setuptools，两者都可以用来打包发布并上传到pypi，后面介绍采用distutils，如果想更多的功能，比如想通过entry points扩展的一些功能，那么就要使用setuptools了．另外，还有一个工具可以用来发布到pypi，叫twine，需要额外安装．最后,需要确保自己的工具都是尽量新的,官方给出的版本参考:twine v1.8.0+ (recommended tool), setuptools 27+, or the distutils included with Python 3.4.6+,Python 3.5.3+, Python 3.6+, and 2.7.13+,升级的参考命令: sudo -H pip install -U pip setuptools twine 写一个 setup.py123456789from distutils.core import setup # 从 Python 发布工具导入 setup 函数setup( name='swfswf', # 要打包的模块名称 version='1.0', description='Python Distribution Utilities', author='shenwenfang', author_email='1978626782@qq.com', url='https://swenfang.github.io/', ) 也可参阅官方文档： https://docs.python.org/2/dis… 注意： 这里只是最基本的参考例子，执行打包会报警告，说缺少一些需要的文件，比如MANIFEST.in、readme.txt等等，暂时忽略即可。正式的项目中会复杂很多，甚至需要用到setuptools来扩展。这部分可以参考其他文档 为了保证效果，在打包之前我们可以验证setup.py的正确性。执行代码python setup.py check，输出一般是running check，如果有错误或者警告，就会在此之后显示.没有任何显示表示Distutils认可你这个setup.py文件 执行 python setup.py sdist upload -r pypi 创建发布并上传,如果想先上传到测试平台，可以执行 python setup.py sdist upload -r pypitest，成功后再执行上面命令上传到正式平台。注意，这一步的配置文件里面由于pypi的发布机制更新导致有一些问题的出现。 410错误：这个是pypi上传机制变更导致的,我虽然参考的是最新的blog，但是还是过时了！！！（.pypirc的repository过时了，很多博客说的repository: https://pypi.python.org/pypi会导致后面步骤操作出现410错误） ​ 打包发布到pypi基本流程： 注册 pypi 账号，如果期望测试发布，同时需要注册pypitest账号（可以采用相同的用户名和密码） 直接通过官网注册 https://pypi.python.org/pypi?…，填写用户名、密码、确认密码、邮箱， 但是需要验证邮件并确认激活。 创建配置文件, 该配置文件里面记录了你的账号信息以及要发布的平台信息，参考如下配置文件创建即可 在自己的用户目录下新建一个空白文件命名为.pypirc，内容如下： 123456789101112[distutils]index-servers=pypi[pypi]repository = https://upload.pypi.org/legacy/username = shenwenfangpassword = **************# 如果期望测试发布，同时需要#repository: https://test.pypi.org/legacy/#username= your_username#password= your_password 相关命令： 1python setup.py check #验证setup.py的正确性 1python setup.py sdist upload -r pypi #打包发布到pypi，返回 "Server response (200) : OK" 说明上传成功 查看上传记录： 如果你的包已经上传成功，那么当你登录PyPI网站后应该能在右侧导航栏看到管理入口。 测试代码块12import swfswf;swfswf.print_lol(参数1，参数2...) 管理你的包如果你的包已经上传成功，那么当你登录PyPI网站后应该能在右侧导航栏看到管理入口。 点击包名进去后你可以对你的包进行管理，当然你也可以从这里删除这个包。 让别人使用你的包包发布完成后，其他人只需要使用pip就可以安装你的包文件。比如： 1pip install package-name 如果你更新了包，别人可以可以通过--update参数来更新： 1pip install package-name --update 可能遇到的错误 410错误：.pypirc的repository过时了，很多博客说的repository: https://pypi.python.org/pypi会导致后面步骤操作出现410错误 403错误：是因为项目和已有 的项目重名了，可以先到 https://pypi.python.org/simple/ 上搜一下看看是否重名。解决的方法自然就是修改一下setup.py中setup函数中的name参数，删除之前生成的dist文件夹并重新生成，然后再upload 参考文章： https://www.xncoding.com/2015/10/26/python/setuptools.html https://segmentfault.com/a/1190000008663126 http://www.cnblogs.com/rongpmcu/p/7662821.html]]></content>
      <categories>
        <category>发布包到pypi</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据迭代的基本用法]]></title>
    <url>%2F2018%2F06%2F03%2Fpython%E5%88%97%E8%A1%A8%E8%BF%AD%E4%BB%A3%2F</url>
    <content type="text"><![CDATA[数据迭代的基本用法：for 循环的使用12345&gt;&gt;&gt;fav_movies = ["The Holy Crail","The Life of Brian"]&gt;&gt;&gt;for each_flick in fav_movies:print(each_flick)The Holy CrailThe Life of Brian while 循环的使用1234567&gt;&gt;&gt;count=2&gt;&gt;&gt;while count&gt;0: 【注意加冒号了再回车】 print("aaaa") 【如果正确会自动缩进】 count = count-1 aaaa 【输出结果】aaaa enumerate，dict，zip 使用通过一个练习，看看这三个函数怎么用的哈。小丽，你也跟着敲一下代码，看看运行效果 题目：现有: 123list1=[&apos;neil&apos;,&apos;mike&apos;,&apos;lucy&apos;]list2=[&apos;123456&apos;,&apos;xuiasj==&apos;,&apos;passWD123&apos;]list3=[&apos;www.abc.com&apos;,&apos;www.mike.org&apos;,&apos;www.lucy.gov&apos;] 需要形成列表： 1fin_list = [&#123;&apos;name&apos;:&apos;neil&apos;,&apos;passwd&apos;:&apos;123456&apos;,&apos;url&apos;:&apos;www.abc.com&apos;&#125;,&#123;&apos;name&apos;:&apos;mike&apos;,&apos;passwd&apos;:&apos;xuiasj==&apos;,&apos;url&apos;:&apos;www.mike.org&apos;&#125;,&#123;&apos;name&apos;:&apos;lucy&apos;,&apos;passwd&apos;:&apos;passWD123&apos;,&apos;url&apos;:&apos;www.lucy.gov&apos;&#125;] python新手的小丽啊，这要如何实现呢？你先自己好好想想怎么实现，动手敲敲代码哈。 在这里我提供两种方法： 方法1: 使用 enumerate 函数来实现 123456789101112131415【按题目定义3个列表】&gt;&gt;&gt;list1 = ['neil','mike','lucy']&gt;&gt;&gt;list2 = ['123456','xuiasj==','passWD123']&gt;&gt;&gt;list3 = ['www.abc.com','www.mike.org','www.lucy.gov']&gt;&gt;&gt;fin_list = []&gt;&gt;&gt;for i , name in enumerate(list1): d = &#123;&#125; d['name'] = name d['password'] = list2[i] d['url'] = list3[i] fin_list.append(d) 【小丽你回车,会空格一行】&gt;&gt;&gt;fin_list 【再输入要输出的这个列表名】【这是输出的结果】[&#123;'name': 'neil', 'password': '123456', 'url': 'www.abc.com'&#125;, &#123;'name': 'mike', 'password': 'xuiasj==', 'url': 'www.mike.org'&#125;, &#123;'name': 'lucy', 'password': 'passWD123', 'url': 'www.lucy.gov'&#125;] 之前你问我:python 中 in 的使用 和 enumerate 函数，在这里我来回答你，你要认真看哈！ enumerate 函数一般情况下我们对一个列表或数组既要遍历索引又要遍历元素时，会这样写： 12&gt;&gt;&gt;for i in range (0,len(list)): 【其实，在 ypthon 中 for... in .. 它就是一个语法】&gt;&gt;&gt;print i ,list[i] 【range 是取一个范围的值】 但是这种方法有些累赘，使用内置enumerrate函数会有更加直接，优美的做法，先看看enumerate的定义： 1234567&gt;&gt;&gt;def enumerate(collection): 【def 函数我在下面会跟你讲】&gt;&gt;&gt; 'Generates an indexed series: (0,coll[0]), (1,coll[1])'&gt;&gt;&gt; i = 0 &gt;&gt;&gt; it = iter(collection) &gt;&gt;&gt; while 1: &gt;&gt;&gt; yield (i, it.next()) &gt;&gt;&gt; i += 1 enumerate会将数组或列表组成一个索引序列。使我们再获取索引和索引内容的时候更加方便如下： 12&gt;&gt;&gt;for index，text in enumerate(list)):&gt;&gt;&gt; print index ,text 如果你要计算文件的行数，可以这样写： 1&gt;&gt;&gt;count = len(open(thefilepath,‘rU’).readlines()) 前面这种方法简单，但是可能比较慢，当文件比较大时甚至不能工作，下面这种循环读取的方法更合适些。 1234&gt;&gt;&gt;Count = -1 &gt;&gt;&gt;For count,line in enumerate(open(thefilepath,‘rU’))：&gt;&gt;&gt; Pass&gt;&gt;&gt;Count += 1 小丽，计算文件的行数的这两种方法，看不懂没关系，你只要知道就可以了。 看到这里你必须掌握的是：for 的迭代 和 enumerate 行数的使用 在来看看 def 函数 def 函数对于某些需要重复调用的程序，可以使用函数进行定义，基本形式为： def 函数名(参数1, 参数2, ……, 参数N): 执行语句函数名为调用的表示名，参数则是传入的参数。 1234567891011# 例1：简单的函数使用# coding=gb2312 # 定义函数def hello(): print 'hello python!' # 调用函数 hello() &gt;&gt;&gt; hello python! 函数可以带参数和返回值，参数将按从左到右的匹配，参数可设置默认值，当使用函数时没给相应的参数时，会按照默认值进行赋值。 1234567891011121314151617181920# 例2：累加计算值# coding=gb2312 # 定义函数def myadd(a=1,b=100): result = 0 i = a while i &lt;= b: # 默认值为1+2+3+……+100 result += i i += 1 return result # 打印1+2+……+10 print myadd(1,10)print myadd() # 使用默认参数1，100print myadd(50) # a赋值50，b使用默认值 &gt;&gt;&gt; 55&gt;&gt;&gt; 5050&gt;&gt;&gt; 3825 Python 函数的参数传递时，值得注意的是参数传入时若为变量会被当作临时赋值给参数变量，如果是对象则会被引用。 12345678910111213141516# 例3：# coding=gb2312 def testpara(p1,p2): p1 = 10 p2.append('hello') l = [] # 定义一数组对像a = 20 # 给变量a赋值testpara(a,l) # 变量a与对象数组l作为参数传入print a # 打印运行参数后的值for v in l: # 打印数组对象的成员 print v &gt;&gt;&gt; 20 # 调用函数后a变量并未被复值&gt;&gt;&gt; hello # 而对象l数组则增加成员hello 方法2: 使用 dict 和 zip 函数来实现** 1234567891011121314【按题目定义3个列表】&gt;&gt;&gt;list1 = ['neil','mike','lucy']&gt;&gt;&gt;list2 = ['123456','xuiasj==','passWD123']&gt;&gt;&gt;list3 = ['www.abc.com','www.mike.org','www.lucy.gov']&gt;&gt;&gt;fin_list = []&gt;&gt;&gt;style = ['name','passwd','url']&gt;&gt;&gt;fin_list.append(dict(zip(style,list1)))&gt;&gt;&gt;fin_list.append(dict(zip(style,list2)))&gt;&gt;&gt;fin_list.append(dict(zip(style,list3)))&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;print(fin_list)【这是输出的结果】[&#123;'name': 'neil', 'password': '123456', 'url': 'www.abc.com'&#125;, &#123;'name': 'mike', 'password': 'xuiasj==', 'url': 'www.mike.org'&#125;, &#123;'name': 'lucy', 'password': 'passWD123', 'url': 'www.lucy.gov'&#125;]]]></content>
      <categories>
        <category>数据迭代</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python3 安装]]></title>
    <url>%2F2018%2F06%2F03%2FPython3%20%E5%AE%89%E8%A3%85%2F</url>
    <content type="text"><![CDATA[Python3 安装Linux下默认安装的是Python2.7，要使用Python3，需要自行安装。Python3最新的版本是Python3.6。 在这里介绍在CentOS，Debian和Windows上安装Python3.6。 小丽，你跳过前面的，只看Windows上安装Python3.6哈。 CentOS安装Python3.6更详细的安装过程查看：CentOS7安装Python3.6 IUS软件源中包含了Python3.6，可以使用IUS软件源安装Python3.6，查看如何安装使用IUS软件源 1）安装IUS软件源 12345复制#安装EPEL依赖sudo yum install epel-release#安装IUS软件源sudo yum install https://centos7.iuscommunity.org/ius-release.rpm 2）安装Python3.6 1复制sudo yum install python36u 安装Python3完成后的shell命令为python3.6，为了使用方便，创建一个到python3的符号链接 1复制sudo ln -s /bin/python3.6 /bin/python3 3）安装pip3 安装完成python36u并没有安装pip，安装pip 1复制sudo yum install python36u-pip 安装pip完成后的shell命令为pip3.6，为了使用方便，创建一个到pip3的符号链接 1复制sudo ln -s /bin/pip3.6 /bin/pip3 Debian安装Python3.6Debian8的软件源中包含了Python3.4，要安装Python3.6，需要下载源文件安装： 1）安装编译，安装Python源文件的依赖包，GCC编译器，Make编译程序，Zlib压缩库： 1复制sudo aptitude -y install gcc make zlib1g-dev 2）运行如下命令安装Python3.6，以下命令依次为获取Python3.6源文件，解压，配置环境，编译，安装Python3.6 123456复制wget https://www.python.org/ftp/python/3.6.0/Python-3.6.0.tar.xztar xJf Python-3.6.0.tar.xzcd Python-3.6.0sudo ./configuresudo makesudo make install 安装完成后，Python安装在了/usr/local文件夹中，可运行文件/usr/local/bin，库文件/usr/local/lib，可以使用如下命令查看安装位置和版本 3）查看安装位置： 123复制anxin@bogon:~$ which python3/usr/local/bin/python3 4）验证Python3.6版本 123复制anxin@bogon:~$ python3 -VPython 3.6.0 Windows安装Python3.61）进入Python下载页面下载对应版本的Python安装包，本例下载Python3.6 64位Windows安装包 python-3.6.2-amd64.exe 如果你的电脑系统是32 位的,你就直接点击箭头的按钮下载就好了! 如果你的电脑系统是64 位的,就按下面流程下载哈 下载下来： 2）双击Python3.6安装包，开始安装Python3.6。 1. 选择 Install Now 安装方式方式 其实你也可以一步就完成 Python3.6 的安装，如果选择 Install Now 安装方式，以后下的步骤你可以全部不用做。 查看安装是否成功，打开cmd 输入 python 即可 2. 选择选择自定义安装（Customize installation）方式，可以选择安装的内容和目录： 注：安装Python3.6时，可以选中 Add Python 3.6 to PATH ，这样Python会自动把Python3.6的路径加入当前用户的PATH路径下，而不是系统的PATH路径下。 3）选择要安装的内容（如果你清除它们是什么），一般可以选择默认，即：所有内容，点击Next 4）选择一些安装选择，一般选择默认，在这一步可以选择安装目录，也可以直接输入目录地址，点击“浏览（Browse）”按钮： 5）选择安装的目录，最好先创建好目录如 python36 ，点击“确定”按钮： 6）如图所示选择好Python3.6的安装目录，点击“Install”按钮，开始安装Python3.6 7）如不出什么以外，会提示你安装完成 恭喜你，成功的在Windows安装了Python 3.6！ 配置Python3.6环境变量你也可以在安装完成Python3.6后，自己手动配置Python3.6的环境变量。 Win7进入控制面板–&gt;系统和安全–&gt;系统–&gt;高级系统设置–&gt;环境变量–&gt;系统变量，选中Path，双击编辑Path环境变量，添加路径\和\Scripts\： 在本例中我们添加的Python3.6环境变量的路径为：;C:\python36\Scripts\;C:\python36\，注意：Windows的环境变量已 ; 分隔。 我们敲代码的地方 打开箭头指向的 IDLE ，就是我们的编辑器。]]></content>
      <categories>
        <category>Python3 安装</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python 的基本要素]]></title>
    <url>%2F2018%2F06%2F03%2FPython%20%E7%9A%84%E5%9F%BA%E6%9C%AC%E8%A6%81%E7%B4%A0%2F</url>
    <content type="text"><![CDATA[Python 的基本要素 基本数据类型 对象引用 组合数据类型 逻辑操作符 控制流语句 算数操作符 输入/输出 函数的创建与调用 要素1：基本数据类型 任何数据语言都必须能够表示基本数据项目 Python 中的基本数据类型有 【1】Integral 类型 ​ （1）.整型：不可变类型 1num = 1 #被保存在内存中，是不可变的 ​ 注意：对象和变量都是不可变的 ​ （2）.布尔型 (Ture、False) ​ 注意：加引号 【2】浮点类型 ​ （1）. 浮点数字 ​ （2）. 复数 【3】字符串 ​ 注意：在这里字符串表示的序列。 1type(放任意的类型) # 输出变量的类型 ​ 要素2：对象引用（变量） Python 将所有数据存为内存对象 Python 中，对象事实上是只想内存对象的引用 动态类型：在任何时刻，只要需要，某个对象都可以重新引用一个不同的对象（可以是不同的数据类型） 内建函数type()用于返回给定数据项的数据类型 “=” 用于将变量名与内存中的某个对象进行绑定：如果对象事先存在，就直接绑定；否则用 “=” 创建引用的对象。 变量的命名规则 只能包含字母下、数字和下划线，且不能以数字开头。 区分数字大小写 禁止使用保留字段（Python2和Python3有所不同） 命名惯例 一单个下划线开头的变量名称 (_x) 不会被 from model import * 语句导入 前后都有下滑线的变量名(_x _) 是系统定义的变量名称，对 Python 解释器有特殊的意义 以两个下划线开头但结尾没有下划线的变量名称(__x)是类的本地变量 交互模式下，变量名 “—” 用于保存最后表达式的结果 1234&gt;&gt;&gt; 1+12&gt;&gt;&gt; print(_)2 ​ 注意： 变量名没有类型，对象才有 变量可以指定任何类型（这是 Python 和其他语言不同的） 要素3：组合数据类型 数据结构：通过某种方式（例如对元素进行编码）组织在一起的数据元素的集合 Python 常用的组合数据类型 序列类型 列表：使用[]创建，如[‘Call’,’me’,’Ishmell’,’_’] 12345&gt;&gt;&gt; l1 = ['Call','me','Ishmell','_']&gt;&gt;&gt; l1[0]'Call'&gt;&gt;&gt; l1[0][0]'C' 注意：列表是可变的，可以在原处进行修改，且内容改变，id 不会改变 1234567891011121314&gt;&gt;&gt; print(l1)['Call', 'me', 'Ishmell', '_']&gt;&gt;&gt; print(l1[1])me&gt;&gt;&gt; l1[1] = 'your'&gt;&gt;&gt; print(l1)['Call', 'your', 'Ishmell', '_']&gt;&gt;&gt; id(l1)1543556324360&gt;&gt;&gt; l1[2]= 'Xshell'&gt;&gt;&gt; print(l1)['Call', 'your', 'Xshell', '_']&gt;&gt;&gt; id(l1)1543556324360 ​ 元组：使用 () 创建，如(‘one’,’two’) 12345&gt;&gt;&gt; t1 = ('This','is')&gt;&gt;&gt; t1[1]'is'&gt;&gt;&gt; t1[0]'This' 注意: 元组是不能做原处修改的，一旦修改就会引发异常 ​ 字符串也属于序列类型 优点：可以做字符串的切块操作 123456789101112131415&gt;&gt;&gt; name = 'jerry'&gt;&gt;&gt; name[0]'j'&gt;&gt;&gt; name[0:1]'j'&gt;&gt;&gt; name[0:2]'je'&gt;&gt;&gt; name[:2]'je'&gt;&gt;&gt; name[2:]'rry'&gt;&gt;&gt; name[0:4]'jerr'&gt;&gt;&gt; name[0:4:2]'jr' 注意：切块本身会创建新的对象（因为字符串本身是不可用的） 集合类型 集合（杂乱的数据） 映射类型 字典 列表是可变序列，元组是不可变序列 Python 中，组合数据类型也是对象，因此其可以嵌套 [‘hello’,’worle’,[1,2,3]] 实质上，列表和元组并不是真正存储数据，而是存放对象引用 Python 对象可以具有其可以被调用的特定 “方法（函数）” 元组、列表以及字符串等数据类型是“有大小的”，也即，其长度可用内置函数 len() 测量 要素4：逻辑操作符 逻辑运算是任何程序设计语言的基本共能 Python 提供了4 组逻辑运算符 身份操作符 is:判断左端对象引用是否等于右端对象引用；也可以与Node进行； 对象引用可以不同，但是对象 所属的类型有可能是相同 1234&gt;&gt;&gt; name="swfswf"&gt;&gt;&gt; test="swfswf"&gt;&gt;&gt; type(name) is type(test)True 比较操作符号 &lt;,&gt;,&lt;=,&gt;=,!=,== 成员操作符 in 或 not in :测试成员关系 逻辑运算符 and、or、not 要素5：控制流语句 控制流语句是过程式编程语言的基本控制机制 Python 的常见控制流语句 if 1234567if boolean_expression1: suite1else if boolean_expression2: suite2.....else: else_suite 注意：冒号是代码块起始的标志 while 12while boolean_expression: suite for…in 12for variable in iterable: suite try 要素6：算数操作符 Python 提供了完整的算数操作符集 很多的 Python 数据类型也可以使用增强的赋值操作符，如+=、-= 等 同样的功能使用增强型赋值操作符的性能较好。 Python 的 int 类型是不可变的，因此，增强型赋值的实际过程是创建了一个新的对象来存储结果后将变量名执行了重新绑定 要素7：输入/输出 现实中，具有实际共能的程序必须能够读取输入（如从键盘或文件中），以及产生输出，并写到终端或文件中 Python 的输入/输出 输出 Python3：print() 函数 Python2: print 语句 输入 input() 12345678910&gt;&gt;&gt; input("plz input a num:")plz input a num:a'a'&gt;&gt;&gt; input("plz input a num:")plz input a num:3'3'&gt;&gt;&gt; a = input("plz input a num:")plz input a num:Hello&gt;&gt;&gt; print(a)Hello row_input() Python 解释器提供了 3 中标准的文件对象，分别为标准输入、标准输出和标准错误，它们 sys 模块中分别以 sys.stdin、sys.stdout 和 sys.stderr 形式提供 Python 的 print 语句实现打印 从技术角度来讲，print 是把一个或多个对象转化为其文本表达形式，然后发送给标准输出或另一个类似文件的流 在 Python 中，打印与文件和流的概念联系紧密 文件写入方法是把字符串写入到任意文件 print 默认把对象打印到 stdout 流，并添加一些自动的格式化 实质上，print 语句只是 Python 的人性化特性的具体实现，它提供了 sys.stdout.write() 的简单接口，再加上一些默认的格式设置 print 接受一个逗号分隔的对象列表，并为行尾自动添加一个换行符，如果不需要，则在最后个元素后面添加逗号 实现格式化输出，print “String %format1%format2…”%(varialbe1,varialbe2,…….) | 字符 | 输出格式 || —- | ————————– || d,i | 十进制整数或长整型 || u | 无符号整数或长整型 || o | 八进制整数或长整型 || x | 十六进制整数或长整型 || X | 十六进制整数 || f | 浮点数 || e | 浮点数 || E | 浮点数 || g,G | 指数小于-4或更高精度时使用%e或%E,否则使用%f || s | 字符串火任意对象，格式化代码使用str()生成字符串 || r | 同 repr() 生成的字符串 || c | 单个字符 || % | 字面量% | ​ 1234567891011121314151617&gt;&gt;&gt; num = 7.9&gt;&gt;&gt; print("The num is %f" %num)The num is 7.900000&gt;&gt;&gt; print("The num is %d" %num)The num is 7&gt;&gt;&gt; num2 = 9.13&gt;&gt;&gt; print("The nums are %d and %f" %(num,num2))The nums are 7 and 9.130000&gt;&gt;&gt; print("The nums are %e and %f"%(num,3.1))The nums are 7.900000e+00 and 3.100000&gt;&gt;&gt; print("The nums are %d and %f"%(num,3.1))The nums are 7 and 3.100000&gt;&gt;&gt; name = "Jerry"&gt;&gt;&gt; print("The name is %s."%name)The name is Jerry.&gt;&gt;&gt; print("The name is %s."%num)The name is 7.9. 注意：Python 的输出是需要转换的。 数据转换类型： 显示转换 123456&gt;&gt;&gt; num = 7.9&gt;&gt;&gt; test3 = str(num)&gt;&gt;&gt; type(test3)&lt;class 'str'&gt;&gt;&gt;&gt; type(num)&lt;class 'float'&gt; 隐式转换 想知道内置有多少种类型 python3 123&gt;&gt;&gt; dir()['__annotations__', '__builtins__', '__doc__', '__loader__', '__name__', '__package__', '__spec__', 'name', 'num', 'num2', 'test3']&gt;&gt;&gt; dir(__builtins__) python2 123&gt;&gt;&gt;dir(builtins)['__annotations__', '__builtins__', '__doc__', '__loader__', '__name__', '__package__', '__spec__', 'name', 'num', 'num2', 'test3']&gt;&gt;&gt; dir(__builtins__) 想明确一个工具怎么使用 1234567891011&gt;&gt;&gt; help(str)Help on class str in module builtins:class str(object) | str(object='') -&gt; str | str(bytes_or_buffer[, encoding[, errors]]) -&gt; str | | Create a new string object from the given object. If encoding or | errors is specified, then the object must expose a data buffer | that will be decoded using the given encoding and error handler. .... % 后面可以使用的修饰符，（如果有只能按如下的顺序） 1%[(name)][flags][width][.precision]typecode 位于括号中的一个属性后面的字典的键名，用于选出一个具体项 下面标志中的一个或多个 减号（-）:表示左对齐，默认为右对齐 1234&gt;&gt;&gt; print("The name are %+10f and %+f"%(num,-3.1))The name are +7.900000 and -3.100000&gt;&gt;&gt; print("The name are %-20f and %+f"%(num,-3.1))The name are 7.900000 and -3.100000 ​ 加号（+）:表示包含数字符号，整数也会带 “+” 1234567891011&gt;&gt;&gt;num=7.9&gt;&gt;&gt; print("The name are %+e and %f"%(num,3.1))The name are +7.900000e+00 and 3.100000&gt;&gt;&gt; print("The name are %+e and %+f"%(num,3.1))The name are +7.900000e+00 and +3.100000&gt;&gt;&gt; print("The name are %+e and %f"%(num,-3.1))The name are +7.900000e+00 and -3.100000&gt;&gt;&gt; print("The name are %+e and %+f"%(num,-3.1))The name are +7.900000e+00 and -3.100000&gt;&gt;&gt; print("The name are %f and %f"%(num,-3.1))The name are 7.900000 and -3.100000 零（0）:表示一个零填充 一个指定最小宽度的数 一个小数,用于按照精度分割字段的宽度(如下例子：15指小数占15位数) 12&gt;&gt;&gt; print("The name are %-20.15f and %+f"%(num,-3.1))The name are 7.900000000000000 and -3.100000 一个数字,指定要打印字符串中的最大字符个数，浮点数中小数点之后的位数，或者整数最小位数。 字典：kv集合（键值对集合） 123456&gt;&gt;&gt; d1=&#123;'a':33,'b':66&#125;&gt;&gt;&gt; d1['a']33&gt;&gt;&gt; d1=&#123;0:33,1:66&#125; &gt;&gt;&gt; d1[0]33 注意字典也是可变对象;键可以是字符也可以是数字 12345&gt;&gt;&gt; d=&#123;'x':32,'y':27.490325,'z':65&#125; &gt;&gt;&gt; print("%(x)-10d %(y)0.3g"%d) 32 27.5 ​ 要素8：函数的创建与调用 函数实现模块化编程的组件 Python 使用 def 语句定义函数 12def functionName(arguments): suite 函数可以参数化，通过传递不同的参数来调用。 1234&gt;&gt;&gt; def printName(name): print(name)&gt;&gt;&gt; printName('Tony')Tony 注意：函数也是对象 每个 Python 函数都有一个返回值，默认为None,也可以使用 “return value” 明确定义返回值 def 会创建一个函数对象，并同时创建一个函数的对象引用 函数也是对象，可以存储在组合数据类型中，也可以作为参数传递给其他函数 callable() 可用于测试函数是否可调用 1234&gt;&gt;&gt; callable(name)False&gt;&gt;&gt; callable(printName)True Python 有众多内置函数 dir()、id()、type() 、str()、help()、len()、callable() 等等 Python 标准库拥有众多内置模块，这些模块拥有大量的函数 Python 模块实际上是包含 Python 代码的 .py 文件，其拥有自定义的函数与类及变量等 导入代码块使用 import 语句进行，后跟模块名称（不能指定模块文件的后缀.py） 导入一个模块后，可以访问其内部的任意函数、类及变量]]></content>
      <categories>
        <category>Python 基础</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Maven仓库理解和优先级]]></title>
    <url>%2F2018%2F06%2F03%2FMaven-Priority%2F</url>
    <content type="text"><![CDATA[5 Seven 2017 前言使用 maven 也有一段时间了，有时候在配置 repository,mirror,profile的时候，总会导致 jar 拉取不到。所以认真的分析了 maven 获取 jar 包时候的优先级。 Maven 仓库的分类仓库分类：本地仓库和远程仓库。Maven根据坐标寻找构件的时候，它先会查看本地仓库，如果本地仓库存在构件，则直接使用；如果没有，则从远程仓库查找，找到后，下载到本地。 1）本地仓库默认情况下，每个用户在自己的用户目录下都有一个路径名为.m2/repository/的仓库目录。我们也可以在 settings.xml 文件配置本地仓库的地址 2）远程仓库本地仓库好比书房，而远程仓库就像是书店。对于Maven来说，每个用户只有一个本地仓库，但是可以配置多个远程仓库。下· 我们可以在 pom 文件配置多个 repository，但是随着项目越来也多我们每次都要在 pom 文件配置比较麻烦，所以我们可以在settings 文件配置 profile （私服）。这样我们每次创建新项目的时候就可以不用配置 repository。 3）中央仓库Maven必须要知道至少一个可用的远程仓库，中央仓库就是这样一个默认的远程仓库，Maven 默认有一个 super pom 文件。maven super pom 文件位置D:\apache-maven-3.0.4\lib 下的 maven-model-builder-3.0.4.jar 中的 org/apache/maven/model/pom-4.0.0.xml12345678910111213··· 省略其他 &lt;repositories&gt; &lt;repository&gt; &lt;id&gt;central&lt;/id&gt; &lt;name&gt;Central Repository&lt;/name&gt; &lt;url&gt;https://repo.maven.apache.org/maven2&lt;/url&gt; &lt;layout&gt;default&lt;/layout&gt; &lt;snapshots&gt; &lt;enabled&gt;false&lt;/enabled&gt; &lt;/snapshots&gt; &lt;/repository&gt; &lt;/repositories&gt;··· 这个时候我们就明白了，我们在 settings 文件配置一个 mirror 的 mirrorOf 为 central 的镜像就会替代 ‘中央仓库’ 的原因了。 Maven 镜像镜像（Mirroring）是冗余的一种类型，一个磁盘上的数据在另一个磁盘上存在一个完全相同的副本即为镜像。为什么配置镜像? 1.一句话，你有的我也有，你没有的我也有。（拥有远程仓库的所有 jar，包括远程仓库没有的 jar）2.还是一句话，我跑的比你快。（有时候远程仓库获取 jar 的速度可能比镜像慢，这也是为什么我们一般要配置中央仓库的原因，外国的 maven 仓库一般获取速度比较慢） 如果你配置 maven 镜像不是为了以上两点，那基本就不用配置镜像了。注意:当远程仓库被镜像匹配到的，则在获取 jar 包将从镜像仓库获取，而不是我们配置的 repository 仓库, repository 将失去作用 mirrorOf 标签mirrorOf 标签里面放置的是 repository 配置的 id,为了满足一些复杂的需求，Maven还支持更高级的镜像配置： external:* = 不在本地仓库的文件才从该镜像获取 repo,repo1 = 远程仓库 repo 和 repo1 从该镜像获取 *,!repo1 = 所有远程仓库都从该镜像获取，除 repo1 远程仓库以外 * = 所用远程仓库都从该镜像获取 私服私服是一种特殊的远程Maven仓库，它是架设在局域网内的仓库服务，私服一般被配置为互联网远程仓库的镜像，供局域网内的Maven用户使用。当Maven需要下载构件的时候，先向私服请求，如果私服上不存在该构件，则从外部的远程仓库下载，同时缓存在私服之上，然后为Maven下载请求提供下载服务，另外，对于自定义或第三方的jar可以从本地上传到私服，供局域网内其他maven用户使用。优点主要有： 1. 节省外网宽带 2. 加速Maven构建 3. 部署第三方构件：可以将公司项目的 jar 发布到私服上，方便项目与项目之间的调用 4. 提高稳定性、增强控制：原因是外网不稳定 5. 降低中央仓库的负荷：原因是中央仓库访问量太大 上面大概介绍了 Maven 仓库概念，接下来我们进入正题 Maven 仓库优先级为了方便测试，我准备了以下几个仓库 172.16.xxx.xxx 远程仓库 （私服） dev.xxx.wiki 远程仓库 （远程） localhost 仓库 是我自己在本机搭建的一个仓库 （镜像） maven.aliyun.com 中央仓库（中央） 本地仓库优先级Maven 本地仓库拥有该包，而远程、镜像、中央、私服都不包含该包。我们来看下 Maven 是怎么获取的123456789101112131415161718192021222324.......// 使用本地仓库，优先级(priority)为 10[DEBUG] Using local repository at E:\OperSource[DEBUG] Using manager EnhancedLocalRepositoryManager with priority 10.0 for E:\OperSource[INFO] Scanning for projects..........[INFO] Installing C:\Users\swipal\Desktop\abc\demo\target\demo-1.0-SNAPSHOT.jar to E:\OperSource\com\cjf\demo\1.0-SNAPSHOT\demo-1.0-SNAPSHOT.jar[DEBUG] Writing tracking file E:\OperSource\com\cjf\demo\1.0-SNAPSHOT\_remote.repositories[INFO] Installing C:\Users\swipal\Desktop\abc\demo\pom.xml to E:\OperSource\com\cjf\demo\1.0-SNAPSHOT\demo-1.0-SNAPSHOT.pom[DEBUG] Writing tracking file E:\OperSource\com\cjf\demo\1.0-SNAPSHOT\_remote.repositories[DEBUG] Installing com.cjf:demo:1.0-SNAPSHOT/maven-metadata.xml to E:\OperSource\com\cjf\demo\1.0-SNAPSHOT\maven-metadata-local.xml[DEBUG] Installing com.cjf:demo/maven-metadata.xml to E:\OperSource\com\cjf\demo\maven-metadata-local.xml[INFO] ------------------------------------------------------------------------[INFO] BUILD SUCCESS[INFO] ------------------------------------------------------------------------[INFO] Total time: 1.874 s[INFO] Finished at: 2017-07-07T10:37:32+08:00[INFO] Final Memory: 23M/219M[INFO] ------------------------------------------------------------------------Process finished with exit code 0 从上面可以看出 Maven 一开始就使用本地仓库，并将本地仓库的优先级定制为 10 , 最后 jar 包也在本地仓库找到，Maven 成功打包。 远程仓库优先级前面我们知道了，本地仓库的优先级是最高的，现在我们继续研究远程仓库的优先级（以下的所有例子，都默认本地仓库不拥有我们需要的包） 这一次我们默认配置 profile（私服）为 172.16.xxx.xxx 远程仓库, repository 为 dev.xxx.wiki 远程仓库,mirror 为本地 localhost 仓库，还配置了一个 mirrorOf 为 central 远程仓库为 maven.aliyun.com 的中央仓库, 以下是配置信息settings.xml 文件1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253······&lt;mirrors&gt; &lt;mirror&gt; &lt;id&gt;localhost&lt;/id&gt; &lt;name&gt;Public Repositories&lt;/name&gt; &lt;mirrorOf&gt;foo&lt;/mirrorOf&gt; &lt;!--拦截 pom 文件配置的 repository--&gt; &lt;url&gt;http://localhost:8081/repository/maven-public/&lt;/url&gt; &lt;/mirror&gt; &lt;mirror&gt; &lt;id&gt;localhost2&lt;/id&gt; &lt;name&gt;Public Repositories&lt;/name&gt; &lt;mirrorOf&gt;foo2&lt;/mirrorOf&gt; &lt;!--配置一个拦截 foo2 的远程仓库的镜像--&gt; &lt;url&gt;http://localhost:8081/repository/maven-snapshots/&lt;/url&gt; &lt;/mirror&gt; &lt;mirror&gt; &lt;id&gt;alimaven&lt;/id&gt; &lt;mirrorOf&gt;central&lt;/mirrorOf&gt; &lt;!--覆盖 Maven 默认的配置的中央仓库--&gt; &lt;name&gt;aliyun maven&lt;/name&gt; &lt;url&gt;http://maven.aliyun.com/nexus/content/groups/public/&lt;/url&gt; &lt;/mirror&gt; &lt;/mirrors&gt;&lt;!--配置私服--&gt;&lt;profiles&gt; &lt;profile&gt; &lt;id&gt;nexus&lt;/id&gt; &lt;repositories&gt; &lt;repository&gt; &lt;id&gt;public&lt;/id&gt; &lt;name&gt;Public Repositories&lt;/name&gt; &lt;url&gt;http://172.16.xxx.xxx:8081/nexus/content/groups/public/&lt;/url&gt; &lt;/repository&gt; &lt;/repositories&gt; &lt;pluginRepositories&gt; &lt;pluginRepository&gt; &lt;id&gt;public&lt;/id&gt; &lt;name&gt;Public Repositories&lt;/name&gt; &lt;url&gt;http://172.16.xxx.xxx:8081/nexus/content/groups/public&lt;/url&gt; &lt;/pluginRepository&gt; &lt;/pluginRepositories&gt; &lt;/profile&gt; &lt;/profiles&gt;&lt;activeProfiles&gt; &lt;activeProfile&gt;nexus&lt;/activeProfile&gt;&lt;/activeProfiles&gt;······ pom.xml 文件 12345678910111213141516171819202122232425262728&lt;dependencies&gt; &lt;!--xxx-cif-api 存在 172.16.xxx.xxx 仓库--&gt; &lt;dependency&gt; &lt;groupId&gt;com.xxx.cif&lt;/groupId&gt; &lt;artifactId&gt;xxx-cif-api&lt;/artifactId&gt; &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt; &lt;/dependency&gt; &lt;!--Chapter1 存在 localhost 仓库--&gt; &lt;dependency&gt; &lt;groupId&gt;com.cjf&lt;/groupId&gt; &lt;artifactId&gt;Chapter1&lt;/artifactId&gt; &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt; &lt;/dependency&gt;&lt;/dependencies&gt;&lt;!--配置远程仓库--&gt;&lt;repositories&gt; &lt;repository&gt; &lt;id&gt;foo&lt;/id&gt; &lt;name&gt;Public Repositories&lt;/name&gt; &lt;url&gt;http://dev.xxx.wiki:8081/nexus/content/groups/public/&lt;/url&gt; &lt;/repository&gt;&lt;/repositories&gt; 以下是 Maven 拉取包的日志123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108······· 省略部分日志信息[DEBUG] Using local repository at E:\OperSource[DEBUG] Using manager EnhancedLocalRepositoryManager with priority 10.0 for E:\OperSource[INFO] Scanning for projects...// 从这里可以看出我们配置的镜像替代了我们在 pom 配置的远程仓库[DEBUG] Using mirror localhost (http://localhost:8081/repository/maven-public/) for foo (http://dev.xxx.wiki:8081/nexus/content/groups/public/).替代了默认的中央仓库[DEBUG] Using mirror alimaven (http://maven.aliyun.com/nexus/content/groups/public/) for central (https://repo.maven.apache.org/maven2).// 从这里可以看出 Maven 使用哪些 dependencies 和 plugins 的地址，我们可以看出优先级最高的是 172.16.xxx.xxx,然后就是 localhost 最后才是 maven.aliyun.com// 注意：alimaven (http://maven.aliyun.com/nexus/content/groups/public/, default, releases) 从这里可以看出中央仓库只能获取 releases 包，所有的 snapshots 包都不从中央仓库获取。（可以看前面 central 的配置信息）[DEBUG] === PROJECT BUILD PLAN ================================================[DEBUG] Project: com.cjf:demo:1.0-SNAPSHOT[DEBUG] Dependencies (collect): [][DEBUG] Dependencies (resolve): [compile, runtime, test][DEBUG] Repositories (dependencies): [public (http://172.16.xxx.xxx:8081/nexus/content/groups/public/, default, releases+snapshots), localhost (http://localhost:8081/repository/maven-public/, default, releases+snapshots), alimaven (http://maven.aliyun.com/nexus/content/groups/public/, default, releases)][DEBUG] Repositories (plugins) : [public (http://172.16.xxx.xxx:8081/nexus/content/groups/public, default, releases+snapshots), alimaven (http://maven.aliyun.com/nexus/content/groups/public/, default, releases)][DEBUG] =======================================================================// 寻找本地是否有 maven-metadata.xml 配置文件 ，从这里可以看出寻找不到（后面会详细讲该文件作用）[DEBUG] Could not find metadata com.xxx.cif:xxx-cif-api:0.0.1-SNAPSHOT/maven-metadata.xml in local (E:\OperSource)// 由于寻找不到 Maven 只能从我们配置的远程仓库寻找，由于 Maven 也不知道那个仓库才有，所以同时寻找两个仓库[DEBUG] Using transporter WagonTransporter with priority -1.0 for http://172.16.xxx.xxx:8081/nexus/content/groups/public/[DEBUG] Using transporter WagonTransporter with priority -1.0 for http://localhost:8081/repository/maven-public/[DEBUG] Using connector BasicRepositoryConnector with priority 0.0 for http://localhost:8081/repository/maven-public/[DEBUG] Using connector BasicRepositoryConnector with priority 0.0 for http://172.16.xxx.xxx:8081/nexus/content/groups/public/Downloading: http://172.16.xxx.xxx:8081/nexus/content/groups/public/com/xxx/cif/xxx-cif-api/0.0.1-SNAPSHOT/maven-metadata.xmlDownloading: http://localhost:8081/repository/maven-public/com/xxx/cif/xxx-cif-api/0.0.1-SNAPSHOT/maven-metadata.xml[DEBUG] Writing tracking file E:\OperSource\com\xxx\cif\xxx-cif-api\0.0.1-SNAPSHOT\resolver-status.properties// 从这里可以看出在 172.16.xxx.xxx 找到 xxx-cif-api 的 maven-metadata.xml 文件并下载下来Downloaded: http://172.16.xxx.xxx:8081/nexus/content/groups/public/com/xxx/cif/xxx-cif-api/0.0.1-SNAPSHOT/maven-metadata.xml (781 B at 7.0 KB/sec)// 追踪文件，resolver-status.properties 配置了 jar 包下载地址和时间[DEBUG] Writing tracking file E:\OperSource\com\xxx\cif\xxx-cif-api\0.0.1-SNAPSHOT\resolver-status.properties[DEBUG] Could not find metadata com.xxx.cif:xxx-cif-api:0.0.1-SNAPSHOT/maven-metadata.xml in localhost (http://localhost:8081/repository/maven-public/)// 在 localhost 远程仓库寻找不到 xxx-cif-api 的 maven-metadata.xml[DEBUG] Could not find metadata com.xxx.cif:xxx-cif-api:0.0.1-SNAPSHOT/maven-metadata.xml in local (E:\OperSource)// 跳过的远程请求 [DEBUG] Skipped remote request for com.xxx.cif:xxx-cif-api:0.0.1-SNAPSHOT/maven-metadata.xml, already updated during this session.[DEBUG] Skipped remote request for com.xxx.cif:xxx-cif-api:0.0.1-SNAPSHOT/maven-metadata.xml, already updated during this session.// 默认以后获取 xxx-cif-api 的时候将不在从 localhost 寻找了，除非强制获取才会再次从 localhost 寻找这个包[DEBUG] Failure to find com.xxx.cif:xxx-cif-api:0.0.1-SNAPSHOT/maven-metadata.xml in http://localhost:8081/repository/maven-public/ was cached in the local repository, resolution will not be reattempted until the update interval of localhost has elapsed or updates are forced// 将 172.16.xxx.xxx 优先级升为 0 ，并下载 xxx-cif-api 的 pom 文件[DEBUG] Using transporter WagonTransporter with priority -1.0 for http://172.16.xxx.xxx:8081/nexus/content/groups/public/[DEBUG] Using connector BasicRepositoryConnector with priority 0.0 for http://172.16.xxx.xxx:8081/nexus/content/groups/public/Downloading: http://172.16.xxx.xxx:8081/nexus/content/groups/public/com/xxx/cif/xxx-cif-api/0.0.1-SNAPSHOT/xxx-cif-api-0.0.1-20170515.040917-89.pomDownloaded: http://172.16.xxx.xxx:8081/nexus/content/groups/public/com/xxx/cif/xxx-cif-api/0.0.1-SNAPSHOT/xxx-cif-api-0.0.1-20170515.040917-89.pom (930 B at 82.6 KB/sec)// _remote.repositories 记录的以后使用那个远程仓库获取 （ps:这个文件作用我要不是很清楚作用，以上观点是自己推测出来的。）[DEBUG] Writing tracking file E:\OperSource\com\xxx\cif\xxx-cif-api\0.0.1-SNAPSHOT\_remote.repositories[DEBUG] Writing tracking file E:\OperSource\com\xxx\cif\xxx-cif-api\0.0.1-SNAPSHOT\xxx-cif-api-0.0.1-20170515.040917-89.pom.lastUpdated// 后面获取 Chapter1 包的流程跟 com.xxx.cif 是一样的，不过最后是在 localhost 寻找到而已，所以这分日志就不贴出来了。// 最后在下载包的时候，都到对应的仓库下载[DEBUG] Using transporter WagonTransporter with priority -1.0 for http://172.16.xxx.xxx:8081/nexus/content/groups/public/[DEBUG] Using connector BasicRepositoryConnector with priority 0.0 for http://172.16.xxx.xxx:8081/nexus/content/groups/public/Downloading: http://172.16.xxx.xxx:8081/nexus/content/groups/public/com/xxx/cif/xxx-cif-api/0.0.1-SNAPSHOT/xxx-cif-api-0.0.1-20170515.040917-89.jarDownloading: http://172.16.xxx.xxx:8081/nexus/content/groups/public/com/xxx/util/xxx-util/0.0.1-SNAPSHOT/xxx-util-0.0.1-20170514.091041-31.jarDownloaded: http://172.16.xxx.xxx:8081/nexus/content/groups/public/com/xxx/util/xxx-util/0.0.1-SNAPSHOT/xxx-util-0.0.1-20170514.091041-31.jar (26 KB at 324.2 KB/sec)Downloaded: http://172.16.xxx.xxx:8081/nexus/content/groups/public/com/xxx/cif/xxx-cif-api/0.0.1-SNAPSHOT/xxx-cif-api-0.0.1-20170515.040917-89.jar (68 KB at 756.6 KB/sec)[DEBUG] Writing tracking file E:\OperSource\com\xxx\cif\xxx-cif-api\0.0.1-SNAPSHOT\_remote.repositories[DEBUG] Writing tracking file E:\OperSource\com\xxx\cif\xxx-cif-api\0.0.1-SNAPSHOT\xxx-cif-api-0.0.1-20170515.040917-89.jar.lastUpdated[DEBUG] Writing tracking file E:\OperSource\com\xxx\util\xxx-util\0.0.1-SNAPSHOT\_remote.repositories[DEBUG] Writing tracking file E:\OperSource\com\xxx\util\xxx-util\0.0.1-SNAPSHOT\xxx-util-0.0.1-20170514.091041-31.jar.lastUpdated[DEBUG] Using transporter WagonTransporter with priority -1.0 for http://localhost:8081/repository/maven-public/[DEBUG] Using connector BasicRepositoryConnector with priority 0.0 for http://localhost:8081/repository/maven-public/Downloading: http://localhost:8081/repository/maven-public/com/cjf/Chapter1/0.0.1-SNAPSHOT/Chapter1-0.0.1-20170708.092339-1.jarDownloaded: http://localhost:8081/repository/maven-public/com/cjf/Chapter1/0.0.1-SNAPSHOT/Chapter1-0.0.1-20170708.092339-1.jar (8 KB at 167.0 KB/sec)[DEBUG] Writing tracking file E:\OperSource\com\cjf\Chapter1\0.0.1-SNAPSHOT\_remote.repositories[DEBUG] Writing tracking file E:\OperSource\com\cjf\Chapter1\0.0.1-SNAPSHOT\Chapter1-0.0.1-20170708.092339-1.jar.lastUpdated[INFO] Installing C:\Users\swipal\Desktop\abc\demo\target\demo-1.0-SNAPSHOT.jar to E:\OperSource\com\cjf\demo\1.0-SNAPSHOT\demo-1.0-SNAPSHOT.jar[DEBUG] Writing tracking file E:\OperSource\com\cjf\demo\1.0-SNAPSHOT\_remote.repositories[INFO] Installing C:\Users\swipal\Desktop\abc\demo\pom.xml to E:\OperSource\com\cjf\demo\1.0-SNAPSHOT\demo-1.0-SNAPSHOT.pom[DEBUG] Writing tracking file E:\OperSource\com\cjf\demo\1.0-SNAPSHOT\_remote.repositories[DEBUG] Installing com.cjf:demo:1.0-SNAPSHOT/maven-metadata.xml to E:\OperSource\com\cjf\demo\1.0-SNAPSHOT\maven-metadata-local.xml[DEBUG] Installing com.cjf:demo/maven-metadata.xml to E:\OperSource\com\cjf\demo\maven-metadata-local.xml[INFO] ------------------------------------------------------------------------[INFO] BUILD SUCCESS[INFO] ------------------------------------------------------------------------[INFO] Total time: 10.549 s[INFO] Finished at: 2017-07-09T18:13:20+08:00[INFO] Final Memory: 26M/219M[INFO] ------------------------------------------------------------------------······· 好了，看了这么多的配置文件信息和日志信息，我们也总结一下 Maven 远程仓库优先级了。 主要有以下几点：1.从日志信息我们得出这几种maven仓库的优先级别为 本地仓库 &gt; 私服 （profile）&gt; 远程仓库（repository）和 镜像 （mirror） &gt; 中央仓库 （central） 2.镜像是一个特殊的配置，其实镜像等同与远程仓库，没有匹配远程仓库的镜像就毫无作用（如 foo2）。3.总结上面所说的，Maven 仓库的优先级就是 私服和远程仓库 的对比，没有其它的仓库类型。为什么这么说是因为，镜像等同远程，而中央其实也是 maven super xml 配置的一个repository 的一个而且。所以 maven 仓库真正的优先级为 本地仓库 &gt; 私服（profile）&gt; 远程仓库（repository） maven-metadata.xml 文件Maven Repository Metadata 可用于表示： 1. 一个没有版本的工件：它提供有关该工件的可用版本的信息 2. 快照伪像：它提供有关快照的精确信息 3. 包含Maven插件工件的组：它提供了有关此组中可用插件的信息。 元数据文件名是： 远程存储库中的 maven-metadata.xml， maven-metadata- &lt;repo-id&gt;.xml在本地存储库中，用于具有repo-id标识符的存储库中的元标记。 以上是 Maven 官网对该文件的解释。 作用问题：有时候我们更新最新包的时候，会发现最新的包被拉取下来的，但是项目使用的包还是旧的包。所以我们要分析下是什么原因导致的。 首先我们先大概的了解下 maven-metadata.xml 文件。1234567891011121314151617181920212223242526&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;metadata modelVersion="1.1.0"&gt; &lt;groupId&gt;com.cjf&lt;/groupId&gt; &lt;artifactId&gt;Chapter1&lt;/artifactId&gt; &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt; &lt;versioning&gt; &lt;snapshot&gt; &lt;!--当前版本下的最新快照信息--&gt; &lt;timestamp&gt;20170710.071727&lt;/timestamp&gt; &lt;!--快照的时间戳--&gt; &lt;buildNumber&gt;6&lt;/buildNumber&gt; &lt;!--构件号--&gt; &lt;/snapshot&gt; &lt;lastUpdated&gt;20170710071727&lt;/lastUpdated&gt;&lt;!--metadata文件被更新的时间--&gt; &lt;snapshotVersions&gt; &lt;snapshotVersion&gt; &lt;!--当前版本下可用的子快照版本信息--&gt; &lt;extension&gt;jar&lt;/extension&gt; &lt;value&gt;0.0.1-20170710.071727-6&lt;/value&gt;&lt;!--子快照版本的信息--&gt; &lt;updated&gt;20170710071727&lt;/updated&gt; &lt;!--这个子快照版本的更新时间--&gt; &lt;/snapshotVersion&gt; &lt;snapshotVersion&gt; &lt;extension&gt;pom&lt;/extension&gt; &lt;value&gt;0.0.1-20170710.071727-6&lt;/value&gt; &lt;updated&gt;20170710071727&lt;/updated&gt; &lt;/snapshotVersion&gt; &lt;/snapshotVersions&gt; &lt;/versioning&gt;&lt;/metadata&gt; 其中 lastUpdated 是最中要的一个属性，Maven 更新工程的 jar包时，会比较 lastUpdated 时间戳值，哪个值更大，就以哪个文件为准。 接下来我们看下 Maven 为我们生成了那些文件我们可以看到 maven-metadata.xml 一共有三个 1. maven-metadata-local.xml 本地的元数据, Maven install 的时候就会生成。 2. maven-metadata-snapshots.xml Maven deploy 时会生成 3. maven-metadata-localhost.xml 远程仓库获取的时候生成 (repository 的 id = localhost) 以上的文件其实都是 Maven 的过渡文件而已 例如 maven-metadata-snapshots 就是 Maven deploy 先从远程仓库对应包的 maven-metadata.xml 下载下来，然后修改快照信息后在上传到远程仓库上。 例如 maven-metadata-localhost 的作用是在 Maven 在拉取包的时候，会先跟本地 maven-metadata-local 比较下 lastUpdated 时间戳值，值大用哪个。如果是 Mavne 强制更新 的时候(没有强制更新是不会) 会下载远程的 maven-metadata.xml 比较远程，本地，和之前远程保存下来的 maven-metadata 文件。 所以有时候 maven 库上的 jar 包已经更新，而我们总是拉取不到 maven 的包原因就是本地的 maven-metadata-local 的 lastUpdated 比较大。 我们验证下 Maven deploy 例子123456789101112131415161718192021222324[INFO] --- maven-deploy-plugin:2.8.2:deploy (default-deploy) @ Chapter1 ---// 先从远程下载快照 maven-metadata.xmlDownloading: http://localhost:8081/repository/maven-snapshots/com/cjf/Chapter1/0.0.1-SNAPSHOT/maven-metadata.xmlDownloaded: http://localhost:8081/repository/maven-snapshots/com/cjf/Chapter1/0.0.1-SNAPSHOT/maven-metadata.xml (768 B at 3.3 KB/sec)// 将项目的 jar 和 pom 文件更新到远程仓库Uploading: http://localhost:8081/repository/maven-snapshots/com/cjf/Chapter1/0.0.1-SNAPSHOT/Chapter1-0.0.1-20170710.121310-15.jarUploaded: http://localhost:8081/repository/maven-snapshots/com/cjf/Chapter1/0.0.1-SNAPSHOT/Chapter1-0.0.1-20170710.121310-15.jar (8 KB at 14.1 KB/sec)Uploading: http://localhost:8081/repository/maven-snapshots/com/cjf/Chapter1/0.0.1-SNAPSHOT/Chapter1-0.0.1-20170710.121310-15.pomUploaded: http://localhost:8081/repository/maven-snapshots/com/cjf/Chapter1/0.0.1-SNAPSHOT/Chapter1-0.0.1-20170710.121310-15.pom (2 KB at 2.0 KB/sec)Downloading: http://localhost:8081/repository/maven-snapshots/com/cjf/Chapter1/maven-metadata.xmlDownloaded: http://localhost:8081/repository/maven-snapshots/com/cjf/Chapter1/maven-metadata.xml (275 B at 1.6 KB/sec)// 上传 maven-metadata.xml 到远程仓库Uploading: http://localhost:8081/repository/maven-snapshots/com/cjf/Chapter1/0.0.1-SNAPSHOT/maven-metadata.xmlUploaded: http://localhost:8081/repository/maven-snapshots/com/cjf/Chapter1/0.0.1-SNAPSHOT/maven-metadata.xml (768 B at 1.0 KB/sec)Uploading: http://localhost:8081/repository/maven-snapshots/com/cjf/Chapter1/maven-metadata.xmlUploaded: http://localhost:8081/repository/maven-snapshots/com/cjf/Chapter1/maven-metadata.xml (275 B at 0.4 KB/sec)[INFO] ------------------------------------------------------------------------[INFO] BUILD SUCCESS[INFO] ------------------------------------------------------------------------[INFO] Total time: 5.231 s[INFO] Finished at: 2017-07-10T20:13:13+08:00[INFO] Final Memory: 19M/226M[INFO] ------------------------------------------------------------------------ 总结原本以为两天就写好这篇文章，在自己理清思路的时候总是被自己绕晕了。比如在 Nexus 的 Central 配置的中央仓库获取，和 maven-metadata.xml 是如何比较的。 如果以上文章有误，等博客的评论系统搭建起来后欢迎大家指认出来。]]></content>
      <categories>
        <category>Maven</category>
      </categories>
      <tags>
        <tag>Maven</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LogBack 使用]]></title>
    <url>%2F2018%2F06%2F03%2FLogback%2F</url>
    <content type="text"><![CDATA[26 Seven 2017 前言之前项目一直使用 logback ,现在大概写下了 logback 基础配置。 简介LogBack是一个日志框架，它是Log4j作者Ceki的又一个日志组件。 LogBack,Slf4j,Log4j之间的关系 slf4j是The Simple Logging Facade for Java的简称，是一个简单日志门面抽象框架，它本身只提供了日志Facade API和一个简单的日志类实现，一般常配合Log4j，LogBack，java.util.logging使用。Slf4j作为应用层的Log接入时，程序可以根据实际应用场景动态调整底层的日志实现框架(Log4j/LogBack/JdkLog…)； LogBack和Log4j都是开源日记工具库，LogBack是Log4j的改良版本，比Log4j拥有更多的特性，同时也带来很大性能提升。 LogBack官方建议配合Slf4j使用，这样可以灵活地替换底层日志框架。 LogBack的结构LogBack分为3个组件，logback-core, logback-classic 和 logback-access。其中logback-core提供了LogBack的核心功能，是另外两个组件的基础。logback-classic则实现了Slf4j的API，所以当想配合Slf4j使用时，则需要引入这个包。logback-access是为了集成Servlet环境而准备的，可提供HTTP-access的日志接口。 Log的行为级别：OFF、FATAL、ERROR、WARN、INFO、DEBUG、ALL从下向上，当选择了其中一个级别，则该级别向下的行为是不会被打印出来。举个例子，当选择了INFO级别，则INFO以下的行为则不会被打印出来。 获取 Logger 对象 我们先从获取 logger 对象开始1Logger logger = LoggerFactory.getLogger(xxx.class.getName()); LoggerFactory 是 slf4j 的日志工厂，获取 logger 方法就来自这里。123456以下代码摘自：org.slf4j.LoggerFactorypublic static Logger getLogger(String name) &#123; ILoggerFactory iLoggerFactory = getILoggerFactory(); return iLoggerFactory.getLogger(name);&#125; 这个方法里面有分为两个过程。第一个过程是获取ILoggerFactory，就是真正的日志工厂。第二个过程就是从真正的日志工厂中获取logger。接下来我们看下到底是怎么获取的12345678910111213141516171819202122232425262728以下代码摘自：org.slf4j.LoggerFactorypublic static ILoggerFactory getILoggerFactory() &#123;if (INITIALIZATION_STATE == UNINITIALIZED) &#123; INITIALIZATION_STATE = ONGOING_INITIALIZATION; // 第一次调用会去加载 StaticLoggerBinder.class 文件来决定 LoggerFactory 的实现类 // （补充下：不同日志包下都有 StaticLoggerBinder 这个类文件，这个会决定 LoggerFactory 初始化那种类型的日志） performInitialization();&#125;// INITIALIZATION_STATE 值判断是否有加载初始化过 ILoggerFactory 实例。switch (INITIALIZATION_STATE) &#123; case SUCCESSFUL_INITIALIZATION: // 返回对应的 ILoggerFactory 实例 return StaticLoggerBinder.getSingleton().getLoggerFactory(); case NOP_FALLBACK_INITIALIZATION: // 当加载不到一个 StaticLoggerBinder 时，会走这里 // 返回一个 NOPLoggerFactory 实例 return NOP_FALLBACK_FACTORY; case FAILED_INITIALIZATION: // 初始化异常 throw new IllegalStateException(UNSUCCESSFUL_INIT_MSG); case ONGOING_INITIALIZATION: // support re-entrant behavior. // See also http://bugzilla.slf4j.org/show_bug.cgi?id=106 return TEMP_FACTORY;&#125;throw new IllegalStateException("Unreachable code");&#125; 接下来我们来看下是怎么加载 StaticLoggerBinder.class 文件的 1234567891011121314151617181920212223242526272829303132333435以下代码摘自：org.slf4j.LoggerFactory private final static void bind() &#123; try &#123; // 加载 StaticLoggerBinder Set staticLoggerBinderPathSet = findPossibleStaticLoggerBinderPathSet(); reportMultipleBindingAmbiguity(staticLoggerBinderPathSet); // 最后会随机选择一个StaticLoggerBinder.class来创建一个单例 StaticLoggerBinder.getSingleton(); // 改变 INITIALIZATION_STATE 值，表示成功初始化 Factory。 // 并且以后在获取 Logger 的时候并不会再次加载该方法 INITIALIZATION_STATE = SUCCESSFUL_INITIALIZATION; reportActualBinding(staticLoggerBinderPathSet); emitSubstituteLoggerWarning(); &#125; catch (NoClassDefFoundError ncde) &#123; String msg = ncde.getMessage(); if (messageContainsOrgSlf4jImplStaticLoggerBinder(msg)) &#123; // 加载不到 StaticLoggerBinder 文件 INITIALIZATION_STATE = NOP_FALLBACK_INITIALIZATION; &#125; else &#123; failedBinding(ncde); throw ncde; &#125; &#125; catch (java.lang.NoSuchMethodError nsme) &#123; String msg = nsme.getMessage(); if (msg != null &amp;&amp; msg.indexOf("org.slf4j.impl.StaticLoggerBinder.getSingleton()") != -1) &#123; // 初始化异常 INITIALIZATION_STATE = FAILED_INITIALIZATION; &#125; throw nsme; &#125; catch (Exception e) &#123; failedBinding(e); throw new IllegalStateException("Unexpected initialization failure", e); &#125; &#125; 加载 StaticLoggerBinder.class12345678910111213141516以下代码摘自：org.slf4j.LoggerFactory.findPossibleStaticLoggerBinderPathSetprivate static String STATIC_LOGGER_BINDER_PATH = "org/slf4j/impl/StaticLoggerBinder.class";···if (loggerFactoryClassLoader == null) &#123; paths = ClassLoader.getSystemResources(STATIC_LOGGER_BINDER_PATH); &#125; else &#123; paths = loggerFactoryClassLoader .getResources(STATIC_LOGGER_BINDER_PATH); &#125; while (paths.hasMoreElements()) &#123; URL path = (URL) paths.nextElement(); staticLoggerBinderPathSet.add(path); &#125;··· 当项目中存在多个StaticLoggerBinder.class文件时，运行项目会出现以下日志：（这里获取的规则就近原则，如果在 maven 先配置 slf4j 而后面在配置 logback，则这里初始化的是 slf4j）12345SLF4J: Class path contains multiple SLF4J bindings.SLF4J: Found binding in [jar:file:/E:/OperSource/org/slf4j/slf4j-log4j12/1.7.12/slf4j-log4j12-1.7.12.jar!/org/slf4j/impl/StaticLoggerBinder.class]SLF4J: Found binding in [jar:file:/E:/OperSource/ch/qos/logback/logback-classic/1.1.3/logback-classic-1.1.3.jar!/org/slf4j/impl/StaticLoggerBinder.class]SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory] 返回实例123以下代码摘自：org.slf4j.LoggerFactory.getILoggerFactoryStaticLoggerBinder.getSingleton().getLoggerFactory(); LogBack 配置123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778&lt;configuration scan="true" scanPeriod="60 seconds" debug="false"&gt; &lt;!-- 项目名称配置 --&gt; &lt;contextName&gt;example&lt;/contextName&gt; &lt;!-- 属性 --&gt; &lt;property name="APP_Name" value="example" /&gt; &lt;!-- 统一的时间格式，用于日志头输出 --&gt; &lt;timestamp key="timeStyle" datePattern="yyyy-MM-dd HH:mm:ss.SSS"/&gt; &lt;!--配置控制台输出,开发环境有--&gt; &lt;appender name="STDOUT" class="ch.qos.logback.core.ConsoleAppender"&gt; &lt;!-- encoder 默认配置为PatternLayoutEncoder --&gt; &lt;encoder&gt; &lt;pattern&gt;[%d&#123;timeStyle&#125;] [%cn] %-5level %logger&#123;35&#125; - %msg%n&lt;/pattern&gt; &lt;/encoder&gt; &lt;/appender&gt; &lt;!--文件输出配置--&gt; &lt;appender name="FILE" class="ch.qos.logback.core.rolling.RollingFileAppender"&gt; &lt;file&gt;../logs/$&#123;APP_Name&#125;_run.log&lt;/file&gt; &lt;rollingPolicy class="ch.qos.logback.core.rolling.FixedWindowRollingPolicy"&gt; &lt;fileNamePattern&gt;../logs/$&#123;APP_Name&#125;_run.%i.log.zip&lt;/fileNamePattern&gt; &lt;minIndex&gt;1&lt;/minIndex&gt; &lt;maxIndex&gt;10&lt;/maxIndex&gt; &lt;/rollingPolicy&gt; &lt;triggeringPolicy class="ch.qos.logback.core.rolling.SizeBasedTriggeringPolicy"&gt; &lt;maxFileSize&gt;10MB&lt;/maxFileSize&gt; &lt;/triggeringPolicy&gt; &lt;encoder&gt; &lt;pattern&gt;[%d&#123;timeStyle&#125;] [%cn] %-5level %logger&#123;35&#125; - %msg%n&lt;/pattern&gt; &lt;/encoder&gt; &lt;/appender&gt; &lt;appender name="BUSINESS_FILE" class="ch.qos.logback.core.rolling.RollingFileAppender"&gt; &lt;file&gt;../logs/$&#123;APP_Name&#125;_business.log&lt;/file&gt; &lt;rollingPolicy class="ch.qos.logback.core.rolling.FixedWindowRollingPolicy"&gt; &lt;fileNamePattern&gt;../logs/$&#123;APP_Name&#125;_business.%i.business.zip&lt;/fileNamePattern&gt; &lt;minIndex&gt;1&lt;/minIndex&gt; &lt;maxIndex&gt;10&lt;/maxIndex&gt; &lt;/rollingPolicy&gt; &lt;triggeringPolicy class="ch.qos.logback.core.rolling.SizeBasedTriggeringPolicy"&gt; &lt;maxFileSize&gt;10MB&lt;/maxFileSize&gt; &lt;/triggeringPolicy&gt; &lt;encoder&gt; &lt;pattern&gt;[%d&#123;timeStyle&#125;] [%cn] %-5level %logger&#123;35&#125; - %msg%n&lt;/pattern&gt; &lt;/encoder&gt; &lt;filter class="ch.qos.logback.classic.filter.LevelFilter"&gt;&lt;!-- 只打印错误日志 --&gt; &lt;level&gt;WARE&lt;/level&gt; &lt;onMatch&gt;ACCEPT&lt;/onMatch&gt; &lt;onMismatch&gt;DENY&lt;/onMismatch&gt; &lt;/filter&gt; &lt;/appender&gt; &lt;!-- 异步输出 --&gt; &lt;appender name="ASYNC" class="ch.qos.logback.classic.AsyncAppender"&gt; &lt;!-- 不丢失日志.默认的,如果队列的80%已满,则会丢弃TRACT、DEBUG、INFO级别的日志 --&gt; &lt;discardingThreshold&gt;0&lt;/discardingThreshold&gt; &lt;!-- 更改默认的队列的深度,该值会影响性能.默认值为256 --&gt; &lt;queueSize&gt;256&lt;/queueSize&gt; &lt;!-- 添加附加的appender,最多只能添加一个 --&gt; &lt;appender-ref ref="FILE"/&gt; &lt;/appender&gt; &lt;appender name="BUSINESS_ASYNC" class="ch.qos.logback.classic.AsyncAppender"&gt; &lt;!-- 不丢失日志.默认的,如果队列的80%已满,则会丢弃TRACT、DEBUG、INFO级别的日志 --&gt; &lt;discardingThreshold&gt;0&lt;/discardingThreshold&gt; &lt;!-- 更改默认的队列的深度,该值会影响性能.默认值为256 --&gt; &lt;queueSize&gt;256&lt;/queueSize&gt; &lt;!-- 添加附加的appender,最多只能添加一个 --&gt; &lt;appender-ref ref="BUSINESS_FILE"/&gt; &lt;/appender&gt; &lt;!-- 为数据库开启显示sql --&gt; &lt;logger name="com.cjf.example.repository" level="DEBUG"&gt;&lt;/logger&gt; &lt;logger name="com.cjf" level="INFO"&gt;&lt;/logger&gt; &lt;!--日志的root目录，用于定位日志输出级别--&gt; &lt;root level="INFO"&gt; &lt;appender-ref ref="ASYNC"/&gt; &lt;appender-ref ref="STDOUT"/&gt; &lt;/root&gt;&lt;/configuration&gt; 上面就是一个常用的日志配置模版，下面就从跟节点来解析每个节点 1.configurationscan：当此属性设置为true时，配置文件如果发生改变，将会被重新加载，默认值为true。scanPeriod：监测配置文件是否有修改的时间间隔，默认 60s。debug：是否打印 logback 内部日志，默认 false.2.contextName 项目名称logger 上下文容器名称，默认 ‘default’，用于区分不同应用程序的记录。（可以在日志输出的时候将项目名称打印处理方便系统间交互 比如上面配置的 %cn）3.property 设置变量定义变量后，可以使${}来使用变量。4.timestamp 设置时间戳格式key:标识此 的名字；datePattern：设置将当前时间（解析配置文件的时间）转换为字符串的模式，遵循Java.txt.SimpleDateFormat的格式。5.logger用来设置某一个包或者具体的某一个类的日志打印级别、以及指定。仅有一个name属性，一个可选的level和一个可选的addtivity属性。name: 用来指定受此loger约束的某一个包或者具体的某一个类。level:用来设置打印级别，大小写无关：TRACE, DEBUG, INFO, WARN, ERROR, ALL 和 OFF，还有一个特俗值INHERITED或者同义词NULL，代表强制执行上级的级别。如果未设置此属性，那么当前loger将会继承上级的级别。addtivity:是否向上级loger传递打印信息。默认是true。loger可以包含零个或多个appender-ref元素，标识这个appender将会添加到这个loger。6.root也是 loger 元素，但是它是根 loger。只有一个 level 属性，应为已经被命名为 “root”.root 可以包含零个或多个 appender-ref 元素，标识这个 appender 将会添加到这个 loger。 什么是 Appender？logback 将日志记录事件写入到名为 appender 的组件的任务,不同 appender 决定了日志的记录方式。appender 有多种实现的方式，下面简单介绍几种比较常用的配置。 更多详情请参考官方文档 ConsoleAppender把日志添加到控制台，有以下子节点： encoder：对日志进行格式化 target：字符串 System.out 或者 System.err ，默认 System.out; withJansi：日志彩色输出 例如123456789101112&lt;configuration&gt; &lt;appender name="STDOUT" class="ch.qos.logback.core.ConsoleAppender"&gt; &lt;encoder&gt; &lt;pattern&gt;[%d&#123;timeStyle&#125;] [%cn] %-5level %logger&#123;35&#125; - %msg%n&lt;/pattern&gt; &lt;/encoder&gt; &lt;/appender&gt; &lt;root level="DEBUG"&gt; &lt;appender-ref ref="STDOUT" /&gt; &lt;/root&gt; &lt;/configuration&gt; RollingFileAppender滚动记录文件，先将日志记录到指定文件，当符合某个条件时，将日志记录到其他文件。file：被写入的文件名，可以是相对目录，也可以是绝对目录，如果上级目录不存在会自动创建，没有默认值。append：如果是 true，日志被追加到文件结尾，如果是 false，清空现存文件，默认是true。encoder：对记录事件进行格式化。rollingPolicy：当发生滚动时，决定 RollingFileAppender 的行为，涉及文件移动和重命名。triggeringPolicy：告知 RollingFileAppender 合适激活滚动。prudent：当为true时，不支持FixedWindowRollingPolicy。支持TimeBasedRollingPolicy，但是有两个限制，1不支持也不允许文件压缩，2不能设置file属性，必须留空。 rollingPolicy 有两种类型，分别是 TimeBasedRollingPolicy，FixedWindowRollingPolicy。 例如 TimeBasedRollingPolicy 配置123456789101112&lt;!--输出到文件--&gt;&lt;appender name="file" class="ch.qos.logback.core.rolling.RollingFileAppender"&gt; &lt;file&gt;$&#123;log.path&#125;&lt;/file&gt; &lt;rollingPolicy class="ch.qos.logback.core.rolling.TimeBasedRollingPolicy"&gt; &lt;fileNamePattern&gt;logback.%d&#123;yyyy-MM-dd&#125;.log&lt;/fileNamePattern&gt; &lt;maxHistory&gt;30&lt;/maxHistory&gt; &lt;totalSizeCap&gt;1GB&lt;/totalSizeCap&gt; &lt;/rollingPolicy&gt; &lt;encoder&gt; &lt;pattern&gt;%d&#123;HH:mm:ss.SSS&#125; %contextName [%thread] %-5level %logger&#123;36&#125; - %msg%n&lt;/pattern&gt; &lt;/encoder&gt;&lt;/appender&gt; fileNamePattern 定义了日志的切分方式——把每一天的日志归档到一个文件中maxHistory 表示只保留最近30天的日志，以防止日志填满整个磁盘空间。totalSizeCap 用来指定日志文件的上限大小，例如设置为1GB的话，那么到了这个值，就会删除旧的日志。 例如 FixedWindowRollingPolicy 配置1234567891011121314&lt;appender name="FILE" class="ch.qos.logback.core.rolling.RollingFileAppender"&gt; &lt;file&gt;../logs/$&#123;APP_Name&#125;_run.log&lt;/file&gt; &lt;rollingPolicy class="ch.qos.logback.core.rolling.FixedWindowRollingPolicy"&gt; &lt;fileNamePattern&gt;../logs/$&#123;APP_Name&#125;_run.%i.log.zip&lt;/fileNamePattern&gt; &lt;minIndex&gt;1&lt;/minIndex&gt; &lt;maxIndex&gt;10&lt;/maxIndex&gt; &lt;/rollingPolicy&gt; &lt;triggeringPolicy class="ch.qos.logback.core.rolling.SizeBasedTriggeringPolicy"&gt; &lt;maxFileSize&gt;10MB&lt;/maxFileSize&gt; &lt;/triggeringPolicy&gt; &lt;encoder&gt; &lt;pattern&gt;[%d&#123;timeStyle&#125;] [%cn] %-5level %logger&#123;35&#125; - %msg%n&lt;/pattern&gt; &lt;/encoder&gt;&lt;/appender&gt; 这里多了一个 SizeBasedTriggeringPolicy 触发机制，但 log 文件大于 10MB 的时候，就开始执行 rolling。minIndex 和 maxIndex 表示保存日志数量，但大于 maxIndex 的时候，会开始删除旧的日志。 必须包含“%i”例如，假设最小值和最大值分别为1和2，命名模式为mylog%i.log,会产生归档文件mylog1.log和mylog2.log。还可以指定文件压缩选项，例如，mylog%i.log.gz或者 没有log%i.log.zip AsyncAppender 异步输出这里就不写了，可以参考文章。 总结由于没有深入去了解 logback ,许多内容都是网上摘来的，写文章的时候也很费劲。思路不清晰。]]></content>
      <categories>
        <category>LogBack</category>
      </categories>
      <tags>
        <tag>LogBack</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LinkedList 源码分析]]></title>
    <url>%2F2018%2F06%2F03%2FLinkedList%20%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%2F</url>
    <content type="text"><![CDATA[LinkedList 源码分析前言有了ArrayList，自然少不了LinkedList了。 下面我就以面试问答的形式学习我们的常用的装载容器——LinkedList（源码分析基于JDK8） 问答内容LinkedList 用来做什么，怎么使用？问：请简单介绍一下您所了解的LinkedList，它可以用来做什么，怎么使用？ 答： LinkedList底层是双向链表，同时实现了List接口和Deque接口，所以它既可以看作是一个顺序容器，也可以看作是一个队列(Queue)，同时也可以看作是一个栈(Stack)，但如果想使用栈或队列等数据结构的话，推荐使用ArrayDeque，它作为栈或队列会比LinkedList有更好的使用性能。 示例代码： 12345678910111213141516171819// 创建一个LinkedList，链表的每个节点的内存空间都是实时分配的，所以无须事先指定容器大小LinkedList&lt;String&gt; linkedList = new LinkedList&lt;String&gt;();// 往容器里面添加元素linkedList.add("张三");linkedList.add("李四");// 在张三与李四之间插入一个王五linkedList.add(1, "王五");// 在头部插入一个小三linkedList.addFirst("小三");// 获取index下标为2的元素 王五String element = linkedList.get(2);// 修改index下标为2的元素 王五 为小四linkedList.set(2, "小四");// 删除index下标为1的元素 张三String removeElement = linkedList.remove(1);// 删除第一个元素String removeFirstElement = linkedList.removeFirst();// 删除最后一个元素String removeLastElement = linkedList.removeLast(); LinkedList底层实现是双向链表，核心组成元素有：int size = 0用于记录链表长度；Node&lt;E&gt; first;用于记录头（第一个）结点（储存的是头结点的引用）；Node&lt;E&gt; last;用于记录尾（最后一个）结点（储存的是尾结点的引用）。 示例代码： 123456789101112131415161718192021public class LinkedList&lt;E&gt; extends AbstractSequentialList&lt;E&gt; implements List&lt;E&gt;, Deque&lt;E&gt;, Cloneable, java.io.Serializable&#123; // 记录链表长度 transient int size = 0; /** * Pointer to first node. 指向第一个结点 * Invariant: (first == null &amp;&amp; last == null) || * (first.prev == null &amp;&amp; first.item != null) */ transient Node&lt;E&gt; first; /** * Pointer to last node. 指向最后一个结点 * Invariant: (first == null &amp;&amp; last == null) || * (last.next == null &amp;&amp; last.item != null) */ transient Node&lt;E&gt; last;&#125; 双向链表的核心组成元素还有一个最重要的Node&lt;E&gt;，Node&lt;E&gt;包含：E item; 用于存储元素数据，Node&lt;E&gt; next; 指向当前元素的后继结点，Node&lt;E&gt; prev; 指向当前元素的前驱结点。 示例代码： 1234567891011121314151617/** * 定义LinkedList底层的结点实现 */private static class Node&lt;E&gt; &#123; E item; // 存储元素数据 Node&lt;E&gt; next;// 指向当前元素的后继结点 Node&lt;E&gt; prev;// 指向当前元素的前驱结点 /** * Node结点构造方法 */ Node(Node&lt;E&gt; prev, E element, Node&lt;E&gt; next) &#123; this.item = element;// 存储的元素 this.next = next;// 后继结点 this.prev = prev;// 前驱结点 &#125;&#125; 双向链表底层实现，图片来自网络 上图中的head即Node first; tail即Node last; LinkedList 的操作和对应的时间复杂度。问：请分别分析一下它是如何获取元素，修改元素，新增元素与删除元素，并分析这些操作对应的时间复杂度。 答： 获取元素：LinkedList提供了三种获取元素的方法，分别是： 获取第一个元素getFirst()，获取第一个元素，直接返回Node&lt;E&gt; first指向的结点即可，所以时间复杂度为O(1)。 获取最后一个元素getLast()，获取最后一个元素，直接返回Node&lt;E&gt; last指向的结点即可，所以时间复杂度也为O(1)。 获取指定索引index位置的元素get(int index)，由于Node&lt;E&gt;结点在内存中存储的空间不是连续存储的，所以查找某一位置的结点，只能通过遍历链表的方式查找结点，因此LinkedList会先通过判断index &lt; (size &gt;&gt; 1)，size&gt;&gt;1即为size/2当前链表长度的一半，判断index的位置是在链表的前半部分还是后半部分。决定是从头部遍历查找数据还是从尾部遍历查找数据。最坏情况下，获取中间元素，则需要遍历n/2次才能获取到对应元素，所以此方法的时间复杂度为O(n)。 综上所述，LinkedList获取元素的时间复杂度为O(n)。 示例代码： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546/** * 返回列表中指定位置的元素 * * @param index 指定index位置 * @return 返回指定位置的元素 * @throws IndexOutOfBoundsException &#123;@inheritDoc&#125; */public E get(int index) &#123; // 检查index下标是否合法[0,size) checkElementIndex(index); // 遍历列表获取对应index位置的元素 return node(index).item;&#125;/** * 检查下标是否合法 */private void checkElementIndex(int index) &#123; if (!isElementIndex(index)) throw new IndexOutOfBoundsException(outOfBoundsMsg(index));&#125;private boolean isElementIndex(int index) &#123; return index &gt;= 0 &amp;&amp; index &lt; size;&#125;/** * 返回指定位置的结点元素（重点） */Node&lt;E&gt; node(int index) &#123; // assert isElementIndex(index); // 判断index位置是在链表的前半部分还是后半部分 if (index &lt; (size &gt;&gt; 1)) &#123; // 从头结点开始，从前往后遍历找到对应位置的结点元素 Node&lt;E&gt; x = first; for (int i = 0; i &lt; index; i++) x = x.next; return x; &#125; else &#123; // 从尾结点开始，从后往前遍历找到对应位置的结点元素 Node&lt;E&gt; x = last; for (int i = size - 1; i &gt; index; i--) x = x.prev; return x; &#125;&#125; 修改元素：LinkedList提供了一种修改元素数据的方法set(int index, E element)，修改元素数据的步骤是：1.检查index索引是否合法[0,size)。2.折半查询获取对应索引元素。3.将新元素赋值，返回旧元素。由获取元素的分析可知，折半查询的时间复杂度为O(n)，故修改元素数据的时间复杂度为O(n)。 示例代码： 123456789101112131415161718/** * 修改指定位置结点的存储数据 * * @param index 指定位置 * @param element 修改的存储数据 * @return 返回未修改前的存储数据 * @throws IndexOutOfBoundsException &#123;@inheritDoc&#125; */public E set(int index, E element) &#123; // 检查index下标是否合法[0,size) checkElementIndex(index); // 折半查询获取对应索引元素 Node&lt;E&gt; x = node(index); // 将新元素赋值，返回旧元素 E oldVal = x.item; x.item = element; return oldVal;&#125; 新增元素：LinkedList提供了四种新增元素的方法，分别是： 将指定元素插入到链表的第一个位置中addFirst(E e)，只需将头结点first指向新元素结点，将原第一结点的前驱指针指向新元素结点即可。不需要移动原数据存储位置，只需交换一下相关结点的指针域信息即可。所以时间复杂度为O(1)。 将指定元素插入到链表的最后一个位置中addLast(E e)，只需将尾结点last指向新元素结点，将原最后一个结点的后继指针指向新元素结点即可。不需要移动原数据存储位置，只需交换一下相关结点的指针域信息即可。所以时间复杂度也为O(1)。 添加元素方法add(E e) 等价于addLast(E e)。 将指定元素插入到链表的指定位置index中add(int index, E element)，需要先根据位置index调用node(index)遍历链表获取该位置的原结点，然后将新结点插入至原该位置结点的前面，不需要移动原数据存储位置，只需交换一下相关结点的指针域信息即可。所以时间复杂度也为O(1)。 综上所述，LinkedList新增元素的时间复杂度为O(1)，单纯论插入新元素，操作是非常高效的，特别是插入至头部或插入到尾部。但如果是通过索引index的方式插入，插入的位置越靠近链表中间所费时间越长，因为需要对链表进行遍历查找。 添加元素结点示意图，图片来自《大话数据结构》 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134/** * 将指定元素插入到链表的第一个位置中 * * @param e 要插入的元素 */public void addFirst(E e) &#123; linkFirst(e);&#125;/** * 将元素e作为第一个元素 */private void linkFirst(E e) &#123; // 获取原头结点 final Node&lt;E&gt; f = first; // 初始化新元素结点 final Node&lt;E&gt; newNode = new Node&lt;&gt;(null, e, f); // 头指针指向新元素结点 first = newNode; // 如果是第一个元素（链表为空） if (f == null) // 将尾指针也指向新元素结点 last = newNode; else // 链表不会空 // 原头结点的前驱指针指向新结点 f.prev = newNode; // 记录链表长度的size + 1 size++; modCount++;&#125;/** * 将指定元素插入到链表的最后一个位置中 * * &lt;p&gt;此方法等同与add(E e)方法 &#123;@link #add&#125;. * * @param e 要插入的元素 */public void addLast(E e) &#123; linkLast(e);&#125;/** * 将指定元素插入到链表的最后一个位置中 * * &lt;p&gt;此方法等同与addLast(E e)方法 &#123;@link #addLast&#125;. * * @param e 要插入的元素 * @return &#123;@code true&#125; (as specified by &#123;@link Collection#add&#125;) */public boolean add(E e) &#123; linkLast(e); return true;&#125;/** * 将元素e作为最后一个元素 */void linkLast(E e) &#123; // 获取原尾结点 final Node&lt;E&gt; l = last; // 初始化新元素结点 final Node&lt;E&gt; newNode = new Node&lt;&gt;(l, e, null); // 位指针指向新元素结点 last = newNode; // 如果是第一个元素（链表为空） if (l == null) // 将头指针也指向新元素结点 first = newNode; else // 链表不会空 // 原尾结点的后继指针指向新结点 l.next = newNode; // 记录链表长度的size + 1 size++; modCount++;&#125;/** * 将指定元素插入到链表的指定位置index中 * * @param index 元素要插入的位置index * @param element 要插入的元素 * @throws IndexOutOfBoundsException &#123;@inheritDoc&#125; */public void add(int index, E element) &#123; // 检查插入位置是否合法[0,size] checkPositionIndex(index); // 如果插入的位置和当前链表长度相等，则直接将元素插入至链表的尾部 if (index == size) // 将元素插入至链表的尾部 linkLast(element); else //将元素插入至指定位置,node(index)先获取占有该index位置的原结点 linkBefore(element, node(index));&#125;/** * 检查位置是否合法 */private void checkPositionIndex(int index) &#123; if (!isPositionIndex(index)) throw new IndexOutOfBoundsException(outOfBoundsMsg(index));&#125;/** * 检查位置是否合法 */private boolean isPositionIndex(int index) &#123; //合法位置为[0,size] return index &gt;= 0 &amp;&amp; index &lt;= size;&#125;/** * 将新元素e插入至旧元素succ前面 */void linkBefore(E e, Node&lt;E&gt; succ) &#123; // assert succ != null; // 记录旧元素结点succ的前驱指针 final Node&lt;E&gt; pred = succ.prev; // 初始化新元素结点 final Node&lt;E&gt; newNode = new Node&lt;&gt;(pred, e, succ); // 旧元素结点的前驱指针指向新元素结点(即新元素结点放至在旧元素结点的前面，取代了原本旧元素的位置) succ.prev = newNode; // 如果旧元素结点的前驱指针为空，则证明旧元素结点是头结点， // 将新元素结点插入至旧元素结点前面，所以现时新的头结点是新元素结点 if (pred == null) first = newNode; else //不是插入至头部 // 旧元素的前驱结点的后继指针指向新元素结点 pred.next = newNode; // 记录链表长度的size + 1 size++; modCount++;&#125; 删除元素：LinkedList提供了四种删除元素的方法，分别是： 删除链表中的第一个元素removeFirst()，只需将头结点first指向删除元素结点的后继结点并将其前驱结点指针信息prev清空即可。不需要移动原数据存储位置，只需操作相关结点的指针域信息即可。所以时间复杂度为O(1)。 删除链表中的最后一个元素removeLast()，只需将尾结点last指向删除元素结点的前驱结点并将其后继结点指针信息next清空即可。不需要移动原数据存储位置，只需操作相关结点的指针域信息即可，所以时间复杂度也为O(1)。 将指定位置index的元素删除remove(int index)，需要先根据位置index调用node(index)遍历链表获取该位置的原结点，然后将删除元素结点的前驱结点的next后继结点指针域指向删除元素结点的后继结点node.prev.next = node.next，删除元素结点的后继结点的prev前驱结点指针域指向删除元素结点的前驱结点即可node.next.prev = node.prev（此处可能有些绕，不太理解的同学自行学习一下双向链表的数据结构吧），不需要移动原数据存储位置，只需交换一下相关结点的指针域信息即可。所以时间复杂度也为O(1)。 删除元素结点示意图，图片来自《大话数据结构》 删除传入的Object o指定对象，比较对象是否一致通过o.equals方法比较remove(Object o)，和3.的思路基本差不多，关键是比较对象是通过o.equals方法，记住这点即可。 综上所述，LinkedList删除元素的时间复杂度为O(1)，单纯论删除元素，操作是非常高效的，特别是删除第一个结点或删除最后一个结点。但如果是通过索引index的方式或者object对象的方式删除，则需要对链表进行遍历查找对应index索引的对象或者利用equals方法判断对象。 示例代码： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169/** * 删除链表中的第一个元素并返回 * * @return 链表中的第一个元素 * @throws NoSuchElementException if this list is empty */public E removeFirst() &#123; //根据头结点获取第一个元素结点 final Node&lt;E&gt; f = first; if (f == null) // 没有元素结点则抛出异常 throw new NoSuchElementException(); return unlinkFirst(f);&#125;/** * 移除第一个元素 */private E unlinkFirst(Node&lt;E&gt; f) &#123; // assert f == first &amp;&amp; f != null; // 记录要移除元素结点的数据域 final E element = f.item; // 记录要移除元素结点的后继结点指针 final Node&lt;E&gt; next = f.next; // 清空要删除结点的数据域和next指针域信息，以帮助垃圾回收 f.item = null; f.next = null; // help GC // 头结点指向要移除元素结点的后继结点 first = next; // 如果要移除元素结点的后继结点为空，则证明链表只有一个元素 // 所以需要将尾结点的指针信息也要清空 if (next == null) last = null; else // 将新的第一个结点的前驱结点指针信息清空 next.prev = null; // 记录链表长度的size - 1 size--; modCount++; // 返回移除元素结点的数据域 return element;&#125;/** * 删除链表中的最后一个元素并返回 * * @return 链表中的最后一个元素 * @throws NoSuchElementException if this list is empty */public E removeLast() &#123; // 根据尾结点获取最后一个元素结点 final Node&lt;E&gt; l = last; if (l == null)// 没有元素结点则抛出异常 throw new NoSuchElementException(); return unlinkLast(l);&#125;/** * 移除最后一个元素 */private E unlinkLast(Node&lt;E&gt; l) &#123; // assert l == last &amp;&amp; l != null; // 记录要移除元素结点的数据域 final E element = l.item; // 记录要移除元素结点的前驱结点指针 final Node&lt;E&gt; prev = l.prev; // 清空要删除结点的数据域和prev指针域信息，以帮助垃圾回收 l.item = null; l.prev = null; // help GC // 头结点指向要移除元素结点的前驱结点 last = prev; // 如果要移除元素结点的前驱结点为空，则证明链表只有一个元素 // 所以需要将头结点的指针信息也要清空 if (prev == null) first = null; else // 将新的最后一个结点的后继结点指针信息清空 prev.next = null; // 记录链表长度的size - 1 size--; modCount++; // 返回移除元素结点的数据域 return element;&#125;/** * 将指定位置index的元素删除 * * @param index 要删除的位置index * @return 要删除位置的原元素 * @throws IndexOutOfBoundsException &#123;@inheritDoc&#125; */public E remove(int index) &#123; // 检查index下标是否合法[0,size) checkElementIndex(index); // 根据index进行遍历链表获取要删除的结点，再调用unlink方法进行删除 return unlink(node(index));&#125;/** * 删除传入的Object o指定对象，比较对象是否一致通过o.equals方法比较 * @param o 要删除的Object o指定对象 * @return &#123;@code true&#125; 是否存在要删除对象o */public boolean remove(Object o) &#123; // 如果删除对象为null，则遍历链表查找node.item数据域为null的结点并移除 if (o == null) &#123; for (Node&lt;E&gt; x = first; x != null; x = x.next) &#123; if (x.item == null) &#123; unlink(x); return true; &#125; &#125; &#125; else &#123; // 从头开始遍历链表，并通过equals方法逐一比较node.item是否相等 // 相等则对象一致，删除此对象。 for (Node&lt;E&gt; x = first; x != null; x = x.next) &#123; if (o.equals(x.item)) &#123; unlink(x); return true; &#125; &#125; &#125; return false;&#125;/** * 移除指定结点x */E unlink(Node&lt;E&gt; x) &#123; // assert x != null; // 记录要移除元素结点的数据域 final E element = x.item; // 记录要移除元素结点的后继结点指针 final Node&lt;E&gt; next = x.next; // 记录要移除元素结点的前驱结点指针 final Node&lt;E&gt; prev = x.prev; // 如果要移除元素结点的前驱结点为空，则证明要删除结点为第一个结点 if (prev == null) &#123; // 头结点指向要删除元素结点的后继结点 first = next; &#125; else &#123; // 要删除元素结点的前驱结点的后继指针指向要删除元素结点的后继结点 prev.next = next; // 清空要删除结点的前驱结点指针信息，以帮助GC x.prev = null; &#125; // 如果要移除元素结点的后继结点为空，则证明要删除结点为最后一个结点 if (next == null) &#123; // 尾结点指向要删除元素结点的前驱结点 last = prev; &#125; else &#123; // 要删除元素结点的后继结点的前驱指针指向要删除元素结点的前驱结点 next.prev = prev; // 清空要删除结点的后继结点指针信息，以帮助GC x.next = null; &#125; // 清空要删除元素的数据域，以帮助GC x.item = null; // 记录链表长度的size - 1 size--; modCount++; // 返回移除元素结点的数据域 return element;&#125; ArrayList和LinkedList 的区别问：那您可以比较一下ArrayList和LinkedList吗? 答： LinkedList内部存储的是Node&lt;E&gt;，不仅要维护数据域，还要维护prev和next，如果LinkedList中的结点特别多，则LinkedList比ArrayList更占内存。 插入删除操作效率：LinkedList在做插入和删除操作时，插入或删除头部或尾部时是高效的，操作越靠近中间位置的元素时，需要遍历查找，速度相对慢一些，如果在数据量较大时，每次插入或删除时遍历查找比较费时。所以LinkedList插入与删除，慢在遍历查找，快在只需要更改相关结点的引用地址。ArrayList在做插入和删除操作时，插入或删除尾部时也一样是高效的，操作其他位置，则需要批量移动元素，所以ArrayList插入与删除，快在遍历查找，慢在需要批量移动元素。 循环遍历效率： 由于ArrayList实现了RandomAccess随机访问接口，所以使用for(int i = 0; i &lt; size; i++)遍历会比使用Iterator迭代器来遍历快： 12345678for (int i=0, n=list.size(); i &lt; n; i++) &#123; list.get(i);&#125;runs faster than this loop:for (Iterator i=list.iterator(); i.hasNext(); ) &#123; i.next();&#125; 而由于LinkedList未实现RandomAccess接口，所以推荐使用Iterator迭代器来遍历数据。 因此，如果我们需要频繁在列表的中部改变插入或删除元素时，建议使用LinkedList，否则，建议使用ArrayList，因为ArrayList遍历查找元素较快，并且只需存储元素的数据域，不需要额外记录其他数据的位置信息，可以节省内存空间。 LinkedList是线程安全的吗？问：LinkedList是线程安全的吗？ 答：LinkedList不是线程安全的，如果多个线程同时对同一个LinkedList更改数据的话，会导致数据不一致或者数据污染。如果出现线程不安全的操作时，LinkedList会尽可能的抛出ConcurrentModificationException防止数据异常，当我们在对一个LinkedList进行遍历时，在遍历期间，我们是不能对LinkedList进行添加，删除等更改数据结构的操作的，否则也会抛出ConcurrentModificationException异常，此为fail-fast（快速失败）机制。从源码上分析，我们在add,remove等更改LinkedList数据时，都会导致modCount的改变，当expectedModCount != modCount时，则抛出ConcurrentModificationException。如果想要线程安全，可以考虑调用Collections.synchronizedCollection(Collection&lt;T&gt; c)方法。 示例代码： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798private class ListItr implements ListIterator&lt;E&gt; &#123; private Node&lt;E&gt; lastReturned; private Node&lt;E&gt; next; private int nextIndex; private int expectedModCount = modCount; ListItr(int index) &#123; // assert isPositionIndex(index); next = (index == size) ? null : node(index); nextIndex = index; &#125; public boolean hasNext() &#123; return nextIndex &lt; size; &#125; public E next() &#123; checkForComodification(); if (!hasNext()) throw new NoSuchElementException(); lastReturned = next; next = next.next; nextIndex++; return lastReturned.item; &#125; public boolean hasPrevious() &#123; return nextIndex &gt; 0; &#125; public E previous() &#123; checkForComodification(); if (!hasPrevious()) throw new NoSuchElementException(); lastReturned = next = (next == null) ? last : next.prev; nextIndex--; return lastReturned.item; &#125; public int nextIndex() &#123; return nextIndex; &#125; public int previousIndex() &#123; return nextIndex - 1; &#125; public void remove() &#123; checkForComodification(); if (lastReturned == null) throw new IllegalStateException(); Node&lt;E&gt; lastNext = lastReturned.next; unlink(lastReturned); if (next == lastReturned) next = lastNext; else nextIndex--; lastReturned = null; expectedModCount++; &#125; public void set(E e) &#123; if (lastReturned == null) throw new IllegalStateException(); checkForComodification(); lastReturned.item = e; &#125; public void add(E e) &#123; checkForComodification(); lastReturned = null; if (next == null) linkLast(e); else linkBefore(e, next); nextIndex++; expectedModCount++; &#125; public void forEachRemaining(Consumer&lt;? super E&gt; action) &#123; Objects.requireNonNull(action); while (modCount == expectedModCount &amp;&amp; nextIndex &lt; size) &#123; action.accept(next.item); lastReturned = next; next = next.next; nextIndex++; &#125; checkForComodification(); &#125; final void checkForComodification() &#123; if (modCount != expectedModCount) throw new ConcurrentModificationException(); &#125;&#125; 总结LinkedList的结论已在第三个问题中展现了一部分了，所以不再重复说明了，我以面试问答的形式和大家一同学习了LinkedList，由于没有时间画图，可能此次没有ArrayList说的那么清楚，如果大家有看不懂的地方，请自行看一下关于链表的数据结构吧。如果此文对你有帮助，麻烦点个喜欢，谢谢各位。]]></content>
      <categories>
        <category>LinkedList</category>
      </categories>
      <tags>
        <tag>基础数据类型</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[jQuery 概述]]></title>
    <url>%2F2018%2F06%2F03%2FjQuery%E6%A6%82%E8%BF%B0%2F</url>
    <content type="text"><![CDATA[jQuery 概述jQuery 简介官网： http://jquery.com/ 中文：https://www.jquery123.com 本质： jQuery是一个快速、简洁的JavaScript框架，是继Prototype之后又一个优秀的JavaScript代码库（或JavaScript框架）。 jQuery有三条产品线jQuery1.x.x : 兼容IE6、7、8，花了很大的气力让IE6、7、8等低级浏览器都兼容。 jQuery2.x.x : 不兼容IE6、7、8，从1代中剔除了所有兼容代码。 jQuery3.x.x : 全面支持HTML5和CSS3。 jQuery的操作过程(1) jQuery操作页面元素一定是从一个$()开始的！ (2) $()函数里面有引号，引号里面写CSS选择器。 (3) 然后加上jQuery自己的方法（不能使用js原生的方法。） 添加事件监听123456789101112131415161718192021222324252627282930313233343536373839&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head lang="en"&gt; &lt;meta charset="UTF-8"&gt; &lt;title&gt;lesson&lt;/title&gt; &lt;style&gt; .div1&#123; width: 200px;height:200px;background-color:skyblue ; &#125; .div2&#123; width: 100px; height: 100px; background-color: pink; &#125; .div3&#123; width: 100px; height:100px; background-color: orange; &#125; &lt;/style&gt;&lt;/head&gt;&lt;body&gt;&lt;div class="div1"&gt;div1 &lt;div class="div2"&gt;div2&lt;/div&gt;&lt;/div&gt;&lt;div class="div3"&gt;div3&lt;/div&gt;&lt;script src="js/jquery-1.12.3.min.js"&gt;&lt;/script&gt;&lt;script&gt; &lt;!--添加事件监听--&gt; $('.div3').click(function () &#123; $(this).css('background-color','red'); &#125;);&lt;/script&gt;&lt;/body&gt;&lt;/html&gt; 批量处理12345678910111213141516171819202122232425262728293031323334353637383940&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head lang="en"&gt; &lt;meta charset="UTF-8"&gt; &lt;title&gt;lesson&lt;/title&gt; &lt;style&gt; .div1&#123; width: 200px;height:200px;background-color:skyblue ; &#125; .div2&#123; width: 100px; height: 100px; background-color: pink; &#125; .div3&#123; width: 100px; height:100px; background-color: orange; &#125; &lt;/style&gt;&lt;/head&gt;&lt;body&gt;&lt;div class="div1"&gt;div1 &lt;div class="div2"&gt;div2&lt;/div&gt;&lt;/div&gt;&lt;div class="div3"&gt;div3&lt;/div&gt;&lt;script src="js/jquery-1.12.3.min.js"&gt;&lt;/script&gt;&lt;script&gt; &lt;!--批量处理--&gt; &lt;!--添加事件监听--&gt; $('div').click(function () &#123; $(this).css('background-color','red'); &#125;);&lt;/script&gt;&lt;/body&gt;&lt;/html&gt; 快速设置动画12345678910111213141516171819202122232425262728293031323334353637&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head lang="en"&gt; &lt;meta charset="UTF-8"&gt; &lt;title&gt;lesson&lt;/title&gt; &lt;style&gt; .div1&#123; width: 200px;height:200px;background-color:skyblue ; &#125; .div2&#123; width: 100px; height: 100px; background-color: pink; &#125; .div3&#123; width: 100px; height:100px; background-color: orange; &#125; &lt;/style&gt;&lt;/head&gt;&lt;body&gt;&lt;div class="div1"&gt;div1 &lt;div class="div2"&gt;div2&lt;/div&gt;&lt;/div&gt;&lt;div class="div3"&gt;div3&lt;/div&gt;&lt;script src="js/jquery-1.12.3.min.js"&gt;&lt;/script&gt;&lt;script&gt; // &lt;!--快速设置动画--&gt; $('.div3').click(function () &#123; $(this).animate(&#123;'width':600&#125;,500); &#125;);&lt;/script&gt;&lt;/body&gt;&lt;/html&gt; 支持css2 、css3 中所有的选择器12345678910111213141516171819202122232425262728293031323334353637&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head lang="en"&gt; &lt;meta charset="UTF-8"&gt; &lt;title&gt;lesson&lt;/title&gt; &lt;style&gt; .div1&#123; width: 200px;height:200px;background-color:skyblue ; &#125; .div2&#123; width: 100px; height: 100px; background-color: pink; &#125; .div3&#123; width: 100px; height:100px; background-color: orange; &#125; &lt;/style&gt;&lt;/head&gt;&lt;body&gt;&lt;div class="div1"&gt;div1 &lt;div class="div2"&gt;div2&lt;/div&gt;&lt;/div&gt;&lt;div class="div3"&gt;div3&lt;/div&gt;&lt;script src="js/jquery-1.12.3.min.js"&gt;&lt;/script&gt;&lt;script&gt; &lt;!--支持css2 、css3 中所有的选择器--&gt; $('.div1 .div2').css('background-color','blue'); &lt;/script&gt;&lt;/body&gt;&lt;/html&gt; 自带的封装到无敌的动画123456789101112131415161718192021222324252627282930313233343536373839404142434445&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head lang="en"&gt; &lt;meta charset="UTF-8"&gt; &lt;title&gt;lesson&lt;/title&gt; &lt;style&gt; .div1&#123; width: 200px;height:200px;background-color:skyblue ; &#125; .div2&#123; width: 100px; height: 100px; background-color: pink; &#125; .div3&#123; width: 100px; height:100px; background-color: orange; &#125; &lt;/style&gt;&lt;/head&gt;&lt;body&gt;&lt;div class="div1"&gt;div1 &lt;div class="div2"&gt;div2&lt;/div&gt;&lt;/div&gt;&lt;div class="div3"&gt;div3&lt;/div&gt;&lt;script src="js/jquery-1.12.3.min.js"&gt;&lt;/script&gt;&lt;script&gt; &lt;!--自带的封装到无敌的动画--&gt; $('.div3').click(function () &#123; //上滑 // $(this).slideUp(); // 先上再下 // $(this).slideUp().slideDown(); // 先上再下 渐变 // $(this).slideUp().slideDown().fadeOut(); $(this).slideUp().slideDown().fadeOut().fadeIn(); &#125;);&lt;/script&gt;&lt;/body&gt;&lt;/html&gt; 总结：jQuery框架整体感知 步骤： (1) jQuery操作页面元素一定是从一个$()开始的！ (2) $()函数里面有引号，引号里面写CSS选择器。 (3) 然后加上jQuery自己的方法（不能使用js原生的方法。） 注意： $()函数的执行结果会返回一个jq对象，jq对象只能调用jq框架中提供的方法 js对象只能调用js中提供的方法，jq对象和js对象两者不能混为一谈。 123456789101112131415161718192021222324252627282930313233343536373839&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head lang="en"&gt; &lt;meta charset="UTF-8"&gt; &lt;title&gt;lesson&lt;/title&gt; &lt;style&gt; .div1&#123; width: 200px;height:200px;background-color:skyblue ; &#125; .div2&#123; width: 100px; height: 100px; background-color: pink; &#125; .div3&#123; width: 100px; height:100px; background-color: orange; &#125; &lt;/style&gt;&lt;/head&gt;&lt;body&gt;&lt;div class="div1"&gt;div1 &lt;div class="div2"&gt;div2&lt;/div&gt;&lt;/div&gt;&lt;div class="div3"&gt;div3&lt;/div&gt;&lt;script src="js/jquery-1.12.3.min.js"&gt;&lt;/script&gt;&lt;script&gt; &lt;!-- js对象只能调用js中提供的方法，jq对象和js对象两者不能混为一谈。--&gt; $('.div3').click(function () &#123; $(this).style.backgroundColor = 'red'; &#125;);&lt;/script&gt;&lt;/body&gt;&lt;/html&gt;]]></content>
      <categories>
        <category>jQuery</category>
      </categories>
      <tags>
        <tag>前端框架</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java选择题【1~60】]]></title>
    <url>%2F2018%2F06%2F03%2FJava%E9%80%89%E6%8B%A9%E9%A2%98%E3%80%901~60%E3%80%91%2F</url>
    <content type="text"><![CDATA[Java选择题【1~60】原文地址：http://blog.csdn.net/qq_36075612/article/details/71126487 1.下面中哪两个可以在A的子类中使用：（ ） 123456789class A &#123;protected int method1 (int a, int b) &#123;return 0;&#125;&#125; A. public int method 1 (int a, int b) { return 0; } B. private int method1 (int a, int b) { return 0; } C. private int method1 (int a, long b) { return 0; } D. public short method1 (int a, int b) { return 0; } 解答：AC 主要考查子类重写父类的方法的原则 B，子类重写父类的方法，访问权限不能降低 C，属于重载 D，子类重写父类的方法 返回值类型要相同或是父类方法返回值类型的子类 2.Abstract method cannot be static. True or False ? A True B False 解答：A 抽象方法可以在子类中被重写，但是静态方法不能在子类中被重写，静态方法和静态属性与对象是无关的，只与类有关，这与abstract是矛盾的，所以abstract是不能被修饰为static，否则就失去了abstract的意义了 3.What will be the output when you compile and execute the following program. 12345678910111213141516171819202122232425262728293031class Base&#123;void test() &#123;System.out.println(“Base.test()”);&#125;&#125;public class Child extends Base &#123;void test() &#123;System.out.println(“Child.test()”);&#125;static public void main(String[] a) &#123;Child anObj = new Child();Base baseObj = (Base)anObj;baseObj.test();&#125;&#125; Select most appropriate answer. A . Child.test() Base.test() B. Base.test() Child.test() C. Base.test() D. Child.test() 解答：D 测试代码相当于：Base baseObj = new Child();父类的引用指向子类的实例，子类又重写了父类 的test方法，因此调用子类的test方法。 4.What will be the output when you compile and execute the following program. 1234567891011121314151617181920212223242526272829class Base&#123;static void test() &#123;System.out.println(“Base.test()”);&#125;&#125;public class Child extends Base &#123;void test() &#123;System.out.println(“Child.test()”);Base.test(); //Call the parent method&#125;static public void main(String[] a) &#123;new Child().test();&#125;&#125; Select most appropriate answer. A . Child.test() 、Base.test() B . Child.test()、Child.test() C. Compilation error. Cannot override a static method by an instance method D. Runtime error. Cannot override a static method by an instance method 解答：C 静态方法不能在子类中被重写 5.What will be the output when you compile and execute the following program. 123456789101112131415public class Base&#123;private void test() &#123;System.out.println(6 + 6 + “(Result)”);&#125;static public void main(String[] a) &#123;new Base().test();&#125;&#125; Select most appropriate answer. A. 66(Result) B . 12(Result) C . Runtime Error.Incompatible type for +. Can’t convert an int to a string. D . Compilation Error.Incompatible type for +. Can’t add a string to an int. 解答：B 字符串与基本数据类型链接的问题,如果第一个是字符串那么后续就都按字符串处理，比如上边例子要是System.out.println(“(Result)”+6 + 6 );那么结果就是(Result)66，如果第一个和第二个。。。第n个都是基本数据第n+1是字符串类型，那么前n个都按加法计算出结果在与字符串连接 6..What will be the output when you compile and execute the following program. The symbol ’ ?’ means space. 12345678910111213141516171819202122232425262728291:public class Base&#123;2:3: private void test() &#123;4:5: String aStr = “?One?”;6: String bStr = aStr;7: aStr.toUpperCase();8: aStr.trim();9: System.out.println(“[" + aStr + "," + bStr + "]“);7: &#125;8:9: static public void main(String[] a) &#123;10: new Base().test();11: &#125;12: &#125; Select most appropriate answer. A. [ONE,?One?] B . [?One?,One] C . [ONE,One] D . [ONE,ONE] E . [?One?,?One?] 解答：E 通过 String bStr = aStr; 这句代码使 bStr 和 aStr 指向同一个地址空间，所以最后 aStr 和 bStr 的结果应该是一样，String 类是定长字符串，调用一个字符串的方法以后会形成一个新的字符串。 7.下面关于变量及其范围的陈述哪些是不正确的（ ）： A．实例变量是类的成员变量 B．实例变量用关键字static声明 C．在方法中定义的局部变量在该方法被执行时创建 D．局部变量在使用前必须被初始化 解答：BC 由static修饰的变量称为类变量或是静态变量 方法加载的时候创建局部变量 8.下列关于修饰符混用的说法，错误的是（ ）： A．abstract不能与final并列修饰同一个类 B．abstract类中可以有private的成员 C．abstract方法必须在abstract类中 D．static方法中能处理非static的属性 解答 D 静态方法中不能引用非静态的成员 9.执行完以下代码 int [ ] x = new int[25]; 后，以下哪项说明是正确的（ ）： A、 x[24]为0 B、 x[24]未定义 C、 x[25]为0 D、 x[0]为空 解答：A x 属于引用类型，该引用类型的每一个成员是 int 类型，默认值为：0 10.编译运行以下程序后，关于输出结果的说明正确的是 （ ）： 1234567891011public class Conditional&#123;public static void main(String args[ ])&#123;int x=4;System.out.println(“value is “+ ((x&gt;4) ? 99.9 :9));&#125;&#125; A、 输出结果为：value is 99.99 B、 输出结果为：value is 9 C、 输出结果为：value is 9.0 D、 编译错误 解答：C 三目运算符中：第二个表达式和第三个表达式中如果都为基本数据类型，整个表达式的运算结果 由容量高的决定。99.9是double类型 而9是int类型，double容量高。 11.关于以下 application 的说明，正确的是（ ）： 123456789101112131415161718191． class StaticStuff2． &#123;3． static int x=10；4． static &#123; x+=5；&#125;5． public static void main（String args[ ]）6． &#123;7． System.out.println(“x=” + x);8． &#125;9． static &#123; x/=3;&#125;10. &#125; A、 4行与9行不能通过编译，因为缺少方法名和返回类型 B、 9行不能通过编译，因为只能有一个静态初始化器 C、 编译通过，执行结果为：x=5 D、编译通过，执行结果为：x=3 解答：C 自由块是类加载的时候就会被执行到的，自由块的执行顺序是按照在类中出现的先后顺序执行。 12.关于以下程序代码的说明正确的是（ ）： 123456789101112131415161718192021222324251．class HasStatic&#123;2． private static int x=100；3． public static void main(String args[ ])&#123;4． HasStatic hs1=new HasStatic( );5． hs1.x++;6． HasStatic hs2=new HasStatic( );7． hs2.x++;8． hs1=new HasStatic( );9． hs1.x++;10． HasStatic.x–;11． System.out.println(“x=”+x);12． &#125;13．&#125; A、5行不能通过编译，因为引用了私有静态变量 B、10行不能通过编译，因为x是私有静态变量 C、程序通过编译，输出结果为：x=103 D、程序通过编译，输出结果为：x=102 解答：D 静态变量是所有对象所共享的，所以上述代码中的几个对象操作是同一静态变量x， 静态变量可以通过类名调用。 13.下列说法正确的有（） A． class 中的constructor不可省略 B． constructor 必须与 class同名，但方法不能与class同名 C． constructor 在一个对象被 new 时执行 D．一个 class 只能定义一个constructor 解答：C 构造方法的作用是在实例化对象的时候给数据成员进行初始化 A．类中如果没有显示的给出构造方法，系统会提供一个无参构造方法 B．构造方法与类同名，类中可以有和类名相同的方法 D．构造方法可以重载 14.下列哪种说法是正确的（） A．实例方法可直接调用超类的实例方法 B．实例方法可直接调用超类的类方法 C．实例方法可直接调用其他类的实例方法 D．实例方法可直接调用本类的类方法 解答：D A. 实例方法不可直接调用超类的私有实例方法 B. 实例方法不可直接调用超类的私有的类方法 C．要看访问权限 15.下列哪一种叙述是正确的（ ） A． abstract修饰符可修饰字段、方法和类 B． 抽象方法的body部分必须用一对大括号{ }包住 C． 声明抽象方法，大括号可有可无 D． 声明抽象方法不可写出大括号 解答：D abstract可以修饰方法和类，不能修饰属性。抽象方法没有方法体，即没有大括号{} 16.下面代码的执行结果是？ 12345678910111213141516171819202122232425import java.util.*;public class ShortSet&#123;public static void main(String args[])&#123;Set&lt;Short&gt; s=new HashSet&lt;Short&gt;();for(Short i=0;i&lt;100;i++)&#123;s.add(i);s.remove(i-1);&#125;System.out.println(s.size());&#125;&#125; A. 1 B. 100 C. Throws Exception D. None of the Above 解答：B i 是 Short 类型 i-1 是int类型,其包装类为 Integer ，所以 s.remove(i-1); 不能移除Set集合中Short类型对象。 17.链表具有的特点是：(选择3项) A、不必事先估计存储空间 B、可随机访问任一元素 C、插入删除不需要移动元素 D、所需空间与线性表长度成正比 解答：ACD A.采用动态存储分配，不会造成内存浪费和溢出。 B. 不能随机访问，查找时要从头指针开始遍历 C. 插入、删除时，只要找到对应前驱结点，修改指针即可，无需移动元素 D. 需要用额外空间存储线性表的关系，存储密度小 18.Java语言中，String类的IndexOf()方法返回的类型是？ A、Int16 B、Int32 C、int D、long 解答：C indexOf方法的声明为：public int indexOf(int ch) 在此对象表示的字符序列中第一次出现该字符的索引；如果未出现该字符，则返回 -1。 19.以下关于面向对象概念的描述中，不正确的一项是（）。(选择1项) A.在现实生活中，对象是指客观世界的实体 B.程序中的对象就是现实生活中的对象 C.在程序中，对象是通过一种抽象数据类型来描述的，这种抽象数据类型称为类（class） D.在程序中，对象是一组变量和相关方法的集合 解答：B 20..执行下列代码后,哪个结论是正确的 String[] s=new String[10]; A． s[9] 为 null; B． s[10] 为 “”; C． s[0] 为 未定义 D． s.length 为10 解答：AD s是引用类型，s中的每一个成员都是引用类型，即String类型，String类型默认的值为null s数组的长度为10。 21.属性的可见性有。(选择3项) A.公有的 B.私有的 C.私有保护的 D.保护的 解答：ABD 属性的可见性有四种：公有的（public） 保护的（protected） 默认的 私有的（private） 22..在字符串前面加上_符号，则字符串中的转义字符将不被处理。(选择1项) A @ B \ C # D % 解答：B 23.下列代码哪行会出错: (选择1项) 123456789101) public void modify() &#123; 2) int I, j, k; 3) I = 100; 4) while ( I &gt; 0 ) &#123; 5) j = I * 2; 6) System.out.println (” The value of j is ” + j ); 7) k = k + 1; 8) I–; 9) &#125; 10) &#125; A. 4 B. 6 C. 7 D. 8 解答：C k没有初始化就使用了 24.对记录序列{314，298，508，123，486，145}按从小到大的顺序进行插入排序，经过两趟排序后的结果为：(选择1项) A {314，298，508，123，145，486} B {298，314，508，123，486，145} C {298，123，314，508，486，145} D {123、298，314，508，486，145} 解答：B 插入排序算法： 123456789101112131415161718192021222324252627public static void injectionSort(int[] number) &#123;// 第一个元素作为一部分，对后面的部分进行循环for (int j = 1; j &lt; number.length; j++) &#123;int tmp = number[j];int i = j – 1;while (tmp &lt; number[i]) &#123;number[i + 1] = number[i];i–;if (i == -1)break;&#125;number[i + 1] = tmp;&#125;&#125; 25.栈是一种。(选择1项) A 存取受限的线性结构 B 存取不受限的线性结构 C 存取受限的非线性结构 D 存取不受限的非线性结构 解答：A 栈（stack）在计算机科学中是限定仅在表尾进行插入或删除操作的线性表。 26.下列哪些语句关于内存回收的说明是正确的。(选择1项) A. 程序员必须创建一个线程来释放内存 B. 内存回收程序负责释放无用内存 C. 内存回收程序允许程序员直接释放内存 D. 内存回收程序可以在指定的时间释放内存对象 解答：B 垃圾收集器在一个Java程序中的执行是自动的，不能强制执行，即使程序员能明确地判断出有一块内存已经无用了，是应该回收的，程序员也不能强制垃圾收集器回收该内存块。程序员唯一能做的就是通过调用System. gc 方法来”建议”执行垃圾收集器，但其是否可以执行，什么时候执行却都是不可知的。 27.Which method must be defined by a class implementing the java.lang.Runnable interface? A. void run() B. public void run() C. public void start() D. void run(int priority) E. public void run(int priority) F. public void start(int priority) 解答：B 实现Runnable接口，接口中有一个抽象方法run，实现类中实现该方法。 28 Given: 123456789101112131415public static void main(String[] args) &#123;Object obj = new Object() &#123;public int hashCode() &#123;return 42;&#125;&#125;;System.out.println(obj.hashCode());&#125; What is the result? A. 42 B. An exception is thrown at runtime. C. Compilation fails because of an error on line 12. D. Compilation fails because of an error on line 16. E. Compilation fails because of an error on line 17. 解答：A 匿名内部类覆盖hashCode方法。 29.哪两个是Java编程语言中的保留字 ? (Choose two) A. run B. import C. default D. implements 解答：BD import导入包的保留字，implements实现接口的保留字。 Which two statements are true regarding the return values of property written hashCodeand equals methods from two instances of the same class? (Choose two) A. If the hashCode values are different, the objects might be equal. B. If the hashCode values are the same, the object must be equal. C. If the hashCode values are the same, the objects might be equal. D. If the hashCode values are different, the objects must be unequal. 解答：CD 先通过 hashcode 来判断某个对象是否存放某个桶里，但这个桶里可能有很多对象，那么我们就需要再通过 equals 来在这个桶里找到我们要的对象。 字符的数字范围是什么? A. 0 … 32767 B. 0 … 65535 C. –256 … 255 D. –32768 … 32767 E. Range is platform dependent. 解答：B 在Java中，char是一个无符号16位类型，取值范围为0到65535。 Given: 1234567891011public class Test &#123;private static float[] f = new float[2];public static void main(String args[]) &#123;System.out.println(“f[0] = “ + f[0]);&#125;&#125; What is the result? A. f[0] = 0 B. f[0] = 0.0 C. Compilation fails. D. An exception is thrown at runtime. 解答：B Given: 1234567891011public class Test &#123;public static void main(String[] args) &#123;String str = NULL;System.out.println(str);&#125;&#125; What is the result? A. NULL B. 编译失败 C. 运行代码没有输出 D. 运行时抛出异常 解答：B null应该小写 34、Exhibit: 123456789101112131415161718192021222324252627282930311.public class X implements Runnable &#123;2. private int x;3. private int y;4. public static void main(String [] args) &#123;5. X that = new X();6. (new Thread(that)).start();7. (new Thread(that)).start();8. &#125;9. public synchronized void run( )&#123;10. for (;;) &#123;11. x++;12. y++;13. System.out.println(“x = “ + x + “, y = “ + y);14. &#125;15. &#125;16.&#125; What is the result? A. 第11行的错误导致编译失败 B. 第7行和第8行的错误导致编译失败。 C. 该程序打印 x 和 y 的值对，这些值在同一行上可能不总是相同的 (例如, “x=2, y=1”) D. 该程序在同一行上打印x和y的值总是一对相同的值(例如, “x=1, y=1”. 此外,每个值出现两次(例如, “x=1, y=1” 其次是“x=1, y=1”) E. 该程序在同一行上打印x和y的值总是一对相同的值 (例如, “x=1, y=1”.此外,每个值出现两次 (例如, “x=1, y=1” 其次 “x=2, y=2”) 解答：E 多线程共享相同的数据，使用 synchronized 实现数据同步。 35、哪两个不能直接导致线程停止执行? (Choose Two) A. 使用 synchronized block. B. 在对象上调用 wait 方法 C. 调用对象的 notify 方法 D. 在InputStream对象上调用read方法。 E. 调用Thread对象上的SetPriority方法。 解答：AD stop方法.这个方法将终止所有未结束的方法，包括run方法。当一个线程停止时候，他会立即释 放 所有他锁住对象上的锁。这会导致对象处于不一致的状态。 当线程想终止另一个线程的时 候，它无法知道何时调用stop是安全的，何时会导致对象被破坏。所以这个方法被弃用了。你应 该中断一个线程而不是停止他。被中断的线程会在安全的时候停止。 36、 关于创建默认构造函数描述正确的是? (Choose Two) A. 默认构造函数初始化方法变量 B. 默认构造函数调用超类的无参数构造函数 C. 默认构造函数初始化在类中声明的实例变量 D. 如果一个类缺少一个无参数构造函数，但有其他构造函数，编译器会创建一个默认构造函数。 E. 编译器只有在没有其他构造函数是才会创建构造函数 解答：CE 构造方法的作用是实例化对象的时候给数据成员初始化，如果类中没有显示的提供构造方法，系统会提供默认的无参构造方法，如果有了其它构造方法，默认的构造方法不再提供。 37、 Given: 1234567public class OuterClass &#123;private double d1 = 1.0;//insert code here&#125; 您需要在第2行插入内部类声明。 哪两个内部类声明是有效的？ A. static class InnerOne { public double methoda() {return d1;} } B. static class InnerOne { static double methoda() {return d1;} } C. private class InnerOne { public double methoda() {return d1;} } D. protected class InnerOne { static double methoda() {return d1;} } E. public abstract class InnerOne { public abstract double methoda(); } 解答：CE AB.内部类可以声明为static的，但此时就不能再使用外层封装类的非static的成员变量； D.非static的内部类中的成员不能声明为static的，只有在顶层类或static的内部类中 才可声明static成员 38、哪两个声明可以防止重写方法？ (Choose Two) A. final void methoda() {} B. void final methoda() {} C. static void methoda() {} D. static final void methoda() {} E. final abstract void methoda() {} 解答：AD final修饰方法，在子类中不能被重写。 39、Given: 12345678910111213141516171819public class Test &#123;public static void main (String args[]) &#123;class Foo &#123;public int i = 3;&#125;Object o = (Object) new Foo();Foo foo = (Foo)o;System.out.println(foo.i);&#125;&#125; What is the result? A. 编译将失败。 B. 编译将成功，程序将打印“3” C. 编译会成功，但程序会在第6行抛出ClassCastException。 D. 编译会成功，但程序会在第7行抛出ClassCastException。 解答：B 局部内部类的使用 40、 Given: 123456789101112131415public class Test &#123;public static void main (String [] args) &#123;String foo = “blue”;String bar = foo;foo = “green”;System.out.println(bar);&#125;&#125; What is the result? A. 抛出异常。 B. 代码不会编译。 C. 该程序打印“null” D. 该程序打印“blue” E. 该程序打印“green” 解答：D 采用String foo = “blue”定义方式定义的字符串放在字符串池中，通过String bar = foo; 他们指向了同一地址空间，就是同一个池子，当执行foo = “green”; foo指向新的地址空间。 41、Which code determines the int value foo closest to a double value bar? A. int foo = (int) Math.max(bar); B. int foo = (int) Math.min(bar); C. int foo = (int) Math.abs(bar); D. int foo = (int) Math.ceil(bar); E. int foo = (int) Math.floor(bar); F. int foo = (int) Math.round(bar); 解答：DEF A B两个选项方法是用错误，都是两个参数。 abs方法是取bar的绝对值， ceil方法返回最小的（最接近负无穷大）double 值，该值大于等于参数，并等于某个整数。 floor方法返回最大的（最接近正无穷大）double 值，该值小于等于参数，并等于某个整数。 round方法 返回最接近参数的 long。 42、 Exhibit: 123456789101112131415161718192021222324252627282930311.package foo;2.import java.util.Vector;3.private class MyVector extends Vector &#123;4.int i = 1;5.public MyVector() &#123;6.i = 2;7. &#125;8.&#125;9.public class MyNewVector extends MyVector &#123;10.public MyNewVector () &#123;11. i = 4;12.&#125;13.public static void main (String args []) &#123;14.MyVector v = new MyNewVector();15. &#125;16.&#125; The file MyNewVector.java is shown in the exhibit. What is the result? A. 汇编将会成功。 B. 编译到第3行的时候失败。 C. 编译到第6行的时候失败。 D. 编译到第9行的时候失败。 E. 编译到第14行的时候失败。 解答：B 类MyVector不能是私有的 43、Given: 12345678910111213141516171819public class Test &#123;public static void main (String[]args) &#123;String foo = args[1];String bar = args[2];String baz = args[3];System.out.println(“baz = ” + baz);&#125;&#125;And the output:Baz = 2 Which command line invocation will produce the output? A. java Test 2222 B. java Test 1 2 3 4 C. java Test 4 2 4 2 D. java Test 4 3 2 1 解答：C 数组下标从0开始 44、 Given: 123451. public interface Foo&#123;2.int k = 4;3. &#125; 哪三个相当于第2行? (Choose Three) A. final int k = 4; B. Public int k = 4; C. static int k = 4; D. Private int k = 4; E. Abstract int k = 4; F. Volatile int k = 4; G. Transient int k = 4; H. protected int k = 4; 解答：BDE static：修饰的静态变量 final 修饰的是常量 abstract不能修饰变量 Volatile修饰的成员变量在每次被线程访问时，都强迫从共享内存中重读该成员变量的值。 而且，当成员变量发生变化时，强迫线程将变化值回写到共享内存。这样在任何时刻， 两个不同的线程总是看到某个成员变量的同一个值。 Transient：对不需序列化的类的域使用transient关键字,以减少序列化的数据量。 int k=4相当于public static final int k=4; 在接口中可以不写static final 45、 Given: 1234567891011public class foo &#123;static String s;public static void main (String[]args) &#123;System.out.println (“s=” + s);&#125;&#125; What is the result? A. 代码编译并打印“s =”。 B. 代码编译并打印“s = null”。 C. 该代码不会编译，因为字符串s未初始化。 D.代码不编译，因为字符串不能被引用。 E. 代码编译，但调用toString时会引发NullPointerException。 解答：B String为禁用数据类型，引用类型数据成员的默认值为null 46、哪两个创建一个数组的实例? (Choose Two) A. int[] ia = new int [15]; B. float fa = new float [20]; C. char[] ca = “Some String”; D. Object oa = new float[20]; E. Int ia [][] = (4, 5, 6) (1, 2, 3) 解答：AD 任何类的父类都是 Object，数组也有数据引用类型，Object oa = new float[20];这种写法相当于父类的用指向之类的实例。 47、Given: 12345678910111213public class ExceptionTest &#123;class TestException extends Exception &#123;&#125;public void runTest () throws TestException &#123;&#125;public void test () /* Point X*/ &#123;runTest ();&#125;&#125; 在第4行的X点上，可以添加哪些代码来编译代码？ A. throws Exception B. Catch (Exception e). C. Throws RuntimeException. D. Catch (TestException e). E. No code is necessary. 解答：A 方法上使用throws抛出异常，Exception是异常类的超类。 48、Exhibit: 123456789101112131415161718192021222324252627282930313233public class SwitchTest &#123;public static void main (String []args) &#123;System.out.println (“value =” +switchIt(4));&#125;public static int switchIt(int x) &#123;int j = 1;switch (x) &#123;case 1: j++;case 2: j++;case 3: j++;case 4: j++;case 5: j++;default:j++;&#125;return j + x;&#125;&#125; 第3行的输出是什么? A. Value =3 B. Value =4 C. Value =5 D. Value =6 E. Value =7 F. Value =8 解答：F 由于case块没有break语句，那么从case 4：向下的代码都会执行。 49、使用 throw 语句可以抛出哪四种类型的对象? (Choose Four) A. Error B. Event C. Object D. Exception E. Throwable F. RuntimeException 解答：ADEF 能够抛出的对象类型要是 Throwable 或是 Throwable 的子类 50．在下面程序的第6行补充上下列哪个方法,会导致在编译过程中发生错误? 1234567891011121314151) class Super&#123;2) public float getNum()&#123;3) return 3.0f;4) &#125;&#125;5) pubhc class Sub extends Super&#123;6)7) &#125; A. public float getNum(){retun 4.0f;} B. public void getNum(){} C. public void getNum(double d){} D. public double getNum(float d){ retun 4.0f ;} 解答：B 方法重写的问题。子类中有和父类的方法名相同，但是参数不同，不会出编译错误，认为是子类 的特有的方法，但是如果子类中方法和父类的方法名，参数，访问权限，异常都相同，只有返回值 类型不同会编译不通过。 51.下面关于import, class和package的声明顺序哪个正确？( ) A. package, import, class B. class, import, package C. import, package, class D. package, class, import 解答：A 52.下面哪个是正确的？( ) A. String temp [] = new String {“a” “b” “c”}; B. String temp [] = {“a” “b” “c”} C. String temp = {“a”, “b”, “c”} D. String temp [] = {“a”, “b”, “c”} 解答：D 53.关于java.lang.String类，以下描述正确的一项是（ ） A. String类是final类故不可以继承； B. String类是final类故可以继承； C. String类不是final类故不可以继承； D. String类不是final类故可以继承； 解答：A String类是final的，在java中final修饰类的不能被继承 54.关于实例方法和类方法，以下描述正确的是：( ) A. 实例方法只能访问实例变量 B. 类方法既可以访问类变量，也可以访问实例变量 C. 类方法只能通过类名来调用 D. 实例方法只能通过对象来调用 解答：D A 实例方法可以访问类变量 B类方法只能访问类变量 C类方法可以通过对象调用 55.接口是Java面向对象的实现机制之一，以下说法正确的是：( ) A. Java支持多重继承，一个类可以实现多个接口； B. Java只支持单重继承，一个类可以实现多个接口； C. Java只支持单重继承，一个类只可以实现一个接口； D. Java支持多重继承，但一个类只可以实现一个接口。 解答：B Java支持单重继承，一个类只能继承自另外的一个类，但是一个类可以实现多个接口。 56.下列关于 interface 的说法正确的是：( ) A. interface中可以有private方法 B. interface中可以有final方法 C. interface中可以有function实现 D. interface可以继承其他interface 解答：D A. 接口中不可以有private的方法 B．接口中不可以有final的方法 接口中的方法默认是 public abstract的 C．接口中的方法不可以有实现 57.已知A类被打包在packageA , B类被打包在packageB ，且B类被声明为public ，且有一个成员变量x被声明为, protected控制方式 。C类也位于packageA包，且继承了B类 。则以下说话正确的是（ ） A. A类的实例不能访问到B类的实例 B. A类的实例能够访问到B类一个实例的x成员 C. C类的实例可以访问到B类一个实例的x成员 D. C类的实例不能访问到B类的实例 解答：C 不同包子类的关系， 可以访问到父类B的protected成员 58.以下程序正确的输出是（ ） 123456789101112131415161718192021222324252627282930313233package test;public class FatherClass &#123;public FatherClass() &#123;System.out.println(“FatherClass Create”);&#125;&#125;package test;import test.FatherClass;public class ChildClass extends FatherClass &#123;public ChildClass() &#123;System.out.println(“ChildClass Create”);&#125;public static void main(String[] args) &#123;FatherClass fc = new FatherClass();ChildClass cc = new ChildClass();&#125;&#125; A. FatherClass Create FatherClass Create ChildClass Create B. FatherClass Create ChildClass Create FatherClass Create C. ChildClass Create ChildClass Create FatherClass Create D. ChildClass Create FatherClass Create FatherClass Create 解答：A 在子类构造方法的开始默认情况下有一句super();来调用父类的构造方法 59.给定如下代码，下面哪个可以作为该类的构造函数 ( ) 123public class Test &#123;&#125; A. public void Test() {?} B. public Test() {?} C. public static Test() {?} D. public static void Test() {?} 解答：B 构造方法：与类同名没有放回类型 60.题目: 123456789101112131. public class test (2. public static void main (String args[]) &#123;3. int i = 0xFFFFFFF1;4. int j = -i;5.6. &#125;7. ) 程序运行到第5行时,j的值为 多少?( ) A. –15 B. 0 C. 1 D. 14 E. 在第三行的错误导致编译失败 解答：D int i = 0xFFFFFFF1;相当于 int i=-15 然后对i进行取反即取绝对值再减一]]></content>
      <categories>
        <category>基础面试题</category>
      </categories>
      <tags>
        <tag>基础面试题</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java线程面试题]]></title>
    <url>%2F2018%2F06%2F03%2FJava%E7%BA%BF%E7%A8%8B%E9%9D%A2%E8%AF%95%E9%A2%98%20Top%2050%2F</url>
    <content type="text"><![CDATA[Java线程面试题前言​ 不管你是新程序员还是老手，你一定在面试中遇到过有关线程的问题。Java语言一个重要的特点就是内置了对并发的支持，让Java大受企业和程序员的欢迎。大多数待遇丰厚的Java开发职位都要求开发者精通多线程技术并且有丰富的Java程序开发、调试、优化经验，所以线程相关的问题在面试中经常会被提到。 在典型的Java面试中， 面试官会从线程的基本概念问起, 如：为什么你需要使用线程， 如何创建线程，用什么方式创建线程比较好（比如：继承thread类还是调用Runnable接口），然后逐渐问到并发问题像在Java并发编程的过程中遇到了什么挑战，Java内存模型，JDK1.5引入了哪些更高阶的并发工具，并发编程常用的设计模式，经典多线程问题如生产者消费者，哲学家就餐，读写器或者简单的有界缓冲区问题。仅仅知道线程的基本概念是远远不够的， 你必须知道如何处理死锁，竞态条件，内存冲突和线程安全等并发问题。掌握了这些技巧，你就可以轻松应对多线程和并发面试了。 多线程三特性 原子性：即一个操作或者多个操作 要么全部执行并且执行的过程不会被任何因素打断，要么就都不执行。 可见性：可见性是指当多个线程访问同一个变量时，一个线程修改了这个变量的值，其他线程能够立即看得到修改的值。 有序性：即程序执行的顺序按照代码的先后顺序执行。 多线程使用场景 多线程使用的主要目的在于：吞吐量/伸缩性： 多线程使用从场景 后台任务 定时向大量（100w以上）的用户发送邮件； 异步处理 volatile关键字的作用 volatile 用来保证该变量对所有线程的可见性，但不保证原子性。具体是在每次使用前都要先刷新，以保证别的线程中的修改已经反映到本线程工作内存中，因此可以保证执行时的一致性。 Java内存模型中，有主内存和每个线程各自的工作内存，虚拟机和硬件可能会让线程工作内存优先存储于寄存器和高速缓存中，以提高性能。 所有变量都存储在主内存中，线程工作内存中保存了此线程使用到的变量的副本。工作内存在线程之间是隔离的，对其他线程不可见。线程对变量的所有操作都必须在工作内存中进行，修改后的变量副本要写回主内存。这样就会出现同一个变量在某个瞬间，在一个线程的工作内存中的值可能与另外一个线程工作内存中的值，或者主内存中的值不一致的情况。 以下例子展现了volatile的作用： 1234567891011121314&gt;public class StoppableTask extends Thread &#123; &gt; private volatile boolean pleaseStop; &gt; &gt; public void run() &#123; &gt; while (!pleaseStop) &#123; &gt; // do some stuff... &gt; &#125; &gt; &#125; &gt; &gt; public void tellMeToStop() &#123; &gt; pleaseStop = true; &gt; &#125; &gt;&#125;&gt; 假如pleaseStop没有被声明为volatile，线程执行run的时候检查的是自己的副本，就不能及时得知其他线程已经调用tellMeToStop()修改了pleaseStop的值。 Volatile一般情况下并不能代替sychronized，因为volatile不能保证操作的原子性，即使只是i++，实际上也是由多个原子操作组成：read i; inc; write i，假如多个线程同时执行i++，依然可能由于不同线程交替执行而出现写入脏数据的情况。也就是说，如果对变量值的修改需要依赖于变量之前的值，那么volatile不能保证一致性，需要用sychronized，或者使用atomic类型(java.util.concurrent.atomic.*)；而上面的代码例子是可以使用volatile的典型场景。 说说自己是怎么使用 synchronized 关键字，在项目中用到了吗 synchronized关键字最主要的三种使用方式： 修饰实例方法，作用于当前对象实例加锁，进入同步代码前要获得当前对象实例的锁 修饰静态方法，作用于当前类对象加锁，进入同步代码前要获得当前类对象的锁 。也就是给当前类加锁，会作用于类的所有对象实例，因为静态成员不属于任何一个实例对象，是类成员（ static 表明这是该类的一个静态资源，不管new了多少个对象，只有一份，所以对该类的所有对象都加了锁）。所以如果一个线程A调用一个实例对象的非静态 synchronized 方法，而线程B需要调用这个实例对象所属类的静态 synchronized 方法，是允许的，不会发生互斥现象，因为访问静态 synchronized 方法占用的锁是当前类的锁，而访问非静态 synchronized 方法占用的锁是当前实例对象锁。 修饰代码块，指定加锁对象，对给定对象加锁，进入同步代码库前要获得给定对象的锁。 和 synchronized 方法一样，synchronized(this)代码块也是锁定当前对象的。synchronized 关键字加到 static 静态方法和 synchronized(class)代码块上都是是给 Class 类上锁。这里再提一下：synchronized关键字加到非 static 静态方法上是给对象实例上锁。另外需要注意的是：尽量不要使用 synchronized(String a) 因为JVM中，字符串常量池具有缓冲功能！ sleep()方法和wait()方法简单对比 Wait通常被用于线程间交互/通信,wait()方法被调用后，线程不会自动苏醒，需要别的线程调用同一个对象上的notify()或者notifyAll()方法 两者最主要的区别在于：sleep方法没有释放锁，而wait方法释放了锁 。 两者都可以暂停线程的执行。 Wait通常被用于线程间交互/通信，sleep通常被用于暂停执行。 wait()方法被调用后，线程不会自动苏醒，需要别的线程调用同一个对象上的notify()或者notifyAll()方法。sleep()方法执行完成后，线程会自动苏醒。 线程有哪些基本状态？这些状态是如何定义的? 新建(new)：新创建了一个线程对象。 可运行(runnable)：线程对象创建后，其他线程(比如main线程）调用了该对象的start()方法。该状态的线程位于可运行线程池中，等待被线程调度选中，获 取cpu的使用权。 运行(running)：可运行状态(runnable)的线程获得了cpu时间片（timeslice），执行程序代码。 阻塞(block)：阻塞状态是指线程因为某种原因放弃了cpu使用权，也即让出了cpu timeslice，暂时停止运行。直到线程进入可运行(runnable)状态，才有 机会再次获得cpu timeslice转到运行(running)状态。阻塞的情况分三种： (一). 等待阻塞：运行(running)的线程执行o.wait()方法，JVM会把该线程放 入等待队列(waiting queue)中。 (二). 同步阻塞：运行(running)的线程在获取对象的同步锁时，若该同步 锁 被别的线程占用，则JVM会把该线程放入锁池(lock pool)中。 (三). 其他阻塞: 运行(running)的线程执行Thread.sleep(long ms)或t.join()方法，或者发出了I/O请求时，JVM会把该线程置为阻塞状态。当sleep()状态超时join()等待线程终止或者超时、或者I/O处理完毕时，线程重新转入可运行(runnable)状态。 死亡(dead)：线程run()、main()方法执行结束，或者因异常退出了run()方法，则该线程结束生命周期。死亡的线程不可再次复生。 备注： 可以用早起坐地铁来比喻这个过程（下面参考自牛客网某位同学的回答）： 还没起床：sleeping 起床收拾好了，随时可以坐地铁出发：Runnable 等地铁来：Waiting 地铁来了，但要排队上地铁：I/O阻塞 上了地铁，发现暂时没座位：synchronized阻塞 地铁上找到座位：Running 到达目的地：Dead 现在有T1、T2、T3三个线程，你怎样保证T2在T1执行完后执行，T3在T2执行完后执行 这个线程问题通常会在第一轮或电话面试阶段被问到，目的是检测你对”join”方法是否熟悉。这个多线程问题比较简单，可以用join方法实现。 在Java中Lock接口比synchronized块的优势是什么？你需要实现一个高效的缓存，它允许多个用户读，但只允许一个用户写，以此来保持它的完整性，你会怎样去实现它？ lock接口在多线程和并发编程中最大的优势是它们为读和写分别提供了锁，它能满足你写像ConcurrentHashMap这样的高性能数据结构和有条件的阻塞。Java线程面试的问题越来越会根据面试者的回答来提问。我强烈建议在你去参加多线程的面试之前认真读一下Locks，因为当前其大量用于构建电子交易终统的客户端缓存和交易连接空间。 在java中wait和sleep方法的不同 通常会在电话面试中经常被问到的Java线程面试问题。最大的不同是在等待时wait会释放锁，而sleep一直持有锁。Wait通常被用于线程间交互，sleep通常被用于暂停执行。 用Java实现阻塞队列 这是一个相对艰难的多线程面试问题，它能达到很多的目的。第一，它可以检测侯选者是否能实际的用Java线程写程序；第二，可以检测侯选者对并发场景的理解，并且你可以根据这个问很多问题。如果他用wait()和notify()方法来实现阻塞队列，你可以要求他用最新的Java 5中的并发类来再写一次。 用Java写代码来解决生产者——消费者问题 与上面的问题很类似，但这个问题更经典，有些时候面试都会问下面的问题。在Java中怎么解决生产者——消费者问题，当然有很多解决方法，我已经分享了一种用阻塞队列实现的方法。有些时候他们甚至会问怎么实现哲学家进餐问题。 用Java编程一个会导致死锁的程序，你将怎么解决 这是我最喜欢的Java线程面试问题，因为即使死锁问题在写多线程并发程序时非常普遍，但是很多侯选者并不能写deadlock free code（无死锁代码？），他们很挣扎。只要告诉他们，你有N个资源和N个线程，并且你需要所有的资源来完成一个操作。为了简单这里的n可以替换为2，越大的数据会使问题看起来更复杂。通过避免Java中的死锁来得到关于死锁的更多信息。 什么是原子操作，Java中的原子操作是什么 非常简单的java线程面试问题，接下来的问题是你需要同步一个原子操作。 Volatile的关键作用？怎样使用它？与synchronized方法有什么不同 自从Java 5和Java内存模型改变以后，基于volatile关键字的线程问题越来越流行。应该准备好回答关于volatile变量怎样在并发环境中确保可见性、顺序性和一致性。 什么是竞争条件？你怎样发现和解决竞争 这是一道出现在多线程面试的高级阶段的问题。大多数的面试官会问最近你遇到的竞争条件，以及你是怎么解决的。有些时间他们会写简单的代码，然后让你检测出代码的竞争条件。可以参考我之前发布的关于Java竞争条件的文章。在我看来这是最好的java线程面试问题之一，它可以确切的检测候选者解决竞争条件的经验，or writing code which is free of data race or any other race condition。关于这方面最好的书是《Concurrency practices in Java》。 你将如何使用thread dump？你将如何分析Thread dump 在UNIX中你可以使用kill -3，然后thread dump将会打印日志，在windows中你可以使用”CTRL+Break”。非常简单和专业的线程面试问题，但是如果他问你怎样分析它，就会很棘手。 为什么我们调用start()方法时会执行run()方法，为什么我们不能直接调用run()方法 这是另一个非常经典的java多线程面试问题。这也是我刚开始写线程程序时候的困惑。现在这个问题通常在电话面试或者是在初中级Java面试的第一轮被问到。这个问题的回答应该是这样的，当你调用start()方法时你将创建新的线程，并且执行在run()方法里的代码。但是如果你直接调用run()方法，它不会创建新的线程也不会执行调用线程的代码。阅读我之前写的《start与run方法的区别》这篇文章来获得更多信息。 怎样唤醒一个阻塞的线程 这是个关于线程和阻塞的棘手的问题，它有很多解决方法。如果线程遇到了IO阻塞，我并且不认为有一种方法可以中止线程。如果线程因为调用wait()、sleep()、或者join()方法而导致的阻塞，你可以中断线程，并且通过抛出InterruptedException来唤醒它。我之前写的《How to deal with blocking methods in java》有很多关于处理线程阻塞的信息。 CycliBarriar和CountdownLatch有什么区别 这个线程问题主要用来检测你是否熟悉JDK5中的并发包。这两个的区别是CyclicBarrier可以重复使用已经通过的障碍，而CountdownLatch不能重复使用。 什么是不可变对象，它对写并发应用有什么帮助 另一个多线程经典面试问题，并不直接跟线程有关，但间接帮助很多。这个java面试问题可以变的非常棘手，如果他要求你写一个不可变对象，或者问你为什么String是不可变的。 在多线程环境中遇到的共同的问题是什么？你是怎么解决它的 多线程和并发程序中常遇到的有Memory-interface、竞争条件、死锁、活锁和饥饿。问题是没有止境的，如果你弄错了，将很难发现和调试。这是大多数基于面试的，而不是基于实际应用的Java线程问题。 什么是线程？ 线程是操作系统能够进行运算调度的最小单位，它被包含在进程之中，是进程中的实际运作单位。程序员可以通过它进行多处理器编程，你可以使用多线程对运算密集型任务提速。比如，如果一个线程完成一个任务要100毫秒，那么用十个线程完成改任务只需10毫秒。Java在语言层面对多线程提供了卓越的支持，它也是一个很好的卖点。 线程和进程有什么区别？ 线程是进程的子集，一个进程可以有很多线程，每条线程并行执行不同的任务。不同的进程使用不同的内存空间，而所有的线程共享一片相同的内存空间。别把它和栈内存搞混，每个线程都拥有单独的栈内存用来存储本地数据。更多详细信息请点击这里。 如何在Java中实现线程？ 在语言层面有两种方式。java.lang.Thread 类的实例就是一个线程但是它需要调用java.lang.Runnable接口来执行，由于线程类本身就是调用的Runnable接口所以你可以继承java.lang.Thread 类或者直接调用Runnable接口来重写run()方法实现线程。更多详细信息请点击这里. 用Runnable还是Thread？ 这个问题是上题的后续，大家都知道我们可以通过继承Thread类或者调用Runnable接口来实现线程，问题是，那个方法更好呢？什么情况下使用它？这个问题很容易回答，如果你知道Java不支持类的多重继承，但允许你调用多个接口。所以如果你要继承其他类，当然是调用Runnable接口好了。更多详细信息请点击这里。 Thread 类中的start() 和 run() 方法有什么区别？ 这个问题经常被问到，但还是能从此区分出面试者对Java线程模型的理解程度。start()方法被用来启动新创建的线程，而且start()内部调用了run()方法，这和直接调用run()方法的效果不一样。当你调用run()方法的时候，只会是在原来的线程中调用，没有新的线程启动，start()方法才会启动新线程。更多讨论请点击这里 Java中Runnable和Callable有什么不同？ Runnable和Callable都代表那些要在不同的线程中执行的任务。Runnable从JDK1.0开始就有了，Callable是在JDK1.5增加的。它们的主要区别是Callable的 call() 方法可以返回值和抛出异常，而Runnable的run()方法没有这些功能。Callable可以返回装载有计算结果的Future对象。我的博客有更详细的说明。 Java中CyclicBarrier 和 CountDownLatch有什么不同？ CyclicBarrier 和 CountDownLatch 都可以用来让一组线程等待其它线程。与 CyclicBarrier 不同的是，CountdownLatch 不能重新使用。点此查看更多信息和示例代码。 Java内存模型是什么？ Java内存模型规定和指引Java程序在不同的内存架构、CPU和操作系统间有确定性地行为。它在多线程的情况下尤其重要。Java内存模型对一个线程所做的变动能被其它线程可见提供了保证，它们之间是先行发生关系。这个关系定义了一些规则让程序员在并发编程时思路更清晰。比如，先行发生关系确保了： 线程内的代码能够按先后顺序执行，这被称为程序次序规则。对于同一个锁，一个解锁操作一定要发生在时间上后发生的另一个锁定操作之前，也叫做管程锁定规则。前一个对volatile的写操作在后一个volatile的读操作之前，也叫volatile变量规则。一个线程内的任何操作必需在这个线程的start()调用之后，也叫作线程启动规则。一个线程的所有操作都会在线程终止之前，线程终止规则。一个对象的终结操作必需在这个对象构造完成之后，也叫对象终结规则。可传递性 我强烈建议大家阅读《Java并发编程实践》第十六章来加深对Java内存模型的理解。 Java中的volatile 变量是什么？ volatile是一个特殊的修饰符，只有成员变量才能使用它。在Java并发程序缺少同步类的情况下，多线程对成员变量的操作对其它线程是透明的。volatile变量可以保证下一个读取操作会在前一个写操作之后发生，就是上一题的volatile变量规则。点击这里查看更多volatile的相关内容。 什么是线程安全？Vector是一个线程安全类吗？ （详见这里) 如果你的代码所在的进程中有多个线程在同时运行，而这些线程可能会同时运行这段代码。如果每次运行结果和单线程运行的结果是一样的，而且其他的变量的值也和预期的是一样的，就是线程安全的。一个线程安全的计数器类的同一个实例对象在被多个线程使用的情况下也不会出现计算失误。很显然你可以将集合类分成两组，线程安全和非线程安全的。Vector 是用同步方法来实现线程安全的, 而和它相似的ArrayList不是线程安全的。 Java中什么是竞态条件？ 举个例子说明。 竞态条件会导致程序在并发情况下出现一些bugs。多线程对一些资源的竞争的时候就会产生竞态条件，如果首先要执行的程序竞争失败排到后面执行了，那么整个程序就会出现一些不确定的bugs。这种bugs很难发现而且会重复出现，因为线程间的随机竞争。一个例子就是无序处理，详见答案。 Java中如何停止一个线程？ Java提供了很丰富的API但没有为停止线程提供API。JDK 1.0本来有一些像stop(), suspend() 和 resume()的控制方法但是由于潜在的死锁威胁因此在后续的JDK版本中他们被弃用了，之后Java API的设计者就没有提供一个兼容且线程安全的方法来停止一个线程。当run() 或者 call() 方法执行完的时候线程会自动结束,如果要手动结束一个线程，你可以用volatile 布尔变量来退出run()方法的循环或者是取消任务来中断线程。点击这里查看示例代码。 一个线程运行时发生异常会怎样？ 这是我在一次面试中遇到的一个很刁钻的Java面试题, 简单的说，如果异常没有被捕获该线程将会停止执行。Thread.UncaughtExceptionHandler是用于处理未捕获异常造成线程突然中断情况的一个内嵌接口。当一个未捕获异常将造成线程中断的时候JVM会使用Thread.getUncaughtExceptionHandler()来查询线程的UncaughtExceptionHandler并将线程和异常作为参数传递给handler的uncaughtException()方法进行处理。 如何在两个线程间共享数据？ 你可以通过共享对象来实现这个目的，或者是使用像阻塞队列这样并发的数据结构。这篇教程《Java线程间通信》(涉及到在两个线程间共享对象)用wait和notify方法实现了生产者消费者模型。 Java中notify 和 notifyAll有什么区别？ 这又是一个刁钻的问题，因为多线程可以等待单监控锁，Java API 的设计人员提供了一些方法当等待条件改变的时候通知它们，但是这些方法没有完全实现。notify()方法不能唤醒某个具体的线程，所以只有一个线程在等待的时候它才有用武之地。而notifyAll()唤醒所有线程并允许他们争夺锁确保了至少有一个线程能继续运行。我的博客有更详细的资料和示例代码。 为什么wait, notify 和 notifyAll这些方法不在thread类里面？ 这是个设计相关的问题，它考察的是面试者对现有系统和一些普遍存在但看起来不合理的事物的看法。回答这些问题的时候，你要说明为什么把这些方法放在Object类里是有意义的，还有不把它放在Thread类里的原因。一个很明显的原因是JAVA提供的锁是对象级的而不是线程级的，每个对象都有锁，通过线程获得。如果线程需要等待某些锁那么调用对象中的wait()方法就有意义了。如果wait()方法定义在Thread类中，线程正在等待的是哪个锁就不明显了。简单的说，由于wait，notify和notifyAll都是锁级别的操作，所以把他们定义在Object类中因为锁属于对象。你也可以查看这篇文章了解更多。 什么是ThreadLocal变量？ ThreadLocal是Java里一种特殊的变量。每个线程都有一个ThreadLocal就是每个线程都拥有了自己独立的一个变量，竞争条件被彻底消除了。它是为创建代价高昂的对象获取线程安全的好方法，比如你可以用ThreadLocal让SimpleDateFormat变成线程安全的，因为那个类创建代价高昂且每次调用都需要创建不同的实例所以不值得在局部范围使用它，如果为每个线程提供一个自己独有的变量拷贝，将大大提高效率。首先，通过复用减少了代价高昂的对象的创建个数。其次，你在没有使用高代价的同步或者不变性的情况下获得了线程安全。线程局部变量的另一个不错的例子是ThreadLocalRandom类，它在多线程环境中减少了创建代价高昂的Random对象的个数。查看答案了解更多。 什么是FutureTask？ 在Java并发程序中FutureTask表示一个可以取消的异步运算。它有启动和取消运算、查询运算是否完成和取回运算结果等方法。只有当运算完成的时候结果才能取回，如果运算尚未完成get方法将会阻塞。一个FutureTask对象可以对调用了Callable和Runnable的对象进行包装，由于FutureTask也是调用了Runnable接口所以它可以提交给Executor来执行。 Java中interrupted 和 isInterruptedd方法的区别？ interrupted() 和 isInterrupted()的主要区别是前者会将中断状态清除而后者不会。Java多线程的中断机制是用内部标识来实现的，调用Thread.interrupt()来中断一个线程就会设置中断标识为true。当中断线程调用静态方法Thread.interrupted()来检查中断状态时，中断状态会被清零。而非静态方法isInterrupted()用来查询其它线程的中断状态且不会改变中断状态标识。简单的说就是任何抛出InterruptedException异常的方法都会将中断状态清零。无论如何，一个线程的中断状态有有可能被其它线程调用中断来改变。 为什么wait和notify方法要在同步块中调用？ 主要是因为Java API强制要求这样做，如果你不这么做，你的代码会抛出IllegalMonitorStateException异常。还有一个原因是为了避免wait和notify之间产生竞态条件。 为什么你应该在循环中检查等待条件? 处于等待状态的线程可能会收到错误警报和伪唤醒，如果不在循环中检查等待条件，程序就会在没有满足结束条件的情况下退出。因此，当一个等待线程醒来时，不能认为它原来的等待状态仍然是有效的，在notify()方法调用之后和等待线程醒来之前这段时间它可能会改变。这就是在循环中使用wait()方法效果更好的原因，你可以在Eclipse中创建模板调用wait和notify试一试。如果你想了解更多关于这个问题的内容，我推荐你阅读《Effective Java》这本书中的线程和同步章节。 Java中的同步集合与并发集合有什么区别？ 同步集合与并发集合都为多线程和并发提供了合适的线程安全的集合，不过并发集合的可扩展性更高。在Java1.5之前程序员们只有同步集合来用且在多线程并发的时候会导致争用，阻碍了系统的扩展性。Java5介绍了并发集合像ConcurrentHashMap，不仅提供线程安全还用锁分离和内部分区等现代技术提高了可扩展性。更多内容详见答案。 Java中堆和栈有什么不同？ 为什么把这个问题归类在多线程和并发面试题里？因为栈是一块和线程紧密相关的内存区域。每个线程都有自己的栈内存，用于存储本地变量，方法参数和栈调用，一个线程中存储的变量对其它线程是不可见的。而堆是所有线程共享的一片公用内存区域。对象都在堆里创建，为了提升效率线程会从堆中弄一个缓存到自己的栈，如果多个线程使用该变量就可能引发问题，这时volatile 变量就可以发挥作用了，它要求线程从主存中读取变量的值。 更多内容详见答案。 什么是线程池？ 为什么要使用它？ 创建线程要花费昂贵的资源和时间，如果任务来了才创建线程那么响应时间会变长，而且一个进程能创建的线程数有限。为了避免这些问题，在程序启动的时候就创建若干线程来响应处理，它们被称为线程池，里面的线程叫工作线程。从JDK1.5开始，Java API提供了Executor框架让你可以创建不同的线程池。比如单线程池，每次处理一个任务；数目固定的线程池或者是缓存线程池（一个适合很多生存期短的任务的程序的可扩展线程池）。更多内容详见这篇文章。 如何写代码来解决生产者消费者问题？ 在现实中你解决的许多线程问题都属于生产者消费者模型，就是一个线程生产任务供其它线程进行消费，你必须知道怎么进行线程间通信来解决这个问题。比较低级的办法是用wait和notify来解决这个问题，比较赞的办法是用Semaphore 或者 BlockingQueue来实现生产者消费者模型，这篇教程有实现它。 如何避免死锁？http://www.cnblogs.com&amp;iframeId=iframe_0.21145907562000632&quot; frameborder=”0” scrolling=”no” height=”20”&gt; Java多线程中的死锁 死锁是指两个或两个以上的进程在执行过程中，因争夺资源而造成的一种互相等待的现象，若无外力作用，它们都将无法推进下去。这是一个严重的问题，因为死锁会让你的程序挂起无法完成任务，死锁的发生必须满足以下四个条件： 互斥条件：一个资源每次只能被一个进程使用。请求与保持条件：一个进程因请求资源而阻塞时，对已获得的资源保持不放。不剥夺条件：进程已获得的资源，在末使用完之前，不能强行剥夺。循环等待条件：若干进程之间形成一种头尾相接的循环等待资源关系。避免死锁最简单的方法就是阻止循环等待条件，将系统中所有的资源设置标志位、排序，规定所有的进程申请资源必须以一定的顺序（升序或降序）做操作来避免死锁。这篇教程有代码示例和避免死锁的讨论细节。 Java中活锁和死锁有什么区别？ 这是上题的扩展，活锁和死锁类似，不同之处在于处于活锁的线程或进程的状态是不断改变的，活锁可以认为是一种特殊的饥饿。一个现实的活锁例子是两个人在狭小的走廊碰到，两个人都试着避让对方好让彼此通过，但是因为避让的方向都一样导致最后谁都不能通过走廊。简单的说就是，活锁和死锁的主要区别是前者进程的状态可以改变但是却不能继续执行。 怎么检测一个线程是否拥有锁？ 我一直不知道我们竟然可以检测一个线程是否拥有锁，直到我参加了一次电话面试。在java.lang.Thread中有一个方法叫holdsLock()，它返回true如果当且仅当当前线程拥有某个具体对象的锁。你可以查看这篇文章了解更多。 你如何在Java中获取线程堆栈？ 对于不同的操作系统，有多种方法来获得Java进程的线程堆栈。当你获取线程堆栈时，JVM会把所有线程的状态存到日志文件或者输出到控制台。在Windows你可以使用Ctrl + Break组合键来获取线程堆栈，Linux下用kill -3命令。你也可以用jstack这个工具来获取，它对线程id进行操作，你可以用jps这个工具找到id。 JVM中哪个参数是用来控制线程的栈堆栈小的 这个问题很简单， -Xss参数用来控制线程的堆栈大小。你可以查看JVM配置列表来了解这个参数的更多信息。 Java中synchronized 和 ReentrantLock 有什么不同？ Java在过去很长一段时间只能通过synchronized关键字来实现互斥，它有一些缺点。比如你不能扩展锁之外的方法或者块边界，尝试获取锁时不能中途取消等。Java 5 通过Lock接口提供了更复杂的控制来解决这些问题。 ReentrantLock 类实现了 Lock，它拥有与 synchronized 相同的并发性和内存语义且它还具有可扩展性。你可以查看这篇文章了解更多 有三个线程T1，T2，T3，怎么确保它们按顺序执行？ 在多线程中有多种方法让线程按特定顺序执行，你可以用线程类的join()方法在一个线程中启动另一个线程，另外一个线程完成该线程继续执行。为了确保三个线程的顺序你应该先启动最后一个(T3调用T2，T2调用T1)，这样T1就会先完成而T3最后完成。你可以查看这篇文章了解更多。 Thread类中的yield方法有什么作用？ Yield方法可以暂停当前正在执行的线程对象，让其它有相同优先级的线程执行。它是一个静态方法而且只保证当前线程放弃CPU占用而不能保证使其它线程一定能占用CPU，执行yield()的线程有可能在进入到暂停状态后马上又被执行。点击这里查看更多yield方法的相关内容。 Java中ConcurrentHashMap的并发度是什么？ ConcurrentHashMap把实际map划分成若干部分来实现它的可扩展性和线程安全。这种划分是使用并发度获得的，它是ConcurrentHashMap类构造函数的一个可选参数，默认值为16，这样在多线程情况下就能避免争用。欲了解更多并发度和内部大小调整请阅读我的文章How ConcurrentHashMap works in Java。 Java中Semaphore是什么？ Java中的Semaphore是一种新的同步类，它是一个计数信号。从概念上讲，从概念上讲，信号量维护了一个许可集合。如有必要，在许可可用前会阻塞每一个 acquire()，然后再获取该许可。每个 release()添加一个许可，从而可能释放一个正在阻塞的获取者。但是，不使用实际的许可对象，Semaphore只对可用许可的号码进行计数，并采取相应的行动。信号量常常用于多线程的代码中，比如数据库连接池。更多详细信息请点击这里。 如果你提交任务时，线程池队列已满。会时发会生什么？ 这个问题问得很狡猾，许多程序员会认为该任务会阻塞直到线程池队列有空位。事实上如果一个任务不能被调度执行那么ThreadPoolExecutor’s submit()方法将会抛出一个RejectedExecutionException异常。 Java线程池中submit() 和 execute()方法有什么区别？ 两个方法都可以向线程池提交任务，execute()方法的返回类型是void，它定义在Executor接口中, 而submit()方法可以返回持有计算结果的Future对象，它定义在ExecutorService接口中，它扩展了Executor接口，其它线程池类像ThreadPoolExecutor和ScheduledThreadPoolExecutor都有这些方法。更多详细信息请点击这里。 什么是阻塞式方法？ 阻塞式方法是指程序会一直等待该方法完成期间不做其他事情，ServerSocket的accept()方法就是一直等待客户端连接。这里的阻塞是指调用结果返回之前，当前线程会被挂起，直到得到结果之后才会返回。此外，还有异步和非阻塞式方法在任务完成前就返回。更多详细信息请点击这里。 Swing是线程安全的吗？ 为什么？ 你可以很肯定的给出回答，Swing不是线程安全的，但是你应该解释这么回答的原因即便面试官没有问你为什么。当我们说swing不是线程安全的常常提到它的组件，这些组件不能在多线程中进行修改，所有对GUI组件的更新都要在AWT线程中完成，而Swing提供了同步和异步两种回调方法来进行更新。点击这里查看更多swing和线程安全的相关内容。 Java中invokeAndWait 和 invokeLater有什么区别？ 这两个方法是Swing API 提供给Java开发者用来从当前线程而不是事件派发线程更新GUI组件用的。InvokeAndWait()同步更新GUI组件，比如一个进度条，一旦进度更新了，进度条也要做出相应改变。如果进度被多个线程跟踪，那么就调用invokeAndWait()方法请求事件派发线程对组件进行相应更新。而invokeLater()方法是异步调用更新组件的。更多详细信息请点击这里。 Swing API中那些方法是线程安全的？ 这个问题又提到了swing和线程安全，虽然组件不是线程安全的但是有一些方法是可以被多线程安全调用的，比如repaint(), revalidate()。 JTextComponent的setText()方法和JTextArea的insert() 和 append() 方法也是线程安全的。 如何在Java中创建Immutable对象？ 这个问题看起来和多线程没什么关系， 但不变性有助于简化已经很复杂的并发程序。Immutable对象可以在没有同步的情况下共享，降低了对该对象进行并发访问时的同步化开销。可是Java没有@Immutable这个注解符，要创建不可变类，要实现下面几个步骤：通过构造方法初始化所有成员、对变量不要提供setter方法、将所有的成员声明为私有的，这样就不允许直接访问这些成员、在getter方法中，不要直接返回对象本身，而是克隆对象，并返回对象的拷贝。我的文章how to make an object Immutable in Java有详细的教程，看完你可以充满自信。 Java中的ReadWriteLock是什么？ 一般而言，读写锁是用来提升并发程序性能的锁分离技术的成果。Java中的ReadWriteLock是Java 5 中新增的一个接口，一个ReadWriteLock维护一对关联的锁，一个用于只读操作一个用于写。在没有写线程的情况下一个读锁可能会同时被多个读线程持有。写锁是独占的，你可以使用JDK中的ReentrantReadWriteLock来实现这个规则，它最多支持65535个写锁和65535个读锁。 多线程中的忙循环是什么? 忙循环就是程序员用循环让一个线程等待，不像传统方法wait(), sleep() 或 yield() 它们都放弃了CPU控制，而忙循环不会放弃CPU，它就是在运行一个空循环。这么做的目的是为了保留CPU缓存，在多核系统中，一个等待线程醒来的时候可能会在另一个内核运行，这样会重建缓存。为了避免重建缓存和减少等待重建的时间就可以使用它了。你可以查看这篇文章获得更多信息。 volatile 变量和 atomic 变量有什么不同？ 这是个有趣的问题。首先，volatile 变量和 atomic 变量看起来很像，但功能却不一样。Volatile变量可以确保先行关系，即写操作会发生在后续的读操作之前, 但它并不能保证原子性。例如用volatile修饰count变量那么 count++ 操作就不是原子性的。而AtomicInteger类提供的atomic方法可以让这种操作具有原子性如getAndIncrement()方法会原子性的进行增量操作把当前值加一，其它数据类型和引用变量也可以进行相似操作。 如果同步块内的线程抛出异常会发生什么？ 这个问题坑了很多Java程序员，若你能想到锁是否释放这条线索来回答还有点希望答对。无论你的同步块是正常还是异常退出的，里面的线程都会释放锁，所以对比锁接口我更喜欢同步块，因为它不用我花费精力去释放锁，该功能可以在finally block里释放锁实现。 单例模式的双检锁是什么？ 这个问题在Java面试中经常被问到，但是面试官对回答此问题的满意度仅为50%。一半的人写不出双检锁还有一半的人说不出它的隐患和Java1.5是如何对它修正的。它其实是一个用来创建线程安全的单例的老方法，当单例实例第一次被创建时它试图用单个锁进行性能优化，但是由于太过于复杂在JDK1.4中它是失败的，我个人也不喜欢它。无论如何，即便你也不喜欢它但是还是要了解一下，因为它经常被问到。你可以查看how double checked locking on Singleton works这篇文章获得更多信息。 如何在Java中创建线程安全的Singleton？ 这是上面那个问题的后续，如果你不喜欢双检锁而面试官问了创建Singleton类的替代方法，你可以利用JVM的类加载和静态变量初始化特征来创建Singleton实例，或者是利用枚举类型来创建Singleton，我很喜欢用这种方法。你可以查看这篇文章获得更多信息。 写出3条你遵循的多线程最佳实践 这种问题我最喜欢了，我相信你在写并发代码来提升性能的时候也会遵循某些最佳实践。以下三条最佳实践我觉得大多数Java程序员都应该遵循： 给你的线程起个有意义的名字。 这样可以方便找bug或追踪。OrderProcessor, QuoteProcessor or TradeProcessor 这种名字比 Thread-1. Thread-2 and Thread-3 好多了，给线程起一个和它要完成的任务相关的名字，所有的主要框架甚至JDK都遵循这个最佳实践。避免锁定和缩小同步的范围 锁花费的代价高昂且上下文切换更耗费时间空间，试试最低限度的使用同步和锁，缩小临界区。因此相对于同步方法我更喜欢同步块，它给我拥有对锁的绝对控制权。多用同步类少用wait 和 notify 首先，CountDownLatch, Semaphore, CyclicBarrier 和 Exchanger 这些同步类简化了编码操作，而用wait和notify很难实现对复杂控制流的控制。其次，这些类是由最好的企业编写和维护在后续的JDK中它们还会不断优化和完善，使用这些更高等级的同步工具你的程序可以不费吹灰之力获得优化。多用并发集合少用同步集合 这是另外一个容易遵循且受益巨大的最佳实践，并发集合比同步集合的可扩展性更好，所以在并发编程时使用并发集合效果更好。如果下一次你需要用到map，你应该首先想到用ConcurrentHashMap。我的文章Java并发集合有更详细的说明。 如何强制启动一个线程？ 这个问题就像是如何强制进行Java垃圾回收，目前还没有觉得方法，虽然你可以使用System.gc()来进行垃圾回收，但是不保证能成功。在Java里面没有办法强制启动一个线程，它是被线程调度器控制着且Java没有公布相关的API。 Java中的fork join框架是什么？ fork join框架是JDK7中出现的一款高效的工具，Java开发人员可以通过它充分利用现代服务器上的多处理器。它是专门为了那些可以递归划分成许多子模块设计的，目的是将所有可用的处理能力用来提升程序的性能。fork join框架一个巨大的优势是它使用了工作窃取算法，可以完成更多任务的工作线程可以从其它线程中窃取任务来执行。你可以查看这篇文章获得更多信息。 多线程中调用wait() 和 sleep()方法有什么不同 sleep是Thread类的方法,wait是Object类中定义的方法; Thread.sleep和Object.wait都会暂停当前的线程，对于CPU资源来说，不管是哪种方式暂停的线程，都表示它暂时不再需要CPU的执行时间。OS会将执行时间分配给其它线程。区别是，调用wait后，需要别的线程执行notify/notifyAll才能够重新获得CPU执行时间; Thread.sleep不会导致锁行为的改变，如果当前线程是拥有锁的，那么Thread.sleep不会让线程释放锁; 线程间的状态转换 线程间的状态转换： 新建(new)：新创建了一个线程对象。 可运行(runnable)：线程对象创建后，其他线程(比如main线程）调用了该对象的start()方法。该状态的线程位于可运行线程池中，等待被线程调度选中，获取cpu 的使用权 。 运行(running)：可运行状态(runnable)的线程获得了cpu 时间片（timeslice） ，执行程序代码。 阻塞(block)：阻塞状态是指线程因为某种原因放弃了cpu 使用权，也即让出了cpu timeslice，暂时停止运行。直到线程进入可运行(runnable)状态，才有机会再次获得cpu timeslice 转到运行(running)状态。阻塞的情况分三种： (一). 等待阻塞：运行(running)的线程执行o.wait()方法，JVM会把该线程放入等待队列(waitting queue)中。 (二). 同步阻塞：运行(running)的线程在获取对象的同步锁时，若该同步锁被别的线程占用，则JVM会把该线程放入锁池(lock pool)中。 (三). 其他阻塞：运行(running)的线程执行Thread.sleep(long ms)或t.join()方法，或者发出了I/O请求时，JVM会把该线程置为阻塞状态。当sleep()状态超时、join()等待线程终止或者超时、或者I/O处理完毕时，线程重新转入可运行(runnable)状态。 死亡(dead)：线程run()、main() 方法执行结束，或者因异常退出了run()方法，则该线程结束生命周期。死亡的线程不可再次复生。]]></content>
      <categories>
        <category>基础面试题</category>
      </categories>
      <tags>
        <tag>基础面试题</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java 红黑树源码解读]]></title>
    <url>%2F2018%2F06%2F03%2FJava%20%E7%BA%A2%E9%BB%91%E6%A0%91%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB%2F</url>
    <content type="text"><![CDATA[Java 红黑树源码解读目录红黑树的介绍红黑树(Red-Black Tree，简称R-B Tree)，它一种特殊的二叉查找树。红黑树是特殊的二叉查找树，意味着它满足二叉查找树的特征: 任意一个节点所包含的键值，大于等于左孩子的键值，小于等于右孩子的键值。除了具备该特性之外，红黑树还包括许多额外的信息。 红黑树的每个节点上都有存储位表示节点的颜色，颜色是红(Red)或黑(Black)。红黑树的特性:【1】 每个节点或者是黑色，或者是红色。【2】 根节点是黑色。【3】 每个叶子节点是黑色。 [注意：这里叶子节点，是指为空的叶子节点！]【4】 如果一个节点是红色的，则它的子节点必须是黑色的。【5】 从一个节点到该节点的子孙节点的所有路径上包含相同数目的黑节点。 关于它的特性，需要注意的是：第一，特性(3)中的叶子节点，是只为空(NIL或null)的节点。第二，特性(5)，确保没有一条路径会比其他路径长出俩倍。因而，红黑树是相对是接近平衡的二叉树。 红黑树示意图如下： 红黑树的原理在研究红黑树原理，我们可以通过以下的实例进行 debug 调试，看它的整个执行过。接着再阅读以下内容，会比较容易理解红黑的原理，才能进一步深入研究清楚。 实例地址：https://github.com/KnIfER/RBTree-java 红黑树的实现(代码说明)红黑树的基本操作是添加、删除和旋转。在对红黑树进行添加或删除后，会用到旋转方法。为什么呢？道理很简单，添加或删除红黑树中的节点之后，红黑树就发生了变化，可能不满足红黑树的5条性质，也就不再是一颗红黑树了，而是一颗普通的树。而通过旋转，可以使这颗树重新成为红黑树。简单点说，旋转的目的是让树保持红黑树的特性。旋转包括两种：左旋 和 右旋。下面分别对红黑树的基本操作进行介绍。 1. 基本定义1234567891011121314151617181920212223242526public class RBTree&lt;T extends Comparable&lt;T&gt;&gt; &#123; private RBTNode&lt;T&gt; mRoot; // 根结点 private static final boolean RED = false; private static final boolean BLACK = true; public class RBTNode&lt;T extends Comparable&lt;T&gt;&gt; &#123; boolean color; // 颜色 T key; // 关键字(键值) RBTNode&lt;T&gt; left; // 左孩子 RBTNode&lt;T&gt; right; // 右孩子 RBTNode&lt;T&gt; parent; // 父结点 public RBTNode(T key, boolean color, RBTNode&lt;T&gt; parent, RBTNode&lt;T&gt; left, RBTNode&lt;T&gt; right) &#123; this.key = key; this.color = color; this.parent = parent; this.left = left; this.right = right; &#125; &#125; ...&#125; RBTree是红黑树对应的类，RBTNode是红黑树的节点类。在RBTree中包含了根节点mRoot和红黑树的相关API。注意：在实现红黑树API的过程中，我重载了许多函数。重载的原因，一是因为有的API是内部接口，有的是外部接口；二是为了让结构更加清晰。 2. 左旋 对x进行左旋，意味着”将x变成一个左节点”。 左旋的实现代码(Java语言) 1234567891011121314151617181920212223242526272829303132333435363738394041/* * 对红黑树的节点(x)进行左旋转 * * 左旋示意图(对节点x进行左旋)： * px px * / / * x y * / \ --(左旋)-. / \ # * lx y x ry * / \ / \ * ly ry lx ly * * */private void leftRotate(RBTNode&lt;T&gt; x) &#123; // 设置x的右孩子为y RBTNode&lt;T&gt; y = x.right; // 将 “y的左孩子” 设为 “x的右孩子”； // 如果y的左孩子非空，将 “x” 设为 “y的左孩子的父亲” x.right = y.left; if (y.left != null) y.left.parent = x; // 将 “x的父亲” 设为 “y的父亲” y.parent = x.parent; if (x.parent == null) &#123; this.mRoot = y; // 如果 “x的父亲” 是空节点，则将y设为根节点 &#125; else &#123; if (x.parent.left == x) x.parent.left = y; // 如果 x是它父节点的左孩子，则将y设为“x的父节点的左孩子” else x.parent.right = y; // 如果 x是它父节点的左孩子，则将y设为“x的父节点的左孩子” &#125; // 将 “x” 设为 “y的左孩子” y.left = x; // 将 “x的父节点” 设为 “y” x.parent = y;&#125; 3. 右旋 对y进行左旋，意味着”将y变成一个右节点”。 右旋的实现代码(Java语言) 1234567891011121314151617181920212223242526272829303132333435363738394041/* * 对红黑树的节点(y)进行右旋转 * * 右旋示意图(对节点y进行左旋)： * py py * / / * y x * / \ --(右旋)-. / \ # * x ry lx y * / \ / \ # * lx rx rx ry * */private void rightRotate(RBTNode&lt;T&gt; y) &#123; // 设置x是当前节点的左孩子。 RBTNode&lt;T&gt; x = y.left; // 将 “x的右孩子” 设为 “y的左孩子”； // 如果"x的右孩子"不为空的话，将 “y” 设为 “x的右孩子的父亲” y.left = x.right; if (x.right != null) x.right.parent = y; // 将 “y的父亲” 设为 “x的父亲” x.parent = y.parent; if (y.parent == null) &#123; this.mRoot = x; // 如果 “y的父亲” 是空节点，则将x设为根节点 &#125; else &#123; if (y == y.parent.right) y.parent.right = x; // 如果 y是它父节点的右孩子，则将x设为“y的父节点的右孩子” else y.parent.left = x; // (y是它父节点的左孩子) 将x设为“x的父节点的左孩子” &#125; // 将 “y” 设为 “x的右孩子” x.right = y; // 将 “y的父节点” 设为 “x” y.parent = x;&#125; 4. 添加将一个节点插入到红黑树中，需要执行哪些步骤呢？首先，将红黑树当作一颗二叉查找树，将节点插入；然后，将节点着色为红色；最后，通过”旋转和重新着色”等一系列操作来修正该树，使之重新成为一颗红黑树。详细描述如下：第一步: 将红黑树当作一颗二叉查找树，将节点插入。​ 红黑树本身就是一颗二叉查找树，将节点插入后，该树仍然是一颗二叉查找树。也就意味着，树的键值仍然是有序的。此外，无论是左旋还是右旋，若旋转之前这棵树是二叉查找树，旋转之后它一定还是二叉查找树。这也就意味着，任何的旋转和重新着色操作，都不会改变它仍然是一颗二叉查找树的事实。好吧？那接下来，我们就来想方设法的旋转以及重新着色，使这颗树重新成为红黑树！ 第二步：将插入的节点着色为”红色”。​ 为什么着色成红色，而不是黑色呢？为什么呢？在回答之前，我们需要重新温习一下红黑树的特性：(1) 每个节点或者是黑色，或者是红色。(2) 根节点是黑色。(3) 每个叶子节点是黑色。 [注意：这里叶子节点，是指为空的叶子节点！](4) 如果一个节点是红色的，则它的子节点必须是黑色的。(5) 从一个节点到该节点的子孙节点的所有路径上包含相同数目的黑节点。​ 将插入的节点着色为红色，不会违背”特性(5)”！少违背一条特性，就意味着我们需要处理的情况越少。接下来，就要努力的让这棵树满足其它性质即可；满足了的话，它就又是一颗红黑树了。o(∩∩)o…哈哈 第三步: 通过一系列的旋转或着色等操作，使之重新成为一颗红黑树。​ 第二步中，将插入节点着色为”红色”之后，不会违背”特性(5)”。那它到底会违背哪些特性呢？​ 对于”特性(1)”，显然不会违背了。因为我们已经将它涂成红色了。​ 对于”特性(2)”，显然也不会违背。在第一步中，我们是将红黑树当作二叉查找树，然后执行的插入操作。而根据二叉查找数的特点，插入操作不会改变根节点。所以，根节点仍然是黑色。​ 对于”特性(3)”，显然不会违背了。这里的叶子节点是指的空叶子节点，插入非空节点并不会对它们造成影响。​ 对于”特性(4)”，是有可能违背的！​ 那接下来，想办法使之”满足特性(4)”，就可以将树重新构造成红黑树了。 添加操作的实现代码 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152/* * 将结点插入到红黑树中 * * 参数说明： * node 插入的结点 // 对应《算法导论》中的node */private void insert(RBTNode&lt;T&gt; node) &#123; int cmp; RBTNode&lt;T&gt; y = null; RBTNode&lt;T&gt; x = this.mRoot; // 1. 将红黑树当作一颗二叉查找树，将节点添加到二叉查找树中。 while (x != null) &#123; y = x; cmp = node.key.compareTo(x.key); if (cmp &lt; 0) x = x.left; else x = x.right; &#125; node.parent = y; if (y!=null) &#123; cmp = node.key.compareTo(y.key); if (cmp &lt; 0) y.left = node; else y.right = node; &#125; else &#123; this.mRoot = node; &#125; // 2. 设置节点的颜色为红色 node.color = RED; // 3. 将它重新修正为一颗二叉查找树 insertFixUp(node);&#125;/* * 新建结点(key)，并将其插入到红黑树中 * * 参数说明： * key 插入结点的键值 */public void insert(T key) &#123; RBTNode&lt;T&gt; node=new RBTNode&lt;T&gt;(key,BLACK,null,null,null); // 如果新建结点失败，则返回。 if (node != null) insert(node);&#125; 内部接口 – insert(node)的作用是将”node”节点插入到红黑树中。外部接口 – insert(key)的作用是将”key”添加到红黑树中。 添加修正操作的实现代码 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172/* * 红黑树插入修正函数 * * 在向红黑树中插入节点之后(失去平衡)，再调用该函数； * 目的是将它重新塑造成一颗红黑树。 * * 参数说明： * node 插入的结点 // 对应《算法导论》中的z */private void insertFixUp(RBTNode&lt;T&gt; node) &#123; RBTNode&lt;T&gt; parent, gparent; // 若“父节点存在，并且父节点的颜色是红色” while (((parent = parentOf(node))!=null) &amp;&amp; isRed(parent)) &#123; // 获得祖父节点 gparent = parentOf(parent); //若“父节点”是“祖父节点的左孩子” if (parent == gparent.left) &#123; // Case 1条件：叔叔节点是红色 RBTNode&lt;T&gt; uncle = gparent.right; if ((uncle!=null) &amp;&amp; isRed(uncle)) &#123; setBlack(uncle); setBlack(parent); setRed(gparent); node = gparent; continue; &#125; // Case 2条件：叔叔是黑色，且当前节点是右孩子 if (parent.right == node) &#123; RBTNode&lt;T&gt; tmp; leftRotate(parent); tmp = parent; parent = node; node = tmp; &#125; // Case 3条件：叔叔是黑色，且当前节点是左孩子。 setBlack(parent); setRed(gparent); rightRotate(gparent); &#125; else &#123; //若“z的父节点”是“z的祖父节点的右孩子” // Case 1条件：叔叔节点是红色 RBTNode&lt;T&gt; uncle = gparent.left; if ((uncle!=null) &amp;&amp; isRed(uncle)) &#123; setBlack(uncle); setBlack(parent); setRed(gparent); node = gparent; continue; &#125; // Case 2条件：叔叔是黑色，且当前节点是左孩子 if (parent.left == node) &#123; RBTNode&lt;T&gt; tmp; rightRotate(parent); tmp = parent; parent = node; node = tmp; &#125; // Case 3条件：叔叔是黑色，且当前节点是右孩子。 setBlack(parent); setRed(gparent); leftRotate(gparent); &#125; &#125; // 将根节点设为黑色 setBlack(this.mRoot);&#125; insertFixUp(node)的作用是对应”上面所讲的第三步”。它是一个内部接口。 5. 删除操作将红黑树内的某一个节点删除。需要执行的操作依次是：首先，将红黑树当作一颗二叉查找树，将该节点从二叉查找树中删除；然后，通过”旋转和重新着色”等一系列来修正该树，使之重新成为一棵红黑树。详细描述如下：第一步：将红黑树当作一颗二叉查找树，将节点删除。​ 这和”删除常规二叉查找树中删除节点的方法是一样的”。分3种情况：① 被删除节点没有儿子，即为叶节点。那么，直接将该节点删除就OK了。② 被删除节点只有一个儿子。那么，直接删除该节点，并用该节点的唯一子节点顶替它的位置。③ 被删除节点有两个儿子。那么，先找出它的后继节点；然后把“它的后继节点的内容”复制给“该节点的内容”；之后，删除“它的后继节点”。在这里，后继节点相当于替身，在将后继节点的内容复制给”被删除节点”之后，再将后继节点删除。这样就巧妙的将问题转换为”删除后继节点”的情况了，下面就考虑后继节点。 在”被删除节点”有两个非空子节点的情况下，它的后继节点不可能是双子非空。既然”的后继节点”不可能双子都非空，就意味着”该节点的后继节点”要么没有儿子，要么只有一个儿子。若没有儿子，则按”情况① “进行处理；若只有一个儿子，则按”情况② “进行处理。 第二步：通过”旋转和重新着色”等一系列来修正该树，使之重新成为一棵红黑树。​ 因为”第一步”中删除节点之后，可能会违背红黑树的特性。所以需要通过”旋转和重新着色”来修正该树，使之重新成为一棵红黑树。 删除操作的实现代码 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105/* * 删除结点(node)，并返回被删除的结点 * * 参数说明： * node 删除的结点 */private void remove(RBTNode&lt;T&gt; node) &#123; RBTNode&lt;T&gt; child, parent; boolean color; // 被删除节点的"左右孩子都不为空"的情况。 if ( (node.left!=null) &amp;&amp; (node.right!=null) ) &#123; // 被删节点的后继节点。(称为"取代节点") // 用它来取代"被删节点"的位置，然后再将"被删节点"去掉。 RBTNode&lt;T&gt; replace = node; // 获取后继节点 replace = replace.right; while (replace.left != null) replace = replace.left; // "node节点"不是根节点(只有根节点不存在父节点) if (parentOf(node)!=null) &#123; if (parentOf(node).left == node) parentOf(node).left = replace; else parentOf(node).right = replace; &#125; else &#123; // "node节点"是根节点，更新根节点。 this.mRoot = replace; &#125; // child是"取代节点"的右孩子，也是需要"调整的节点"。 // "取代节点"肯定不存在左孩子！因为它是一个后继节点。 child = replace.right; parent = parentOf(replace); // 保存"取代节点"的颜色 color = colorOf(replace); // "被删除节点"是"它的后继节点的父节点" if (parent == node) &#123; parent = replace; &#125; else &#123; // child不为空 if (child!=null) setParent(child, parent); parent.left = child; replace.right = node.right; setParent(node.right, replace); &#125; replace.parent = node.parent; replace.color = node.color; replace.left = node.left; node.left.parent = replace; if (color == BLACK) removeFixUp(child, parent); node = null; return ; &#125; if (node.left !=null) &#123; child = node.left; &#125; else &#123; child = node.right; &#125; parent = node.parent; // 保存"取代节点"的颜色 color = node.color; if (child!=null) child.parent = parent; // "node节点"不是根节点 if (parent!=null) &#123; if (parent.left == node) parent.left = child; else parent.right = child; &#125; else &#123; this.mRoot = child; &#125; if (color == BLACK) removeFixUp(child, parent); node = null;&#125;/* * 删除结点(z)，并返回被删除的结点 * * 参数说明： * tree 红黑树的根结点 * z 删除的结点 */public void remove(T key) &#123; RBTNode&lt;T&gt; node; if ((node = search(mRoot, key)) != null) remove(node);&#125; 内部接口 – remove(node)的作用是将”node”节点插入到红黑树中。外部接口 – remove(key)删除红黑树中键值为key的节点。 删除修正操作的实现代码(Java语言) 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687/* * 红黑树删除修正函数 * * 在从红黑树中删除插入节点之后(红黑树失去平衡)，再调用该函数； * 目的是将它重新塑造成一颗红黑树。 * * 参数说明： * node 待修正的节点 */private void removeFixUp(RBTNode&lt;T&gt; node, RBTNode&lt;T&gt; parent) &#123; RBTNode&lt;T&gt; other; while ((node==null || isBlack(node)) &amp;&amp; (node != this.mRoot)) &#123; if (parent.left == node) &#123; other = parent.right; if (isRed(other)) &#123; // Case 1: x的兄弟w是红色的 setBlack(other); setRed(parent); leftRotate(parent); other = parent.right; &#125; if ((other.left==null || isBlack(other.left)) &amp;&amp; (other.right==null || isBlack(other.right))) &#123; // Case 2: x的兄弟w是黑色，且w的俩个孩子也都是黑色的 setRed(other); node = parent; parent = parentOf(node); &#125; else &#123; if (other.right==null || isBlack(other.right)) &#123; // Case 3: x的兄弟w是黑色的，并且w的左孩子是红色，右孩子为黑色。 setBlack(other.left); setRed(other); rightRotate(other); other = parent.right; &#125; // Case 4: x的兄弟w是黑色的；并且w的右孩子是红色的，左孩子任意颜色。 setColor(other, colorOf(parent)); setBlack(parent); setBlack(other.right); leftRotate(parent); node = this.mRoot; break; &#125; &#125; else &#123; other = parent.left; if (isRed(other)) &#123; // Case 1: x的兄弟w是红色的 setBlack(other); setRed(parent); rightRotate(parent); other = parent.left; &#125; if ((other.left==null || isBlack(other.left)) &amp;&amp; (other.right==null || isBlack(other.right))) &#123; // Case 2: x的兄弟w是黑色，且w的俩个孩子也都是黑色的 setRed(other); node = parent; parent = parentOf(node); &#125; else &#123; if (other.left==null || isBlack(other.left)) &#123; // Case 3: x的兄弟w是黑色的，并且w的左孩子是红色，右孩子为黑色。 setBlack(other.right); setRed(other); leftRotate(other); other = parent.left; &#125; // Case 4: x的兄弟w是黑色的；并且w的右孩子是红色的，左孩子任意颜色。 setColor(other, colorOf(parent)); setBlack(parent); setBlack(other.left); rightRotate(parent); node = this.mRoot; break; &#125; &#125; &#125; if (node!=null) setBlack(node);&#125; removeFixup(node, parent)是对应”上面所讲的第三步”。它是一个内部接口。 红黑树的Java实现(完整源码)下面是红黑树实现的完整代码和相应的测试程序。(1) 除了上面所说的”左旋”、”右旋”、”添加”、”删除”等基本操作之后，还实现了”遍历”、”查找”、”打印”、”最小值”、”最大值”、”创建”、”销毁”等接口。(2) 函数接口大多分为内部接口和外部接口。内部接口是private函数，外部接口则是public函数。(3) 测试代码中提供了”插入”和”删除”动作的检测开关。默认是关闭的，打开方法可以参考”代码中的说明”。建议在打开开关后，在草稿上自己动手绘制一下红黑树。 红黑树的实现文件(RBTree.java) 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382383384385386387388389390391392393394395396397398399400401402403404405406407408409410411412413414415416417418419420421422423424425426427428429430431432433434435436437438439440441442443444445446447448449450451452453454455456457458459460461462463464465466467468469470471472473474475476477478479480481482483484485486487488489490491492493494495496497498499500501502503504505506507508509510511512513514515516517518519520521522523524525526527528529530531532533534535536537538539540541542543544545546547548549550551552553554555556557558559560561562563564565566567568569570571572573574575576577578579580581582583584585586587588589590591592593594595596597598599600601602603604605606607608609610611612613614615616617618619620621622623624625626627628629630631632633634635636637638639640641642643644645646647648649650651652653654655656657658659660661662663664665666667668669670671672673674675676677678679680681682683684685686687688689690691692 1 /** 2 * Java 语言: 红黑树 3 * 4 * @author skywang 5 * @date 2013/11/07 6 */ 7 8 public class RBTree&lt;T extends Comparable&lt;T&gt;&gt; &#123; 9 10 private RBTNode&lt;T&gt; mRoot; // 根结点 11 12 private static final boolean RED = false; 13 private static final boolean BLACK = true; 14 15 public class RBTNode&lt;T extends Comparable&lt;T&gt;&gt; &#123; 16 boolean color; // 颜色 17 T key; // 关键字(键值) 18 RBTNode&lt;T&gt; left; // 左孩子 19 RBTNode&lt;T&gt; right; // 右孩子 20 RBTNode&lt;T&gt; parent; // 父结点 21 22 public RBTNode(T key, boolean color, RBTNode&lt;T&gt; parent, RBTNode&lt;T&gt; left, RBTNode&lt;T&gt; right) &#123; 23 this.key = key; 24 this.color = color; 25 this.parent = parent; 26 this.left = left; 27 this.right = right; 28 &#125; 29 30 public T getKey() &#123; 31 return key; 32 &#125; 33 34 public String toString() &#123; 35 return ""+key+(this.color==RED?"(R)":"B"); 36 &#125; 37 &#125; 38 39 public RBTree() &#123; 40 mRoot=null; 41 &#125; 42 43 private RBTNode&lt;T&gt; parentOf(RBTNode&lt;T&gt; node) &#123; 44 return node!=null ? node.parent : null; 45 &#125; 46 private boolean colorOf(RBTNode&lt;T&gt; node) &#123; 47 return node!=null ? node.color : BLACK; 48 &#125; 49 private boolean isRed(RBTNode&lt;T&gt; node) &#123; 50 return ((node!=null)&amp;&amp;(node.color==RED)) ? true : false; 51 &#125; 52 private boolean isBlack(RBTNode&lt;T&gt; node) &#123; 53 return !isRed(node); 54 &#125; 55 private void setBlack(RBTNode&lt;T&gt; node) &#123; 56 if (node!=null) 57 node.color = BLACK; 58 &#125; 59 private void setRed(RBTNode&lt;T&gt; node) &#123; 60 if (node!=null) 61 node.color = RED; 62 &#125; 63 private void setParent(RBTNode&lt;T&gt; node, RBTNode&lt;T&gt; parent) &#123; 64 if (node!=null) 65 node.parent = parent; 66 &#125; 67 private void setColor(RBTNode&lt;T&gt; node, boolean color) &#123; 68 if (node!=null) 69 node.color = color; 70 &#125; 71 72 /* 73 * 前序遍历"红黑树" 74 */ 75 private void preOrder(RBTNode&lt;T&gt; tree) &#123; 76 if(tree != null) &#123; 77 System.out.print(tree.key+" "); 78 preOrder(tree.left); 79 preOrder(tree.right); 80 &#125; 81 &#125; 82 83 public void preOrder() &#123; 84 preOrder(mRoot); 85 &#125; 86 87 /* 88 * 中序遍历"红黑树" 89 */ 90 private void inOrder(RBTNode&lt;T&gt; tree) &#123; 91 if(tree != null) &#123; 92 inOrder(tree.left); 93 System.out.print(tree.key+" "); 94 inOrder(tree.right); 95 &#125; 96 &#125; 97 98 public void inOrder() &#123; 99 inOrder(mRoot);100 &#125;101 102 103 /*104 * 后序遍历"红黑树"105 */106 private void postOrder(RBTNode&lt;T&gt; tree) &#123;107 if(tree != null)108 &#123;109 postOrder(tree.left);110 postOrder(tree.right);111 System.out.print(tree.key+" ");112 &#125;113 &#125;114 115 public void postOrder() &#123;116 postOrder(mRoot);117 &#125;118 119 120 /*121 * (递归实现)查找"红黑树x"中键值为key的节点122 */123 private RBTNode&lt;T&gt; search(RBTNode&lt;T&gt; x, T key) &#123;124 if (x==null)125 return x;126 127 int cmp = key.compareTo(x.key);128 if (cmp &lt; 0)129 return search(x.left, key);130 else if (cmp &gt; 0)131 return search(x.right, key);132 else133 return x;134 &#125;135 136 public RBTNode&lt;T&gt; search(T key) &#123;137 return search(mRoot, key);138 &#125;139 140 /*141 * (非递归实现)查找"红黑树x"中键值为key的节点142 */143 private RBTNode&lt;T&gt; iterativeSearch(RBTNode&lt;T&gt; x, T key) &#123;144 while (x!=null) &#123;145 int cmp = key.compareTo(x.key);146 147 if (cmp &lt; 0) 148 x = x.left;149 else if (cmp &gt; 0) 150 x = x.right;151 else152 return x;153 &#125;154 155 return x;156 &#125;157 158 public RBTNode&lt;T&gt; iterativeSearch(T key) &#123;159 return iterativeSearch(mRoot, key);160 &#125;161 162 /* 163 * 查找最小结点：返回tree为根结点的红黑树的最小结点。164 */165 private RBTNode&lt;T&gt; minimum(RBTNode&lt;T&gt; tree) &#123;166 if (tree == null)167 return null;168 169 while(tree.left != null)170 tree = tree.left;171 return tree;172 &#125;173 174 public T minimum() &#123;175 RBTNode&lt;T&gt; p = minimum(mRoot);176 if (p != null)177 return p.key;178 179 return null;180 &#125;181 182 /* 183 * 查找最大结点：返回tree为根结点的红黑树的最大结点。184 */185 private RBTNode&lt;T&gt; maximum(RBTNode&lt;T&gt; tree) &#123;186 if (tree == null)187 return null;188 189 while(tree.right != null)190 tree = tree.right;191 return tree;192 &#125;193 194 public T maximum() &#123;195 RBTNode&lt;T&gt; p = maximum(mRoot);196 if (p != null)197 return p.key;198 199 return null;200 &#125;201 202 /* 203 * 找结点(x)的后继结点。即，查找"红黑树中数据值大于该结点"的"最小结点"。204 */205 public RBTNode&lt;T&gt; successor(RBTNode&lt;T&gt; x) &#123;206 // 如果x存在右孩子，则"x的后继结点"为 "以其右孩子为根的子树的最小结点"。207 if (x.right != null)208 return minimum(x.right);209 210 // 如果x没有右孩子。则x有以下两种可能：211 // (01) x是"一个左孩子"，则"x的后继结点"为 "它的父结点"。212 // (02) x是"一个右孩子"，则查找"x的最低的父结点，并且该父结点要具有左孩子"，找到的这个"最低的父结点"就是"x的后继结点"。213 RBTNode&lt;T&gt; y = x.parent;214 while ((y!=null) &amp;&amp; (x==y.right)) &#123;215 x = y;216 y = y.parent;217 &#125;218 219 return y;220 &#125;221 222 /* 223 * 找结点(x)的前驱结点。即，查找"红黑树中数据值小于该结点"的"最大结点"。224 */225 public RBTNode&lt;T&gt; predecessor(RBTNode&lt;T&gt; x) &#123;226 // 如果x存在左孩子，则"x的前驱结点"为 "以其左孩子为根的子树的最大结点"。227 if (x.left != null)228 return maximum(x.left);229 230 // 如果x没有左孩子。则x有以下两种可能：231 // (01) x是"一个右孩子"，则"x的前驱结点"为 "它的父结点"。232 // (01) x是"一个左孩子"，则查找"x的最低的父结点，并且该父结点要具有右孩子"，找到的这个"最低的父结点"就是"x的前驱结点"。233 RBTNode&lt;T&gt; y = x.parent;234 while ((y!=null) &amp;&amp; (x==y.left)) &#123;235 x = y;236 y = y.parent;237 &#125;238 239 return y;240 &#125;241 242 /* 243 * 对红黑树的节点(x)进行左旋转244 *245 * 左旋示意图(对节点x进行左旋)：246 * px px247 * / /248 * x y 249 * / \ --(左旋)-. / \ #250 * lx y x ry 251 * / \ / \252 * ly ry lx ly 253 *254 *255 */256 private void leftRotate(RBTNode&lt;T&gt; x) &#123;257 // 设置x的右孩子为y258 RBTNode&lt;T&gt; y = x.right;259 260 // 将 “y的左孩子” 设为 “x的右孩子”；261 // 如果y的左孩子非空，将 “x” 设为 “y的左孩子的父亲”262 x.right = y.left;263 if (y.left != null)264 y.left.parent = x;265 266 // 将 “x的父亲” 设为 “y的父亲”267 y.parent = x.parent;268 269 if (x.parent == null) &#123;270 this.mRoot = y; // 如果 “x的父亲” 是空节点，则将y设为根节点271 &#125; else &#123;272 if (x.parent.left == x)273 x.parent.left = y; // 如果 x是它父节点的左孩子，则将y设为“x的父节点的左孩子”274 else275 x.parent.right = y; // 如果 x是它父节点的左孩子，则将y设为“x的父节点的左孩子”276 &#125;277 278 // 将 “x” 设为 “y的左孩子”279 y.left = x;280 // 将 “x的父节点” 设为 “y”281 x.parent = y;282 &#125;283 284 /* 285 * 对红黑树的节点(y)进行右旋转286 *287 * 右旋示意图(对节点y进行左旋)：288 * py py289 * / /290 * y x 291 * / \ --(右旋)-. / \ #292 * x ry lx y 293 * / \ / \ #294 * lx rx rx ry295 * 296 */297 private void rightRotate(RBTNode&lt;T&gt; y) &#123;298 // 设置x是当前节点的左孩子。299 RBTNode&lt;T&gt; x = y.left;300 301 // 将 “x的右孩子” 设为 “y的左孩子”；302 // 如果"x的右孩子"不为空的话，将 “y” 设为 “x的右孩子的父亲”303 y.left = x.right;304 if (x.right != null)305 x.right.parent = y;306 307 // 将 “y的父亲” 设为 “x的父亲”308 x.parent = y.parent;309 310 if (y.parent == null) &#123;311 this.mRoot = x; // 如果 “y的父亲” 是空节点，则将x设为根节点312 &#125; else &#123;313 if (y == y.parent.right)314 y.parent.right = x; // 如果 y是它父节点的右孩子，则将x设为“y的父节点的右孩子”315 else316 y.parent.left = x; // (y是它父节点的左孩子) 将x设为“x的父节点的左孩子”317 &#125;318 319 // 将 “y” 设为 “x的右孩子”320 x.right = y;321 322 // 将 “y的父节点” 设为 “x”323 y.parent = x;324 &#125;325 326 /*327 * 红黑树插入修正函数328 *329 * 在向红黑树中插入节点之后(失去平衡)，再调用该函数；330 * 目的是将它重新塑造成一颗红黑树。331 *332 * 参数说明：333 * node 插入的结点 // 对应《算法导论》中的z334 */335 private void insertFixUp(RBTNode&lt;T&gt; node) &#123;336 RBTNode&lt;T&gt; parent, gparent;337 338 // 若“父节点存在，并且父节点的颜色是红色”339 while (((parent = parentOf(node))!=null) &amp;&amp; isRed(parent)) &#123;340 gparent = parentOf(parent);341 342 //若“父节点”是“祖父节点的左孩子”343 if (parent == gparent.left) &#123;344 // Case 1条件：叔叔节点是红色345 RBTNode&lt;T&gt; uncle = gparent.right;346 if ((uncle!=null) &amp;&amp; isRed(uncle)) &#123;347 setBlack(uncle);348 setBlack(parent);349 setRed(gparent);350 node = gparent;351 continue;352 &#125;353 354 // Case 2条件：叔叔是黑色，且当前节点是右孩子355 if (parent.right == node) &#123;356 RBTNode&lt;T&gt; tmp;357 leftRotate(parent);358 tmp = parent;359 parent = node;360 node = tmp;361 &#125;362 363 // Case 3条件：叔叔是黑色，且当前节点是左孩子。364 setBlack(parent);365 setRed(gparent);366 rightRotate(gparent);367 &#125; else &#123; //若“z的父节点”是“z的祖父节点的右孩子”368 // Case 1条件：叔叔节点是红色369 RBTNode&lt;T&gt; uncle = gparent.left;370 if ((uncle!=null) &amp;&amp; isRed(uncle)) &#123;371 setBlack(uncle);372 setBlack(parent);373 setRed(gparent);374 node = gparent;375 continue;376 &#125;377 378 // Case 2条件：叔叔是黑色，且当前节点是左孩子379 if (parent.left == node) &#123;380 RBTNode&lt;T&gt; tmp;381 rightRotate(parent);382 tmp = parent;383 parent = node;384 node = tmp;385 &#125;386 387 // Case 3条件：叔叔是黑色，且当前节点是右孩子。388 setBlack(parent);389 setRed(gparent);390 leftRotate(gparent);391 &#125;392 &#125;393 394 // 将根节点设为黑色395 setBlack(this.mRoot);396 &#125;397 398 /* 399 * 将结点插入到红黑树中400 *401 * 参数说明：402 * node 插入的结点 // 对应《算法导论》中的node403 */404 private void insert(RBTNode&lt;T&gt; node) &#123;405 int cmp;406 RBTNode&lt;T&gt; y = null;407 RBTNode&lt;T&gt; x = this.mRoot;408 409 // 1. 将红黑树当作一颗二叉查找树，将节点添加到二叉查找树中。410 while (x != null) &#123;411 y = x;412 cmp = node.key.compareTo(x.key);413 if (cmp &lt; 0)414 x = x.left;415 else416 x = x.right;417 &#125;418 419 node.parent = y;420 if (y!=null) &#123;421 cmp = node.key.compareTo(y.key);422 if (cmp &lt; 0)423 y.left = node;424 else425 y.right = node;426 &#125; else &#123;427 this.mRoot = node;428 &#125;429 430 // 2. 设置节点的颜色为红色431 node.color = RED;432 433 // 3. 将它重新修正为一颗二叉查找树434 insertFixUp(node);435 &#125;436 437 /* 438 * 新建结点(key)，并将其插入到红黑树中439 *440 * 参数说明：441 * key 插入结点的键值442 */443 public void insert(T key) &#123;444 RBTNode&lt;T&gt; node=new RBTNode&lt;T&gt;(key,BLACK,null,null,null);445 446 // 如果新建结点失败，则返回。447 if (node != null)448 insert(node);449 &#125;450 451 452 /*453 * 红黑树删除修正函数454 *455 * 在从红黑树中删除插入节点之后(红黑树失去平衡)，再调用该函数；456 * 目的是将它重新塑造成一颗红黑树。457 *458 * 参数说明：459 * node 待修正的节点460 */461 private void removeFixUp(RBTNode&lt;T&gt; node, RBTNode&lt;T&gt; parent) &#123;462 RBTNode&lt;T&gt; other;463 464 while ((node==null || isBlack(node)) &amp;&amp; (node != this.mRoot)) &#123;465 if (parent.left == node) &#123;466 other = parent.right;467 if (isRed(other)) &#123;468 // Case 1: x的兄弟w是红色的 469 setBlack(other);470 setRed(parent);471 leftRotate(parent);472 other = parent.right;473 &#125;474 475 if ((other.left==null || isBlack(other.left)) &amp;&amp;476 (other.right==null || isBlack(other.right))) &#123;477 // Case 2: x的兄弟w是黑色，且w的俩个孩子也都是黑色的 478 setRed(other);479 node = parent;480 parent = parentOf(node);481 &#125; else &#123;482 483 if (other.right==null || isBlack(other.right)) &#123;484 // Case 3: x的兄弟w是黑色的，并且w的左孩子是红色，右孩子为黑色。 485 setBlack(other.left);486 setRed(other);487 rightRotate(other);488 other = parent.right;489 &#125;490 // Case 4: x的兄弟w是黑色的；并且w的右孩子是红色的，左孩子任意颜色。491 setColor(other, colorOf(parent));492 setBlack(parent);493 setBlack(other.right);494 leftRotate(parent);495 node = this.mRoot;496 break;497 &#125;498 &#125; else &#123;499 500 other = parent.left;501 if (isRed(other)) &#123;502 // Case 1: x的兄弟w是红色的 503 setBlack(other);504 setRed(parent);505 rightRotate(parent);506 other = parent.left;507 &#125;508 509 if ((other.left==null || isBlack(other.left)) &amp;&amp;510 (other.right==null || isBlack(other.right))) &#123;511 // Case 2: x的兄弟w是黑色，且w的俩个孩子也都是黑色的 512 setRed(other);513 node = parent;514 parent = parentOf(node);515 &#125; else &#123;516 517 if (other.left==null || isBlack(other.left)) &#123;518 // Case 3: x的兄弟w是黑色的，并且w的左孩子是红色，右孩子为黑色。 519 setBlack(other.right);520 setRed(other);521 leftRotate(other);522 other = parent.left;523 &#125;524 525 // Case 4: x的兄弟w是黑色的；并且w的右孩子是红色的，左孩子任意颜色。526 setColor(other, colorOf(parent));527 setBlack(parent);528 setBlack(other.left);529 rightRotate(parent);530 node = this.mRoot;531 break;532 &#125;533 &#125;534 &#125;535 536 if (node!=null)537 setBlack(node);538 &#125;539 540 /* 541 * 删除结点(node)，并返回被删除的结点542 *543 * 参数说明：544 * node 删除的结点545 */546 private void remove(RBTNode&lt;T&gt; node) &#123;547 RBTNode&lt;T&gt; child, parent;548 boolean color;549 550 // 被删除节点的"左右孩子都不为空"的情况。551 if ( (node.left!=null) &amp;&amp; (node.right!=null) ) &#123;552 // 被删节点的后继节点。(称为"取代节点")553 // 用它来取代"被删节点"的位置，然后再将"被删节点"去掉。554 RBTNode&lt;T&gt; replace = node;555 556 // 获取后继节点557 replace = replace.right;558 while (replace.left != null)559 replace = replace.left;560 561 // "node节点"不是根节点(只有根节点不存在父节点)562 if (parentOf(node)!=null) &#123;563 if (parentOf(node).left == node)564 parentOf(node).left = replace;565 else566 parentOf(node).right = replace;567 &#125; else &#123;568 // "node节点"是根节点，更新根节点。569 this.mRoot = replace;570 &#125;571 572 // child是"取代节点"的右孩子，也是需要"调整的节点"。573 // "取代节点"肯定不存在左孩子！因为它是一个后继节点。574 child = replace.right;575 parent = parentOf(replace);576 // 保存"取代节点"的颜色577 color = colorOf(replace);578 579 // "被删除节点"是"它的后继节点的父节点"580 if (parent == node) &#123;581 parent = replace;582 &#125; else &#123;583 // child不为空584 if (child!=null)585 setParent(child, parent);586 parent.left = child;587 588 replace.right = node.right;589 setParent(node.right, replace);590 &#125;591 592 replace.parent = node.parent;593 replace.color = node.color;594 replace.left = node.left;595 node.left.parent = replace;596 597 if (color == BLACK)598 removeFixUp(child, parent);599 600 node = null;601 return ;602 &#125;603 604 if (node.left !=null) &#123;605 child = node.left;606 &#125; else &#123;607 child = node.right;608 &#125;609 610 parent = node.parent;611 // 保存"取代节点"的颜色612 color = node.color;613 614 if (child!=null)615 child.parent = parent;616 617 // "node节点"不是根节点618 if (parent!=null) &#123;619 if (parent.left == node)620 parent.left = child;621 else622 parent.right = child;623 &#125; else &#123;624 this.mRoot = child;625 &#125;626 627 if (color == BLACK)628 removeFixUp(child, parent);629 node = null;630 &#125;631 632 /* 633 * 删除结点(z)，并返回被删除的结点634 *635 * 参数说明：636 * tree 红黑树的根结点637 * z 删除的结点638 */639 public void remove(T key) &#123;640 RBTNode&lt;T&gt; node; 641 642 if ((node = search(mRoot, key)) != null)643 remove(node);644 &#125;645 646 /*647 * 销毁红黑树648 */649 private void destroy(RBTNode&lt;T&gt; tree) &#123;650 if (tree==null)651 return ;652 653 if (tree.left != null)654 destroy(tree.left);655 if (tree.right != null)656 destroy(tree.right);657 658 tree=null;659 &#125;660 661 public void clear() &#123;662 destroy(mRoot);663 mRoot = null;664 &#125;665 666 /*667 * 打印"红黑树"668 *669 * key -- 节点的键值 670 * direction -- 0，表示该节点是根节点;671 * -1，表示该节点是它的父结点的左孩子;672 * 1，表示该节点是它的父结点的右孩子。673 */674 private void print(RBTNode&lt;T&gt; tree, T key, int direction) &#123;675 676 if(tree != null) &#123;677 678 if(direction==0) // tree是根节点679 System.out.printf("%2d(B) is root\n", tree.key);680 else // tree是分支节点681 System.out.printf("%2d(%s) is %2d's %6s child\n", tree.key, isRed(tree)?"R":"B", key, direction==1?"right" : "left");682 683 print(tree.left, tree.key, -1);684 print(tree.right,tree.key, 1);685 &#125;686 &#125;687 688 public void print() &#123;689 if (mRoot != null)690 print(mRoot, mRoot.key, 0);691 &#125;692 &#125; 红黑树的测试文件(RBTreeTest.java) 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465 1 /** 2 * Java 语言: 二叉查找树 3 * 4 * @author skywang 5 * @date 2013/11/07 6 */ 7 public class RBTreeTest &#123; 8 9 private static final int a[] = &#123;10, 40, 30, 60, 90, 70, 20, 50, 80&#125;;10 private static final boolean mDebugInsert = false; // "插入"动作的检测开关(false，关闭；true，打开)11 private static final boolean mDebugDelete = false; // "删除"动作的检测开关(false，关闭；true，打开)12 13 public static void main(String[] args) &#123;14 int i, ilen = a.length;15 RBTree&lt;Integer&gt; tree=new RBTree&lt;Integer&gt;();16 17 System.out.printf("== 原始数据: ");18 for(i=0; i&lt;ilen; i++)19 System.out.printf("%d ", a[i]);20 System.out.printf("\n");21 22 for(i=0; i&lt;ilen; i++) &#123;23 tree.insert(a[i]);24 // 设置mDebugInsert=true,测试"添加函数"25 if (mDebugInsert) &#123;26 System.out.printf("== 添加节点: %d\n", a[i]);27 System.out.printf("== 树的详细信息: \n");28 tree.print();29 System.out.printf("\n");30 &#125;31 &#125;32 33 System.out.printf("== 前序遍历: ");34 tree.preOrder();35 36 System.out.printf("\n== 中序遍历: ");37 tree.inOrder();38 39 System.out.printf("\n== 后序遍历: ");40 tree.postOrder();41 System.out.printf("\n");42 43 System.out.printf("== 最小值: %s\n", tree.minimum());44 System.out.printf("== 最大值: %s\n", tree.maximum());45 System.out.printf("== 树的详细信息: \n");46 tree.print();47 System.out.printf("\n");48 49 // 设置mDebugDelete=true,测试"删除函数"50 if (mDebugDelete) &#123;51 for(i=0; i&lt;ilen; i++)52 &#123;53 tree.remove(a[i]);54 55 System.out.printf("== 删除节点: %d\n", a[i]);56 System.out.printf("== 树的详细信息: \n");57 tree.print();58 System.out.printf("\n");59 &#125;60 &#125;61 62 // 销毁二叉树63 tree.clear();64 &#125;65 &#125; 红黑树的Java测试程序前面已经给出了红黑树的测试代码(RBTreeTest.java)，这里就不再重复说明。下面是测试程序的运行结果： 12345678910111213141516== 原始数据: 10 40 30 60 90 70 20 50 80 == 前序遍历: 30 10 20 60 40 50 80 70 90 == 中序遍历: 10 20 30 40 50 60 70 80 90 == 后序遍历: 20 10 50 40 70 90 80 60 30 == 最小值: 10== 最大值: 90== 树的详细信息: 30(B) is root10(B) is 30&apos;s left child20(R) is 10&apos;s right child60(R) is 30&apos;s right child40(B) is 60&apos;s left child50(R) is 40&apos;s right child80(B) is 60&apos;s right child70(R) is 80&apos;s left child90(R) is 80&apos;s right child]]></content>
      <categories>
        <category>红黑树</category>
      </categories>
      <tags>
        <tag>数据结构</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java Map 用法、遍历、排序和常用 API]]></title>
    <url>%2F2018%2F06%2F03%2FJava%20Map%20%E7%9A%84%E5%B8%B8%E7%94%A8%E6%96%B9%E6%B3%95%2F</url>
    <content type="text"><![CDATA[Java map 用法、遍历、排序和常用APIjava.util 中的集合类包含 Java 中某些最常用的类。最常用的集合类是 List 和 Map。 Map 提供了一个更通用的元素存储方法。Map 集合类用于存储元素对（称作“键”和“值”），其中每个键映射到一个值。 本文主要介绍java map的初始化、用法、map的四种常用的遍历方式、map的排序以及常用api。 Map用法类型介绍Java 自带了各种 Map 类。这些 Map 类可归为三种类型： 通用Map，用于在应用程序中管理映射，通常在 java.util 程序包中实现HashMap、Hashtable、Properties、LinkedHashMap、IdentityHashMap、TreeMap、WeakHashMap、ConcurrentHashMap 专用Map，通常我们不必亲自创建此类Map，而是通过某些其他类对其进行访 java.util.jar.Attributes、 javax.print.attribute.standard.PrinterStateReasons、 java.security.Provider、 java.awt.RenderingHints、 javax.swing.UIDefaults 一个用于帮助我们实现自己的Map类的抽象类 AbstractMap 类型区别HashMap 最常用的 Map ,它根据键的 HashCode 值存储数据,根据键可以直接获取它的值，具有很快的访问速度。HashMap 最多只允许一条记录的键为 Null(多条会覆盖);不允许多条记录的值为 Null。非同步的。 TreeMap 能够把它保存的记录根据键(key)排序,默认是按升序排序，也可以指定排序的比较器，当用Iterator 遍历TreeMap时，得到的记录是排过序的。TreeMap不允许key的值为null。非同步的。Hashtable 与 HashMap类似,不同的是:key和value的值均不允许为null;它支持线程的同步，即任一时刻只有一个线程能写Hashtable,因此也导致了Hashtale在写入时会比较慢。LinkedHashMap 保存了记录的插入顺序，在用 Iterator 遍历 LinkedHashMap 时，先得到的记录肯定是先插入的.在遍历的时候会比 HashMap 慢。key 和 value 均允许为空，非同步的。 四种常用Map插入与读取性能比较测试环境jdk1.7.0_80 测试结果 插入10次平均(ms) 读取10次平均(ms) 1W 10W 100W 1W 10W 100W HashMap 56 261 3030 2 21 220 LinkedHashMap 25 229 3069 2 20 216 TreeMap 29 295 4117 5 103 1446 Hashtable 24 234 3275 2 22 259 测试代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119public class Test &#123; static int hashMapW = 0; static int hashMapR = 0; static int linkMapW = 0; static int linkMapR = 0; static int treeMapW = 0; static int treeMapR = 0; static int hashTableW = 0; static int hashTableR = 0; public static void main(String[] args) &#123; for (int i = 0; i &lt; 10; i++) &#123; Test test = new Test(); test.test(100 * 10000); System.out.println(); &#125; System.out.println("hashMapW = " + hashMapW / 10); System.out.println("hashMapR = " + hashMapR / 10); System.out.println("linkMapW = " + linkMapW / 10); System.out.println("linkMapR = " + linkMapR / 10); System.out.println("treeMapW = " + treeMapW / 10); System.out.println("treeMapR = " + treeMapR / 10); System.out.println("hashTableW = " + hashTableW / 10); System.out.println("hashTableR = " + hashTableR / 10); &#125; public void test(int size) &#123; int index; Random random = new Random(); String[] key = new String[size]; // HashMap 插入 Map&lt;String, String&gt; map = new HashMap&lt;String, String&gt;(); long start = System.currentTimeMillis(); for (int i = 0; i &lt; size; i++) &#123; key[i] = UUID.randomUUID().toString(); map.put(key[i], UUID.randomUUID().toString()); &#125; long end = System.currentTimeMillis(); hashMapW += (end - start); System.out.println("HashMap插入耗时 = " + (end - start) + " ms"); // HashMap 读取 start = System.currentTimeMillis(); for (int i = 0; i &lt; size; i++) &#123; index = random.nextInt(size); map.get(key[index]); &#125; end = System.currentTimeMillis(); hashMapR += (end - start); System.out.println("HashMap读取耗时 = " + (end - start) + " ms"); // LinkedHashMap 插入 map = new LinkedHashMap&lt;String, String&gt;(); start = System.currentTimeMillis(); for (int i = 0; i &lt; size; i++) &#123; key[i] = UUID.randomUUID().toString(); map.put(key[i], UUID.randomUUID().toString()); &#125; end = System.currentTimeMillis(); linkMapW += (end - start); System.out.println("LinkedHashMap插入耗时 = " + (end - start) + " ms"); // LinkedHashMap 读取 start = System.currentTimeMillis(); for (int i = 0; i &lt; size; i++) &#123; index = random.nextInt(size); map.get(key[index]); &#125; end = System.currentTimeMillis(); linkMapR += (end - start); System.out.println("LinkedHashMap读取耗时 = " + (end - start) + " ms"); // TreeMap 插入 key = new String[size]; map = new TreeMap&lt;String, String&gt;(); start = System.currentTimeMillis(); for (int i = 0; i &lt; size; i++) &#123; key[i] = UUID.randomUUID().toString(); map.put(key[i], UUID.randomUUID().toString()); &#125; end = System.currentTimeMillis(); treeMapW += (end - start); System.out.println("TreeMap插入耗时 = " + (end - start) + " ms"); // TreeMap 读取 start = System.currentTimeMillis(); for (int i = 0; i &lt; size; i++) &#123; index = random.nextInt(size); map.get(key[index]); &#125; end = System.currentTimeMillis(); treeMapR += (end - start); System.out.println("TreeMap读取耗时 = " + (end - start) + " ms"); // Hashtable 插入 key = new String[size]; map = new Hashtable&lt;String, String&gt;(); start = System.currentTimeMillis(); for (int i = 0; i &lt; size; i++) &#123; key[i] = UUID.randomUUID().toString(); map.put(key[i], UUID.randomUUID().toString()); &#125; end = System.currentTimeMillis(); hashTableW += (end - start); System.out.println("Hashtable插入耗时 = " + (end - start) + " ms"); // Hashtable 读取 start = System.currentTimeMillis(); for (int i = 0; i &lt; size; i++) &#123; index = random.nextInt(size); map.get(key[index]); &#125; end = System.currentTimeMillis(); hashTableR += (end - start); System.out.println("Hashtable读取耗时 = " + (end - start) + " ms"); &#125;&#125; Map 遍历初始化数据123Map&lt;String, String&gt; map = new HashMap&lt;String, String&gt;();map.put("key1", "value1");map.put("key2", "value2"); 增强for循环遍历使用keySet()遍历 12345Iterator&lt;String&gt; iterator = map.keySet().iterator();while (iterator.hasNext()) &#123; String key = iterator.next(); System.out.println(key + " ：" + map.get(key));&#125; 使用entrySet()遍历 12345Iterator&lt;Map.Entry&lt;String, String&gt;&gt; iterator = map.entrySet().iterator();while (iterator.hasNext()) &#123; Map.Entry&lt;String, String&gt; entry = iterator.next(); System.out.println(entry.getKey() + " ：" + entry.getValue());&#125; 迭代器遍历使用keySet()遍历 12345Iterator&lt;String&gt; iterator = map.keySet().iterator();while (iterator.hasNext()) &#123; String key = iterator.next(); System.out.println(key + " ：" + map.get(key));&#125; 使用entrySet()遍历 12345Iterator&lt;Map.Entry&lt;String, String&gt;&gt; iterator = map.entrySet().iterator();while (iterator.hasNext()) &#123; Map.Entry&lt;String, String&gt; entry = iterator.next(); System.out.println(entry.getKey() + " ：" + entry.getValue());&#125; HashMap四种遍历方式性能比较比较方式 分别对四种遍历方式进行10W次迭代，比较用时。 代码 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657package net.xsoftlab.baike; import java.util.HashMap;import java.util.Iterator;import java.util.Map;import java.util.Map.Entry; public class TestMap &#123; public static void main(String[] args) &#123; // 初始化，10W次赋值 Map&lt;Integer, Integer&gt; map = new HashMap&lt;Integer, Integer&gt;(); for (int i = 0; i &lt; 100000; i++) map.put(i, i); /** 增强for循环，keySet迭代 */ long start = System.currentTimeMillis(); for (Integer key : map.keySet()) &#123; map.get(key); &#125; long end = System.currentTimeMillis(); System.out.println("增强for循环，keySet迭代 -&gt; " + (end - start) + " ms"); /** 增强for循环，entrySet迭代 */ start = System.currentTimeMillis(); for (Entry&lt;Integer, Integer&gt; entry : map.entrySet()) &#123; entry.getKey(); entry.getValue(); &#125; end = System.currentTimeMillis(); System.out.println("增强for循环，entrySet迭代 -&gt; " + (end - start) + " ms"); /** 迭代器，keySet迭代 */ start = System.currentTimeMillis(); Iterator&lt;Integer&gt; iterator = map.keySet().iterator(); Integer key; while (iterator.hasNext()) &#123; key = iterator.next(); map.get(key); &#125; end = System.currentTimeMillis(); System.out.println("迭代器，keySet迭代 -&gt; " + (end - start) + " ms"); /** 迭代器，entrySet迭代 */ start = System.currentTimeMillis(); Iterator&lt;Map.Entry&lt;Integer, Integer&gt;&gt; iterator1 = map.entrySet().iterator(); Map.Entry&lt;Integer, Integer&gt; entry; while (iterator1.hasNext()) &#123; entry = iterator1.next(); entry.getKey(); entry.getValue(); &#125; end = System.currentTimeMillis(); System.out.println("迭代器，entrySet迭代 -&gt; " + (end - start) + " ms"); &#125;&#125; 运行三次，比较结果 第一次 1234增强 for 循环 keySet 迭代 -&gt; 18ms增强 for 循环， entrySet - &gt;4ms迭代器，keySet 迭代 - &gt;4ms迭代器，entrySet 迭代 - &gt;3ms 第二次 1234增强 for 循环 keySet 迭代 -&gt; 8ms增强 for 循环， entrySet - &gt;5ms迭代器，keySet 迭代 - &gt;5ms迭代器，entrySet 迭代 - &gt;4ms 第三次 1234增强 for 循环 keySet 迭代 -&gt; 30ms增强 for 循环， entrySet - &gt;38ms迭代器，keySet 迭代 - &gt;7ms迭代器，entrySet 迭代 - &gt;2ms 平均值 1234增强for循环，keySet迭代 -&gt; 31 ms增强for循环，entrySet迭代 -&gt; 20 ms迭代器，keySet迭代 -&gt; 17 ms迭代器，entrySet迭代 -&gt; 10.33 ms 总结 增强for循环使用方便，但性能较差，不适合处理超大量级的数据。 迭代器的遍历速度要比增强for循环快很多，是增强for循环的2倍左右。 使用entrySet遍历的速度要比keySet快很多，是keySet的1.5倍左右。 Map 排序HashMap、Hashtable、LinkedHashMap 排序注： TreeMap 也可以使用此方法进行排序，但是更推荐下面的方法。 1234567891011121314151617Map&lt;String, String&gt; map = new HashMap&lt;String, String&gt;();map.put("a", "c");map.put("b", "b");map.put("c", "a"); // 通过ArrayList构造函数把map.entrySet()转换成listList&lt;Map.Entry&lt;String, String&gt;&gt; list = new ArrayList&lt;Map.Entry&lt;String, String&gt;&gt;(map.entrySet());// 通过比较器实现比较排序Collections.sort(list, new Comparator&lt;Map.Entry&lt;String, String&gt;&gt;() &#123; public int compare(Map.Entry&lt;String, String&gt; mapping1, Map.Entry&lt;String, String&gt; mapping2) &#123; return mapping1.getKey().compareTo(mapping2.getKey()); &#125;&#125;); for (Map.Entry&lt;String, String&gt; mapping : list) &#123; System.out.println(mapping.getKey() + " ：" + mapping.getValue());&#125; TreeMap排序TreeMap默认按key进行升序排序，如果想改变默认的顺序，可以使用比较器: 12345678910111213Map&lt;String, String&gt; map = new TreeMap&lt;String, String&gt;(new Comparator&lt;String&gt;() &#123; public int compare(String obj1, String obj2) &#123; return obj2.compareTo(obj1);// 降序排序 &#125;&#125;);map.put("a", "c");map.put("b", "b");map.put("c", "a"); for (String key : map.keySet()) &#123; System.out.println(key + " ：" + map.get(key));&#125; 按value排序(通用)1234567891011121314151617Map&lt;String, String&gt; map = new TreeMap&lt;String, String&gt;(); map.put("a", "c"); map.put("b", "b"); map.put("c", "a"); // 通过ArrayList构造函数把map.entrySet()转换成list List&lt;Map.Entry&lt;String, String&gt;&gt; list = new ArrayList&lt;Map.Entry&lt;String, String&gt;&gt;(map.entrySet()); // 通过比较器实现比较排序 Collections.sort(list, new Comparator&lt;Map.Entry&lt;String, String&gt;&gt;() &#123; public int compare(Map.Entry&lt;String, String&gt; mapping1, Map.Entry&lt;String, String&gt; mapping2) &#123; return mapping1.getValue().compareTo(mapping2.getValue()); &#125; &#125;); for (String key : map.keySet()) &#123; System.out.println(key + " ：" + map.get(key)); &#125; 常用API clear() 从 Map 中删除所有映射 remove(Object key) 从 Map 中删除键和关联的值 put(Object key, Object value) 将指定值与指定键相关联 putAll(Map t) 将指定 Map 中的所有映射复制到此 map entrySet() 返回 Map 中所包含映射的 Set 视图。Set 中的每个元素都是一个 Map.Entry 对象，可以使用 getKey() 和 getValue() 方法（还有一个 setValue() 方法）访问后者的键元素和值元素 keySet() 返回 Map 中所包含键的 Set 视图。删除 Set 中的元素还将删除 Map 中相应的映射（键和值） values() 返回 map 中所包含值的 Collection 视图。删除 Collection 中的元素还将删除 Map 中相应的映射（键和值） get(Object key) 返回与指定键关联的值 containsKey(Object key) 如果 Map 包含指定键的映射，则返回 true containsValue(Object value) 如果此 Map 将一个或多个键映射到指定值，则返回 true isEmpty() 如果 Map 不包含键-值映射，则返回 true size() 返回 Map 中的键-值映射的数目]]></content>
      <categories>
        <category>Map</category>
      </categories>
      <tags>
        <tag>基础数据类型</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ConcurrentHashMap 源码解读]]></title>
    <url>%2F2018%2F06%2F03%2FJava%208%20ConcurrentHashMap%20%E6%BA%90%E7%A0%81%E8%A7%A3%E8%AF%BB%2F</url>
    <content type="text"><![CDATA[Java 8 ConcurrentHashMap 源码解读 ConcurrentHashMap 当之无愧是支持并发最好的键值对（Map）集合。在日常编码中，出场率也相当之高。在jdk8中，集合类 ConcurrentHashMap 经 Doug Lea 大师之手，借助volatile语义以及CAS操作进行优化，使得该集合类更好地发挥出了并发的优势。与jdk7中相比，在原有段锁（Segment）的基础上，引入了数组＋链表＋红黑树的存储模型，在查询效率上花费了不少心思。 基础数据结构ConcurrentHashMap内存存储结构图大致如下： 概述1、设计首要目的：维护并发可读性（get、迭代相关）；次要目的：使空间消耗比HashMap相同或更好，且支持多线程高效率的初始插入（empty table）。 2、HashTable线程安全，但采用synchronized，多线程下效率低下。线程1put时，线程2无法put或get。 阅前了解在真正阅读 ConcurrentHashMap 源码之前，我们简单复习下关于volatile和CAS的概念，这样才能更好地帮助我们理解源码中的关键方法。 volatile语义java提供的关键字volatile是最轻量级的同步机制。当定义一个变量为volatile时，它就具备了三层语义： - 可见性（Visibility）：在多线程环境下，一个变量的写操作总是对其后的读取线程可见 - 原子性（Atomicity）：volatile的读/写操作具有原子性 - 有序性（Ordering）：禁止指令的重排序优化，JVM会通过插入内存屏障（Memory Barrier）指令来保证 就同步性能而言，大多数场景下volatile的总开销是要比锁低的。在ConcurrentHashMap的源码中，我们能看到频繁的volatile变量读取与写入。 CAS操作CAS一般被理解为原子操作。在java中，正是利用了处理器的CMPXCHG（intel）指令实现CAS操作。CAS需要接受原有期望值expected以及想要修改的新值x，只有在原有期望值与当前值相等时才会更新为x，否则为失败。在ConcurrentHashMap的方法中，大量使用CAS获取/修改互斥量，以达到多线程并发环境下的正确性。 ConcurrentHashMap 的常量12// maximum_capacity table的最大容量，必须为2次幂形式private static final int MAXIMUM_CAPACITY = 1 &lt;&lt; 30 12// default_capacity table的默认初始容量，必须为2次幂形式private static final int DEFAULT_CAPACITY = 16 12// max_array_size MAX_VALUE=2^31-1=2147483647static final int MAX_ARRAY_SIZE = Integer.MAX_VALUE - 8 12// default_concurrency_leve 未被用到，用来兼容之前版本private static finalint DEFAULT_CONCURRENCY_LEVEL = 16 123// load_factor table的负载因子，当前节点数量超过 n * LOAD_FACTOR，执行扩容// 位操作表达式为 n - (n &gt;&gt;&gt; 2)private static final float LOAD_FACTOR = 0.75f 12// treeify_threshold 针对每个桶（bin），链表转换为红黑树的节点数阈值static final int TREEIFY_THRESHOLD = 8 12// 针对每个桶（bin），红黑树退化为链表的节点数阈值static final int UNTREEIFY_THRESHOLD = 6 12// min_treeify_capacity 最小的树的容量static final int MIN_TREEIFY_CAPACITY = 64 1234// 扩容线程每次最少要迁移16个hash桶// min_transfer_stride 在扩容中，参与的单个线程允许处理的最少table桶首节点个数// 虽然适当添加线程，会使得整个扩容过程变快，但需要考虑多线程内存同时分配的问题private static final int MIN_TRANSFER_STRIDE = 16 12// resize stamp bits sizeCtl 中记录 size 的 bit 数private static int RESIZE_STAMP_BITS = 16 12// max_resizers 2^15-1 参与扩容的最大线程数private static final int MAX_RESIZERS = (1&lt;&lt;(32-RESIZE_STAMP_BITS))-1 12// 32 - 16 = 16, sizeCtl 中记录 size 大小的偏移量private static final int RESIZE_STAMP_SHIFT = 32 - RESIZE_STAMP_BITS = 16 12// 转为 nodes的hash值、标示位static final int MOVED = -1 12// 树的根节点的 hash 值static final int TREEBIN = -2 12// ReservationNode 的 hash 值static final int RESERVED = -3 12345// 一些特定的哈希值代表不同含义static final int MOVED = -1; // hash for forwarding nodesstatic final int TREEBIN = -2; // hash for roots of treesstatic final int RESERVED = -3; // hash for transient reservationsstatic final int HASH_BITS = 0x7fffffff; // usable bits of normal node hash 12// CPU数static final int NCPU = Runtime.getRuntime().availableProcessors() 123456789101112131415161718192021222324252627/** * 真正存储Node数据（桶首）节点的数组table * 所有Node节点根据hash分桶存储 * table数组中存储的是所有桶（bin）的首节点 * hash值相同的节点以链表形式分装在桶中 * 当一个桶中节点数达到8个时，转换为红黑树，提高查询效率 * 装载Node的数组，作为ConcurrentHashMap的数据容器，采用懒加载的方式 * 直到第一次插入数据的时候才会进行初始化操作，数组的大小总是为2的幂次方。 */transient volatile Node&lt;K,V&gt;[] table;// 扩容时候使用,平时为null，只有在扩容的时候才为非nullprivate transient volatile Node&lt;K,V&gt;[] nextTable;// 没有竞争条件时，使用private transient volatile long baseCount;// 扩容时，将table中的元素迁移至nextTable . 扩容时非空private transient volatile Node&lt;K,V&gt;[] nextTable;/** * 重要控制变量 * 根据变量的数值不同，类实例处于不同阶段 * 1. = -1 : 正在初始化 * 2. &lt; -1 : 正在扩容，数值为 -(1 + 参与扩容的线程数) * 3. = 0 : 创建时初始为0 * 4. &gt; 0 : 下一次扩容的大小 */private transient volatile int sizeCtl; ConcurrentHashMap 重要属性NodeKey-value entry, 继承自Map.Entry&lt;K,V&gt;对象。 Node&lt;K,V&gt;节点是ConcurrentHashMap存储数据的最基本结构。一个数据mapping节点中，存储4个变量：当前节点hash值、节点的key值、节点的value值、指向下一个节点的指针next。其中在子类中的hash可以为负数，具有特殊的并发处理意义，后文会解释。除了具有特殊意义的子类，Node中的key和val不允许为null。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152static class Node&lt;K,V&gt; implements Map.Entry&lt;K,V&gt; &#123; // Node节点的hash值和key的hash值相同 // TreeNode节点的hash值 final int hash; final K key; volatile V val; //带有同步锁的value(保证可见性) volatile Node&lt;K,V&gt; next;//带有同步锁的next指针 Node(inthash, K key, V val, Node&lt;K,V&gt; next) &#123; this.hash = hash; this.key = key; this.val = val; this.next = next; &#125; public final K getKey() &#123; return key; &#125; public final V getValue() &#123; return val; &#125; // HashMap调用Objects.hashCode()，最终也是调用Object.hashCode()；效果一样 public final int hashCode() &#123; returnkey.hashCode() ^ val.hashCode(); &#125; public final String toString()&#123; returnkey + "=" + val; &#125; //不允许直接改变value的值 public final V setValue(V value) &#123; // 不允许修改value值，HashMap允许 throw new UnsupportedOperationException(); &#125; // HashMap使用if (o == this)，且嵌套if；concurrent使用&amp;&amp; public final boolean equals(Object o) &#123; Object k, v, u; Map.Entry&lt;?,?&gt; e; return ((oinstanceof Map.Entry) &amp;&amp; (k = (e = (Map.Entry&lt;?,?&gt;)o).getKey()) != null &amp;&amp; (v = e.getValue()) != null &amp;&amp; (k == key || k.equals(key)) &amp;&amp; (v == (u = val) || v.equals(u))); &#125; Node&lt;K,V&gt; find(inth, Object k) &#123; // 增加find方法辅助get方法 Node&lt;K,V&gt; e = this; if (k != null) &#123; do &#123; K ek; if (e.hash == h &amp;&amp; ((ek = e.key) == k || (ek != null &amp;&amp; k.equals(ek)))) return e; /** * 以链表形式查找桶中下一个Node信息 * 当转换为subclass红黑树节点TreeNode * 则使用TreeNode中的find进行查询操作 */ &#125; while ((e = e.next) != null); &#125; returnnull; &#125; &#125; 另外可以看出很多属性都是用volatile进行修饰的，也就是为了保证内存可见性。 这个Node内部类与HashMap中定义的Node类很相似，但是有一些差别 它对value和next属性设置了volatile同步锁 它不允许调用setValue方法直接改变Node的value域 它增加了find方法辅助map.get()方法 TreeNodeNode的子类，红黑树节点，当Node链表过长时，会转换成红黑树。 位于 ConcurrentHashMap 类的 2653行 或 搜索 / —————- TreeNodes ————– / 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455// Nodes for use in TreeBins，链表&gt;8，才可能转为TreeNode. // HashMap的TreeNode继承至LinkedHashMap.Entry；而这里继承至自己实现的Node，将带有next指针，便于treebin访问。 static final class TreeNode&lt;K,V&gt; extends Node&lt;K,V&gt; &#123; TreeNode&lt;K,V&gt; parent; // red-black tree links TreeNode&lt;K,V&gt; left; TreeNode&lt;K,V&gt; right; TreeNode&lt;K,V&gt; prev; // needed to unlink next upon deletion boolean red; TreeNode(inthash, K key, V val, Node&lt;K,V&gt; next, TreeNode&lt;K,V&gt; parent) &#123; super(hash, key, val, next); this.parent = parent; &#125; Node&lt;K,V&gt; find(inth, Object k) &#123; return findTreeNode(h, k, null); &#125; /** * Returns the TreeNode (or null if not found) for the given key * starting at given root. */ // 查找hash为h，key为k的节点 final TreeNode&lt;K,V&gt; findTreeNode(int h, Object k, Class&lt;?&gt; kc) &#123; if (k != null) &#123; // 比HMap增加判空 TreeNode&lt;K,V&gt; p = this; do &#123; intph, dir; K pk; TreeNode&lt;K,V&gt; q; TreeNode&lt;K,V&gt; pl = p.left, pr = p.right; if ((ph = p.hash) &gt; h) p = pl; elseif (ph &lt; h) p = pr; elseif ((pk = p.key) == k || (pk != null &amp;&amp; k.equals(pk))) returnp; elseif (pl == null) p = pr; elseif (pr == null) p = pl; elseif ((kc != null || (kc = comparableClassFor(k)) != null) &amp;&amp; (dir = compareComparables(kc, k, pk)) != 0) p = (dir &lt; 0) ? pl : pr; elseif ((q = pr.findTreeNode(h, k, kc)) != null) returnq; else p = pl; &#125; while (p != null); &#125; return null; &#125; &#125; // 和HashMap相比，这里的TreeNode相当简洁；ConcurrentHashMap链表转树时，并不会直接转，// 正如注释（Nodes for use in TreeBins）所说，只是把这些节点包装成TreeNode放到TreeBin中，// 再由TreeBin来转化红黑树。 树节点，继承于承载数据的Node类。而红黑树的操作是针对TreeBin类的，从该类的注释也可以看出，也就是TreeBin会将TreeNode进行再一次封装 TreeBin位于 ConcurrentHashMap 类的 2709 行 或 搜索 / —————- TreeBins ————– / 12345678910111213141516171819202122232425262728293031323334353637383940414243444546// TreeBin用于封装维护TreeNode，包含putTreeVal、lookRoot、UNlookRoot、remove、// balanceInsetion、balanceDeletion等方法，这里只分析其构造函数。当链表转树时，// 用于封装TreeNode，也就是说，ConcurrentHashMap的红黑树存放的是TreeBin，而不是treeNode。 TreeBin(TreeNode&lt;K,V&gt; b) &#123; super(TREEBIN, null, null, null);//hash值为常量TREEBIN=-2,表示roots of trees this.first = b; TreeNode&lt;K,V&gt; r = null; for (TreeNode&lt;K,V&gt; x = b, next; x != null; x = next) &#123; next = (TreeNode&lt;K,V&gt;)x.next; x.left = x.right = null; if (r == null) &#123; x.parent = null; x.red = false; r = x; &#125; else &#123; K k = x.key; inth = x.hash; Class&lt;?&gt; kc = null; for (TreeNode&lt;K,V&gt; p = r;;) &#123; intdir, ph; K pk = p.key; if ((ph = p.hash) &gt; h) dir = -1; elseif (ph &lt; h) dir = 1; elseif ((kc == null &amp;&amp; (kc = comparableClassFor(k)) == null) || (dir = compareComparables(kc, k, pk)) == 0) dir = tieBreakOrder(k, pk); TreeNode&lt;K,V&gt; xp = p; if ((p = (dir &lt;= 0) ? p.left : p.right) == null) &#123; x.parent = xp; if (dir &lt;= 0) xp.left = x; else xp.right = x; r = balanceInsertion(r, x); break; &#125; &#125; &#125; &#125; this.root = r; assert checkInvariants(root); &#125; 这个类并不负责包装用户的key、value信息，而是包装的很多TreeNode节点。实际的ConcurrentHashMap“数组”中，存放的是TreeBin对象，而不是TreeNode对象。 threeifyBin位于 ConcurrentHashMap 类的 2611 行 或 搜索 “private final void treeifyBin” 1234567891011121314151617181920212223242526private final void treeifyBin(Node&lt;K,V&gt;[] tab, int index) &#123; Node&lt;K,V&gt; b; intn, sc; if (tab != null) &#123; // 数组的大小还未超过64 if ((n = tab.length) &lt; MIN_TREEIFY_CAPACITY) tryPresize(n &lt;&lt; 1); // 容量&lt;64，则table两倍扩容，不转树了 else if ((b = tabAt(tab, index)) != null &amp;&amp; b.hash &gt;= 0) &#123; synchronized (b) &#123; // 读写锁 if (tabAt(tab, index) == b) &#123; TreeNode&lt;K,V&gt; hd = null, tl = null; for (Node&lt;K,V&gt; e = b; e != null; e = e.next) &#123; TreeNode&lt;K,V&gt; p = new TreeNode&lt;K,V&gt;(e.hash, e.key, e.val, null, null); if ((p.prev = tl) == null) hd = p; else tl.next = p; tl = p; &#125; setTabAt(tab, index, new TreeBin&lt;K,V&gt;(hd)); &#125; &#125; &#125; &#125; &#125; ForwardingNode位于 ConcurrentHashMap 类的 2163 行 或 搜索 “static final class ForwardingNode” 123456789101112131415161718192021222324252627282930313233343536// A node inserted at head of bins during transfer operations.连接两个table // 并不是我们传统的包含key-value的节点，只是一个标志节点，并且指向nextTable，提供find方法而已。// 生命周期：仅存活于扩容操作且bin不为null时，一定会出现在每个bin的首位。 static final class ForwardingNode&lt;K,V&gt; extends Node&lt;K,V&gt; &#123; final Node&lt;K,V&gt;[] nextTable; ForwardingNode(Node&lt;K,V&gt;[] tab) &#123; super(MOVED, null, null, null); // 此节点hash=-1，key、value、next均为null this.nextTable = tab; &#125; Node&lt;K,V&gt; find(int h, Object k) &#123; // 查nextTable节点，outer避免深度递归 outer: for (Node&lt;K,V&gt;[] tab = nextTable;;) &#123; Node&lt;K,V&gt; e; intn; if (k == null || tab == null || (n = tab.length) == 0 || (e = tabAt(tab, (n - 1) &amp; h)) == null) returnnull; for (;;) &#123; // CAS算法多和死循环搭配！直到查到或null int eh; K ek; if ((eh = e.hash) == h &amp;&amp; ((ek = e.key) == k || (ek != null &amp;&amp; k.equals(ek)))) returne; if (eh &lt; 0) &#123; if (e instanceof ForwardingNode) &#123; tab = ((ForwardingNode&lt;K,V&gt;)e).nextTable; continue outer; &#125; else return e.find(h, k); &#125; if ((e = e.next) == null) return null; &#125; &#125; &#125; &#125; 在扩容时才会出现的特殊节点，其key,value,hash全部为null。并拥有nextTable指针引用新的table数组。 Traverser123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102static class Traverser&lt;K,V&gt; &#123; Node&lt;K,V&gt;[] tab; //下一个要访问的entry Node&lt;K,V&gt; next; //发现forwardingNode时，保存当前tab相关信息 TableStack&lt;K,V&gt; stack, spare; //下一个要访问的hash桶索引 int index; //当前正在访问的初始tab的hash桶索引 int baseIndex; //初始tab的hash桶索引边界 int baseLimit; //初始tab的长度 final int baseSize; Traverser(Node&lt;K,V&gt;[] tab, int size, int index, int limit) &#123; this.tab = tab; this.baseSize = size; this.baseIndex = this.index = index; this.baseLimit = limit; this.next = null; &#125; // 如果有可能，返回下一个有效节点，否则返回null。 final Node&lt;K,V&gt; advance() &#123; Node&lt;K,V&gt; e; //获取Node链表的下一个元素e if ((e = next) != null) e = e.next; for (;;) &#123; Node&lt;K,V&gt;[] t; int i, n; // e不为空，返回e if (e != null) return next = e; //e为空，说明此链表已经遍历完成，准备遍历下一个hash桶 if (baseIndex &gt;= baseLimit || (t = tab) == null || (n = t.length) &lt;= (i = index) || i &lt; 0) //到达边界，返回null return next = null; //获取下一个hash桶对应的node链表的头节点 if ((e = tabAt(t, i)) != null &amp;&amp; e.hash &lt; 0) &#123; //转发节点,说明此hash桶中的节点已经迁移到了nextTable if (e instanceof ForwardingNode) &#123; tab = ((ForwardingNode&lt;K,V&gt;)e).nextTable; e = null; //保存当前tab的遍历状态 pushState(t, i, n); continue; &#125; //红黑树 else if (e instanceof TreeBin) e = ((TreeBin&lt;K,V&gt;)e).first; else e = null; &#125; if (stack != null) // 此时遍历的是迁移目标nextTable,尝试回退到源table， // 继续遍历源table中的节点 recoverState(n); else if ((index = i + baseSize) &gt;= n) //初始tab的hash桶索引+1 ，即遍历下一个hash桶 index = ++baseIndex; &#125; &#125; // 在遇到转发节点时保存遍历状态。 private void pushState(Node&lt;K,V&gt;[] t, int i, int n) &#123; TableStack&lt;K,V&gt; s = spare; // reuse if possible if (s != null) spare = s.next; else s = new TableStack&lt;K,V&gt;(); s.tab = t; s.length = n; s.index = i; s.next = stack; stack = s; &#125;// 可能会弹出遍历状态 private void recoverState(int n) &#123; TableStack&lt;K,V&gt; s; int len; // (s = stack) != null :stack不空，说明此时遍历的是nextTable // (index += (len = s.length)) &gt;= n: 确保了按照index, //index+tab.length的顺序遍历nextTable,条件成立表示nextTable已经遍历完毕 //nextTable中的桶遍历完毕 while ((s = stack) != null &amp;&amp; (index += (len = s.length)) &gt;= n) &#123; //弹出tab，获取tab的遍历状态，开始遍历tab中的桶 n = len; index = s.index; tab = s.tab; s.tab = null; TableStack&lt;K,V&gt; next = s.next; s.next = spare; // save for reuse stack = next; spare = s; &#125; if (s == null &amp;&amp; (index += baseSize) &gt;= n) index = ++baseIndex; &#125;&#125; tryPresize(扩容)协调多个线程如何调用transfer方法进行hash桶的迁移（addCount，helpTransfer 方法中也有类似的逻辑） tryPresize在putAll以及treeifyBin中调用 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061private final void tryPresize(int size) &#123; //计算扩容的目标size // 给定的容量若&gt;=MAXIMUM_CAPACITY的一半，直接扩容到允许的最大值，否则调用函数扩容 int c = (size &gt;= (MAXIMUM_CAPACITY &gt;&gt;&gt; 1)) ? MAXIMUM_CAPACITY : tableSizeFor(size + (size &gt;&gt;&gt; 1) + 1); int sc; while ((sc = sizeCtl) &gt;= 0) &#123; //没有正在初始化或扩容，或者说表还没有被初始化 Node&lt;K,V&gt;[] tab = table; int n; //tab没有初始化 if(tab == null || (n = tab.length) == 0) &#123; n = (sc &gt; c) ? sc : c; // 扩容阀值取较大者 // 期间没有其他线程对表操作，则CAS将SIZECTL状态置为-1，表示正在进行初始化 //初始化之前，CAS设置sizeCtl=-1 if (U.compareAndSwapInt(this, SIZECTL, sc, -1)) &#123; try &#123; if (table == tab) &#123; @SuppressWarnings("unchecked") Node&lt;K,V&gt;[] nt = (Node&lt;K,V&gt;[])new Node&lt;?,?&gt;[n]; table = nt; //sc=0.75n,相当于扩容阈值 sc = n - (n &gt;&gt;&gt; 2); //无符号右移2位，此即0.75*n &#125; &#125; finally &#123; // 此时并没有通过CAS赋值，因为其他想要执行初始化的线程， // 发现sizeCtl=-1，就直接返回，从而确保任何情况， // 只会有一个线程执行初始化操作。 sizeCtl = sc; &#125; &#125; &#125;// 若欲扩容值不大于原阀值，或现有容量&gt;=最值，什么都不用做了 //目标扩容size小于扩容阈值，或者容量超过最大限制时，不需要扩容 else if (c &lt;= sc || n &gt;= MAXIMUM_CAPACITY) break; //扩容 else if (tab == table) &#123; int rs = resizeStamp(n); // sc&lt;0表示，已经有其他线程正在扩容 if (sc &lt; 0) &#123; Node&lt;K,V&gt;[] nt;//RESIZE_STAMP_SHIFT=16,MAX_RESIZERS=2^15-1 // 1. (sc &gt;&gt;&gt; RESIZE_STAMP_SHIFT) != rs ：扩容线程数 &gt; MAX_RESIZERS-1 // 2. sc == rs + 1 和 sc == rs + MAX_RESIZERS ：表示什么？？？ // 3. (nt = nextTable) == null ：表示nextTable正在初始化 // transferIndex &lt;= 0 ：表示所有hash桶均分配出去 if ((sc &gt;&gt;&gt; RESIZE_STAMP_SHIFT) != rs || sc == rs + 1 || sc == rs + MAX_RESIZERS || (nt = nextTable) == null || transferIndex &lt;= 0) //如果不需要帮其扩容，直接返回 break; //CAS设置sizeCtl=sizeCtl+1 if (U.compareAndSwapInt(this, SIZECTL, sc, sc + 1)) //帮其扩容 transfer(tab, nt); &#125; // 第一个执行扩容操作的线程，将sizeCtl设置为： // (resizeStamp(n) &lt;&lt; RESIZE_STAMP_SHIFT) + 2) else if (U.compareAndSwapInt(this, SIZECTL, sc, (rs &lt;&lt; RESIZE_STAMP_SHIFT) + 2)) transfer(tab, null); &#125; &#125; &#125; 123456789private static final int tableSizeFor(int c)&#123;//和HashMap一样,返回&gt;=n的最小2的自然数幂 int n = c - 1; n |= n &gt;&gt;&gt; 1; n |= n &gt;&gt;&gt; 2; n |= n &gt;&gt;&gt; 4; n |= n &gt;&gt;&gt; 8; n |= n &gt;&gt;&gt; 16; return (n &lt; 0) ? 1 : (n &gt;= MAXIMUM_CAPACITY) ? MAXIMUM_CAPACITY : n + 1; &#125; spread 重新哈希spread()重哈希，以减小Hash冲突。我们知道对于一个hash表来说，hash值分散的不够均匀的话会大大增加哈希冲突的概率，从而影响到hash表的性能。因此通过spread方法进行了一次重hash从而大大减小哈希冲突的可能性。spread方法为： 123static final int spread(int h) &#123; return (h ^ (h &gt;&gt;&gt; 16)) &amp; HASH_BITS;&#125; 该方法主要是将key的hashCode的低16位于高16位进行异或运算，这样不仅能够使得hash值能够分散能够均匀减小hash冲突的概率，另外另外只用到了异或运算，在性能开销上也能兼顾，做到平衡的trade-off。 get(查找)1234567891011121314151617181920212223242526272829303132public V get(Object key) &#123; Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; e, p; int n, eh; K ek; // 1. 重hash int h = spread(key.hashCode()); // 2. table[i]桶节点的key与查找的key相同，则直接返回 if ((tab = table) != null &amp;&amp; (n = tab.length) &gt; 0 &amp;&amp; // 唯一一处volatile读操作 (e = tabAt(tab, (n - 1) &amp; h)) != null) &#123; // 注意：因为容器大小为2的次方，所以 h mod n = h &amp; (n -1) if ((eh = e.hash) == h) &#123;// 如果hash值相等 // 检查第一个Node if ((ek = e.key) == key || (ek != null &amp;&amp; key.equals(ek))) return e.val; &#125; // hash为负表示是扩容中的ForwardingNode节点 // 直接调用ForwardingNode的find方法(可以是代理到扩容中的nextTable) // 3. 当前节点hash小于0说明为树节点，在红黑树中查找即可 else if (eh &lt; 0) return (p = e.find(h, key)) != null ? p.val : null; // 遍历链表，对比key值 // 通过next指针，逐一查找 while ((e = e.next) != null) &#123; //4. 从链表中查找，查找到则返回该节点的value，否则就返回null即可 if (e.hash == h &amp;&amp; ((ek = e.key) == key || (ek != null &amp;&amp; key.equals(ek)))) return e.val; &#125; &#125; return null; &#125; 代码的逻辑请看注释，首先先看当前的hash桶数组节点即table[i]是否为查找的节点，若是则直接返回；若不是，则继续再看当前是不是树节点？通过看节点的hash值是否为小于0，如果小于0则为树节点。如果是树节点在红黑树中查找节点；如果不是树节点，那就只剩下为链表的形式的一种可能性了，就向后遍历查找节点，若查找到则返回节点的value即可，若没有找到就返回null。 这个 get 请求，我们需要 cas 来保证变量的原子性。如果 tab[i] 正被锁住，那么 CAS 就会失败，失败之后就会不断的重试。这也保证了在高并发情况下不会出错。 我们来分析一下哪些情况会导致 get 在并发的情况下可能取不到值。 一个线程在 get 的时候，另一个线程在对同一个 key 的 node 进行 remove 操作 一个线程在 get 的时候，另一个线程正在重排 table 。可能导致旧 table 取不到值 那么本质是，我在get的时候，有其他线程在对同一桶的链表或树进行修改。那么get是怎么保证同步性的呢？我们看到e = tabAt(tab, (n - 1) &amp; h)) != null，在看下tablAt到底是干嘛的： 123static final &lt;K,V&gt; Node&lt;K,V&gt; tabAt(Node&lt;K,V&gt;[] tab, int i) &#123; return (Node&lt;K,V&gt;)U.getObjectVolatile(tab, ((long)i &lt;&lt; ASHIFT) + ABASE);&#125; 它是对tab[i]进行原子性的读取，因为我们知道putVal等对table的桶操作是有加锁的，那么一般情况下我们对桶的读也是要加锁的，但是我们这边为什么不需要加锁呢？因为我们用了Unsafe的getObjectVolatile，因为table是volatile类型，所以对tab[i]的原子请求也是可见的。因为如果同步正确的情况下，根据happens-before原则，对volatile域的写入操作happens-before于每一个后续对同一域的读操作。所以不管其他线程对table链表或树的修改，都对get读取可见。用一张图说明，协调读-写线程可见示意图： jdk7是没有用到CAS操作和Unsafe类的，下面是jdk7的get方法： 12345678910111213141516V get(Object key, int hash) &#123; if(count != 0) &#123; // 首先读 count 变量 HashEntry&lt;K,V&gt; e = getFirst(hash); while(e != null) &#123; if(e.hash == hash &amp;&amp; key.equals(e.key)) &#123; V v = e.value; if(v != null) return v; // 如果读到 value 域为 null，说明发生了重排序，加锁后重新读取 return readValueUnderLock(e); &#125; e = e.next; &#125; &#125; return null; &#125; 为什么我们在get的时候需要判断count不等于0呢？如果是在HashMap的源码中是没有这个判断的，不用判断不是也是可以的吗？这个就是用到线程安全发布情况下happens-before原则之volatile变量法则：对volatile域的写入操作happens-before于每一个后续对同一域的读操作，看下面的示意图： tabAt以 volatile 读的方式读取 table 数组中的元素 12345// 这边为什么i要等于((long)i &lt;&lt; ASHIFT) + ABASE呢,计算偏移量static final &lt;K,V&gt; Node&lt;K,V&gt; tabAt(Node&lt;K,V&gt;[] tab, int i) &#123; // Key对应的数组元素的可见性，由Unsafe的getObjectVolatile方法保证。 return (Node&lt;K,V&gt;)U.getObjectVolatile(tab, ((long)i &lt;&lt; ASHIFT) + ABASE);&#125; tabAt 方法用来获取table数组中索引为i的Node元素。 put/putValputVal是将一个新key-value mapping插入到当前ConcurrentHashMap的关键方法。 此方法的具体流程如下图： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091final V putVal(K key, V value, boolean onlyIfAbsent) &#123; // 不允许 key 和 value 为空 if (key == null || value == null) throw new NullPointerException(); // 1.计算 key 的 hash 值(计算新节点的hash值) int hash = spread(key.hashCode()); // 返回 (h^(h&gt;&gt;&gt;16))&amp;HASH_BITS int binCount = 0; // 获取当前table，进入死循环,直到插入成功！ for (Node&lt;K,V&gt;[] tab = table;;) &#123; Node&lt;K,V&gt; f; int n, i, fh; // 2. 如果当前 table 还没初始化先调用 initTable 方法将 tab 进行初始化 if (tab == null || (n = tab.length) == 0) tab = initTable(); // 如果table为空，执行初始化，也即是延迟初始化 // 3. tab中索引为i的位置的元素为null,则直接使用 CAS 将值插入即可 // 如果bin为空，则采用cas算法赋值，无需加锁 else if ((f = tabAt(tab, i = (n - 1) &amp; hash)) == null) &#123; if (casTabAt(tab, i, null,new Node&lt;K,V&gt;(hash, key, value, null))) // 直接设置为桶首节点成功，退出死循环（出口之一） break; &#125; // 4. 当前正在扩容 // 当前桶首节点正在特殊的扩容状态下，当前线程尝试参与扩容 // 然后重新进入死循环 //f.hash == MOVED 表示为：ForwardingNode，说明其他线程正在扩容 else if ((fh = f.hash) == MOVED) // MOVED = -1 tab = helpTransfer(tab, f); // 当发现其他线程扩容时，帮其扩容 // 通过桶首节点，将新节点加入table else &#123; V oldVal = null; // 获取桶首节点实例对象锁，进入临界区进行添加操作 synchronized (f) &#123; // 再判断以此f是否仍是第一个Node，如果不是，退出临界区，重复添加操作 if (tabAt(tab, i) == f) &#123; //5. 当前为链表，在链表中插入新的键值对 if (fh &gt;= 0) &#123; // 桶首节点hash值&gt;0，表示为链表 binCount = 1; for (Node&lt;K,V&gt; e = f;; ++binCount) &#123; K ek; // 找到hash值相同的key,覆盖旧值即可 if (e.hash == hash &amp;&amp; ((ek = e.key) == key || (ek != null &amp;&amp; key.equals(ek)))) &#123; oldVal = e.val; // 仅 putIfAbsent() 方法中的 onlyIfAbsend 为 true; if (!onlyIfAbsent) // putIfAbsend() 包含 key 则返回 get ,否则 put 并返回 e.val = value; break; &#125; Node&lt;K,V&gt; pred = e; //如果到链表末尾仍未找到，则直接将新值插入到链表末尾即可 if ((e = e.next) == null) &#123; pred.next = new Node&lt;K,V&gt;(hash, key, value, null); break; &#125; &#125; &#125; // 桶首节点为Node子类型TreeBin，表示为红黑树 // 6.当前为红黑树，将新的键值对插入到红黑树中 else if (f instanceof TreeBin) &#123; Node&lt;K,V&gt; p; binCount = 2; // 调用putTreeVal方法，插入新值 if ((p = ((TreeBin&lt;K,V&gt;)f).putTreeVal(hash, key, value)) != null) &#123; // key已经存在，则替换 oldVal = p.val; if (!onlyIfAbsent) p.val = value; &#125; &#125; &#125; &#125; // 7.插入完键值对后再根据实际大小看是否需要转换成红黑树 if (binCount != 0) &#123; if (binCount &gt;= TREEIFY_THRESHOLD) // 插入新节点后，达到链表转换红黑树阈值，则执行转换操作 // 此函数内部会判断是树化，还是扩容：tryPresize treeifyBin(tab, i); // 退出死循环（出口之二） if (oldVal != null) return oldVal; break; &#125; &#125; &#125; // 更新计算count时的base和counterCells数组 //8.对当前容量大小进行检查，如果超过了临界值（实际大小*加载因子）就需要扩容 addCount(1L, binCount); return null;&#125; 当table[i]为链表的头结点，在链表中插入新值在table[i]不为null并且不为forwardingNode时，并且当前Node f的hash值大于0（fh &gt;= 0）的话说明当前节点f为当前桶的所有的节点组成的链表的头结点。那么接下来，要想向ConcurrentHashMap插入新值的话就是向这个链表插入新值。通过synchronized (f)的方式进行加锁以实现线程安全性。往链表中插入节点的部分代码为： 12345678910111213141516171819202122if (fh &gt;= 0) &#123; binCount = 1; for (Node&lt;K,V&gt; e = f;; ++binCount) &#123; K ek; // 找到hash值相同的key,覆盖旧值即可 if (e.hash == hash &amp;&amp; ((ek = e.key) == key || (ek != null &amp;&amp; key.equals(ek)))) &#123; oldVal = e.val; if (!onlyIfAbsent) e.val = value; break; &#125; Node&lt;K,V&gt; pred = e; if ((e = e.next) == null) &#123; //如果到链表末尾仍未找到，则直接将新值插入到链表末尾即可 pred.next = new Node&lt;K,V&gt;(hash, key, value, null); break; &#125; &#125;&#125; 这部分代码很好理解，就是两种情况：1. 在链表中如果找到了与待插入的键值对的key相同的节点，就直接覆盖即可；2. 如果直到找到了链表的末尾都没有找到的话，就直接将待插入的键值对追加到链表的末尾即可。 当table[i]为红黑树的根节点，在红黑树中插入新值按照之前的数组+链表的设计方案，这里存在一个问题，即使负载因子和Hash算法设计的再合理，也免不了会出现拉链过长的情况，一旦出现拉链过长，甚至在极端情况下，查找一个节点会出现时间复杂度为O(n)的情况，则会严重影响ConcurrentHashMap的性能，于是，在JDK1.8版本中，对数据结构做了进一步的优化，引入了红黑树。而当链表长度太长（默认超过8）时，链表就转换为红黑树，利用红黑树快速增删改查的特点提高ConcurrentHashMap的性能，其中会用到红黑树的插入、删除、查找等算法。当table[i]为红黑树的树节点时的操作为： 12345678910if (f instanceof TreeBin) &#123; Node&lt;K,V&gt; p; binCount = 2; if ((p = ((TreeBin&lt;K,V&gt;)f).putTreeVal(hash, key, value)) != null) &#123; oldVal = p.val; if (!onlyIfAbsent) p.val = value; &#125;&#125; 首先在if中通过f instanceof TreeBin判断当前table[i]是否是树节点，这下也正好验证了我们在最上面介绍时说的TreeBin会对TreeNode做进一步封装，对红黑树进行操作的时候针对的是TreeBin而不是TreeNode。这段代码很简单，调用putTreeVal方法完成向红黑树插入新节点，同样的逻辑，如果在红黑树中存在于待插入键值对的Key相同（hash值相等并且equals方法判断为true）的节点的话，就覆盖旧值，否则就向红黑树追加新节点。 当table[i]为红黑树的根节点，在红黑树中插入新值。按照之前的数组+链表的设计方案，这里存在一个问题，即使负载因子和Hash算法设计的再合理，也免不了会出现拉链过长的情况，一旦出现拉链过长，甚至在极端情况下，查找一个节点会出现时间复杂度为O(n)的情况，则会严重影响ConcurrentHashMap的性能，于是，在JDK1.8版本中，对数据结构做了进一步的优化，引入了红黑树。而当链表长度太长（默认超过8）时，链表就转换为红黑树，利用红黑树快速增删改查的特点提高ConcurrentHashMap的性能，其中会用到红黑树的插入、删除、查找等算法。当table[i]为红黑树的树节点时的操作为： 123456789if (f instanceof TreeBin) &#123; Node&lt;K,V&gt; p; binCount = 2; if ((p = ((TreeBin&lt;K,V&gt;)f).putTreeVal(hash, key,value)) != null) &#123; oldVal = p.val; if (!onlyIfAbsent) p.val = value; &#125;&#125; 首先在if中通过f instanceof TreeBin判断当前table[i]是否是树节点，这下也正好验证了我们在最上面介绍时说的TreeBin会对TreeNode做进一步封装，对红黑树进行操作的时候针对的是TreeBin而不是TreeNode。这段代码很简单，调用putTreeVal方法完成向红黑树插入新节点，同样的逻辑，如果在红黑树中存在于待插入键值对的Key相同（hash值相等并且equals方法判断为true）的节点的话，就覆盖旧值，否则就向红黑树追加新节点。 根据当前节点个数进行调整当完成数据新节点插入之后，会进一步对当前链表大小进行调整，这部分代码为： 1234567if (binCount != 0) &#123; if (binCount &gt;= TREEIFY_THRESHOLD) treeifyBin(tab, i); if (oldVal != null) return oldVal; break;&#125; 很容易理解，如果当前链表节点个数大于等于8（TREEIFY_THRESHOLD）的时候，就会调用treeifyBin方法将tabel[i]（第i个散列桶）拉链转换成红黑树。 关于Put方法的逻辑就基本说的差不多了，现在来做一些总结： 整体流程： 首先对于每一个放入的值，首先利用spread方法对key的hashcode进行一次hash计算，由此来确定这个值在 table中的位置； 如果当前table数组还未初始化，先将table数组进行初始化操作； 如果这个位置是null的，那么使用CAS操作直接放入； 如果这个位置存在结点，说明发生了hash碰撞，首先判断这个节点的类型。如果该节点fh==MOVED(代表forwardingNode,数组正在进行扩容)的话，说明正在进行扩容； 如果是链表节点（fh&gt;0）,则得到的结点就是hash值相同的节点组成的链表的头节点。需要依次向后遍历确定这个新加入的值所在位置。如果遇到hash值与key值都与新加入节点是一致的情况，则只需要更新value值即可。否则依次向后遍历，直到链表尾插入这个结点； 如果这个节点的类型是TreeBin的话，直接调用红黑树的插入方法进行插入新的节点； 插入完节点之后再次检查链表长度，如果长度大于8，就把这个链表转换成红黑树； 对当前容量大小进行检查，如果超过了临界值（实际大小*加载因子）就需要扩容。 该流程中，可以细细品味的环节有： - 初始化方法 initTable - 扩容方法 transfer (在多线程扩容方法 helpTransfer 中被调用) initTableinitTable方法允许多线程同时进入，但只有一个线程可以完成table的初始化，其他线程都会通过yield方法让出cpu。 12345678910111213141516171819202122232425262728293031323334353637private final Node&lt;K,V&gt;[] initTable() &#123; Node&lt;K,V&gt;[] tab; int sc; while ((tab = table) == null || tab.length == 0) &#123; // 前文提及sizeCtl是重要的控制变量 // sizeCtl = -1 表示正在初始化 if ((sc = sizeCtl) &lt; 0) // 已经有其他线程在执行初始化，则主动让出cpu // 1. 保证只有一个线程正在进行初始化操作 Thread.yield(); // 利用CAS操作设置sizeCtl为-1 // 设置成功表示当前线程为执行初始化的唯一线程 // 此处进入临界区 else if (U.compareAndSwapInt(this, SIZECTL, sc, -1)) &#123; try &#123; // 由于让出cpu的线程也会后续进入该临界区 // 需要进行再次确认table是否为null if ((tab = table) == null || tab.length == 0) &#123; // 2. 得出数组的大小 int n = (sc &gt; 0) ? sc : DEFAULT_CAPACITY; @SuppressWarnings("unchecked") // 3. 这里才真正的初始化数组，即分配Node数组 Node&lt;K,V&gt;[] nt = (Node&lt;K,V&gt;[])new Node&lt;?,?&gt;[n]; table = tab = nt; // 默认负载为0.75 // 4. 计算数组中可用的大小：实际大小n*0.75（加载因子） sc = n - (n &gt;&gt;&gt; 2); &#125; &#125; finally &#123; sizeCtl = sc; &#125; // 退出死循环的唯一出口 break; &#125; &#125; return tab;&#125; 代码的逻辑请见注释，有可能存在一个情况是多个线程同时走到这个方法中，为了保证能够正确初始化，在第1步中会先通过if进行判断，若当前已经有一个线程正在初始化即sizeCtl值变为-1，这个时候其他线程在If判断为true从而调用Thread.yield()让出CPU时间片。正在进行初始化的线程会调用U.compareAndSwapInt方法将sizeCtl改为-1即正在初始化的状态。另外还需要注意的事情是，在第四步中会进一步计算数组中可用的大小即为数组实际大小n乘以加载因子0.75.可以看看这里乘以0.75是怎么算的，0.75为四分之三，这里n - (n &gt;&gt;&gt; 2)是不是刚好是n-(1/4)n=(3/4)n，挺有意思的吧:)。如果选择是无参的构造器的话，这里在new Node数组的时候会使用默认大小为DEFAULT_CAPACITY（16），然后乘以加载因子0.75为12，也就是说数组的可用大小为12。 casTabAt(原子操作方法)以 CAS 的方式，将元素插入到 table 数组 123456789101112/* *这边为什么i要等于((long)i &lt;&lt; ASHIFT) + ABASE呢,计算偏移量 *ASHIFT是指tab[i]中第i个元素在相对于数组第一个元素的偏移量，而ABASE就算第一数组的内存素的偏移地址 *所以呢，((long)i &lt;&lt; ASHIFT) + ABASE就算i最后的地址 * 那么compareAndSwapObject的作用就算tab[i]和c比较，如果相等就tab[i]=v否则tab[i]=c; */ // 利用CAS算法设置i位置上的Node节点（将c和table[i]比较，相同则插入v）。 static final &lt;K,V&gt; boolean casTabAt(Node&lt;K,V&gt;[] tab, int i, Node&lt;K,V&gt; c, Node&lt;K,V&gt; v) &#123; //原子的执行如下逻辑：如果tab[i]==c,则设置tab[i]=v，并返回ture.否则返回false return U.compareAndSwapObject(tab, ((long)i &lt;&lt; ASHIFT) + ABASE, c, v); &#125; 利用CAS操作设置table数组中索引为i的元素 setTabAt以 valatile 写的方式，将元素插入 table 数组 123static final &lt;K,V&gt; void setTabAt(Node&lt;K,V&gt;[] tab, int i, Node&lt;K,V&gt; v) &#123; U.putObjectVolatile(tab, ((long)i &lt;&lt; ASHIFT) + ABASE, v);&#125; 该方法用来设置table数组中索引为i的元素 实例构造器方法在使用ConcurrentHashMap第一件事自然而然就是new 出来一个ConcurrentHashMap对象，一共提供了如下几个构造器方法： 12345678910// 1. 构造一个空的map，即table数组还未初始化，初始化放在第一次插入数据时，默认大小为16ConcurrentHashMap()// 2. 给定map的大小ConcurrentHashMap(int initialCapacity) // 3. 给定一个mapConcurrentHashMap(Map&lt;? extends K, ? extends V&gt; m)// 4. 给定map的大小以及加载因子ConcurrentHashMap(int initialCapacity, float loadFactor)// 5. 给定map大小，加载因子以及并发度（预计同时操作数据的线程）ConcurrentHashMap(int initialCapacity,float loadFactor, int concurrencyLevel) ConcurrentHashMap一共给我们提供了5中构造器方法，具体使用请看注释，我们来看看第2种构造器，传入指定大小时的情况，该构造器源码为： 1234567891011public ConcurrentHashMap(int initialCapacity) &#123; //1. 小于0直接抛异常 if (initialCapacity &lt; 0) throw new IllegalArgumentException(); //2. 判断是否超过了允许的最大值，超过了话则取最大值，否则再对该值进一步处理 int cap = ((initialCapacity &gt;= (MAXIMUM_CAPACITY &gt;&gt;&gt; 1)) ? MAXIMUM_CAPACITY : tableSizeFor(initialCapacity + (initialCapacity &gt;&gt;&gt; 1) + 1)); //3. 赋值给sizeCtl this.sizeCtl = cap;&#125; 这段代码的逻辑请看注释，很容易理解，如果小于0就直接抛出异常，如果指定值大于了所允许的最大值的话就取最大值，否则，在对指定值做进一步处理。最后将cap赋值给sizeCtl,关于sizeCtl的说明请看上面的说明，当调用构造器方法之后，sizeCtl的大小应该就代表了ConcurrentHashMap的大小，即table数组长度。tableSizeFor做了哪些事情了？源码为： tableSizeFor12345678910private static final int tableSizeFor(int c) &#123; int n = c - 1; n |= n &gt;&gt;&gt; 1; n |= n &gt;&gt;&gt; 2; n |= n &gt;&gt;&gt; 4; n |= n &gt;&gt;&gt; 8; n |= n &gt;&gt;&gt; 16; return (n &lt; 0) ? 1 : (n &gt;= MAXIMUM_CAPACITY) ? MAXIMUM_CAPACITY : n + 1;&#125; 通过注释就很清楚了，该方法会将调用构造器方法时指定的大小转换成一个2的幂次方数，也就是说ConcurrentHashMap的大小一定是2的幂次方，比如，当指定大小为18时，为了满足2的幂次方特性，实际上concurrentHashMapd的大小为2的5次方（32）。另外，需要注意的是，调用构造器方法的时候并未构造出table数组（可以理解为ConcurrentHashMap的数据容器），只是算出table数组的长度，当第一次向ConcurrentHashMap插入数据的时候才真正的完成初始化创建table数组的工作。 helpTransfer(协助扩容)123456789101112131415161718192021// 协助扩容方法。多线程下，当前线程检测到其他线程正进行扩容操作，则协助其一起扩容；（只有这种情况会被调用）从某种程度上说，其“优先级”很高，只要检测到扩容，就会放下其他工作，先扩容。 // 调用之前，nextTable一定已存在。 final Node&lt;K,V&gt;[] helpTransfer(Node&lt;K,V&gt;[] tab, Node&lt;K,V&gt; f) &#123; Node&lt;K,V&gt;[] nextTab; intsc; if (tab != null &amp;&amp; (finstanceof ForwardingNode) &amp;&amp; (nextTab = ((ForwardingNode&lt;K,V&gt;)f).nextTable) != null) &#123; intrs = resizeStamp(tab.length); //标志位 while (nextTab == nextTable &amp;&amp; table == tab &amp;&amp; (sc = sizeCtl) &lt; 0) &#123; if ((sc &gt;&gt;&gt; RESIZE_STAMP_SHIFT) != rs || sc == rs + 1 || sc == rs + MAX_RESIZERS || transferIndex &lt;= 0) break; if (U.compareAndSwapInt(this, SIZECTL, sc, sc + 1)) &#123; transfer(tab, nextTab);//调用扩容方法，直接进入复制阶段 break; &#125; &#125; return nextTab; &#125; return table; &#125; addCount在put方法结尾处调用了addCount方法，把当前ConcurrentHashMap的元素个数+1这个方法一共做了两件事,更新baseCount的值，检测是否进行扩容。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849private final void addCount(long x, int check) &#123; CounterCell[] as; long b, s; //利用CAS方法更新baseCount的值 if ((as = counterCells) != null || !U.compareAndSwapLong(this, BASECOUNT, b = baseCount, s = b + x)) &#123;// 1 CounterCell a; long v; int m; boolean uncontended = true; if (as == null || (m = as.length - 1) &lt; 0 || (a = as[ThreadLocalRandom.getProbe() &amp; m]) == null || !(uncontended = U.compareAndSwapLong(a, CELLVALUE, v = a.value, v + x))) &#123; // 多线程 CAS 发生失败的时候执行 fullAddCount(x, uncontended); // 2 return; &#125; if (check &lt;= 1) return; s = sumCount(); &#125; //如果check值大于等于0 则需要检验是否需要进行扩容操作 if (check &gt;= 0) &#123; Node&lt;K,V&gt;[] tab, nt; int n, sc; // 当条件满足的时候开始扩容 while (s &gt;= (long)(sc = sizeCtl) &amp;&amp; (tab = table) != null &amp;&amp; (n = tab.length) &lt; MAXIMUM_CAPACITY) &#123; int rs = resizeStamp(n); // 如果小于0 说明已经有线程在进行扩容了 if (sc &lt; 0) &#123; // 一下的情况说明已经有在扩容或者多线程进行了扩容，其他线程直接 break 不要进入扩容 if ((sc &gt;&gt;&gt; RESIZE_STAMP_SHIFT) != rs || sc == rs + 1 || sc == rs + MAX_RESIZERS || (nt = nextTable) == null || transferIndex &lt;= 0) break; // 如果已经有其他线程在执行扩容操作 // 如果相等说明已经完成，可以继续扩容 if (U.compareAndSwapInt(this, SIZECTL, sc, sc + 1)) transfer(tab, nt); &#125; // 当前线程是唯一的或是第一个发起扩容的线程 此时nextTable=null // 这个时候 sizeCtl 已经等于(rs&lt;&lt;RESIZE_STAMP_SHIFT)+2 等于一个大的负数，这边 // 加上2很巧，因为 transfer 后面对 sizeCtl-- 操作的时候，最多只能减两个就结束 else if (U.compareAndSwapInt(this, SIZECTL, sc, (rs &lt;&lt; RESIZE_STAMP_SHIFT) + 2)) transfer(tab, null); s = sumCount(); &#125; &#125; &#125; 看上面的注释1,每次都会对 baseCount 加1,如果并发竞争太大，那么可能导致 U.compareAndSwapLong(this,BASECOUNT,b=baseCount,s = b + x) 失败,那么为了提高高并发的时候 baseCount 可见性的失败的问题,又避免一直重试，这样性能会有很大的影响,那么在 jdk 8的时候是有引入一个类 Striped64 ,其中 LongAdder 和 DoubleAdder 就是对这个类的实现。这两个方法都是为了解决高并发场景而生的，是 AtomicLong 的加强版,AtomicLong 在高并发场景性能会比 LongAdder 差。但是 LongAdder 的空间复杂度会高点。 fullAddCount123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293private final void fullAddCount(long x, boolean wasUncontended) &#123; int h; // 获取当前线程的 probe 值作为 hash 值,如果0则强制初始化当前线程的 Probe 值， // 初始化 probe 值不为 0 if ((h = ThreadLocalRandom.getProbe()) == 0) &#123; ThreadLocalRandom.localInit(); // force initialization h = ThreadLocalRandom.getProbe(); // 设置未竞争标记为true wasUncontended = true; &#125; boolean collide = false; // True if last slot nonempty for (;;) &#123; CounterCell[] as; CounterCell a; int n; long v; if ((as = counterCells) != null &amp;&amp; (n = as.length) &gt; 0) &#123; if ((a = as[(n - 1) &amp; h]) == null) &#123; // Try to attach new Cell 如果当前没有 CounterCell 就创建一个 if (cellsBusy == 0) &#123; CounterCell r = new CounterCell(x); // Optimistic create if (cellsBusy == 0 &amp;&amp; // 这边加上 cellsBusy 锁 U.compareAndSwapInt(this, CELLSBUSY, 0, 1)) &#123; boolean created = false; try &#123; // Recheck under lock CounterCell[] rs; int m, j; if ((rs = counterCells) != null &amp;&amp; (m = rs.length) &gt; 0 &amp;&amp; rs[j = (m - 1) &amp; h] == null) &#123; rs[j] = r; created = true; &#125; &#125; finally &#123; // 释放 cellsBusy 锁定，让其他线程可以进来 cellsBusy = 0; &#125; if (created) break; continue; // Slot is now non-empty &#125; &#125; collide = false; &#125; // wasUncontended 为 false 说明已经发生了竞争，重置为true重新执行上面代码 else if (!wasUncontended) // CAS already known to fail wasUncontended = true; // Continue after rehash // 对 cell 的值进行累计x(1) else if (U.compareAndSwapLong(a, CELLVALUE, v = a.value, v + x)) break; // 表明 as 已经过时，说明 cells 已经初始化完成，看下面， // 重置 collide 为 false 表明已经存在竞争 else if (counterCells != as || n &gt;= NCPU) collide = false; // At max size or stale else if (!collide) collide = true; else if (cellsBusy == 0 &amp;&amp; U.compareAndSwapInt(this, CELLSBUSY, 0, 1)) &#123; try &#123; // 下面的方法主要是给 counterCells 扩容，尽可能避免冲突 if (counterCells == as) &#123;// Expand table unless stale CounterCell[] rs = new CounterCell[n &lt;&lt; 1]; for (int i = 0; i &lt; n; ++i) rs[i] = as[i]; counterCells = rs; &#125; &#125; finally &#123; cellsBusy = 0; &#125; collide = false; continue; // Retry with expanded table &#125; h = ThreadLocalRandom.advanceProbe(h); &#125; // 表明 counterCells 还没初始化，则初始化，这边用 cellsBusy 加锁 else if (cellsBusy == 0 &amp;&amp; counterCells == as &amp;&amp; U.compareAndSwapInt(this, CELLSBUSY, 0, 1)) &#123; boolean init = false; try &#123; // Initialize table if (counterCells == as) &#123; CounterCell[] rs = new CounterCell[2]; rs[h &amp; 1] = new CounterCell(x); counterCells = rs; init = true; &#125; &#125; finally &#123; cellsBusy = 0; &#125; if (init) break; &#125; // 最终如果上面的都失败就把 x 累计到 baseCount else if (U.compareAndSwapLong(this, BASECOUNT, v = baseCount, v + x)) break; // Fall back on using base &#125; &#125; 回到 addCount 来,我们每次竞争都对 baseCount 进行加 1 当达到一定的容量时，就需要对 table 进行扩容。 使用 transfer 方法。 transfer负责迁移node节点 扩容transfer方法是一个设计极为精巧的方法。通过互斥读写ForwardingNode，多线程可以协同完成扩容任务。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210private final void transfer(Node&lt;K,V&gt;[] tab, Node&lt;K,V&gt;[] nextTab) &#123; int n = tab.length, stride; //计算每次迁移的node个数（MIN_TRANSFER_STRIDE该值作为下限，以避免扩容线程过多） if ((stride = (NCPU &gt; 1) ? (n &gt;&gt;&gt; 3) / NCPU : n) &lt; MIN_TRANSFER_STRIDE) // 确保每次迁移的node个数不少于16个 stride = MIN_TRANSFER_STRIDE; // nextTab为扩容中的临时table if (nextTab == null) &#123; try &#123; //扩容一倍 @SuppressWarnings("unchecked") // 1. 新建一个 node 数组，容量为之前的两倍 Node&lt;K,V&gt;[] nt = (Node&lt;K,V&gt;[])new Node&lt;?,?&gt;[n &lt;&lt; 1]; nextTab = nt; &#125; catch (Throwable ex) &#123; // try to cope with OOME sizeCtl = Integer.MAX_VALUE; return; &#125; nextTable = nextTab; // transferIndex为扩容复制过程中的桶首节点遍历索引 // 所以从n开始，表示从后向前遍历 transferIndex = n; &#125; int nextn = nextTab.length; // ForwardingNode是Node节点的直接子类，是扩容过程中的特殊桶首节点 // 该类中没有key,value,next // hash值为特定的-1 // 附加Node&lt;K,V&gt;[] nextTable变量指向扩容中的nextTab // 在find方法中，将扩容中的查询操作导入到nextTab上 //2. 新建forwardingNode引用，在之后会用到 ForwardingNode&lt;K,V&gt; fwd = new ForwardingNode&lt;K,V&gt;(nextTab); boolean advance = true; // 循环的关键变量，判断是否已经扩容完成，完成就 return , 退出循环 boolean finishing = false; //【1】逆序迁移已经获取到的hash桶集合，如果迁移完毕，则更新transferIndex， // 获取下一批待迁移的hash桶 //【2】如果transferIndex=0，表示所以hash桶均被分配，将i置为-1， // 准备退出transfer方法 for (int i = 0, bound = 0;;) &#123; Node&lt;K,V&gt; f; int fh; // 3. 确定遍历中的索引i（更新待迁移的hash桶索引） // 循环的关键 i , i-- 操作保证了倒叙遍历数组 while (advance) &#123; int nextIndex, nextBound; // 更新迁移索引i if (--i &gt;= bound || finishing) advance = false; // transferIndex = 0表示table中所有数组元素都已经有其他线程负责扩容 // nextIndex=transferIndex=n=tab.length(默认16) else if ((nextIndex = transferIndex) &lt;= 0) &#123; // transferIndex&lt;=0表示已经没有需要迁移的hash桶， // 将i置为-1，线程准备退出 i = -1; advance = false; &#125; //cas无锁算法设置 transferIndex = transferIndex - stride // 尝试更新transferIndex，获取当前线程执行扩容复制的索引区间 // 更新成功，则当前线程负责完成索引为(nextBound，nextIndex)之间的桶首节点扩容 //当迁移完bound这个桶后，尝试更新transferIndex，获取下一批待迁移的hash桶 else if (U.compareAndSwapInt (this, TRANSFERINDEX, nextIndex, nextBound = (nextIndex &gt; stride ? nextIndex - stride : 0))) &#123; bound = nextBound; i = nextIndex - 1; advance = false; &#125; &#125; //退出transfer //4.将原数组中的元素复制到新数组中去 //4.5 for循环退出，扩容结束修改sizeCtl属性// i&lt;0 说明已经遍历完旧的数组tab;i&gt;=n什么时候有可能呢？在下面看到i=n,所以目前i最大应该是n吧// i+n&gt;=nextn,nextn=nextTab.length,所以如果满足i+n&gt;=nextn说明已经扩容完成 if (i &lt; 0 || i &gt;= n || i + n &gt;= nextn) &#123; int sc; if (finishing) &#123; // a //最后一个迁移的线程，recheck后，做收尾工作，然后退出 nextTable = null; table = nextTab; // 扩容成功，设置新sizeCtl，仍然为总大小的0.75 sizeCtl = (n &lt;&lt; 1) - (n &gt;&gt;&gt; 1); return; &#125; // 第一个扩容的线程，执行transfer方法之前，会设置 sizeCtl = // (resizeStamp(n) &lt;&lt; RESIZE_STAMP_SHIFT) + 2) // 后续帮其扩容的线程，执行transfer方法之前，会设置 sizeCtl = sizeCtl+1 // 每一个退出transfer的方法的线程，退出之前，会设置 sizeCtl = sizeCtl-1 // 那么最后一个线程退出时： // 必然有sc == (resizeStamp(n) &lt;&lt; RESIZE_STAMP_SHIFT) + 2)， // 即 (sc - 2) == resizeStamp(n) &lt;&lt; RESIZE_STAMP_SHIFT if (U.compareAndSwapInt(this, SIZECTL, sc = sizeCtl, sc - 1)) &#123; // 如果有多个线程进行扩容，那么这个值在第二个线程以后就不会相等，因为 // sizeCtl 已经被减1了，所以后面的线程只能直接返回， // 始终保证只有一个线程执行了a(上面的注释a) if ((sc - 2) != resizeStamp(n) &lt;&lt; RESIZE_STAMP_SHIFT) return; // finishing 和 advance 保证线程已经扩容完成了可以退出循环 finishing = advance = true; //最后退出的线程要重新check下是否全部迁移完毕 i = n; &#125; &#125; // 当前table节点为空，不需要复制，直接放入ForwardingNode //4.1 当前数组中第i个元素为null，用CAS设置成特殊节点forwardingNode(可以理解成占位符) // 如果 tab[i] 为 null,那么就把 fwd 插入到 tab[i],表明这个节点已经处理过了 else if ((f = tabAt(tab, i)) == null) advance = casTabAt(tab, i, null, fwd); // 当前table节点已经是ForwardingNode // 表示已经被其他线程处理了，则直接往前遍历 // 通过CAS读写ForwardingNode节点状态，达到多线程互斥处理 // 4.2 如果遍历到ForwardingNode节点说明这个点已经被处理过了直接跳过 // 这里是控制并发扩容的核心 // 如果 f.hash=-1 的话说明该节点为 ForwardingNode,说明该节点已经处理过了 else if ((fh = f.hash) == MOVED) advance = true; //迁移node节点 else &#123; // 锁住当前桶首节点 synchronized (f) &#123; if (tabAt(tab, i) == f) &#123; Node&lt;K,V&gt; ln, hn; // 链表节点复制(链表迁移) if (fh &gt;= 0) &#123; // 4.3 处理当前节点为链表的头结点的情况，构造两个链表，一个是原链表 // 另一个是原链表的反序排列 int runBit = fh &amp; n; Node&lt;K,V&gt; lastRun = f; //将node链表，分成2个新的node链表 // 这边还对链表进行遍历，这边的算法和hashMap的算法又不一样了，对半拆分 // 把链表拆分为，hash&amp;n 等于0和不等于0的，然后分别放在新表的i和i+n位置 // 此方法同 HashMap 的 resize for (Node&lt;K,V&gt; p = f.next; p != null; p = p.next) &#123; int b = p.hash &amp; n; if (b != runBit) &#123; runBit = b; lastRun = p; &#125; &#125; if (runBit == 0) &#123; ln = lastRun; hn = null; &#125; else &#123; hn = lastRun; ln = null; &#125; for (Node&lt;K,V&gt; p = f; p != lastRun; p = p.next) &#123; int ph = p.hash; K pk = p.key; V pv = p.val; if ((ph &amp; n) == 0) ln = new Node&lt;K,V&gt;(ph, pk, pv, ln); else hn = new Node&lt;K,V&gt;(ph, pk, pv, hn); &#125; //将新node链表赋给nextTab //在nextTable的i位置上插入一个链表 setTabAt(nextTab, i, ln); //在nextTable的i+n的位置上插入另一个链表 setTabAt(nextTab, i + n, hn); // 扩容成功后，设置ForwardingNode节点 //在table的i位置上插入forwardNode节点表示已经处理过该节点 // 把已经替换的节点的旧tab的i的位置用fwd替换，fwd包含nextTab setTabAt(tab, i, fwd); //设置advance为true 返回到上面的while循环中 就可以执行i--操作 advance = true; &#125; // 红黑树节点复制(红黑树迁移) //4.4 处理当前节点是TreeBin时的情况，操作和上面的类似 else if (f instanceof TreeBin) &#123; TreeBin&lt;K,V&gt; t = (TreeBin&lt;K,V&gt;)f; TreeNode&lt;K,V&gt; lo = null, loTail = null; TreeNode&lt;K,V&gt; hi = null, hiTail = null; int lc = 0, hc = 0; for (Node&lt;K,V&gt; e = t.first; e != null; e = e.next) &#123; int h = e.hash; TreeNode&lt;K,V&gt; p = new TreeNode&lt;K,V&gt; (h, e.key, e.val, null, null); if ((h &amp; n) == 0) &#123; if ((p.prev = loTail) == null) lo = p; else loTail.next = p; loTail = p; ++lc; &#125; else &#123; if ((p.prev = hiTail) == null) hi = p; else hiTail.next = p; hiTail = p; ++hc; &#125; &#125; // 判断扩容后是否还需要红黑树 ln = (lc &lt;= UNTREEIFY_THRESHOLD) ? untreeify(lo) : (hc != 0) ? new TreeBin&lt;K,V&gt;(lo) : t; hn = (hc &lt;= UNTREEIFY_THRESHOLD) ? untreeify(hi) : (lc != 0) ? new TreeBin&lt;K,V&gt;(hi) : t; setTabAt(nextTab, i, ln); setTabAt(nextTab, i + n, hn); // 扩容成功后，设置ForwardingNode节点 setTabAt(tab, i, fwd); advance = true; &#125; &#125; &#125; &#125; &#125; &#125; 代码逻辑请看注释,整个扩容操作分为两个部分： 第一部分是构建一个nextTable,它的容量是原来的两倍，这个操作是单线程完成的。新建table数组的代码为:Node&lt;K,V&gt;[] nt = (Node&lt;K,V&gt;[])new Node&lt;?,?&gt;[n &lt;&lt; 1],在原容量大小的基础上右移一位。 第二个部分就是将原来table中的元素复制到nextTable中，主要是遍历复制的过程。根据运算得到当前遍历的数组的位置i，然后利用tabAt方法获得i位置的元素再进行判断： 如果这个位置为空，就在原table中的i位置放入forwardNode节点，这个也是触发并发扩容的关键点； 如果这个位置是Node节点（fh&gt;=0），如果它是一个链表的头节点，就构造一个反序链表，把他们分别放在nextTable的i和i+n的位置上 如果这个位置是TreeBin节点（fh&lt;0），也做一个反序处理，并且判断是否需要untreefi，把处理的结果分别放在nextTable的i和i+n的位置上 遍历过所有的节点以后就完成了复制工作，这时让nextTable作为新的table，并且更新sizeCtl为新容量的0.75倍 ，完成扩容。设置为新容量的0.75倍代码为 sizeCtl = (n &lt;&lt; 1) - (n &gt;&gt;&gt; 1)，仔细体会下是不是很巧妙，n&lt;&lt;1相当于n右移一位表示n的两倍即2n,n&gt;&gt;&gt;1左右一位相当于n除以2即0.5n,然后两者相减为2n-0.5n=1.5n,是不是刚好等于新容量的0.75倍即2n*0.75=1.5n。最后用一个示意图来进行总结（图片摘自网络）： mappingCount 与 sizemappingCount与size方法的类似 从给出的注释来看，应该使用mappingCount代替size方法 两个方法都没有直接返回basecount 而是统计一次这个值，而这个值其实也是一个大概的数值，因此可能在统计的时候有其他线程正在执行插入或删除操作。 1234567891011121314151617181920212223242526272829303132public int size() &#123; long n = sumCount(); return ((n &lt; 0L) ? 0 : (n &gt; (long)Integer.MAX_VALUE) ? Integer.MAX_VALUE : (int)n);&#125; /** * Returns the number of mappings. This method should be used * instead of &#123;@link #size&#125; because a ConcurrentHashMap may * contain more mappings than can be represented as an int. The * value returned is an estimate; the actual count may differ if * there are concurrent insertions or removals. * * @return the number of mappings * @since 1.8 */public long mappingCount() &#123; long n = sumCount(); return (n &lt; 0L) ? 0L : n; // ignore transient negative values&#125; final long sumCount() &#123; CounterCell[] as = counterCells; CounterCell a; long sum = baseCount; if (as != null) &#123; for (int i = 0; i &lt; as.length; ++i) &#123; if ((a = as[i]) != null) sum += a.value;//所有counter的值求和 &#125; &#125; return sum;&#125; remove和put方法一样，多个remove线程请求不同的hash桶时，可以并发执行 如图所示：删除的node节点的next依然指着下一个元素。此时若有一个遍历线程正在遍历这个已经删除的节点，这个遍历线程依然可以通过next属性访问下一个元素。从遍历线程的角度看，他并没有感知到此节点已经删除了，这说明了ConcurrentHashMap提供了弱一致性的迭代器。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495public V remove(Object key) &#123; return replaceNode(key, null, null); &#125; // 当参数 value == null 时，删除节点。否则更新节点的值为value // cv 是个期望值，当 map[key].value 等于期望值 cv 或 cv == null 时， // 删除节点，或者更新节点的值 final V replaceNode(Object key, V value, Object cv) &#123; int hash = spread(key.hashCode()); for (Node&lt;K,V&gt;[] tab = table;;) &#123; Node&lt;K,V&gt; f; int n, i, fh; // table 还没初始化或key对应的 hash 桶为空 if (tab == null || (n = tab.length) == 0 || (f = tabAt(tab, i = (n - 1) &amp; hash)) == null) break; // 正在扩容 else if ((fh = f.hash) == MOVED) tab = helpTransfer(tab, f); else &#123; V oldVal = null; boolean validated = false; synchronized (f) &#123; // CAS 获取 tab[i] ,如果此时 tab[i] != f,说明其他线程修改了 tab[i] // 回到 for 循环开始处，重新执行 if (tabAt(tab, i) == f) &#123; // node 链表 if (fh &gt;= 0) &#123; validated = true; for (Node&lt;K,V&gt; e = f, pred = null;;) &#123; K ek; if (e.hash == hash &amp;&amp; ((ek = e.key) == key || (ek != null &amp;&amp; key.equals(ek)))) &#123; V ev = e.val; // ev 代表参数期望值 // cv == null:直接更新value/删除节点 // cv 不为空，则只有在 key 的 oldVal 等于 // 期望值的时候，才更新 value/删除节点 if (cv == null || cv == ev || (ev != null &amp;&amp; cv.equals(ev))) &#123; oldVal = ev; //更新value if (value != null) e.val = value; //删除非头节点 else if (pred != null) pred.next = e.next; //删除头节点 else // 因为已经获取了头结点锁，所以此时 // 不需要使用casTabAt setTabAt(tab, i, e.next); &#125; break; &#125; //当前节点不是目标节点，继续遍历下一个节点 pred = e; if ((e = e.next) == null) //到达链表尾部，依旧没有找到，跳出循环 break; &#125; &#125; //红黑树 else if (f instanceof TreeBin) &#123; validated = true; TreeBin&lt;K,V&gt; t = (TreeBin&lt;K,V&gt;)f; TreeNode&lt;K,V&gt; r, p; if ((r = t.root) != null &amp;&amp; (p = r.findTreeNode(hash, key, null)) != null) &#123; V pv = p.val; if (cv == null || cv == pv || (pv != null &amp;&amp; cv.equals(pv))) &#123; oldVal = pv; if (value != null) p.val = value; else if (t.removeTreeNode(p)) setTabAt(tab, i, untreeify(t.first)); &#125; &#125; &#125; &#125; &#125; if (validated) &#123; if (oldVal != null) &#123; //如果删除了节点，更新size if (value == null) addCount(-1L, -1); return oldVal; &#125; break; &#125; &#125; &#125; return null; &#125; ForwardingNode1234567891011121314151617181920212223242526272829303132333435static final class ForwardingNode&lt;K,V&gt; extends Node&lt;K,V&gt; &#123; final Node&lt;K,V&gt;[] nextTable; ForwardingNode(Node&lt;K,V&gt;[] tab) &#123; //hash值为MOVED（-1）的节点就是ForwardingNode super(MOVED, null, null, null); this.nextTable = tab; &#125; //通过此方法，访问被迁移到nextTable中的数据 Node&lt;K,V&gt; find(int h, Object k) &#123; // loop to avoid arbitrarily deep recursion on forwarding nodes outer: for (Node&lt;K,V&gt;[] tab = nextTable;;) &#123; Node&lt;K,V&gt; e; int n; if (k == null || tab == null || (n = tab.length) == 0 || (e = tabAt(tab, (n - 1) &amp; h)) == null) return null; for (;;) &#123; int eh; K ek; if ((eh = e.hash) == h &amp;&amp; ((ek = e.key) == k || (ek != null &amp;&amp; k.equals(ek)))) return e; if (eh &lt; 0) &#123; if (e instanceof ForwardingNode) &#123; tab = ((ForwardingNode&lt;K,V&gt;)e).nextTable; continue outer; &#125; else return e.find(h, k); &#125; if ((e = e.next) == null) return null; &#125; &#125; &#125;&#125; 总结JDK6,7中的ConcurrentHashmap主要使用Segment来实现减小锁粒度，分割成若干个Segment，在put的时候需要锁住Segment，get时候不加锁，使用volatile来保证可见性，当要统计全局时（比如size），首先会尝试多次计算modcount来确定，这几次尝试中，是否有其他线程进行了修改操作，如果没有，则直接返回size。如果有，则需要依次锁住所有的Segment来计算。 而在1.8的时候摒弃了segment臃肿的设计，这种设计在定位到具体的桶时，要先定位到具体的segment，然后再在segment中定位到具体的桶。而到了1.8的时候是针对的是Node[] tale数组中的每一个桶，进一步减小了锁粒度。并且防止拉链过长导致性能下降，当链表长度大于8的时候采用红黑树的设计。 主要设计上的变化有以下几点: 不采用segment而采用node，锁住node来实现减小锁粒度。 设计了MOVED状态 当resize的中过程中 线程2还在put数据，线程2会帮助resize。 使用3个CAS操作来确保node的一些操作的原子性，这种方式代替了锁。 sizeCtl的不同值来代表不同含义，起到了控制的作用。 采用synchronized而不是ReentrantLock volatile语义提供更细颗粒度的轻量级锁，使得多线程可以(几乎)同时读写实例中的关键量，正确理解当前类所处的状态，进入对应if语句中执行相关逻辑。 采用更加细粒度的hash桶级别锁，扩容期间，依然可以保证写操作的并发度。 多线程无锁扩容的关键就是通过CAS设置sizeCtl与transferIndex变量，协调多个线程对table数组中的node进行迁移。 参考文章：http://www.cnblogs.com/huaizuo/p/5413069.html 参考文章：http://www.bijishequ.com/detail/560964?p= 参考文章：https://bentang.me/tech/2016/12/01/jdk8-concurrenthashmap-1/ 参考文章: http://www.jianshu.com/p/5bc70d9e5410 扩容原理: http://www.jianshu.com/p/487d00afe6ca 遍历操作：http://www.jianshu.com/p/3e85ac8f8662 改进说明带例子:http://www.voidcn.com/article/p-gdbewnlb-qh.html http://nannan408.iteye.com/blog/2217042]]></content>
      <categories>
        <category>ConcurrentHashMap</category>
      </categories>
      <tags>
        <tag>基础数据类型</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[IntelliJ IDEA 调试教程]]></title>
    <url>%2F2018%2F06%2F03%2FIntelliJ%20IDEA%20%20%E8%B0%83%E8%AF%95%E6%95%99%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[IntelliJ IDEA 调试教程在调试代码的时候，你的项目得debug模式启动，也就是点那个绿色的甲虫启动服务器，然后，就可以在代码里面断点调试啦。下面不要在意，这个快捷键具体是啥，因为，这个keymap是可以自己配置的，有的人keymap是mac版的，有的是Windows版的。我的就是Windows，而且修改keymap为eclipse的keymap，因为我算是eclipse转过来的吧。 看图，每一个按钮（按钮对应图中的数字）都是什么功能 rerun XXX，这个就是直接重新跑某个程序。 这个相当于eclipse里面的f8，直接跑完，到下一个断点停下，没有就直接跑完程序。 停止项目或者程序。要是自己的main呢，点一下就停下了，要是Java web项目，则点2下，就把服务器给停了。 查看所有的断点设置情况。具体详情，下面有示意图，再细细解释。 直接取消所有断点，让所有断点无效。 要是你一不小心把这个下面的布局给弄乱了，你点这个，就把下面的布局给还原咯。 跳转到当前代码所执行的地方，也就是说你在看代码的时候，点到其他地方，一点这个按钮，就到了程序执行到当前哪行的代码的地方。 下一步，如果是方法，他是不会跳进去的。就是一行行的往下走。（eclipse里面的快捷键就是f6） 跳转到详情，如果下一行调试代码是可执行方法，就可以f7进去，查看这个方法的运行详细情况。重点就是点进去执行 从详情跳出去，和上面的9相反。 看字面意思就是跳转到下一个断点 这个点开之后，可以计算你想要看的代码段的值，后面详细上图。 看意思，同eclipse里面的watch，查看某个对象的值，自定义的对象。 把自定义的查看对象的值，分开到另一个tab页。 有时候当我们步入方法体之后，还想回退到方法体外，点这个按钮后，断点重新回到方法体之外。在继续还是可以再次进到方法内 查看断点处的某个对象的值，可以 如下几个方法：1，选中对象后，鼠标悬停在对象上 2 秒左右2，在watch里面添加这个对象，3，下面也许会自动列出来你代码里面有的4，使用上面图上标注的12的那个按钮 下面就再详细说下 4，12，13，144，查看所有的断点的详情，点开如下所示。在图中condition中可以设置断点的条件，当i==4的时候，才停下。查看具体断点内容。 关于设置断点条件，还可以，直接在代码断点处，右键设置，完啦之后，done，设置完成。 12，这个用的也比较多，这个就比较随意。可以根据你的输入，计算你要的结果，不局限代码里面的变量啥的。这个在debug的时候，使用起来是很方便的。 13，14，这2个点完之后，效果如下图，只是把自定义的变量和代码里面自带的变量分在两个tab页面展示。你可以点13号按钮，自行添加，你想查看的变量的值。 还有个需求，就是在调试代码的时候，实时的修改，运行状态的代码变量的值。 仔细看下图，就知道，怎么在实时调试代码的时候，怎么设置某些变量的值，可以看到，我上面在输入a之后，下面就有类似你写代码时候的提示，你就可以在这地方修改变量的值啦 关于调试的时候，设置运行时的参数，如下： 入口如下，2个地方都可以。 一般都是跑简单的main方法，跑main方法的时候，还带参数文件的，还是第一次，顺带做个记录吧。 更新：这个编辑器为了方便从eclipse编辑器转过来的同学们，他可以设置keymap的。具体看图。 因为我就是刚刚开始的时候，使用的就是eclipse，后来转过来的，所以，在使用的时候，就先设置了一下，这个键盘映射。使用的还是以前在eclipse上使用的快捷键。不需要再次去记一遍新的快捷键映射。这个也是极其方便的。 所以，在这个debug的快捷键上和使用eclipse时候，是一样的f5进去，f6是下一步。]]></content>
      <categories>
        <category>IntelliJ IDEA</category>
      </categories>
      <tags>
        <tag>开发工具</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[IntelliJ IDEA 版本控制的使用]]></title>
    <url>%2F2018%2F06%2F03%2FIDEA%20%E7%89%88%E6%9C%AC%E6%8E%A7%E5%88%B6%2F</url>
    <content type="text"><![CDATA[版本控制的使用IntelliJ IDEA 下的版本控制介绍这一章节放在这么靠前位置来讲是因为版本控制在我心目中的地位比后面的实战知识点都来得重要。不管是个人开发或是团队开发，版本控制都是可以很好地被使用的，目前我找不到任何开发者不使用版本控制的理由。而且对于 IDE 来讲，集成版本控制的本身就是它最大的亮点之一，很多开发者也是为此而使用它。 在本章节中也会对 IntelliJ IDEA 的相关版本控制进行了介绍，会开始涉及到一些 IntelliJ IDEA 人性化设置，也希望你能从这一讲开始认识到 IntelliJ IDEA 的优雅。 很多人认为 IntelliJ IDEA 自带了 SVN 或是 Git 等版本控制工具，认为只要安装了 IntelliJ IDEA 就可以完全使用版本控制应有的功能。这完全是一种错误的解读，IntelliJ IDEA 是自带对这些版本控制工具的支持插件，但是该装什么版本控制客户端还是要照样装的。 如上图标注 1 所示，IntelliJ IDEA 对版本控制的支持是以插件化的方式来实现的。旗舰版默认支持目前主流的版本控制软件：CVS、Subversion（SVN）、Git、ClearCase、Mercurial、Perforce、TFS。又因为目前太多人使用 Github 进行协同或是项目版本管理，所以 IntelliJ IDEA 同时自带了 Github 插件，方便 Checkout 和管理你的 Github 项目。 SVN 的配置要在 IntelliJ IDEA 中使用 SVN，需要先安装 SVN 客户端或是 TortoiseSVN 这类图形化工具，Windows 系统这里推荐安装 TortoiseSVN，即使在不使用 IntelliJ IDEA 也可以方便管理我们的项目。 SVN 主要使用的版本有 1.6、1.7、1.8，最新的是 1.9。推荐大家使用 1.8 的。如果你的项目使用的是 1.6 的版本，在安装 1.8 之后是可以直接对项目文件进行升级的，所以无需担心，也因此更加推荐大家使用 1.8。 Subversion 官网下载：https://subversion.apache.org/download/#recommended-release TortoiseSVN 官网下载：http://tortoisesvn.net/downloads.zh.html 如上图箭头所示，在安装 TortoiseSVN 的时候，默认 command line client tools，是不安装的，这里建议勾选上。 如上图标注 1 所示，勾选 Use command line client 如上图标注 2 所示，建议 svn 的路径自己根据安装后的路径进行选择，不然有时候 IntelliJ IDEA 无法识别到会报：Cannot run program &quot;svn&quot; 这类错误。 如上图标注 3 所示，当使用一段时间 SVN 以后，发现各种 SVN 相关问题无法解决，可以考虑点击此按钮进行清除一下缓存。 根据目前的使用经验来看，IntelliJ IDEA 下 SVN 的使用经历并不算愉快，至少比 Git 不好用很多，经常遇到很多问题，所以这里也算是先给大家提个醒。如果紧急情况下 IntelliJ IDEA 无法更新、提交的时候，要记得使用 TortoiseSVN 来操作。 Git 的配置要在 IntelliJ IDEA 中使用 Git，需要先安装 Git 客户端，这里推荐安装官网版本。 Git 主要的版本有 1.X、2.X，最新的是 2.X，使用版本随意，但是不要太新了，不然可能 IntelliJ IDEA 小旧版本会无法支持可能。 Git 官网下载：http://git-scm.com/ TortoiseGit 官网下载：http://download.tortoisegit.org/tgit/ 如上图标注 1 所示，确定好该路径下是否有对应的可执行文件。 Github 的配置和使用 如上图标注 1 所示，填写你的 Github 登录账号和密码，点击 Test 可以进行测试是否可以正确连上。 如上图标注 1 所示，支持直接从你当前登录的 Github 账号上 Checkout 项目。 如上图标注 1 所示，支持把当前本地项目分享到你的 Github 账号上。 如上图标注 1 所示，支持创建 Gist。Github 的 Gist 官网地址：https://gist.github.com/ 版本控制主要操作按钮 如上图标注 1 所示，对目录进行右键弹出的菜单选项。 如上图标注 1 所示，对文件进行右键弹出的菜单选项。 如上图标注红圈所示，为工具栏上版本控制操作按钮，基本上大家也都是使用这里进行操作。 第一个按钮：Update Project 更新项目。 第二个按钮：Commit changes 提交项目上所有变化文件。点击这个按钮不会立马提交所有文件，而是先弹出一个被修改文件的一个汇总框，具体操作下面会有图片进行专门介绍。 第三个按钮：Compare with the Same Repository Version 当前文件与服务器上该文件通版本的内容进行比较。如果当前编辑的文件没有修改，则是灰色不可点击。 第四个按钮：Show history 显示当前文件的历史记录。 第五个按钮：Revert 还原当前被修改的文件到未被修改的版本状态下。如果当前编辑的文件没有修改，则是灰色不可点击。 如上图标注 1 所示，菜单栏上的版本控制操作区。 版本控制相关的常用设置说明 如上图标注 1 所示，当前项目使用的版本控制是 Git。如果你不愿意这个项目继续使用版本控制可以点击旁边的减号按钮，如果你要切换版本控制，可以点击 Git，会出现 IntelliJ IDEA 支持的各种版本控制选择列表，但是我们一般情况下一个项目不会有多个版本控制的。 如上图标注 2 所示，Show directories with changed descendants 表示子目录有文件被修改了，则该文件的所有上层目录都显示版本控制被概念的颜色。默认是不勾选的，我一般建议勾选此功能。 如上图标注 1 所示，When files are created 表示当有新文件放进项目中的时候 IntelliJ IDEA 做如何处理，默认是 Show options before adding to version control 表示弹出提示选项，让开发者决定这些新文件是加入到版本控制中还是不加入。如果不想弹出提示，则选择下面两个选项进行默认操作。 如上图标注 2 所示，When files are deleted 表示当有新文件在项目中被删除的时候 IntelliJ IDEA 做如何处理，默认是 Show options before removing from version control 表示弹出提示选项，让开发者决定这些被删除的是否从版本控制中删除。如果不想弹出提示，则选择下面两个选项进行默认操作。 如上图标注 1 所示，对于不想加入到版本控制的文件，可以添加要此忽略的列表中。但是如果已经加入到版本控制的文件使用此功能，则表示该文件 或 目录无法再使用版本控制相关的操作，比如提交、更新等。我个人使用过程中发现在 SVN 上此功能不太好用，Git 上是可以用的。 上图所示的弹出层就是本文上面说的 Commit Changes 点击后弹出的变动文件汇总弹出层。 如上图标注 1 所示，可以在文件上右键进行操作。 Show Diff 当前文件与服务器上该文件通版本的内容进行比较。 Move to Another Changelist 将选中的文件转移到其他的 Change list 中。Change list 是一个重要的概念，这里需要进行重点说明。很多时候，我们开发一个项目同时并发的任务可能有很多，每个任务涉及到的文件可能都是基于业务来讲的。所以就会存在一个这样的情况：我改了 30 个文件，其中 15 个文件是属于订单问题，剩下 15 个是会员问题，那我希望提交代码的时候是根据业务区分这些文件的，这样我填写 Commit Message 是好描述的，同时在文件多的情况下，我也好区分这些要提交的文件业务模块。所以我一般会把属于订单的 15 个文件转移到其他的 Change list中，先把专注点集中在 15 个会员问题的文件，先提交会员问题的 Change list，然后在提交订单会员的 Change list。我个人还有一种用法是把一些文件暂时不提交的文件转移到一个我指定的 Change list，等后面我觉得有必要提交了，再做提交操作，这样这些文件就不会干扰我当前修改的文件提交。总结下 Change list 的功能就是为了让你更好地管理你的版本控制文件，让你的专注点得到更好的集中，从而提供效率。 Jump to Source 打开并跳转到被选中。 如上图标注 2 所示，可以根据工具栏按钮进行操作，操作的对象会鼠标选中的文件，多选可以按 Ctrl后不放，需要注意的是这个更前面的复选框是没有多大关系的。 如上图标注 3 所示，可以在提交前自动对被提交的文件进行一些操作事件（该项目使用的 Git，使用其他版本控制可能有些按钮有差异。）： Reformat code 格式化代码，如果是 Web 开发建议不要勾选，因为格式化 JSP 类文件，格式化效果不好。如果都是 Java 类则可以安心格式化。 Rearrange code 重新编排代码，IntelliJ IDEA 支持各种复杂的编排设置选项，这个会在后面说。设置好了编码功能之后，这里就可以尝试勾选这个进行自动编排。 Optimize imports 优化导入包，会在自动去掉没有使用的包。这个建议都勾选，这个只对 Java 类有作用，所以不用担心有副作用。 Perform code analysis 进行代码分析，这个建议不用在提交的时候处理，而是在开发完之后，要专门养成对代码进行分析的习惯。IntelliJ IDEA 集成了代码分析功能。 Check TODO 检查代码中的 TODO。TODO 功能后面也会有章节进行讲解，这里简单介绍：这是一个记录待办事项的功能。 Cleanup 清除下版本控制系统，去掉一些版本控制系统的错误信息，建议勾选（主要针对 SVN，Git 不适用）。 如上图标注 4 所示，填写提交的信息。 如上图标注 5 所示，Change list 改变列表，这是一个下拉选项，说明我们可以切换不同的 Change list，提交不同的 Change list 文件。 如上图标注箭头所示，我们可以查看我们提交历史中使用的 Commit Message，有些时候，我们做得是同一个任务，但是需要提交多次，为了更好管理项目，建议是提交的 Message 是保持一致的。 如上图标注箭头所示，如果你使用的 Git，点击此位置可以切换分支和创建分支，以及合并、删除分支等操作。 SVN 的使用SVN 的这个窗口有的 IntelliJ IDEA 上叫 Changes，有的叫 Version Control，具体是什么原因引起这样的差异，我暂时还不清楚。但是不管叫法如何里面的结构是一样的，所以对使用者来讲没多大影响，但是你需要知道他们其实是一样的功能即可。 上图 Local Changes 这个 Tab 表示当前项目的 SVN 中各个文件的总的情况预览。这里的 Default 是 IntelliJ IDEA 的默认 change list 名称，no commit 是我自己创建的一个change list，我个人有一个习惯是把一些暂时不需要提交的先放这个 list 里面。change list 很常用而且重要，本文前面也有强调过了，所以一定好认真对待。unversioned Files表示项目中未加到版本控制系统中的文件，你可以点击 Click to browse，会弹出一个弹出框列表显示这些未被加入的文件。 上图 Repository 这个 Tab 表示项目的 SVN 信息汇总，内容非常的详细，也是我平时用最多的地方。如果你点击这个 Tab 没看到数据，是因为你需要点击上图红圈这个刷新按钮。初次使用下默认的过滤条件不是我上图这样的，我习惯根据 User 进行过滤筛选，所以上图箭头中的 Filter 我是选择 User。选择之后，如上图标注 1 所示，显示了这个项目中参与提交的各个用户名，选择一个用户之后，上图标注 2 所以会显示出该用户提交了哪些记录。选择标注 2 区域中的某个提交记录后，标注 3 显示对应的具体提交细节，我们可以对这些文件进行右键操作，具体操作内容跟本文上面提到的那些提交时的操作按钮差不多，这里不多讲。 总的来说，SVN 这个功能用来管理和审查开发团队中人员的代码是非常好用的，所以非常非常建议你一定要学会该功能。 Git 常见问题 更新的时候报： 1Can&apos;t update: no tracked branch 解决办法：打开 git-bash（路径：C:\Program Files\Git\git-bash.exe），切换到这个更新不下来的项目的根目录，然后输入：git branch --set-upstream-to origin/master master，回车之后重新回到 IntelliJ IDEA 进行更新，正常就可以了。 输错密码后，弹出验证的登录框没有再出现： 解决办法如下图：选择 Do not save, forget passwords after restart 等你确定你的密码没错后再选择保存密码方案。 Git Flow 的介绍Git Flow 概念 Git Flow 是一个 git 扩展集，按 Vincent Driessen 的分支模型提供高层次的库操作。这里的重点是 Vincent Driessen 的分支模型思想，下面讲解的内容也是基于 Vincent Driessen 思想。 Vincent Driessen 的观点：http://nvie.com/posts/a-successful-git-branching-model/ Git Flow 是一个 git 扩展集 你可以理解 Git Flow 是一个基于 Git 的插件，这个插件简化了 Git 一些复杂的命令，比如 Git Flow 用一条命令，就可以代替 Git 原生 10 条命令。 Git Flow 对原生的 Git 不会有任何影响，你可以照旧用 Git 原生命令，也可以使用 Git Flow 命令。 还有其他的一些分支管理模型思想，具体可以看：http://www.ruanyifeng.com/blog/2015/12/git-workflow.html Git Flow 核心概念 必须有的两个核心分支（长期分支）： master，Git 代码仓库中默认的一条主分支。这条分支上的代码一般都建议为是正式版本的代码，并且这条分支不能进行代码修改，只能用来合并其他分支。 develop，一般用于存储开发过程的代码分支，并且这条分支也不能进行代码修改，只能用来合并其他辅助分支。 根据情况创建的辅助分支（临时分支） feature branches（功能分支） 基于 develop 分支上创建 开发完成后合并到 develop 分支上 当要开始一个新功能的开发时，我门可以创建一个 Feature branches 。等待这个新功能开发完成并确定应用到新版本中就合并回 develop 对于单人开发的 feature branches，start 之后，开发完成后可以直接 finish。 对于多人开发的 feature branches，start 之后，开发完成后先 publish 给其他开发人员进行合并，最后大家都开发完成后再 finish。这个思路也同样适用下面几个辅助分支场景。 feature branches 开发过程有 bug，直接在 feature branches 上修改、提交。 release branches（预发布分支） 基于 develop 分支上创建 测试确定新功能没有问题，合并到 develop 分支和 master 分支上 用来做新版本发布前的准备工作，在上面可以做一些小的 bug 修复、准备发布版本号等等和发布有关的小改动，其实已经是一个比较成熟的版本了。另外这样我们既可以在预发布分支上做一些发布前准备，也不会影响 “develop” 分支上下一版本的新功能开发。 hotfix branches（基于 master 基础上的生产环境 bug 的修复分支） 基于 master 分支上创建 修复测试无误后合并到 master 分支和 develop 分支上 主要用于处理线上版本出现的一些需要立刻修复的 bug 情况 Git Flow 安装 Windows：如果你安装 Git 用的是 Git for Windows，那它已经内置了。 Mac：brew install git-flow-avh Linux：wget --no-check-certificate -q https://raw.githubusercontent.com/petervanderdoes/gitflow-avh/develop/contrib/gitflow-installer.sh &amp;&amp; sudo bash gitflow-installer.sh install stable; rm gitflow-installer.sh 更多版本：https://github.com/petervanderdoes/gitflow-avh/wiki/Installation 在系统环境上支持之后，再安装 IntelliJ IDEA 对 Git Flow 支持的插件：https://plugins.jetbrains.com/plugin/7315-git-flow-integration Git Flow 基础命令资料 https://danielkummer.github.io/git-flow-cheatsheet/index.zh_CN.html http://www.jianshu.com/p/9e4291078853 http://stormzhang.com/git/2014/01/29/git-flow/ Hotfix/Release/Develop/Feature/ Git Flow Integration 插件的使用· 如果你已经理解了上面的理论，再看下面这些截图你能理解对应的是什么意思。]]></content>
      <categories>
        <category>IntelliJ IDEA</category>
      </categories>
      <tags>
        <tag>开发工具</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HashMap 的源码分析]]></title>
    <url>%2F2018%2F06%2F03%2FHashMap%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%2F</url>
    <content type="text"><![CDATA[JDK1.8 HashMap源码分析HahsMap实现了Map接口。其继承关系如下图： HashMap有两个影响性能的重要参数：初始容量和加载因子。容量是Hash表中桶的个数，当HashMap初始化时，容量就是初始容量。加载因子是衡量hash表多满的一个指标，用来判断是否需要增加容量。当HashMap需要增加容量时，将会导致rehash操作。默认情况下，0.75的加载因子在时间和空间方面提供了很好的平衡。加载因子越大，增加了空间利用率但是也增加了查询的时间。 构造器底层结构JDK1.8之前的结构在JDK1.7之前，HashMap采用的是数组+链表的结构，其结构图如下：左边部分代表Hash表，数组的每一个元素都是一个单链表的头节点，链表是用来解决冲突的，如果不同的key映射到了数组的同一位置处，就将其放入单链表中。 JDK1.8的结构JDK1.8 之前的 HashMap 都采用上图的结构，都是基于一个数组和多个单链表，hash 值冲突的时候，就将对应节点以链表形式存储。如果在一个链表中查找一个节点时，将会花费 O(n) 的查找时间，会有很大的性能损失。到了JDK1.8，当同一个Hash值的节点数不小于8时，不再采用单链表形式存储，而是采用红黑树，如下图所示： Node介绍Node是map的接口中的内部接口Map.Entry的实现类,用于存储HashMap中键值对的对象,是HashMap中非常重要的一个内部类,随便提一下,HashMap中有非常多的内部类,本文没做过多的介绍,读者可以自己翻看一下源码,因为篇幅实在太长了…在这里就简单的讲一下,大部分的内部类都是用于集合操作的,如keySet,values,entrySet等方法. 内部组成 12345678910static class Node&lt;K,V&gt; implements Map.Entry&lt;K,V&gt; &#123;//key是不可变的final K key;//valueV value;//指向下一个entry对象,具体作用后面介绍Node&lt;K,V&gt; next;//hash值int hash;&#125; 重要的字段HashMap中有几个重要的字段，如下： 1234567891011121314151617181920212223242526 //Hash表结构 transient Node&lt;K,V&gt;[] table; //元素个数 transient int size; //确保fail-fast机制 transient int modCount; //下一次增容前的阈值 int threshold; //加载因子 final float loadFactor; //默认初始容量 static final int DEFAULT_INITIAL_CAPACITY = 1 &lt;&lt; 4; // aka 16//最大容量 static final int MAXIMUM_CAPACITY = 1 &lt;&lt; 30; //加载因子 static final float DEFAULT_LOAD_FACTOR = 0.75f; //链表转红黑树的阈值 static final int TREEIFY_THRESHOLD = 8; 红黑树的关键参数123456789101112131415//一个桶的树化阈值//当桶中元素个数超过这个值时，需要使用红黑树节点替换链表节点//这个值必须为 8，要不然频繁转换效率也不高static final int TREEIFY_THRESHOLD = 8;//一个树的链表还原阈值//当扩容时，桶中元素个数小于这个值，就会把树形的桶元素 还原（切分）为链表结构//这个值应该比上面那个小，至少为 6，避免频繁转换static final int UNTREEIFY_THRESHOLD = 6;//哈希表的最小树形化容量//当哈希表中的容量大于这个值时，表中的桶才能进行树形化//否则桶内元素太多时会扩容，而不是树形化//为了避免进行扩容、树形化选择的冲突，这个值不能小于 4 * TREEIFY_THRESHOLDstatic final int MIN_TREEIFY_CAPACITY = 64; 构造方法HashMap一共有4个构造方法，主要的工作就是完成容量和加载因子的赋值。Hash表都是采用的懒加载方式，当第一次插入数据时才会创建。 1234567891011121314151617181920212223242526272829//构造方法 初始化负载因子和阈值public HashMap(int initialCapacity, float loadFactor) &#123; if (initialCapacity &lt; 0) throw new IllegalArgumentException("Illegal initial capacity: " + initialCapacity); //容量判断 if (initialCapacity &gt; MAXIMUM_CAPACITY) initialCapacity = MAXIMUM_CAPACITY; //负载银子判断 if (loadFactor &lt;= 0 || Float.isNaN(loadFactor)) throw new IllegalArgumentException("Illegal load factor: " + loadFactor); this.loadFactor = loadFactor; this.threshold = tableSizeFor(initialCapacity); &#125; public HashMap(int initialCapacity) &#123; this(initialCapacity, DEFAULT_LOAD_FACTOR); &#125;//为空时候使用默认分配的大小16,负载因子0.75f,默认的容量为12,当size&gt;threshold默认容量时候就会去扩容 public HashMap() &#123; this.loadFactor = DEFAULT_LOAD_FACTOR; &#125; public HashMap(Map&lt;? extends K, ? extends V&gt; m) &#123; this.loadFactor = DEFAULT_LOAD_FACTOR; putMapEntries(m, false); &#125; 基本操作确定哈希桶数组索引位置不管增加、删除、查找键值对，定位到哈希桶数组的位置都是很关键的第一步。前面说过 HashMap 的数据结构是数组和链表的结合，所以我们当然希望这个 HashMap 里面的元素位置尽量分布均匀些，尽量使得每个位置上的元素数量只有一个，那么当我们用 hash 算法求得这个位置的时候，马上就可以知道对应位置的元素就是我们要的，不用遍历链表，大大优化了查询的效率。HashMap 定位数组索引位置，直接决定了 hash 方法的离散性能。先看看源码的实现(方法一+方法二): 1234567891011方法一：static final int hash(Object key) &#123; //jdk1.8 &amp; jdk1.7 int h; // h = key.hashCode() 为第一步 取hashCode值 // h ^ (h &gt;&gt;&gt; 16) 为第二步 高位参与运算 return (key == null) ? 0 : (h = key.hashCode()) ^ (h &gt;&gt;&gt; 16);&#125;方法二：static int indexFor(int h, int length) &#123; //jdk1.7的源码，jdk1.8没有这个方法，但是实现原理一样的 return h &amp; (length-1); //第三步 取模运算&#125; 这里的 Hash 算法本质上就是三步：取 key 的 hashCode 值、高位运算、取模运算。 对于任意给定的对象，只要它的 hashCode() 返回值相同，那么程序调用方法一所计算得到的 Hash 码值总是相同的。我们首先想到的就是把 hash 值对数组长度取模运算，这样一来，元素的分布相对来说是比较均匀的。但是，模运算的消耗还是比较大的，在 HashMap 中是这样做的：调用方法二来计算该对象应该保存在 table 数组的哪个索引处。 这个方法非常巧妙，它通过 h &amp; (table.length -1) 来得到该对象的保存位，而 HashMap 底层数组的长度总是 2 的n 次方，这是 HashMap 在速度上的优化。当 length 总是 2 的 n 次方时，h&amp; (length-1) 运算等价于对 length 取模，也就是 h%length ，但是&amp;比%具有更高的效率。 在JDK1.8的实现中，优化了高位运算的算法，通过hashCode()的高16位异或低16位实现的：(h = k.hashCode()) ^ (h &gt;&gt;&gt; 16)，主要是从速度、功效、质量来考虑的，这么做可以在数组table的length比较小的时候，也能保证考虑到高低Bit都参与到Hash的计算中，同时不会有太大的开销。 下面举例说明下，n为table的长度。 添加一个元素put(K k,V v)HashMap 允许K和V为 null，添加一个键值对时使用 put 方法，如果之前已经存在K的键值，那么旧值将会被新值替换。实现如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465public V put(K key, V value) &#123; return putVal(hash(key), key, value, false, true); &#125;/** * onlyIfAbsent 是否替换,为true时,如果存在值则替换 * evict 主要用于LinkedHashMap移除最近未使用的节点 */final V putVal(int hash, K key, V value, boolean onlyIfAbsent, boolean evict) &#123; Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; p; int n, i; //如果哈希表为空或长度为0，调用resize()方法创建哈希表 if ((tab = table) == null || (n = tab.length) == 0) n = (tab = resize()).length; //如果哈希表中K对应的桶为空，那么该K，V对将成为该桶的头节点 if ((p = tab[i = (n - 1) &amp; hash]) == null) tab[i] = newNode(hash, key, value, null); //该桶处已有节点，即发生了哈希冲突 else &#123; Node&lt;K,V&gt; e; K k; //如果添加的值与头节点相同，将e指向p if (p.hash == hash &amp;&amp; ((k = p.key) == key || (key != null &amp;&amp; key.equals(k)))) e = p; //如果与头节点不同，并且该桶目前已经是红黑树状态，调用putTreeVal()方法 else if (p instanceof TreeNode) e = ((TreeNode&lt;K,V&gt;)p).putTreeVal(this, tab, hash, key, value); //桶中仍是链表阶段 else &#123; //遍历，要比较是否与已有节点相同 for (int binCount = 0; ; ++binCount) &#123; //将e指向下一个节点，如果是null，说明链表中没有相同节点，添加到链表尾部即可 if ((e = p.next) == null) &#123; p.next = newNode(hash, key, value, null); //如果此时链表个数达到了8，那么需要将该桶处链表转换成红黑树，treeifyBin()方法将hash处的桶转成红黑树 if (binCount &gt;= TREEIFY_THRESHOLD - 1) // -1 for 1st treeifyBin(tab, hash); break; &#125; //如果与已有节点相同，跳出循环 if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) break; p = e; &#125; &#125; //如果有重复节点，那么需要返回旧值 if (e != null) &#123; V oldValue = e.value; if (!onlyIfAbsent || oldValue == null) e.value = value; //子类实现(用于linkedHashMap) afterNodeAccess(e); return oldValue; &#125; &#125; //是一个全新节点，那么size需要+1 ++modCount; //如果超过了阈值，那么需要resize()扩大容量 if (++size &gt; threshold) resize(); //子类实现(用于linkedHashMap) afterNodeInsertion(evict); return null; &#125; 从上面代码可以看到 putVal() 方法的流程： 判断哈希表是否为空，如果为空，调用 resize() 方法进行创建哈希表 根据 hash 值得到哈希表中桶的头节点，如果为 null ，说明是第一个节点，直接调用 newNode() 方法添加节点即可 如果发生了哈希冲突，那么首先会得到头节点，比较是否相同，如果相同，则进行节点值的替换返回 如果头节点不相同，但是头节点已经是 TreeNode 了，说明该桶处已经是红黑树了，那么调用 putTreeVal() 方法将该结点加入到红黑树中 如果头节点不是 TreeNode，说明仍然是链表阶段，那么就需要从头开始遍历，一旦找到了相同的节点就跳出循环或者直到了链表尾部，那么将该节点插入到链表尾部 如果插入到链表尾部后，链表个数达到了阈值8，那么将会将该链表转换成红黑树，调用 treeifyBin() 方法 如果是新加一个数据，那么将 size+1，此时如果 size 超过了阈值，那么需要调用 resize() 方法进行扩容 resize()方法下面我们一个一个分析上面提到的方法。首先是 resize() 方法，resize() 在哈希表为 null 时将会初始化，但是在已经初始化后就会进行容量扩展。下面是resize()的具体实现： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146/** * 有几种情况 * 1.当为空的时候,也就是没有初始化的时候 * 2.当到达最大值时候 * 3.普通扩容时候 */final Node&lt;K,V&gt;[] resize() &#123; Node&lt;K,V&gt;[] oldTab = table; int oldCap = (oldTab == null) ? 0 : oldTab.length;//旧表容量 int oldThr = threshold;//旧表与之 int newCap, newThr = 0; //旧表存在 if (oldCap &gt; 0) &#123; // 超过最大值就不再扩充了，就只好随你碰撞去吧 //旧表已经达到了最大容量，不能再大，直接返回旧表 //大于2&lt;&lt;30 最大容量设置为2&lt;&lt;32 - 1 if (oldCap &gt;= MAXIMUM_CAPACITY) &#123; //但是不移动.,没有空间移动 threshold = Integer.MAX_VALUE; return oldTab; &#125; // 没超过最大值，就扩充为原来的2倍 //否则，新容量为旧容量2倍，新阈值为旧阈值2倍 else if ((newCap = oldCap &lt;&lt; 1) &lt; MAXIMUM_CAPACITY &amp;&amp; oldCap &gt;= DEFAULT_INITIAL_CAPACITY) newThr = oldThr &lt;&lt; 1; // double threshold &#125; //如果就阈值&gt;0，说明构造方法中指定了容量 //用户自己设定了初始化大小 else if (oldThr &gt; 0) newCap = oldThr; //初始化时没有指定阈值和容量，使用默认的容量16和阈值16*0.75=12 //如果没使用,使用默认值初始化 else &#123; newCap = DEFAULT_INITIAL_CAPACITY; newThr = (int)(DEFAULT_LOAD_FACTOR * DEFAULT_INITIAL_CAPACITY); &#125; // 计算新的resize上限 //用户自定义了map的初始化操作 if (newThr == 0) &#123; float ft = (float)newCap * loadFactor; newThr = (newCap &lt; MAXIMUM_CAPACITY &amp;&amp; ft &lt; (float)MAXIMUM_CAPACITY ? (int)ft : Integer.MAX_VALUE); &#125; //更新阈值 //新的容量 threshold = newThr; //创建表,初始化或更新表 @SuppressWarnings(&#123;"rawtypes","unchecked"&#125;) Node&lt;K,V&gt;[] newTab = (Node&lt;K,V&gt;[])new Node[newCap]; table = newTab; //如果属于容量扩展，rehash操作 if (oldTab != null) &#123; // 把每个bucket都移动到新的buckets中 //遍历旧表 for (int j = 0; j &lt; oldCap; ++j) &#123; Node&lt;K,V&gt; e; //如果该桶处存在数据 if ((e = oldTab[j]) != null) &#123; //将旧表数据置为null，帮助gc oldTab[j] = null; //如果只有一个节点，直接在新表中赋值 //如果当前位置只有一个元素,直接移动到新的位置 if (e.next == null) newTab[e.hash &amp; (newCap - 1)] = e; //如果该节点已经为红黑树 else if (e instanceof TreeNode) ((TreeNode&lt;K,V&gt;)e).split(this, newTab, j, oldCap); //如果没超过8个 是链表 //如果该桶处仍为链表 // 链表优化重hash的代码块 else &#123; //下面这段暂时没有太明白，通过e.hash &amp; oldCap将链表分为两队，参考知乎上的一段解释 /** * 把链表上的键值对按hash值分成lo和hi两串，lo串的新索引位置与原先相同[原先位 * j]，hi串的新索引位置为[原先位置j+oldCap]； * 链表的键值对加入lo还是hi串取决于 判断条件if ((e.hash &amp; oldCap) == 0)，因为* capacity是2的幂，所以oldCap为10...0的二进制形式，若判断条件为真，意味着 * oldCap为1的那位对应的hash位为0，对新索引的计算没有影响（新索引 * =hash&amp;(newCap-*1)，newCap=oldCap&lt;&lt;2）；若判断条件为假，则 oldCap为1的那位* 对应的hash位为1， * 即新索引=hash&amp;( newCap-1 )= hash&amp;( (oldCap&lt;&lt;2) - 1)，相当于多了10...0， * 即 oldCap * 例子： * 旧容量=16，二进制10000；新容量=32，二进制100000 * 旧索引的计算： * hash = xxxx xxxx xxxy xxxx * 旧容量-1 1111 * &amp;运算 xxxx * 新索引的计算： * hash = xxxx xxxx xxxy xxxx * 新容量-1 1 1111 * &amp;运算 y xxxx * 新索引 = 旧索引 + y0000，若判断条件为真，则y=0(lo串索引不变)，否则y=1(hi串 * 索引=旧索引+旧容量10000) */ Node&lt;K,V&gt; loHead = null, loTail = null; Node&lt;K,V&gt; hiHead = null, hiTail = null; Node&lt;K,V&gt; next;//此处的操作是这样子的 因为是扩容一倍的操作,所以与旧的容量进行与操作后只有两个值0 和 1//如果是0就位置不变,如果是1就移动n+old的位置,//个人认为这么做的好处是:/*** 1.不会像之前1.7发生循环依赖的问题* 2.从概率的角度上来看可以均匀分配,(一般来说高位和低位比例差不多)* 3.提高效率*/ do &#123; next = e.next; // 原索引 //如果和旧容量位运算后的值是0,记得当前节点和存放在链表的尾部 if ((e.hash &amp; oldCap) == 0) &#123; if (loTail == null) loHead = e; else loTail.next = e; loTail = e; &#125; //同上 else &#123; if (hiTail == null) hiHead = e; else hiTail.next = e; hiTail = e; &#125; &#125; while ((e = next) != null); // 原索引放到bucket里 //为0的还是存放在当前位置 if (loTail != null) &#123; loTail.next = null; newTab[j] = loHead; &#125; // 原索引+oldCap放到bucket里 //为1的就放在扩容的j + oldCap那边去 if (hiTail != null) &#123; hiTail.next = null; newTab[j + oldCap] = hiHead; &#125; &#125; &#125; &#125; &#125; return newTab; &#125; 因为不像Java8之前的HashMap有初始化操作,此处选择将初始化和扩容放在了一起,并且又增加了红黑树的概念,所以导致整个方法的判断次数非常多,也是这个方法比较庞大的主要原因. 值得一体的是,在扩容后重新计算位置的时候,对链表进行优化,有兴趣可以搜索一下HashMap导致cpu百分之百的问题 而在Java中通过巧妙的进行&amp;操作,然后获得高位是为0还是1.最终移动的位置就是低位的链表留在原地,高位的放在index+oldsize的地方就可以了,不用为每一个元素计算hash值,然后移动到对应的位置,再判断是否是链表,是否需要转换成树的操作.如下所示. 12hashcode: 1111111111111101212oldcap: 0000000000000010000 很容易知道这个&amp;操作之后就是为0,因为oldcap都是2的倍数,只有高位为1,所以通过&amp;确认高位要比%取余高效. 此时在看一下上面的扩容操作也许就更清晰了. resize()首先获取新容量以及新阈值，然后根据新容量创建新表。如果是扩容操作，则需要进行rehash操作，通过e.hash&amp;oldCap将链表分为两列，更好地均匀分布在新表中。 newNode()方法下面介绍一些newNode方法.就是新建一个节点.可以思考一下为什么要把newNode抽离出来? 123Node&lt;K,V&gt; newNode(int hash, K key, V value, Node&lt;K,V&gt; next) &#123; return new Node&lt;&gt;(hash, key, value, next);&#125; putTreeVal() 方法添加节点到红黑树的方法是Java8中新添加的,需要满足链表的长度到8,才会转换成红黑树,其主要目的是防止某个下标处的链表太长,导致在找到的时候速度很慢,下面看一下实现 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950//尝试着往树节点添加值final TreeNode&lt;K,V&gt; putTreeVal(HashMap&lt;K,V&gt; map, Node&lt;K,V&gt;[] tab,int h, K k, V v) &#123; Class&lt;?&gt; kc = null; boolean searched = false; //找到根节点 TreeNode&lt;K,V&gt; root = (parent != null) ? root() : this; for (TreeNode&lt;K,V&gt; p = root;;) &#123; int dir, ph; K pk; if ((ph = p.hash) &gt; h) dir = -1; else if (ph &lt; h) dir = 1; //存在的话直接返回,用于替换 else if ((pk = p.key) == k || (k != null &amp;&amp; k.equals(pk))) return p; //判断节点类型是否相同,不相同 else if ((kc == null &amp;&amp; (kc = comparableClassFor(k)) == null) || (dir = compareComparables(kc, k, pk)) == 0) &#123; //没有搜索过,搜索子节点,搜过了说明没有就跳过. if (!searched) &#123; TreeNode&lt;K,V&gt; q, ch; searched = true; //去子节点去找 if (((ch = p.left) != null &amp;&amp;(q = ch.find(h, k, kc)) != null) ||((ch = p.right) != null &amp;&amp; (q = ch.find(h, k, kc)) != null)) return q; &#125; //对比hash值,决定查找的方向 dir = tieBreakOrder(k, pk); &#125; TreeNode&lt;K,V&gt; xp = p; //找到子节点为空,就可以加进去,设置层级关系 if ((p = (dir &lt;= 0) ? p.left : p.right) == null) &#123; Node&lt;K,V&gt; xpn = xp.next; TreeNode&lt;K,V&gt; x = map.newTreeNode(h, k, v, xpn); if (dir &lt;= 0) xp.left = x; else xp.right = x; xp.next = x; x.parent = x.prev = xp; if (xpn != null) ((TreeNode&lt;K,V&gt;)xpn).prev = x; moveRootToFront(tab, balanceInsertion(root, x)); return null; &#125; &#125;&#125; 这里简单的梳理一下流程. 1.从根节点查找,找到了返回,如果没找到,找字节点 2.判断往哪个方向去查找 3.如果不存在,在子节点末端添加新节点 split() 方法树的 split() 方法,主要是扩容操作,重新结算位置需要分裂树,之前讲过,扩容会根据和旧 map 容量进行&amp;操作,移动高位为1的节点.并且验证新的节点列表是否需要重新转换成链表的形式. 当头节点是TreeNode时，将调用TreeNode的split方法将红黑树复制到新表中，代码实现如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576final void split(HashMap&lt;K,V&gt; map, Node&lt;K,V&gt;[] tab, int index, int bit) &#123; TreeNode&lt;K,V&gt; b = this;//就是上面的头结点e // 设置记录高低位的node,和链表一样都是计算高位是0还是1 //与链表rehash时类似，将红黑树分为两部分 TreeNode&lt;K,V&gt; loHead = null, loTail = null; TreeNode&lt;K,V&gt; hiHead = null, hiTail = null; int lc = 0, hc = 0; //遍历 for (TreeNode&lt;K,V&gt; e = b, next; e != null; e = next) &#123; next = (TreeNode&lt;K,V&gt;)e.next; e.next = null; //还是和旧的容量做位运算,为0的放在lo中 //分散规则与rehash中相同 if ((e.hash &amp; bit) == 0) &#123; //判断是否为头部 if ((e.prev = loTail) == null) loHead = e; else loTail.next = e; loTail = e; ++lc; &#125; //获取为1的放在hi中 else &#123; if ((e.prev = hiTail) == null) hiHead = e; else hiTail.next = e; hiTail = e; ++hc; &#125; &#125; //lo链表的处理 //如果存在低端 if (loHead != null) &#123; //如果小于7,那么当做链表处理 //如果分散后的红黑树节点小于等于6，将红黑树节点转换成链表节点 if (lc &lt;= UNTREEIFY_THRESHOLD) tab[index] = loHead.untreeify(map); else &#123; //转换成树 tab[index] = loHead; //将链表转换成红黑树 if (hiHead != null) // (else is already treeified) loHead.treeify(tab); &#125; &#125; //同上 //如果存在高端 if (hiHead != null) &#123; //如果分散后的红黑树节点小于等于6，将红黑树节点转换成链表节点 if (hc &lt;= UNTREEIFY_THRESHOLD) tab[index + bit] = hiHead.untreeify(map); else &#123; tab[index + bit] = hiHead; //将链表转换成红黑树节点 if (loHead != null) hiHead.treeify(tab); &#125; &#125; &#125;//把树转换成链表final Node&lt;K,V&gt; untreeify(HashMap&lt;K,V&gt; map) &#123; Node&lt;K,V&gt; hd = null, tl = null; for (Node&lt;K,V&gt; q = this; q != null; q = q.next) &#123; Node&lt;K,V&gt; p = map.replacementNode(q, null); if (tl == null) hd = p; else tl.next = p; tl = p; &#125; return hd;&#125; TreeNode的split方法首先将头节点从头开始遍历，区分出两条单链表，再根据如果节点数小于等于6，那么将单链表的每个TreeNode转换成Node节点；否则将单链表转换成红黑树结构。至此，resize()方法结束。需要注意的是rehash时，由于容量扩大一倍，本来一条链表有可能会分成两条链表，而如果将红黑树结构复制到新表时，有可能需要完成红黑树到单链表的转换。 treeifyBin()方法treeifyBin()方法将表中某一个桶处的单链表结果转换成红黑树结构，其实现如下： 12345678910111213141516171819202122232425final void treeifyBin(Node&lt;K,V&gt;[] tab, int hash) &#123; int n, index; Node&lt;K,V&gt; e; //如果哈希表不存在，或者哈希表尺寸小于64，进行resize()扩容 if (tab == null || (n = tab.length) &lt; MIN_TREEIFY_CAPACITY) resize(); //如果桶处头节点不为null else if ((e = tab[index = (n - 1) &amp; hash]) != null) &#123; TreeNode&lt;K,V&gt; hd = null, tl = null; //将单链表节点转换成TreeNode结构的单链表 do &#123; //将Node转换成TreeNode TreeNode&lt;K,V&gt; p = replacementTreeNode(e, null); if (tl == null) hd = p; else &#123; p.prev = tl; tl.next = p; &#125; tl = p; &#125; while ((e = e.next) != null); //调用treeify将该TreeNode结构的单链表转换成红黑树 if ((tab[index] = hd) != null) hd.treeify(tab); &#125; &#125; 上述操作做了这些事: 根据哈希表中元素个数确定是扩容还是树形化 如果是树形化 遍历桶中的元素，创建相同个数的树形节点，复制内容，建立起联系 然后让桶第一个元素指向新建的树头结点，替换桶的链表内容为树形内容 treeify() 方法但是我们发现，之前的操作并没有设置红黑树的颜色值，现在得到的只能算是个二叉树。在 最后调用树形节点 hd.treeify(tab) 方法进行塑造红黑树，来看看代码： 123456789101112131415161718192021222324252627282930313233343536373839404142434445 final void treeify(Node[] tab) &#123; TreeNode root = null; for (TreeNode x = this, next; x != null; x = next) &#123; next = (TreeNode)x.next; x.left = x.right = null; if (root == null) &#123; //头回进入循环，确定头结点，为黑色 x.parent = null; x.red = false; root = x; &#125; else &#123; //后面进入循环走的逻辑，x 指向树中的某个节点 K k = x.key; int h = x.hash; Class kc = null; //又一个循环，从根节点开始，遍历所有节点跟当前节点 x 比较，调整位置，有点像冒泡排序 for (TreeNode p = root;;) &#123; int dir, ph; //这个 dir K pk = p.key; if ((ph = p.hash) &gt; h) //当比较节点的哈希值比 x 大时， dir 为 -1 dir = -1; else if (ph &lt; h) //哈希值比 x 小时 dir 为 1 dir = 1; else if ((kc == null &amp;&amp; (kc = comparableClassFor(k)) == null) || (dir = compareComparables(kc, k, pk)) == 0) // 如果比较节点的哈希值、 x dir = tieBreakOrder(k, pk); //把 当前节点变成 x 的父亲 //如果当前比较节点的哈希值比 x 大，x 就是左孩子，否则 x 是右孩子 TreeNode xp = p; if ((p = (dir &lt;= 0) ? p.left : p.right) == null) &#123; x.parent = xp; if (dir &lt;= 0) xp.left = x; else xp.right = x; root = balanceInsertion(root, x); break; &#125; &#125; &#125; &#125; moveRootToFront(tab, root);&#125; put操作总结当调用put插入一个键值对时，在表为空时，会创建表。如果桶为空时，直接插入节点，如果桶不为空时，则需要对当前桶中包含的结构做判断，如果已是红黑树结构，那么需要使用红黑树的插入方法；如果不是红黑树结构，则需要遍历链表，如果添加到链表后端，如果该条链表达到了8，那么需要将该链表转换成红黑树，从treeifyBin方法可以看到，当容量小于64时，不会进行红黑树转换，只会扩容。当成功新加一个桶，那么需要将尺寸和阈值进行判断，是否需要进行resize()操作。 get(K k)操作get(K k)根据键得到值，如果值不存在，那么返回null。其实现如下： 12345678910111213141516171819202122232425262728293031public V get(Object key) &#123; Node&lt;K,V&gt; e; return (e = getNode(hash(key), key)) == null ? null : e.value; &#125;//根据键的hash值和键得到对应节点final Node&lt;K,V&gt; getNode(int hash, Object key) &#123; Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; first, e; int n; K k; //可以从桶中得到对应hash值的第一个节点 if ((tab = table) != null &amp;&amp; (n = tab.length) &gt; 0 &amp;&amp; (first = tab[(n - 1) &amp; hash]) != null) &#123; //检查首节点，如果首节点匹配，那么直接返回首节点 if (first.hash == hash &amp;&amp; // always check first node ((k = first.key) == key || (key != null &amp;&amp; key.equals(k)))) return first; //如果首节点还有后续节点 if ((e = first.next) != null) &#123; //如果首节点是红黑树节点，调用getTreeNode()方法 if (first instanceof TreeNode) return ((TreeNode&lt;K,V&gt;)first).getTreeNode(hash, key); //首节点是链表结构，从前往后遍历 do &#123; //一旦匹配，返回节点 if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) return e; &#125; while ((e = e.next) != null); &#125; &#125; return null; &#125; 从上面代码可以看到getNode()方法中有多种情况：\1. 表为空或表的长度为0或表中不存在key对应的hash值桶，那么返回null\2. 如果表中有key对应hash值的桶，得到首节点，如果首节点匹配，那么直接返回；\3. 如果首节点不匹配，并且没有后续节点，那么返回null\4. 如果首节点有后续节点并且首节点是TreeNode,调用getTreeNode方法寻找节点\5. 如果首节点有后续节点并且是链表结构，那么从前往后遍历，一旦找到则返回节点，否则返回null remove()操作remove(K k)用于根据键值删除键值对，如果哈希表中存在该键，那么返回键对应的值，否则返回null。其实现如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758public V remove(Object key) &#123; Node&lt;K,V&gt; e; return (e = removeNode(hash(key), key, null, false, true)) == null ? null : e.value; &#125;//按照hash和key删除节点，如果不存在节点，则返回nullfinal Node&lt;K,V&gt; removeNode(int hash, Object key, Object value, boolean matchValue, boolean movable) &#123; Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; p; int n, index; //如果哈希表不为空并且存在桶与hash值匹配,p为桶中的头节点 if ((tab = table) != null &amp;&amp; (n = tab.length) &gt; 0 &amp;&amp; (p = tab[index = (n - 1) &amp; hash]) != null) &#123; Node&lt;K,V&gt; node = null, e; K k; V v; //case 1：如果头节点匹配 if (p.hash == hash &amp;&amp; ((k = p.key) == key || (key != null &amp;&amp; key.equals(k)))) node = p; else if ((e = p.next) != null) &#123; //case2：如果头节点不匹配，且头节点是TreeNode，即桶中的结构为红黑树结构 if (p instanceof TreeNode) node = ((TreeNode&lt;K,V&gt;)p).getTreeNode(hash, key); else &#123; //case 3:如果头节点不匹配，且头节点是Node，即桶中的结构为链表结构，遍历链表 do &#123; //一旦匹配，跳出循环 if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) &#123; node = e; break; &#125; p = e; &#125; while ((e = e.next) != null); &#125; &#125; //如果存在待删除节点节点 if (node != null &amp;&amp; (!matchValue || (v = node.value) == value || (value != null &amp;&amp; value.equals(v)))) &#123; //如果节点是TreeNode，使用红黑树的方法 if (node instanceof TreeNode) ((TreeNode&lt;K,V&gt;)node).removeTreeNode(this, tab, movable); //如果待删除节点是头节点，更改桶中的头节点即可 else if (node == p) tab[index] = node.next; //在链表遍历过程中，p代表node节点的前驱节点 else p.next = node.next; ++modCount; --size; //子类实现 afterNodeRemoval(node); return node; &#125; &#125; return null; &#125; 从上面的代码可以看出，removeNode()方法首先是找到待删除的节点，如果存在待删除节点，接下来再执行删除操作。查询时流程与getNode()方法的流程类似，只不过多了在遍历链表时还需要保存前驱节点，因为后面删除时要用到（单链表结构）。删除节点时就比较简单了，三种情况三种处理方式,分别是：\1. 如果待删除节点是TreeNode，那么调用removeTreeNode()方法\2. 如果待删除节点是Node，并且待删除节点就是头节点，那么将头节点更改为原有节点的下一个节点就可以了\3. 如果待删除节点是Node且待删除节点不是头节点，那么将遍历过程中保存的前驱节点p的后继节点设为node的后继节点就可以了 红黑树中查找元素 getTreeNode()HashMap 的查找方法是 get(): 1234public V get(Object key) &#123; Node e; return (e = getNode(hash(key), key)) == null ? null : e.value;&#125; 它通过计算指定 key 的哈希值后，调用内部方法 getNode()； 12345678910111213141516171819final Node getNode(int hash, Object key) &#123; Node[] tab; Node first, e; int n; K k; if ((tab = table) != null &amp;&amp; (n = tab.length) &gt; 0 &amp;&amp; (first = tab[(n - 1) &amp; hash]) != null) &#123; if (first.hash == hash &amp;&amp; // always check first node ((k = first.key) == key || (key != null &amp;&amp; key.equals(k)))) return first; if ((e = first.next) != null) &#123; if (first instanceof TreeNode) return ((TreeNode)first).getTreeNode(hash, key); do &#123; if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) return e; &#125; while ((e = e.next) != null); &#125; &#125; return null;&#125; 这个 getNode() 方法就是根据哈希表元素个数与哈希值求模（使用的公式是 (n - 1) &amp;hash）得到 key 所在的桶的头结点，如果头节点恰好是红黑树节点，就调用红黑树节点的 getTreeNode() 方法，否则就遍历链表节点。 123final TreeNode getTreeNode(int h, Object k) &#123; return ((parent != null) ? root() : this).find(h, k, null);&#125; getTreeNode 方法使通过调用树形节点的 find() 方法进行查找： 123456789101112131415161718192021222324252627//从根节点根据 哈希值和 key 进行查找final TreeNode find(int h, Object k, Class kc) &#123; TreeNode p = this; do &#123; int ph, dir; K pk; TreeNode pl = p.left, pr = p.right, q; if ((ph = p.hash) &gt; h) p = pl; else if (ph &lt; h) p = pr; else if ((pk = p.key) == k || (k != null &amp;&amp; k.equals(pk))) return p; else if (pl == null) p = pr; else if (pr == null) p = pl; else if ((kc != null || (kc = comparableClassFor(k)) != null) &amp;&amp; (dir = compareComparables(kc, k, pk)) != 0) p = (dir &lt; 0) ? pl : pr; else if ((q = pr.find(h, k, kc)) != null) return q; else p = pl; &#125; while (p != null); return null;&#125; 由于之前添加时已经保证这个树是有序的，因此查找时基本就是折半查找，效率很高。 这里和插入时一样，如果对比节点的哈希值和要查找的哈希值相等，就会判断 key 是否相等，相等就直接返回（也没有判断值哎）；不相等就从子树中递归查找。 HashMap总结至此，我们分析完了HashMap的主要方法：构造器、put、get和remove。只需要明白JDK1.8的HashMap底层结构，那么就很好理解了。需要注意的是什么时候应该将链表结构转换成红黑树结构，什么时候又应该将红黑树结构重新转换成链表结构，本文没有具体解释有关红黑树的结构，但是这并不影响理解HashMap的基本原理。另外需要注意的是，本文的源码是基于JDK1.8的。]]></content>
      <categories>
        <category>HashMap</category>
      </categories>
      <tags>
        <tag>基础数据类型</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HashMap 的扩容机制]]></title>
    <url>%2F2018%2F06%2F03%2FHashMap1.7%20%E5%92%8C%201.8%20%E6%89%A9%E5%AE%B9%E6%9C%BA%E5%88%B6%E7%9A%84%E6%AF%94%E8%BE%83%2F</url>
    <content type="text"><![CDATA[扩容机制扩容(resize)就是重新计算容量，向HashMap对象里不停的添加元素，而HashMap对象内部的数组无法装载更多的元素时，对象就需要扩大数组的长度，以便能装入更多的元素。当然Java里的数组是无法自动扩容的，方法是使用一个新的数组代替已有的容量小的数组，就像我们用一个小桶装水，如果想装更多的水，就得换大水桶。 我们分析下resize的源码，鉴于JDK1.8融入了红黑树，较复杂，为了便于理解我们仍然使用JDK1.7的代码，好理解一些，本质上区别不大，具体区别后文再说。 12345678910111213 1 void resize(int newCapacity) &#123; //传入新的容量 2 Entry[] oldTable = table; //引用扩容前的Entry数组 3 int oldCapacity = oldTable.length; 4 if (oldCapacity == MAXIMUM_CAPACITY) &#123; //扩容前的数组大小如果已经达到最大(2^30)了 5 threshold = Integer.MAX_VALUE; //修改阈值为int的最大值(2^31-1)，这样以后就不会扩容了 6 return; 7 &#125; 8 9 Entry[] newTable = new Entry[newCapacity]; //初始化一个新的Entry数组10 transfer(newTable); //！！将数据转移到新的Entry数组里11 table = newTable; //HashMap的table属性引用新的Entry数组12 threshold = (int)(newCapacity * loadFactor);//修改阈值13 &#125; 这里就是使用一个容量更大的数组来代替已有的容量小的数组，transfer()方法将原有Entry数组的元素拷贝到新的Entry数组里。 1234567891011121314151617 1 void transfer(Entry[] newTable) &#123; 2 Entry[] src = table; //src引用了旧的Entry数组 3 int newCapacity = newTable.length; 4 for (int j = 0; j &lt; src.length; j++) &#123; //遍历旧的Entry数组 5 Entry&lt;K,V&gt; e = src[j]; //取得旧Entry数组的每个元素 6 if (e != null) &#123; 7 src[j] = null;//释放旧Entry数组的对象引用（for循环后，旧的Entry数组不再引用任何对象） 8 do &#123; 9 Entry&lt;K,V&gt; next = e.next;10 int i = indexFor(e.hash, newCapacity); //！！重新计算每个元素在数组中的位置11 e.next = newTable[i]; //标记[1]12 newTable[i] = e; //将元素放在数组上13 e = next; //访问下一个Entry链上的元素14 &#125; while (e != null);15 &#125;16 &#125;17 &#125; newTable[i]的引用赋给了e.next，也就是使用了单链表的头插入方式，同一位置上新元素总会被放在链表的头部位置；这样先放在一个索引上的元素终会被放到Entry链的尾部(如果发生了hash冲突的话），这一点和Jdk1.8有区别，下文详解。在旧数组中同一条Entry链上的元素，通过重新计算索引位置后，有可能被放到了新数组的不同位置上。 下面举个例子说明下扩容过程。假设了我们的hash算法就是简单的用key mod 一下表的大小（也就是数组的长度）。其中的哈希桶数组table的size=2， 所以key = 3、7、5，put顺序依次为 5、7、3。在mod 2以后都冲突在table[1]这里了。这里假设负载因子 loadFactor=1，即当键值对的实际大小size 大于 table的实际大小时进行扩容。接下来的三个步骤是哈希桶数组 resize成4，然后所有的Node重新rehash的过程。 下面我们讲解下JDK1.8做了哪些优化。经过观测可以发现，我们使用的是2次幂的扩展(指长度扩为原来2倍)，所以，元素的位置要么是在原位置，要么是在原位置再移动2次幂的位置。看下图可以明白这句话的意思，n为table的长度，图（a）表示扩容前的key1和key2两种key确定索引位置的示例，图（b）表示扩容后key1和key2两种key确定索引位置的示例，其中hash1是key1对应的哈希与高位运算结果。 元素在重新计算hash之后，因为n变为2倍，那么n-1的mask范围在高位多1bit(红色)，因此新的index就会发生这样的变化： 因此，我们在扩充HashMap的时候，不需要像JDK1.7的实现那样重新计算hash，只需要看看原来的hash值新增的那个bit是1还是0就好了，是0的话索引没变，是1的话索引变成“原索引+oldCap”，可以看看下图为16扩充为32的resize示意图： 这个设计确实非常的巧妙，既省去了重新计算hash值的时间，而且同时，由于新增的1bit是0还是1可以认为是随机的，因此resize的过程，均匀的把之前的冲突的节点分散到新的bucket了。这一块就是JDK1.8新增的优化点。有一点注意区别，JDK1.7中rehash的时候，旧链表迁移新链表的时候，如果在新表的数组索引位置相同，则链表元素会倒置，但是从上图可以看出，JDK1.8不会倒置。有兴趣的同学可以研究下JDK1.8的resize源码，写的很赞，如下: 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182 1 final Node&lt;K,V&gt;[] resize() &#123; 2 Node&lt;K,V&gt;[] oldTab = table; 3 int oldCap = (oldTab == null) ? 0 : oldTab.length; 4 int oldThr = threshold; 5 int newCap, newThr = 0; 6 if (oldCap &gt; 0) &#123; 7 // 超过最大值就不再扩充了，就只好随你碰撞去吧 8 if (oldCap &gt;= MAXIMUM_CAPACITY) &#123; 9 threshold = Integer.MAX_VALUE;10 return oldTab;11 &#125;12 // 没超过最大值，就扩充为原来的2倍13 else if ((newCap = oldCap &lt;&lt; 1) &lt; MAXIMUM_CAPACITY &amp;&amp;14 oldCap &gt;= DEFAULT_INITIAL_CAPACITY)15 newThr = oldThr &lt;&lt; 1; // double threshold16 &#125;17 else if (oldThr &gt; 0) // initial capacity was placed in threshold18 newCap = oldThr;19 else &#123; // zero initial threshold signifies using defaults20 newCap = DEFAULT_INITIAL_CAPACITY;21 newThr = (int)(DEFAULT_LOAD_FACTOR * DEFAULT_INITIAL_CAPACITY);22 &#125;23 // 计算新的resize上限24 if (newThr == 0) &#123;25 26 float ft = (float)newCap * loadFactor;27 newThr = (newCap &lt; MAXIMUM_CAPACITY &amp;&amp; ft &lt; (float)MAXIMUM_CAPACITY ?28 (int)ft : Integer.MAX_VALUE);29 &#125;30 threshold = newThr;31 @SuppressWarnings(&#123;"rawtypes"，"unchecked"&#125;)32 Node&lt;K,V&gt;[] newTab = (Node&lt;K,V&gt;[])new Node[newCap];33 table = newTab;34 if (oldTab != null) &#123;35 // 把每个bucket都移动到新的buckets中36 for (int j = 0; j &lt; oldCap; ++j) &#123;37 Node&lt;K,V&gt; e;38 if ((e = oldTab[j]) != null) &#123;39 oldTab[j] = null;40 if (e.next == null)41 newTab[e.hash &amp; (newCap - 1)] = e;42 else if (e instanceof TreeNode)43 ((TreeNode&lt;K,V&gt;)e).split(this, newTab, j, oldCap);44 else &#123; // 链表优化重hash的代码块45 Node&lt;K,V&gt; loHead = null, loTail = null;46 Node&lt;K,V&gt; hiHead = null, hiTail = null;47 Node&lt;K,V&gt; next;48 do &#123;49 next = e.next;50 // 原索引51 if ((e.hash &amp; oldCap) == 0) &#123;52 if (loTail == null)53 loHead = e;54 else55 loTail.next = e;56 loTail = e;57 &#125;58 // 原索引+oldCap59 else &#123;60 if (hiTail == null)61 hiHead = e;62 else63 hiTail.next = e;64 hiTail = e;65 &#125;66 &#125; while ((e = next) != null);67 // 原索引放到bucket里68 if (loTail != null) &#123;69 loTail.next = null;70 newTab[j] = loHead;71 &#125;72 // 原索引+oldCap放到bucket里73 if (hiTail != null) &#123;74 hiTail.next = null;75 newTab[j + oldCap] = hiHead;76 &#125;77 &#125;78 &#125;79 &#125;80 &#125;81 return newTab;82 &#125;]]></content>
      <categories>
        <category>HashMap</category>
      </categories>
      <tags>
        <tag>基础数据类型</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HashMap 的详细分析]]></title>
    <url>%2F2018%2F06%2F03%2FHashMap%20%E7%9A%84%E8%AF%A6%E7%BB%86%E5%88%86%E6%9E%90%2F</url>
    <content type="text"><![CDATA[HashMap 的详细分析前言这次我和大家一起学习HashMap，HashMap我们在工作中经常会使用，而且面试中也很频繁会问到，因为它里面蕴含着很多知识点，可以很好的考察个人基础。但一个这么重要的东西，我为什么没有在一开始就去学习它呢，因为它是由多种基础的数据结构和一些代码设计思想组成的。我们要学习了这些基础，再学习HashMap，这样我们才能更好的去理解它。古人云：无欲速，无见小利。欲速则不达，见小利则大事不成。 HashMap其实就是ArrayList和LinkedList的数据结构加上hashCode和equals方法的思想设计出来的。没有理解上述说的知识点的同学可以翻开我过往的文章记录。 下面我就以面试问答的形式学习我们的——HashMap（源码分析基于JDK8，辅以JDK7），问答内容只是对HashMap的一个总结归纳，因为现时已经有大牛把HashMap通俗易懂的剖析了一遍，我学习HashMap也是主要通过这篇文章学习的，强烈推荐：美团点评技术团队的Java 8系列之重新认识HashMap 问答内容HashMap 的主要用途问：HashMap有用过吗？您能给我说说他的主要用途吗？ 答： 有用过，我在平常工作中经常会用到HashMap这种数据结构，HashMap是基于Map接口实现的一种键-值对&lt;key,value&gt;的存储结构，允许null值，同时非有序，非同步(即线程不安全)。HashMap的底层实现是数组 + 链表 + 红黑树（JDK1.8增加了红黑树部分）。它存储和查找数据时，是根据键key的hashCode的值计算出具体的存储位置。HashMap最多只允许一条记录的键key为null，HashMap增删改查等常规操作都有不错的执行效率，是ArrayList和LinkedList等数据结构的一种折中实现。 示例代码： 12345678910111213141516171819202122232425262728293031323334// 创建一个HashMap，如果没有指定初始大小，默认底层hash表数组的大小为16HashMap&lt;String, String&gt; hashMap = new HashMap&lt;String, String&gt;();// 往容器里面添加元素hashMap.put("小明", "好帅");hashMap.put("老王", "坑爹货");hashMap.put("老铁", "没毛病");hashMap.put("掘金", "好地方");hashMap.put("王五", "别搞事");// 获取key为小明的元素 好帅String element = hashMap.get("小明");// value : 好帅System.out.println(element);// 移除key为王五的元素String removeElement = hashMap.remove("王五");// value : 别搞事System.out.println(removeElement);// 修改key为小明的元素的值value 为 其实有点丑hashMap.replace("小明", "其实有点丑");// &#123;老铁=没毛病, 小明=其实有点丑, 老王=坑爹货, 掘金=好地方&#125;System.out.println(hashMap);// 通过put方法也可以达到修改对应元素的值的效果hashMap.put("小明", "其实还可以啦,开玩笑的");// &#123;老铁=没毛病, 小明=其实还可以啦,开玩笑的, 老王=坑爹货, 掘金=好地方&#125;System.out.println(hashMap);// 判断key为老王的元素是否存在(捉奸老王)boolean isExist = hashMap.containsKey("老王");// true , 老王竟然来搞事System.out.println(isExist);// 判断是否有 value = "坑爹货" 的人boolean isHasSomeOne = hashMap.containsValue("坑爹货");// true 老王是坑爹货System.out.println(isHasSomeOne);// 查看这个容器里面还有几个家伙 value : 4System.out.println(hashMap.size()); HashMap的底层实现是数组 + 链表 + 红黑树（JDK1.8增加了红黑树部分），核心组成元素有： int size;用于记录HashMap实际存储元素的个数； float loadFactor;负载因子（默认是0.75，此属性后面详细解释）。 int threshold;下一次扩容时的阈值，达到阈值便会触发扩容机制resize（阈值 threshold = 容器容量 capacity * 负载因子 load factor）。也就是说，在容器定义好容量之后，负载因子越大，所能容纳的键值对元素个数就越多。 Node&lt;K,V&gt;[] table; 底层数组，充当哈希表的作用，用于存储对应hash位置的元素Node&lt;K,V&gt;，此数组长度总是2的N次幂。（具体原因后面详细解释） 示例代码： 12345678910111213141516171819202122232425262728293031323334public class HashMap&lt;K,V&gt; extends AbstractMap&lt;K,V&gt; implements Map&lt;K,V&gt;, Cloneable, Serializable &#123;····· /* ---------------- Fields -------------- */ /** * 哈希表，在第一次使用到时进行初始化，重置大小是必要的操作， * 当分配容量时，长度总是2的N次幂。 */ transient Node&lt;K,V&gt;[] table; /** * 实际存储的key - value 键值对 个数 */ transient int size; /** * 下一次扩容时的阈值 * (阈值 threshold = 容器容量 capacity * 负载因子 load factor). * @serial */ int threshold; /** * 哈希表的负载因子 * * @serial */ final float loadFactor;·····&#125; 其中Node&lt;K,V&gt;[] table;哈希表存储的核心元素是Node&lt;K,V&gt;,Node&lt;K,V&gt;包含： final int hash;元素的哈希值，决定元素存储在Node&lt;K,V&gt;[] table;哈希表中的位置。由final修饰可知，当hash的值确定后，就不能再修改。 final K key; 键，由final修饰可知，当key的值确定后，就不能再修改。 V value; 值 Node&lt;K,V&gt; next; 记录下一个元素结点(单链表结构，用于解决hash冲突) 示例代码： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556/** * 定义HashMap存储元素结点的底层实现 */static class Node&lt;K,V&gt; implements Map.Entry&lt;K,V&gt; &#123; final int hash;//元素的哈希值 由final修饰可知，当hash的值确定后，就不能再修改 final K key;// 键，由final修饰可知，当key的值确定后，就不能再修改 V value; // 值 Node&lt;K,V&gt; next; // 记录下一个元素结点(单链表结构，用于解决hash冲突) /** * Node结点构造方法 */ Node(int hash, K key, V value, Node&lt;K,V&gt; next) &#123; this.hash = hash;//元素的哈希值 this.key = key;// 键 this.value = value; // 值 this.next = next;// 记录下一个元素结点 &#125; public final K getKey() &#123; return key; &#125; public final V getValue() &#123; return value; &#125; public final String toString() &#123; return key + "=" + value; &#125; /** * 为Node重写hashCode方法，值为：key的hashCode 异或 value的hashCode * 运算作用就是将2个hashCode的二进制中，同一位置相同的值为0，不同的为1。 */ public final int hashCode() &#123; return Objects.hashCode(key) ^ Objects.hashCode(value); &#125; /** * 修改某一元素的值 */ public final V setValue(V newValue) &#123; V oldValue = value; value = newValue; return oldValue; &#125; /** * 为Node重写equals方法 */ public final boolean equals(Object o) &#123; if (o == this) return true; if (o instanceof Map.Entry) &#123; Map.Entry&lt;?,?&gt; e = (Map.Entry&lt;?,?&gt;)o; if (Objects.equals(key, e.getKey()) &amp;&amp; Objects.equals(value, e.getValue())) return true; &#125; return false; &#125;&#125; hashMap内存结构图 - 图片来自于《美团点评技术团队文章》 HashMap 常用操作的底层实现原理问：您能说说HashMap常用操作的底层实现原理吗？如存储put(K key, V value)，查找get(Object key)，删除remove(Object key)，修改replace(K key, V value)等操作。 答： 调用put(K key, V value)操作添加key-value键值对时，进行了如下操作： 判断哈希表Node&lt;K,V&gt;[] table是否为空或者null，是则执行resize()方法进行扩容。 根据插入的键值key的hash值，通过(n - 1) &amp; hash当前元素的hash值 &amp; hash表长度 - 1（实际就是 hash值 % hash表长度） 计算出存储位置table[i]。如果存储位置没有元素存放，则将新增结点存储在此位置table[i]。 如果存储位置已经有键值对元素存在，则判断该位置元素的hash值和key值是否和当前操作元素一致，一致则证明是修改value操作，覆盖value即可。 当前存储位置即有元素，又不和当前操作元素一致，则证明此位置table[i]已经发生了hash冲突，则通过判断头结点是否是treeNode，如果是treeNode则证明此位置的结构是红黑树，已红黑树的方式新增结点。 如果不是红黑树，则证明是单链表，将新增结点插入至链表的最后位置，随后判断当前链表长度是否 大于等于 8，是则将当前存储位置的链表转化为红黑树。遍历过程中如果发现key已经存在，则直接覆盖value。 插入成功后，判断当前存储键值对的数量 大于 阈值threshold 是则扩容。 hashMap put方法执行流程图- 图片来自于《美团点评技术团队文章》 示例代码： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102/** * 添加key-value键值对 * * @param key 键 * @param value 值 * @return 如果原本存在此key，则返回旧的value值，如果是新增的key- * value，则返回nulll */public V put(K key, V value) &#123; //实际调用putVal方法进行添加 key-value 键值对操作 return putVal(hash(key), key, value, false, true);&#125;/** * 根据key 键 的 hashCode 通过 “扰动函数” 生成对应的 hash值 * 经过此操作后，使每一个key对应的hash值生成的更均匀， * 减少元素之间的碰撞几率（后面详细说明） */static final int hash(Object key) &#123; int h; return (key == null) ? 0 : (h = key.hashCode()) ^ (h &gt;&gt;&gt; 16);&#125;/** * 添加key-value键值对的实际调用方法（重点） * * @param hash key 键的hash值 * @param key 键 * @param value 值 * @param onlyIfAbsent 此值如果是true, 则如果此key已存在value，则不执 * 行修改操作 * @param evict 此值如果是false，哈希表是在初始化模式 * @return 返回原本的旧值, 如果是新增，则返回null */final V putVal(int hash, K key, V value, boolean onlyIfAbsent, boolean evict) &#123; // 用于记录当前的hash表 Node&lt;K,V&gt;[] tab; // 用于记录当前的链表结点 Node&lt;K,V&gt; p; // n用于记录hash表的长度，i用于记录当前操作索引index int n, i; // 当前hash表为空 if ((tab = table) == null || (n = tab.length) == 0) // 初始化hash表，并把初始化后的hash表长度值赋值给n n = (tab = resize()).length; // 1）通过 (n - 1) &amp; hash 当前元素的hash值 &amp; hash表长度 - 1 // 2）确定当前元素的存储位置，此运算等价于 当前元素的hash值 % hash表的长度 // 3）计算出的存储位置没有元素存在 if ((p = tab[i = (n - 1) &amp; hash]) == null) // 4) 则新建一个Node结点，在该位置存储此元素 tab[i] = newNode(hash, key, value, null); else &#123; // 当前存储位置已经有元素存在了(不考虑是修改的情况的话，就代表发生hash冲突了) // 用于存放新增结点 Node&lt;K,V&gt; e; // 用于临时存在某个key值 K k; // 1)如果当前位置已存在元素的hash值和新增元素的hash值相等 // 2)并且key也相等，则证明是同一个key元素，想执行修改value操作 if (p.hash == hash &amp;&amp; ((k = p.key) == key || (key != null &amp;&amp; key.equals(k)))) e = p;// 将当前结点引用赋值给e else if (p instanceof TreeNode) // 如果当前结点是树结点 // 则证明当前位置的链表已变成红黑树结构，则已红黑树结点结构新增元素 e = ((TreeNode&lt;K,V&gt;)p).putTreeVal(this, tab, hash, key, value); else &#123;// 排除上述情况，则证明已发生hash冲突，并hash冲突位置现时的结构是单链表结构 for (int binCount = 0; ; ++binCount) &#123; //遍历单链表，将新元素结点放置此链表的最后一位 if ((e = p.next) == null) &#123; // 将新元素结点放在此链表的最后一位 p.next = newNode(hash, key, value, null); // 新增结点后，当前结点数量是否大于等于 阈值 8 if (binCount &gt;= TREEIFY_THRESHOLD - 1) // -1 for 1st // 大于等于8则将链表转换成红黑树 treeifyBin(tab, hash); break; &#125; // 如果链表中已经存在对应的key，则覆盖value if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) break; p = e; &#125; &#125; if (e != null) &#123; // 已存在对应key V oldValue = e.value; if (!onlyIfAbsent || oldValue == null) //如果允许修改，则修改value为新值 e.value = value; afterNodeAccess(e); return oldValue; &#125; &#125; ++modCount; // 当前存储键值对的数量 大于 阈值 是则扩容 if (++size &gt; threshold) // 重置hash大小，将旧hash表的数据逐一复制到新的hash表中（后面详细讲解） resize(); afterNodeInsertion(evict); // 返回null，则证明是新增操作，而不是修改操作 return null;&#125; 调用get(Object key)操作根据键key查找对应的key-value键值对时，进行了如下操作： 1.先调用 hash(key)方法计算出 key 的 hash值 2.根据查找的键值key的hash值，通过(n - 1) &amp; hash当前元素的hash值 &amp; hash表长度 - 1（实际就是 hash值 % hash表长度） 计算出存储位置table[i]，判断存储位置是否有元素存在 。 如果存储位置有元素存放，则首先比较头结点元素，如果头结点的key的hash值 和 要获取的key的hash值相等，并且 头结点的key本身 和要获取的 key 相等，则返回该位置的头结点。 如果存储位置没有元素存放，则返回null。 3.如果存储位置有元素存放，但是头结点元素不是要查找的元素，则需要遍历该位置进行查找。 4.先判断头结点是否是treeNode，如果是treeNode则证明此位置的结构是红黑树，以红色树的方式遍历查找该结点，没有则返回null。 5.如果不是红黑树，则证明是单链表。遍历单链表，逐一比较链表结点，链表结点的key的hash值 和 要获取的key的hash值相等，并且 链表结点的key本身 和要获取的 key 相等，则返回该结点，遍历结束仍未找到对应key的结点，则返回null。 示例代码： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566/** * 返回指定 key 所映射的 value 值 * 或者 返回 null 如果容器里不存在对应的key * * 更确切地讲，如果此映射包含一个满足 (key==null ? k==null :key.equals(k)) * 的从 k 键到 v 值的映射关系， * 则此方法返回 v；否则返回 null。（最多只能有一个这样的映射关系。） * * 返回 null 值并不一定 表明该映射不包含该键的映射关系； * 也可能该映射将该键显示地映射为 null。可使用containsKey操作来区分这两种情况。 * * @see #put(Object, Object) */public V get(Object key) &#123; Node&lt;K,V&gt; e; // 1.先调用 hash(key)方法计算出 key 的 hash值 // 2.随后调用getNode方法获取对应key所映射的value值 return (e = getNode(hash(key), key)) == null ? null : e.value;&#125;/** * 获取哈希表结点的方法实现 * * @param hash key 键的hash值 * @param key 键 * @return 返回对应的结点，如果结点不存在，则返回null */final Node&lt;K,V&gt; getNode(int hash, Object key) &#123; // 用于记录当前的hash表 Node&lt;K,V&gt;[] tab; // first用于记录对应hash位置的第一个结点，e充当工作结点的作用 Node&lt;K,V&gt; first, e; // n用于记录hash表的长度 int n; // 用于临时存放Key K k; // 通过 (n - 1) &amp; hash 当前元素的hash值 &amp; hash表长度 - 1 // 判断当前元素的存储位置是否有元素存在 if ((tab = table) != null &amp;&amp; (n = tab.length) &gt; 0 &amp;&amp; (first = tab[(n - 1) &amp; hash]) != null) &#123;//元素存在的情况 // 如果头结点的key的hash值 和 要获取的key的hash值相等 // 并且 头结点的key本身 和要获取的 key 相等 if (first.hash == hash &amp;&amp; // always check first node 总是检查头结点 ((k = first.key) == key || (key != null &amp;&amp; key.equals(k)))) // 返回该位置的头结点 return first; if ((e = first.next) != null) &#123;// 头结点不相等 if (first instanceof TreeNode) // 如果当前结点是树结点 // 则证明当前位置的链表已变成红黑树结构 // 通过红黑树结点的方式获取对应key结点 return ((TreeNode&lt;K,V&gt;)first).getTreeNode(hash, key); do &#123;// 当前位置不是红黑树，则证明是单链表 // 遍历单链表，逐一比较链表结点 // 链表结点的key的hash值 和 要获取的key的hash值相等 // 并且 链表结点的key本身 和要获取的 key 相等 if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) // 找到对应的结点则返回 return e; &#125; while ((e = e.next) != null); &#125; &#125; // 通过上述查找均无找到，则返回null return null;&#125; 调用remove(Object key)操作根据键key删除对应的key-value键值对时，进行了如下操作： 1.先调用 hash(key)方法计算出 key 的 hash值 2.根据查找的键值key的hash值，通过(n - 1) &amp; hash当前元素的hash值 &amp; hash表长度 - 1（实际就是 hash值 % hash表长度） 计算出存储位置table[i]，判断存储位置是否有元素存在 。 如果存储位置有元素存放，则首先比较头结点元素，如果头结点的key的hash值 和 要获取的key的hash值相等，并且 头结点的key本身 和要获取的 key 相等，则该位置的头结点即为要删除的结点，记录此结点至变量node中。 如果存储位置没有元素存放，则没有找到对应要删除的结点，则返回null。 3.如果存储位置有元素存放，但是头结点元素不是要删除的元素，则需要遍历该位置进行查找。 4.先判断头结点是否是treeNode，如果是treeNode则证明此位置的结构是红黑树，以红色树的方式遍历查找并删除该结点，没有则返回null。 5.如果不是红黑树，则证明是单链表。遍历单链表，逐一比较链表结点，链表结点的key的hash值 和 要获取的key的hash值相等，并且 链表结点的key本身 和要获取的 key 相等，则此为要删除的结点，记录此结点至变量node中，遍历结束仍未找到对应key的结点，则返回null。 6.如果找到要删除的结点node，则判断是否需要比较value也是否一致，如果value值一致或者不需要比较value值，则执行删除结点操作，删除操作根据不同的情况与结构进行不同的处理。 如果当前结点是树结点，则证明当前位置的链表已变成红黑树结构，通过红黑树结点的方式删除对应结点。 如果不是红黑树，则证明是单链表。如果要删除的是头结点，则当前存储位置table[i]的头结点指向删除结点的下一个结点。 如果要删除的结点不是头结点，则将要删除的结点的后继结点node.next赋值给要删除结点的前驱结点的next域，即p.next = node.next;。 7.HashMap当前存储键值对的数量 - 1，并返回删除结点。 示例代码： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495/** * 从此映射中移除指定键的映射关系（如果存在）。 * * @param key 其映射关系要从映射中移除的键 * @return 与 key 关联的旧值；如果 key 没有任何映射关系，则返回 null。 * （返回 null 还可能表示该映射之前将 null 与 key 关联。） */public V remove(Object key) &#123; Node&lt;K,V&gt; e; // 1.先调用 hash(key)方法计算出 key 的 hash值 // 2.随后调用removeNode方法删除对应key所映射的结点 return (e = removeNode(hash(key), key, null, false, true)) == null ? null : e.value;&#125;/** * 删除哈希表结点的方法实现 * * @param hash 键的hash值 * @param key 键 * @param value 用于比较的value值，当matchValue 是 true时才有效, 否则忽略 * @param matchValue 如果是 true 只有当value相等时才会移除 * @param movable 如果是 false当执行移除操作时，不删除其他结点 * @return 返回删除结点node，不存在则返回null */final Node&lt;K,V&gt; removeNode(int hash, Object key, Object value, boolean matchValue, boolean movable) &#123; // 用于记录当前的hash表 Node&lt;K,V&gt;[] tab; // 用于记录当前的链表结点 Node&lt;K,V&gt; p; // n用于记录hash表的长度，index用于记录当前操作索引index int n, index; // 通过 (n - 1) &amp; hash 当前元素的hash值 &amp; hash表长度 - 1 // 判断当前元素的存储位置是否有元素存在 if ((tab = table) != null &amp;&amp; (n = tab.length) &gt; 0 &amp;&amp; (p = tab[index = (n - 1) &amp; hash]) != null) &#123;// 元素存在的情况 // node 用于记录找到的结点，e为工作结点 Node&lt;K,V&gt; node = null, e; K k; V v; // 如果头结点的key的hash值 和 要获取的key的hash值相等 // 并且 头结点的key本身 和要获取的 key 相等 // 则证明此头结点就是要删除的结点 if (p.hash == hash &amp;&amp; ((k = p.key) == key || (key != null &amp;&amp; key.equals(k)))) // 记录要删除的结点的引用地址至node中 node = p; else if ((e = p.next) != null) &#123;// 头结点不相等 if (p instanceof TreeNode)// 如果当前结点是树结点 // 则证明当前位置的链表已变成红黑树结构 // 通过红黑树结点的方式获取对应key结点 // 记录要删除的结点的引用地址至node中 node = ((TreeNode&lt;K,V&gt;)p).getTreeNode(hash, key); else &#123;// 当前位置不是红黑树，则证明是单链表 do &#123; // 遍历单链表，逐一比较链表结点 // 链表结点的key的hash值 和 要获取的key的hash值相等 // 并且 链表结点的key本身 和要获取的 key 相等 if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) &#123; // 找到则记录要删除的结点的引用地址至node中，中断遍历 node = e; break; &#125; p = e; &#125; while ((e = e.next) != null); &#125; &#125; // 如果找到要删除的结点，则判断是否需要比较value也是否一致 if (node != null &amp;&amp; (!matchValue || (v = node.value) == value || (value != null &amp;&amp; value.equals(v)))) &#123; // value值一致或者不需要比较value值，则执行删除结点操作 if (node instanceof TreeNode) // 如果当前结点是树结点 // 则证明当前位置的链表已变成红黑树结构 // 通过红黑树结点的方式删除对应结点 ((TreeNode&lt;K,V&gt;)node).removeTreeNode(this, tab, movable); else if (node == p) // node 和 p相等，则证明删除的是头结点 // 当前存储位置的头结点指向删除结点的下一个结点 tab[index] = node.next; else // 删除的不是头结点 // p是删除结点node的前驱结点，p的next改为记录要删除结点node的后继结点 p.next = node.next; ++modCount; // 当前存储键值对的数量 - 1 --size; afterNodeRemoval(node); // 返回删除结点 return node; &#125; &#125; // 不存在要删除的结点，则返回null return null;&#125; 调用replace(K key, V value)操作根据键key查找对应的key-value键值对，随后替换对应的值value，进行了如下操作： 先调用 hash(key)方法计算出 key 的 hash值 随后调用getNode方法获取对应key所映射的value值 。 记录元素旧值，将新值赋值给元素，返回元素旧值，如果没有找到元素，则返回null。 示例代码： 123456789101112131415161718192021222324/** * 替换指定 key 所映射的 value 值 * * @param key 对应要替换value值元素的key键 * @param value 要替换对应元素的新value值 * @return 返回原本的旧值，如果没有找到key对应的元素，则返回null * @since 1.8 JDK1.8新增方法 */public V replace(K key, V value) &#123; Node&lt;K,V&gt; e; // 1.先调用 hash(key)方法计算出 key 的 hash值 // 2.随后调用getNode方法获取对应key所映射的value值 if ((e = getNode(hash(key), key)) != null) &#123;// 如果找到对应的元素 // 元素旧值 V oldValue = e.value; // 将新值赋值给元素 e.value = value; afterNodeAccess(e); // 返回元素旧值 return oldValue; &#125; // 没有找到元素，则返回null return null;&#125; HashMap 若要新增的这个元素存在了或hash冲突了怎么办问 1：您上面说，存放一个元素时，先计算它的hash值确定它的存储位置，然后再把这个元素放到对应的位置上，那万一这个位置上面已经有元素存在呢，新增的这个元素怎么办？ 问 2：hash冲突（或者叫hash碰撞）是什么？为什么会出现这种现象，如何解决hash冲突？ 答： hash冲突： 当我们调用put(K key, V value)操作添加key-value键值对，这个key-value键值对存放在的位置是通过扰动函数(key == null) ? 0 : (h = key.hashCode()) ^ (h &gt;&gt;&gt; 16)计算键key的hash值。随后将 这个hash值 % 模上 哈希表Node&lt;K,V&gt;[] table的长度 得到具体的存放位置。所以put(K key, V value)多个元素，是有可能计算出相同的存放位置。此现象就是hash冲突或者叫hash碰撞。 例子如下：元素 A 的hash值 为 9，元素 B 的hash值 为 17。哈希表Node&lt;K,V&gt;[] table的长度为8。则元素 A 的存放位置为9 % 8 = 1，元素 B 的存放位置为17 % 8 = 1。两个元素的存放位置均为table[1]，发生了hash冲突。 hash冲突的避免：既然会发生hash冲突，我们就应该想办法避免此现象的发生，解决这个问题最关键就是如果生成元素的hash值。Java是使用“扰动函数”生成元素的hash值。 示例代码： 123456789101112131415161718/** * JDK 7 的 hash方法 */ final int hash(int h) &#123; h ^= k.hashCode(); h ^= (h &gt;&gt;&gt; 20) ^ (h &gt;&gt;&gt; 12); return h ^ (h &gt;&gt;&gt; 7) ^ (h &gt;&gt;&gt; 4); &#125;/** * JDK 8 的 hash方法 */ static final int hash(Object key) &#123; int h; return (key == null) ? 0 : (h = key.hashCode()) ^ (h &gt;&gt;&gt; 16); &#125; Java7做了4次16位右位移异或混合，Java 8中这步已经简化了，只做一次16位右位移异或混合，而不是四次，但原理是不变的。例子如下： 扰动函数执行例子 - 图片来自于《知乎》 右位移16位，正好是32bit的一半，自己的高半区和低半区做异或，就是为了混合原始哈希码的高位和低位，以此来加大低位的随机性。而且混合后的低位掺杂了高位的部分特征，这样高位的信息也被变相保留下来。 上述扰动函数的解释参考自：JDK 源码中 HashMap 的 hash 方法原理是什么？ hash冲突解决：解决hash冲突的方法有很多，常见的有：开发定址法，再散列法，链地址法，公共溢出区法（详细说明请查看我的文章JAVA基础-自问自答学hashCode和equals）。HashMap是使用链地址法解决hash冲突的，当有冲突元素放进来时，会将此元素插入至此位置链表的最后一位，形成单链表。但是由于是单链表的缘故，每当通过hash % length找到该位置的元素时，均需要从头遍历链表，通过逐一比较hash值，找到对应元素。如果此位置元素过多，造成链表过长，遍历时间会大大增加，最坏情况下的时间复杂度为O(N)，造成查找效率过低。所以当存在位置的链表长度 大于等于 8 时，HashMap会将链表 转变为 红黑树，红黑树最坏情况下的时间复杂度为O(logn)。以此提高查找效率。 HashMap 的容量为什么一定要是2的n次方问：HashMap的容量为什么一定要是2的n次方？ 答： 因为调用put(K key, V value)操作添加key-value键值对时，具体确定此元素的位置是通过 hash值 % 模上 哈希表Node&lt;K,V&gt;[] table的长度 hash % length 计算的。但是”模”运算的消耗相对较大，通过位运算h &amp; (length-1)也可以得到取模后的存放位置，而位运算的运行效率高，但只有length的长度是2的n次方时，h &amp; (length-1) 才等价于 h % length。 而且当数组长度为2的n次幂的时候，不同的key算出的index相同的几率较小，那么数据在数组上分布就比较均匀，也就是说碰撞的几率小，相对的，查询的时候就不用遍历某个位置上的链表，这样查询效率也就较高了。 例子： hash &amp; (length-1)运算过程.jpg 上图中，左边两组的数组长度是16（2的4次方），右边两组的数组长度是15。两组的hash值均为8和9。 当数组长度是15时，当它们和1110进行&amp;与运算（相同为1，不同为0）时，计算的结果都是1000，所以他们都会存放在相同的位置table[8]中，这样就发生了hash冲突，那么查询时就要遍历链表，逐一比较hash值，降低了查询的效率。 同时，我们可以发现，当数组长度为15的时候，hash值均会与14（1110）进行&amp;与运算，那么最后一位永远是0，而0001，0011，0101，1001，1011，0111，1101这几个位置永远都不能存放元素了，空间浪费相当大，更糟的是这种情况中，数组可以使用的位置比数组长度小了很多，这意味着进一步增加了碰撞的几率，减慢了查询的效率。 所以，HashMap的容量是2的n次方，有利于提高计算元素存放位置时的效率，也降低了hash冲突的几率。因此，我们使用HashMap存储大量数据的时候，最好先预先指定容器的大小为2的n次方，即使我们不指定为2的n次方，HashMap也会把容器的大小设置成最接近设置数的2的n次方，如，设置HashMap的大小为 7 ，则HashMap会将容器大小设置成最接近7的一个2的n次方数，此值为 8 。 上述回答参考自：深入理解HashMap 示例代码： 12345678910111213/** * 返回一个比指定数cap大的，并且大小是2的n次方的数 * Returns a power of two size for the given target capacity. */static final int tableSizeFor(int cap) &#123; int n = cap - 1; n |= n &gt;&gt;&gt; 1; n |= n &gt;&gt;&gt; 2; n |= n &gt;&gt;&gt; 4; n |= n &gt;&gt;&gt; 8; n |= n &gt;&gt;&gt; 16; return (n &lt; 0) ? 1 : (n &gt;= MAXIMUM_CAPACITY) ? MAXIMUM_CAPACITY : n + 1;&#125; HashMap 的负载因子是什么，有什么作用问：HashMap的负载因子是什么，有什么作用？ 答：负载因子表示哈希表空间的使用程度（或者说是哈希表空间的利用率）。 例子如下：底层哈希表Node&lt;K,V&gt;[] table的容量大小capacity为 16，负载因子load factor为 0.75，则当存储的元素个数size = capacity 16 * load factor 0.75等于 12 时，则会触发HashMap的扩容机制，调用resize()方法进行扩容。 当负载因子越大，则HashMap的装载程度就越高。也就是能容纳更多的元素，元素多了，发生hash碰撞的几率就会加大，从而链表就会拉长，此时的查询效率就会降低。 当负载因子越小，则链表中的数据量就越稀疏，此时会对空间造成浪费，但是此时查询效率高。 我们可以在创建HashMap 时根据实际需要适当地调整load factor 的值；如果程序比较关心空间开销、内存比较紧张，可以适当地增加负载因子；如果程序比较关心时间开销，内存比较宽裕则可以适当的减少负载因子。通常情况下，默认负载因子 (0.75) 在时间和空间成本上寻求一种折衷，程序员无需改变负载因子的值。 因此，如果我们在初始化HashMap时，就预估知道需要装载key-value键值对的容量size，我们可以通过size / load factor 计算出我们需要初始化的容量大小initialCapacity，这样就可以避免HashMap因为存放的元素达到阈值threshold而频繁调用resize()方法进行扩容。从而保证了较好的性能。 HashMap 和 HashTable 的区别问：您能说说HashMap和HashTable的区别吗？ 答：HashMap和HashTable有如下区别： 1）容器整体结构： HashMap的key和value都允许为null，HashMap遇到key为null的时候，调用putForNullKey方法进行处理，而对value没有处理。 Hashtable的key和value都不允许为null。Hashtable遇到null，直接返回NullPointerException。 2） 容量设定与扩容机制： HashMap默认初始化容量为 16，并且容器容量一定是2的n次方，扩容时，是以原容量 2倍 的方式 进行扩容。 Hashtable默认初始化容量为 11，扩容时，是以原容量 2倍 再加 1的方式进行扩容。即int newCapacity = (oldCapacity &lt;&lt; 1) + 1;。 3） 散列分布方式（计算存储位置）： HashMap是先将key键的hashCode经过扰动函数扰动后得到hash值，然后再利用 hash &amp; (length - 1)的方式代替取模，得到元素的存储位置。 Hashtable则是除留余数法进行计算存储位置的（因为其默认容量也不是2的n次方。所以也无法用位运算替代模运算），int index = (hash &amp; 0x7FFFFFFF) % tab.length;。 由于HashMap的容器容量一定是2的n次方，所以能使用hash &amp; (length - 1)的方式代替取模的方式计算元素的位置提高运算效率，但Hashtable的容器容量不一定是2的n次方，所以不能使用此运算方式代替。 4）线程安全（最重要）： HashMap 不是线程安全，如果想线程安全，可以通过调用synchronizedMap(Map&lt;K,V&gt; m)使其线程安全。但是使用时的运行效率会下降，所以建议使用ConcurrentHashMap容器以此达到线程安全。 Hashtable则是线程安全的，每个操作方法前都有synchronized修饰使其同步，但运行效率也不高，所以还是建议使用ConcurrentHashMap容器以此达到线程安全。 因此，Hashtable是一个遗留容器，如果我们不需要线程同步，则建议使用HashMap，如果需要线程同步，则建议使用ConcurrentHashMap。 此处不再对Hashtable的源码进行逐一分析了，如果想深入了解的同学，可以参考此文章Hashtable源码剖析 HashMap 在多线程下如何处理，啥时会发生线程不安全问：您说HashMap不是线程安全的，那如果多线程下，它是如何处理的？并且什么情况下会发生线程不安全的情况？ 答： HashMap不是线程安全的，如果多个线程同时对同一个HashMap更改数据的话，会导致数据不一致或者数据污染。如果出现线程不安全的操作时，HashMap会尽可能的抛出ConcurrentModificationException防止数据异常，当我们在对一个HashMap进行遍历时，在遍历期间，我们是不能对HashMap进行添加，删除等更改数据的操作的，否则也会抛出ConcurrentModificationException异常，此为fail-fast（快速失败）机制。从源码上分析，我们在put,remove等更改HashMap数据时，都会导致modCount的改变，当expectedModCount != modCount时，则抛出ConcurrentModificationException。如果想要线程安全，可以考虑使用ConcurrentHashMap。 而且，在多线程下操作HashMap，由于存在扩容机制，当HashMap调用resize()进行自动扩容时，可能会导致死循环的发生。 由于时间关系，我暂不带着大家一起去分析resize()方法导致死循环发生的现象造成原因了，迟点有空我会再补充上去，请见谅，大家可以参考如下文章： Java 8系列之重新认识HashMap 谈谈HashMap线程不安全的体现 使用 HashMap ，选取什么对象作为 key 键比较好问：我们在使用HashMap时，选取什么对象作为key键比较好，为什么？ 答： 可变对象：指创建后自身状态能改变的对象。换句话说，可变对象是该对象在创建后它的哈希值可能被改变。 我们在使用HashMap时，最好选择不可变对象作为key。例如String，Integer等不可变类型作为key是非常明智的。 如果key对象是可变的，那么key的哈希值就可能改变。在HashMap中可变对象作为Key会造成数据丢失。因为我们再进行hash &amp; (length - 1)取模运算计算位置查找对应元素时，位置可能已经发生改变，导致数据丢失。 详细例子说明请参考：危险！在HashMap中将可变对象用作Key 总结 HashMap是基于Map接口实现的一种键-值对&lt;key,value&gt;的存储结构，允许null值，同时非有序，非同步(即线程不安全)。HashMap的底层实现是数组 + 链表 + 红黑树（JDK1.8增加了红黑树部分）。 HashMap定位元素位置是通过键key经过扰动函数扰动后得到hash值，然后再通过hash &amp; (length - 1)代替取模的方式进行元素定位的。 HashMap是使用链地址法解决hash冲突的，当有冲突元素放进来时，会将此元素插入至此位置链表的最后一位，形成单链表。当存在位置的链表长度 大于等于 8 时，HashMap会将链表 转变为 红黑树，以此提高查找效率。 HashMap的容量是2的n次方，有利于提高计算元素存放位置时的效率，也降低了hash冲突的几率。因此，我们使用HashMap存储大量数据的时候，最好先预先指定容器的大小为2的n次方，即使我们不指定为2的n次方，HashMap也会把容器的大小设置成最接近设置数的2的n次方，如，设置HashMap的大小为 7 ，则HashMap会将容器大小设置成最接近7的一个2的n次方数，此值为 8 。 HashMap的负载因子表示哈希表空间的使用程度（或者说是哈希表空间的利用率）。当负载因子越大，则HashMap的装载程度就越高。也就是能容纳更多的元素，元素多了，发生hash碰撞的几率就会加大，从而链表就会拉长，此时的查询效率就会降低。当负载因子越小，则链表中的数据量就越稀疏，此时会对空间造成浪费，但是此时查询效率高。 HashMap不是线程安全的，Hashtable则是线程安全的。但Hashtable是一个遗留容器，如果我们不需要线程同步，则建议使用HashMap，如果需要线程同步，则建议使用ConcurrentHashMap。 在多线程下操作HashMap，由于存在扩容机制，当HashMap调用resize()进行自动扩容时，可能会导致死循环的发生。 我们在使用HashMap时，最好选择不可变对象作为key。例如String，Integer等不可变类型作为key是非常明智的。 由于最近工作较忙，也有拖延症发作的问题，所以文章迟迟未能完成发布，现时完成的文章其实对我而言，也不算太好，但还是打算先发出来让大家看看，一起学习学习，看有什么不好的地方，我再慢慢改进，如果此文对你有帮助，请给个赞，谢谢大家。 参考文章Java 8系列之重新认识HashMapJDK 源码中 HashMap 的 hash 方法原理是什么？深入理解HashMapHashMap负载因子Hashtable源码剖析危险！在HashMap中将可变对象用作Key谈谈HashMap线程不安全的体现]]></content>
      <categories>
        <category>HashMap</category>
      </categories>
      <tags>
        <tag>基础数据类型</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ConcurrentHashMap 的锁定分离技术]]></title>
    <url>%2F2018%2F06%2F03%2FConcurrentHashMap%E7%9A%84%E9%94%81%E5%88%86%E7%A6%BB%E6%8A%80%E6%9C%AF%2F</url>
    <content type="text"><![CDATA[ConcurrentHashMap的锁分离技术 ​ 对比上图，HashTable实现锁的方式是锁整个hash表，而ConcurrentHashMap的实现方式是锁桶（简单理解就是将整个hash表想象成一大缸水，现在将这大缸里的水分到了几个水桶里，hashTable每次都锁定这个大缸，而ConcurrentHashMap则每次只锁定其中一个 桶）。 ​ ConcurrentHashMap将hash表分为16个桶（默认值），诸如get,put,remove等常用操作只锁当前需要用到的桶。试想，原来 只能一个线程进入，现在却能同时16个写线程进入，并发性的提升是显而易见的。 ​ 值得一提的是当对ConcurrentHashMap进行remove操作时，并不是进行简单的节点删除操作，对比上图，当对ConcurrentHashMap的一个segment也就是一个桶中的节点进行remove后，例如删除节点C，C节点实际并没有被销毁，而是将C节点前面的反转并拷贝到新的链表中，C节点后面的不需要被克隆。这样来保持并发的读线程不受并发的写线程的干扰。例如现在有一个读线程读到了A节点，写线程把C删掉了，但是看上图，读线程仍然可以继续读下去；当然，如果在删除C之前读线程读到的是D，那么更不会有影响。 ​ 根据上面所提到的ConcurrentHashMap中删除一个节点并不会立刻被读线程感受到的效果，就是传说中的弱一致性，所以ConcurrentHashMap的迭代器是弱一致性迭代器]]></content>
      <categories>
        <category>ConcurrentHashMap</category>
      </categories>
      <tags>
        <tag>基础数据类型</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ConcurrentHashMap的使用]]></title>
    <url>%2F2018%2F06%2F03%2FConcurrentHashMap%E7%9A%84%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[ConcurrentHashMap的使用缓存的使用 高性能本地缓存：对系统中常用到的业务数据放到缓存中以提高系统性能，限制是单服务器模式 分布式缓存：常用分布式缓存技术memcached、redis等 ConcurrentHashMap就是常用的高并发下的缓存对象。接下来直接上例子：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105public class ConcurrentMapTest &#123; public static ConcurrentMap&lt;String, Future&lt;String&gt;&gt; cMap = new ConcurrentHashMap&lt;&gt;(); public static ConcurrentHashMap&lt;String, String&gt; cMap2 = new ConcurrentHashMap&lt;&gt;(); public static int index = 0; public static void main(String[] args) &#123; for (int i = 0; i &lt; 5; i++) &#123; new Thread(new Runnable() &#123; @Override public void run() &#123; concurrentMap2("3"); try &#123; concurrentMap("123"); &#125; catch (InterruptedException | ExecutionException | TimeoutException e) &#123; e.printStackTrace(); &#125; &#125; &#125;).start(); &#125; &#125; /** * 解决并发写的线程安全问题。但是高并发可读取会造成重复写的问题... * 如果put的业务计算复杂将耗费不必要的资源 * 解决缓存读取问题，但可能会出现缓存重复写 */ private static void concurrentMap2(String key) &#123; System.out.println(Thread.currentThread().getName() + " start ...."); String f = cMap2.get(key); // ConcurrentMap读不加锁，写加锁。 // 当并发量高时会出现重复compute的操作，然后才put到map中 if (f == null) &#123; try &#123; Thread.sleep(500); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; cMap2.put(key, "dataTest" + index); System.out.println(Thread.currentThread().getName() + " compute , index================== " + index++); &#125; System.out.println(Thread.currentThread().getName() + " end .... " + cMap2.get(key)); &#125; /** * 解决并发写问题，同时避免了重复put计算的问题 解决缓存读写的问题 * * @param key * @throws InterruptedException * @throws ExecutionException * @throws TimeoutException */ private static void concurrentMap(String key) throws InterruptedException, ExecutionException, TimeoutException &#123; System.out.println(Thread.currentThread().getName() + " start ...."); Future&lt;String&gt; f = null; f = cMap.get(key); if (f == null) &#123; try &#123; Thread.sleep(500); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; FutureTask&lt;String&gt; fTask = new FutureTask&lt;String&gt; (new Callable&lt;String&gt;() &#123; public String call() throws Exception &#123; try &#123; Thread.sleep(2000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println(Thread.currentThread().getName() + " compute , index=============== " + index++); return "456789123"; &#125; &#125;); f = cMap.putIfAbsent(key, fTask); // 相当于get-if-absent-compute， //而且是原子执行，解决了并发读的问题。（FutureTask解决compute步骤） if (f == null) &#123; f = fTask; // f的值是FutureTask对象引用，解决了call的重复调用问题, // 只用一个线程会执行run()方法 fTask.run(); &#125; &#125; System.out.println("end ==========="); // get会等待FutureTask的计算结果，可以设置等待超时事件,超时会抛出超时异常 System.out.println(Thread.currentThread().getName() + " end ....====== " + f.get(3000, TimeUnit.MILLISECONDS)); // get会等待FutureTask的计算结果，永久等待 System.out.println(Thread.currentThread().getName() + " end .... " + f.get()); &#125;&#125; concurrentMap2的执行结果 Thread-0 start ….Thread-2 start ….Thread-4 start ….Thread-1 start ….Thread-3 start ….Thread-4 compute , index================== 0Thread-0 compute , index================== 2Thread-0 end …. dataTest0Thread-2 compute , index================== 1Thread-2 end …. dataTest0Thread-4 end …. dataTest0Thread-3 compute , index================== 3Thread-1 compute , index================== 4Thread-1 end …. dataTest3Thread-3 end …. dataTest3 可以看到put方法被重复执行…. concurrentMap的执行结果 Thread-1 start ….Thread-3 start ….Thread-0 start ….Thread-2 start ….Thread-4 start ….end ===========end ===========end ===========end ===========Thread-1 compute , index=============== 0Thread-3 end ….====== 456789123Thread-3 end …. 456789123end ===========Thread-1 end ….====== 456789123Thread-1 end …. 456789123Thread-0 end ….====== 456789123Thread-0 end …. 456789123Thread-2 end ….====== 456789123Thread-2 end …. 456789123Thread-4 end ….====== 456789123Thread-4 end …. 456789123 可以看到put运算只执行一次…. 总结： 如果缓存对象可以在系统启动时进行初始化加载，可以不使用ConcurrentHashMap 如果缓存在put时计算比较复杂，那么推荐直接使用concurrentMap写法 ConcurrentHashMap缺陷就是缓存无法回收，导致内存溢出问题。此问题在google发布Guava的Cache很好的进行了处理，可查看另一篇文章Guava Cache的使用]]></content>
      <categories>
        <category>ConcurrentHashMap</category>
      </categories>
      <tags>
        <tag>基础数据类型</tag>
      </tags>
  </entry>
  <entry>
    <title></title>
    <url>%2F2018%2F06%2F03%2FConcurrentHashMap%E6%89%A9%E5%AE%B9%E5%AE%9E%E7%8E%B0%E6%9C%BA%E5%88%B6%2F</url>
    <content type="text"><![CDATA[ConcurrentHashMap扩容实现机制jdk8中，采用多线程扩容。整个扩容过程，通过CAS设置sizeCtl，transferIndex等变量协调多个线程进行并发扩容。 扩容相关的属性nextTable扩容期间，将table数组中的元素 迁移到 nextTable。 12345/** * The next table to use; non-null only while resizing. 扩容时，将table中的元素迁移至nextTable . 扩容时非空 */private transient volatile Node&lt;K,V&gt;[] nextTable; sizeCtl属性1private transient volatile int sizeCtl; 多线程之间，以volatile的方式读取sizeCtl属性，来判断ConcurrentHashMap当前所处的状态。通过cas设置sizeCtl属性，告知其他线程ConcurrentHashMap的状态变更。 不同状态，sizeCtl所代表的含义也有所不同。 未初始化： sizeCtl=0：表示没有指定初始容量。 sizeCtl&gt;0：表示初始容量。 初始化中： sizeCtl=-1,标记作用，告知其他线程，正在初始化 正常状态： sizeCtl=0.75n ,扩容阈值 扩容中: sizeCtl &lt; 0 : 表示有其他线程正在执行扩容 sizeCtl = (resizeStamp(n) &lt;&lt; RESIZE_STAMP_SHIFT) + 2 :表示此时只有一个线程在执行扩容 ConcurrentHashMap的状态图如下： transferIndex属性1234567private transient volatile int transferIndex; /** 扩容线程每次最少要迁移16个hash桶 */private static final int MIN_TRANSFER_STRIDE = 16; 扩容索引，表示已经分配给扩容线程的table数组索引位置。主要用来协调多个线程，并发安全地获取迁移任务（hash桶）。 1 在扩容之前，transferIndex 在数组的最右边 。此时有一个线程发现已经到达扩容阈值，准备开始扩容。 2 扩容线程，在迁移数据之前，首先要将transferIndex右移（以cas的方式修改 transferIndex=transferIndex-stride(要迁移hash桶的个数)），获取迁移任务。每个扩容线程都会通过for循环+CAS的方式设置transferIndex，因此可以确保多线程扩容的并发安全。 换个角度，我们可以将待迁移的table数组，看成一个任务队列，transferIndex看成任务队列的头指针。而扩容线程，就是这个队列的消费者。扩容线程通过CAS设置transferIndex索引的过程，就是消费者从任务队列中获取任务的过程。为了性能考虑，我们当然不会每次只获取一个任务（hash桶），因此ConcurrentHashMap规定，每次至少要获取16个迁移任务（迁移16个hash桶，MIN_TRANSFER_STRIDE = 16） cas设置transferIndex的源码如下： 123456789101112131415161718private final void transfer(Node&lt;K,V&gt;[] tab, Node&lt;K,V&gt;[] nextTab) &#123; //计算每次迁移的node个数 if ((stride = (NCPU &gt; 1) ? (n &gt;&gt;&gt; 3) / NCPU : n) &lt; MIN_TRANSFER_STRIDE) stride = MIN_TRANSFER_STRIDE; // 确保每次迁移的node个数不少于16个 ... for (int i = 0, bound = 0;;) &#123; ... //cas无锁算法设置 transferIndex = transferIndex - stride if (U.compareAndSwapInt (this, TRANSFERINDEX, nextIndex, nextBound = (nextIndex &gt; stride ? nextIndex - stride : 0))) &#123; ... ... &#125; ...//省略迁移逻辑 &#125; &#125; ForwardingNode节点 标记作用，表示其他线程正在扩容，并且此节点已经扩容完毕 关联了nextTable,扩容期间可以通过find方法，访问已经迁移到了nextTable中的数据 123456789101112 static final class ForwardingNode&lt;K,V&gt; extends Node&lt;K,V&gt; &#123; final Node&lt;K,V&gt;[] nextTable; ForwardingNode(Node&lt;K,V&gt;[] tab) &#123; //hash值为MOVED（-1）的节点就是ForwardingNode super(MOVED, null, null, null); this.nextTable = tab; &#125; //通过此方法，访问被迁移到nextTable中的数据 Node&lt;K,V&gt; find(int h, Object k) &#123; ... &#125;&#125; 何时扩容1 当前容量超过阈值12345final V putVal(K key, V value, boolean onlyIfAbsent) &#123; ... addCount(1L, binCount); ...&#125; 123456789101112private final void addCount(long x, int check) &#123; ... if (check &gt;= 0) &#123; Node&lt;K,V&gt;[] tab, nt; int n, sc; //s&gt;=sizeCtl 即容量达到扩容阈值，需要扩容 while (s &gt;= (long)(sc = sizeCtl) &amp;&amp; (tab = table) != null &amp;&amp; (n = tab.length) &lt; MAXIMUM_CAPACITY) &#123; //调用transfer()扩容 ... &#125; &#125; &#125; 2 当链表中元素个数超过默认设定（8个），当数组的大小还未超过64的时候，此时进行数组的扩容，如果超过则将链表转化成红黑树123456789101112final V putVal(K key, V value, boolean onlyIfAbsent) &#123; ... if (binCount != 0) &#123; //链表中元素个数超过默认设定（8个） if (binCount &gt;= TREEIFY_THRESHOLD) treeifyBin(tab, i); if (oldVal != null) return oldVal; break; &#125; ...&#125; 12345678910111213private final void treeifyBin(Node&lt;K,V&gt;[] tab, int index) &#123; Node&lt;K,V&gt; b; int n, sc; if (tab != null) &#123; //数组的大小还未超过64 if ((n = tab.length) &lt; MIN_TREEIFY_CAPACITY) //扩容 tryPresize(n &lt;&lt; 1); else if ((b = tabAt(tab, index)) != null &amp;&amp; b.hash &gt;= 0) &#123; //转换成红黑树 ... &#125; &#125;&#125; 3 当发现其他线程扩容时，帮其扩容1234567final V putVal(K key, V value, boolean onlyIfAbsent) &#123; ... //f.hash == MOVED 表示为：ForwardingNode，说明其他线程正在扩容 else if ((fh = f.hash) == MOVED) tab = helpTransfer(tab, f); ...&#125; 扩容过程分析 线程执行put操作，发现容量已经达到扩容阈值，需要进行扩容操作，此时transferindex=tab.length=32 扩容线程A 以cas的方式修改transferindex=32-16=16 ,然后按照降序迁移table[32]–table[16]这个区间的hash桶 迁移hash桶时，会将桶内的链表或者红黑树，按照一定算法，拆分成2份，将其插入nextTable[i]和nextTable[i+n]（n是table数组的长度）。 迁移完毕的hash桶,会被设置成ForwardingNode节点，以此告知访问此桶的其他线程，此节点已经迁移完毕。 相关代码如下： 123456789101112131415161718private final void transfer(Node&lt;K,V&gt;[] tab, Node&lt;K,V&gt;[] nextTab) &#123; ...//省略无关代码 synchronized (f) &#123; //将node链表，分成2个新的node链表 for (Node&lt;K,V&gt; p = f; p != lastRun; p = p.next) &#123; int ph = p.hash; K pk = p.key; V pv = p.val; if ((ph &amp; n) == 0) ln = new Node&lt;K,V&gt;(ph, pk, pv, ln); else hn = new Node&lt;K,V&gt;(ph, pk, pv, hn); &#125; //将新node链表赋给nextTab setTabAt(nextTab, i, ln); setTabAt(nextTab, i + n, hn); setTabAt(tab, i, fwd); &#125; ...//省略无关代码&#125; 此时线程2访问到了ForwardingNode节点，如果线程2执行的put或remove等写操作，那么就会先帮其扩容。如果线程2执行的是get等读方法，则会调用ForwardingNode的find方法，去nextTable里面查找相关元素。 线程2加入扩容操作 如果准备加入扩容的线程，发现以下情况，放弃扩容，直接返回。 发现transferIndex=0,即所有node均已分配 发现扩容线程已经达到最大扩容线程数 部分源码分析tryPresize方法协调多个线程如何调用transfer方法进行hash桶的迁移（addCount，helpTransfer 方法中也有类似的逻辑） 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960private final void tryPresize(int size) &#123; //计算扩容的目标size int c = (size &gt;= (MAXIMUM_CAPACITY &gt;&gt;&gt; 1)) ? MAXIMUM_CAPACITY : tableSizeFor(size + (size &gt;&gt;&gt; 1) + 1); int sc; while ((sc = sizeCtl) &gt;= 0) &#123; Node&lt;K,V&gt;[] tab = table; int n; //tab没有初始化 if (tab == null || (n = tab.length) == 0) &#123; n = (sc &gt; c) ? sc : c; //初始化之前，CAS设置sizeCtl=-1 if (U.compareAndSwapInt(this, SIZECTL, sc, -1)) &#123; try &#123; if (table == tab) &#123; @SuppressWarnings("unchecked") Node&lt;K,V&gt;[] nt = (Node&lt;K,V&gt;[])new Node&lt;?,?&gt;[n]; table = nt; //sc=0.75n,相当于扩容阈值 sc = n - (n &gt;&gt;&gt; 2); &#125; &#125; finally &#123; // 此时并没有通过CAS赋值，因为其他想要执行初始化的线程， // 发现sizeCtl=-1，就直接返回，从而确保任何情况， // 只会有一个线程执行初始化操作。 sizeCtl = sc; &#125; &#125; &#125; //目标扩容size小于扩容阈值，或者容量超过最大限制时，不需要扩容 else if (c &lt;= sc || n &gt;= MAXIMUM_CAPACITY) break; //扩容 else if (tab == table) &#123; int rs = resizeStamp(n); //sc&lt;0表示，已经有其他线程正在扩容 if (sc &lt; 0) &#123; Node&lt;K,V&gt;[] nt; //1 (sc &gt;&gt;&gt; RESIZE_STAMP_SHIFT) != rs ：扩容线程数 &gt; MAX_RESIZERS-1 //2 sc == rs + 1 和 sc == rs + MAX_RESIZERS ：表示什么？？？ //3 (nt = nextTable) == null ：表示nextTable正在初始化 //4 transferIndex &lt;= 0 ：表示所有hash桶均分配出去 //如果不需要帮其扩容，直接返回 if ((sc &gt;&gt;&gt; RESIZE_STAMP_SHIFT) != rs || sc == rs + 1 || sc == rs + MAX_RESIZERS || (nt = nextTable) == null || transferIndex &lt;= 0) break; //CAS设置sizeCtl=sizeCtl+1 if (U.compareAndSwapInt(this, SIZECTL, sc, sc + 1)) //帮其扩容 transfer(tab, nt); &#125; // 第一个执行扩容操作的线程，将sizeCtl设置为： // (resizeStamp(n) &lt;&lt; RESIZE_STAMP_SHIFT) + 2) else if (U.compareAndSwapInt(this, SIZECTL, sc, (rs &lt;&lt; RESIZE_STAMP_SHIFT) + 2)) transfer(tab, null); &#125; &#125;&#125; transfer方法负责迁移node节点 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164private final void transfer(Node&lt;K,V&gt;[] tab, Node&lt;K,V&gt;[] nextTab) &#123; int n = tab.length, stride; //计算需要迁移多少个hash桶（MIN_TRANSFER_STRIDE该值作为下限，以避免扩容线程过多） if ((stride = (NCPU &gt; 1) ? (n &gt;&gt;&gt; 3) / NCPU : n) &lt; MIN_TRANSFER_STRIDE) stride = MIN_TRANSFER_STRIDE; // subdivide range if (nextTab == null) &#123; // initiating try &#123; //扩容一倍 @SuppressWarnings("unchecked") Node&lt;K,V&gt;[] nt = (Node&lt;K,V&gt;[])new Node&lt;?,?&gt;[n &lt;&lt; 1]; nextTab = nt; &#125; catch (Throwable ex) &#123; // try to cope with OOME sizeCtl = Integer.MAX_VALUE; return; &#125; nextTable = nextTab; transferIndex = n; &#125; int nextn = nextTab.length; ForwardingNode&lt;K,V&gt; fwd = new ForwardingNode&lt;K,V&gt;(nextTab); boolean advance = true; boolean finishing = false; // 1.逆序迁移已经获取到的hash桶集合，如果迁移完毕， // 则更新transferIndex，获取下一批待迁移的hash桶 // 2.如果transferIndex=0，表示所以hash桶均被分配， // 将i置为-1，准备退出transfer方法 for (int i = 0, bound = 0;;) &#123; Node&lt;K,V&gt; f; int fh; //更新待迁移的hash桶索引 while (advance) &#123; int nextIndex, nextBound; //更新迁移索引i。 if (--i &gt;= bound || finishing) advance = false; else if ((nextIndex = transferIndex) &lt;= 0) &#123; // transferIndex&lt;=0表示已经没有需要迁移的hash桶， // 将i置为-1，线程准备退出 i = -1; advance = false; &#125; // 当迁移完bound这个桶后，尝试更新transferIndex， // 获取下一批待迁移的hash桶 else if (U.compareAndSwapInt (this, TRANSFERINDEX, nextIndex, nextBound = (nextIndex &gt; stride ? nextIndex - stride : 0))) &#123; bound = nextBound; i = nextIndex - 1; advance = false; &#125; &#125; //退出transfer if (i &lt; 0 || i &gt;= n || i + n &gt;= nextn) &#123; int sc; if (finishing) &#123; //最后一个迁移的线程，recheck后，做收尾工作，然后退出 nextTable = null; table = nextTab; sizeCtl = (n &lt;&lt; 1) - (n &gt;&gt;&gt; 1); return; &#125; if (U.compareAndSwapInt(this, SIZECTL, sc = sizeCtl, sc - 1)) &#123; // 第一个扩容的线程，执行transfer方法之前，会设置 sizeCtl = // (resizeStamp(n) &lt;&lt; RESIZE_STAMP_SHIFT) + 2) // 后续帮其扩容的线程，执行transfer方法之前，会设置 sizeCtl = sizeCtl+1 // 每一个退出transfer的方法的线程，退出之前，会设置 sizeCtl = sizeCtl-1 // 那么最后一个线程退出时： // 必然有sc == (resizeStamp(n) &lt;&lt; RESIZE_STAMP_SHIFT) + 2)， // 即 (sc - 2) == resizeStamp(n) &lt;&lt; RESIZE_STAMP_SHIFT //不相等，说明不到最后一个线程，直接退出transfer方法 if ((sc - 2) != resizeStamp(n) &lt;&lt; RESIZE_STAMP_SHIFT) return; finishing = advance = true; //最后退出的线程要重新check下是否全部迁移完毕 i = n; // recheck before commit &#125; &#125; else if ((f = tabAt(tab, i)) == null) advance = casTabAt(tab, i, null, fwd); else if ((fh = f.hash) == MOVED) advance = true; // already processed //迁移node节点 else &#123; synchronized (f) &#123; if (tabAt(tab, i) == f) &#123; Node&lt;K,V&gt; ln, hn; //链表迁移 if (fh &gt;= 0) &#123; int runBit = fh &amp; n; Node&lt;K,V&gt; lastRun = f; for (Node&lt;K,V&gt; p = f.next; p != null; p = p.next) &#123; int b = p.hash &amp; n; if (b != runBit) &#123; runBit = b; lastRun = p; &#125; &#125; if (runBit == 0) &#123; ln = lastRun; hn = null; &#125; else &#123; hn = lastRun; ln = null; &#125; //将node链表，分成2个新的node链表 for (Node&lt;K,V&gt; p = f; p != lastRun; p = p.next) &#123; int ph = p.hash; K pk = p.key; V pv = p.val; if ((ph &amp; n) == 0) ln = new Node&lt;K,V&gt;(ph, pk, pv, ln); else hn = new Node&lt;K,V&gt;(ph, pk, pv, hn); &#125; //将新node链表赋给nextTab setTabAt(nextTab, i, ln); setTabAt(nextTab, i + n, hn); setTabAt(tab, i, fwd); advance = true; &#125; //红黑树迁移 else if (f instanceof TreeBin) &#123; TreeBin&lt;K,V&gt; t = (TreeBin&lt;K,V&gt;)f; TreeNode&lt;K,V&gt; lo = null, loTail = null; TreeNode&lt;K,V&gt; hi = null, hiTail = null; int lc = 0, hc = 0; for (Node&lt;K,V&gt; e = t.first; e != null; e = e.next) &#123; int h = e.hash; TreeNode&lt;K,V&gt; p = new TreeNode&lt;K,V&gt; (h, e.key, e.val, null, null); if ((h &amp; n) == 0) &#123; if ((p.prev = loTail) == null) lo = p; else loTail.next = p; loTail = p; ++lc; &#125; else &#123; if ((p.prev = hiTail) == null) hi = p; else hiTail.next = p; hiTail = p; ++hc; &#125; &#125; ln = (lc &lt;= UNTREEIFY_THRESHOLD) ? untreeify(lo) : (hc != 0) ? new TreeBin&lt;K,V&gt;(lo) : t; hn = (hc &lt;= UNTREEIFY_THRESHOLD) ? untreeify(hi) : (lc != 0) ? new TreeBin&lt;K,V&gt;(hi) : t; setTabAt(nextTab, i, ln); setTabAt(nextTab, i + n, hn); setTabAt(tab, i, fwd); advance = true; &#125; &#125; &#125; &#125; &#125;&#125; 总结多线程无锁扩容的关键就是通过CAS设置sizeCtl与transferIndex变量，协调多个线程对table数组中的node进行迁移。 勘误：tab.length为32，扩容阈值是32*0.75=24]]></content>
  </entry>
  <entry>
    <title><![CDATA[ArrayList 源码分析]]></title>
    <url>%2F2018%2F06%2F03%2FArrayList%20%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%2F</url>
    <content type="text"><![CDATA[ArrayList 源码分析 前言以面试问答的形式学习我们的最常用的装载容器——ArrayList（源码分析基于JDK8） 问答内容ArrayList是什么，可以用来干嘛？问：ArrayList有用过吗？它是一个什么东西？可以用来干嘛？ 答：有用过，ArrayList就是数组列表，主要用来装载数据，当我们装载的是基本类型的数据int,long,boolean,short,byte...的时候我们只能存储他们对应的包装类，它的主要底层实现是数组Object[] elementData。与它类似的是LinkedList，和LinkedList相比，它的查找和访问元素的速度较快，但新增，删除的速度较慢。 示例代码： 12345678910// 创建一个ArrayList，如果没有指定初始大小，默认容器大小为10ArrayList&lt;String&gt; arrayList = new ArrayList&lt;String&gt;();// 往容器里面添加元素arrayList.add("张三");arrayList.add("李四");arrayList.add("王五");// 获取index下标为0的元素 张三String element = arrayList.get(0);// 删除index下标为1的元素 李四String removeElement = arrayList.remove(1); ArrayList底层实现示意图 ArrayList中不断添加数据会有什么问题吗？问：您说它的底层实现是数组，但是数组的大小是定长的，如果我们不断的往里面添加数据的话，不会有问题吗？ 答：ArrayList可以通过构造方法在初始化的时候指定底层数组的大小。 通过无参构造方法的方式ArrayList()初始化，则赋值底层数组Object[] elementData为一个默认空数组Object[] DEFAULTCAPACITY_EMPTY_ELEMENTDATA = {}所以数组容量为0，只有真正对数据进行添加add时，才分配默认DEFAULT_CAPACITY = 10的初始容量。示例代码： 12345678910111213141516// 定义ArrayList默认容量为10private static final int DEFAULT_CAPACITY = 10;// 空数组，当调用无参构造方法时默认复制这个空数组private static final Object[] DEFAULTCAPACITY_EMPTY_ELEMENTDATA = &#123;&#125;;// 真正保存数据的底层数组transient Object[] elementData; // ArrayList的实际元素数量private int size;public ArrayList() &#123; // 无参构造方法默认为空数组 this.elementData = DEFAULTCAPACITY_EMPTY_ELEMENTDATA;&#125; 通过指定容量初始大小的构造方法方式ArrayList(int initialCapacity)初始化，则赋值底层数组Object[] elementData为指定大小的数组this.elementData = new Object[initialCapacity];示例代码： 12345678910111213private static final Object[] EMPTY_ELEMENTDATA = &#123;&#125;;// 通过构造方法出入指定的容量来设置默认底层数组大小 public ArrayList(int initialCapacity) &#123; if (initialCapacity &gt; 0) &#123; this.elementData = new Object[initialCapacity]; &#125; else if (initialCapacity == 0) &#123; this.elementData = EMPTY_ELEMENTDATA; &#125; else &#123; throw new IllegalArgumentException("Illegal Capacity: "+ initialCapacity); &#125;&#125; 当我们添加的元素数量已经达到底层数组Object[] elementData的上限时，我们再往ArrayList元素，则会触发ArrayList的自动扩容机制，ArrayList会通过位运算int newCapacity = oldCapacity + (oldCapacity &gt;&gt; 1);以1.5倍的方式初始化一个新的数组（如初始化数组大小为10，则扩容后的数组大小为15），然后使用Arrays.copyOf(elementData, newCapacity);方法将原数据的数据逐一复制到新数组上面去，以此达到ArrayList扩容的效果。虽然，Arrays.copyOf(elementData, newCapacity);方法最终调用的是native void arraycopy(Object src, int srcPos, Object dest, int destPos, int length)是一个底层方法，效率还算可以，但如果我们在知道ArrayList想装多少个元素的情况下，却没有指定容器大小，则就会导致ArrayList频繁触发扩容机制，频繁进行底层数组之间的数据复制，大大降低使用效率。示例代码： 123456789101112131415161718192021222324252627282930313233public boolean add(E e) &#123; //确保底层数组容量，如果容量不足，则扩容 ensureCapacityInternal(size + 1); elementData[size++] = e; return true;&#125;private void ensureExplicitCapacity(int minCapacity) &#123; modCount++; // 容量不足，则调用grow方法进行扩容 if (minCapacity - elementData.length &gt; 0) grow(minCapacity);&#125;/** * 扩容方法(重点) */private void grow(int minCapacity) &#123; // 获得原容量大小 int oldCapacity = elementData.length; // 新容量为原容量的1.5倍 int newCapacity = oldCapacity + (oldCapacity &gt;&gt; 1); // 再判断新容量是否已足够，如果扩容后仍然不足够，则复制为最小容量长度 if (newCapacity - minCapacity &lt; 0) newCapacity = minCapacity; // 判断是否超过最大长度限制 if (newCapacity - MAX_ARRAY_SIZE &gt; 0) newCapacity = hugeCapacity(minCapacity); // 将原数组的数据复制至新数组， ArrayList的底层数组引用指向新数组 // 如果数据量很大，重复扩容，则会影响效率 elementData = Arrays.copyOf(elementData, newCapacity);&#125; 因此，在我们使用ArrayList的时候，如果知道最终的存储容量capacity，则应该在初始化的时候就指定ArrayList的容量ArrayList(int initialCapacity)，如果初始化时无法预知装载容量，但在使用过程中，得知最终容量，我们可以通过调用ensureCapacity(int minCapacity)方法来指定ArrayList的容量，并且，如果我们在使用途中，如果确定容量大小，但是由于之前每次扩容都扩充50%，所以会造成一定的存储空间浪费，我们可以调用trimToSize()方法将容器最小化到存储元素容量，进而消除这些存储空间浪费。例如：我们当前存储了11个元素，我们不会再添加但是当前的ArrayList的大小为15，有4个存储空间没有被使用，则调用trimToSize()方法后，则会重新创建一个容量为11的数组Object[] elementData，将原有的11个元素复制至新数组，达到节省内存空间的效果。示例代码： 123456789101112131415161718192021222324252627/** * 将底层数组一次性指定到指定容量的大小 */public void ensureCapacity(int minCapacity) &#123; int minExpand = (elementData != DEFAULTCAPACITY_EMPTY_ELEMENTDATA) // any size if not default element table ? 0 // larger than default for default empty table. It's already // supposed to be at default size. : DEFAULT_CAPACITY; if (minCapacity &gt; minExpand) &#123; ensureExplicitCapacity(minCapacity); &#125;&#125;/** * 将容器最小化到存储元素容量 */public void trimToSize() &#123; modCount++; if (size &lt; elementData.length) &#123; elementData = (size == 0) ? EMPTY_ELEMENTDATA : Arrays.copyOf(elementData, size); &#125;&#125; ArrayList怎么 删除数据，为什么访问速度快，删除新增速度慢 ？问：那它是怎么样删除元素的？您上面说到ArrayList访问元素速度较快，但是新增和删除的速度较慢，为什么呢？ 答： 通过源码我们可以得知，ArrayList删除元素时，先获取对应的删除元素，然后把要删除元素对应索引index后的元素逐一往前移动1位，最后将最后一个存储元素清空并返回删除元素，以此达到删除元素的效果。 当我们通过下标的方式去访问元素时，我们假设访问一个元素所花费的时间为K，则通过下标一步到位的方式访问元素，时间则为1K，用“大O”表示法表示，则时间复杂度为O(1)。所以ArrayList的访问数据的数据是比较快的。 当我们去添加元素add(E e)时，我们是把元素添加至末尾，不需要移动元素，此时的时间复杂度为O(1)，但我们把元素添加到指定位置，最坏情况下，我们将元素添加至第一个位置add(int index, E element)，则整个ArrayList的n-1个元素都要往前移动位置，导致底层数组发生n-1次复制。通常情况下，我们说的时间复杂度都是按最坏情况度量的，此时的时间复杂度为O(n)。删除元素同理，删除最后一个元素不需要移动元素，时间复杂度为O(1)，但删除第一个元素，则需要移动n-1个元素，最坏情况下的时间复杂度也是O(n)。 所以ArrayList访问元素速度较快，但是新增和删除的速度较慢。 示例代码： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748/** * 将元素添加至末尾 */public boolean add(E e) &#123; // 确保底层数组容量，如果容量不足，则扩容 ensureCapacityInternal(size + 1); // Increments modCount!! elementData[size++] = e; return true;&#125;/** * 将元素添加至指定下标位置 */public void add(int index, E element) &#123; // 检查下标是否在合法范围内 rangeCheckForAdd(index); // 确保底层数组容量，如果容量不足，则扩容 ensureCapacityInternal(size + 1); // Increments modCount!! // 将要添加的元素下标后的元素通过复制的方式逐一往后移动，腾出对应index下标的存储位置 System.arraycopy(elementData, index, elementData, index + 1, size - index); // 将新增元素存储至指定下标索引index elementData[index] = element; // ArrayList的大小 + 1 size++;&#125;/** * 通过下标索引的方式删除元素 */public E remove(int index) &#123; // 检查下标是否在合法范围内 rangeCheck(index); modCount++; // 直接通过下标去访问底层数组的元素 E oldValue = elementData(index); // 计算数组需要移动的元素个数 int numMoved = size - index - 1; if (numMoved &gt; 0) // 将要删除的元素下标后的元素通过复制的方式逐一往前移动 System.arraycopy(elementData, index+1, elementData, index, numMoved); //将底层数组长度减1，并清空最后一个存储元素。 elementData[--size] = null; // clear to let GC do its work // 返回移除元素 return oldValue;&#125; ArrayList是线程安全的吗？问：ArrayList是线程安全的吗？ 答：ArrayList不是线程安全的，如果多个线程同时对同一个ArrayList更改数据的话，会导致数据不一致或者数据污染。如果出现线程不安全的操作时，ArrayList会尽可能的抛出ConcurrentModificationException防止数据异常，当我们在对一个ArrayList进行遍历时，在遍历期间，我们是不能对ArrayList进行添加，修改，删除等更改数据的操作的，否则也会抛出ConcurrentModificationException异常，此为fail-fast（快速失败）机制。从源码上分析，我们在add,remove,clear等更改ArrayList数据时，都会导致modCount的改变，当expectedModCount != modCount时，则抛出ConcurrentModificationException。如果想要线程安全，可以考虑使用Vector、CopyOnWriteArrayList。 示例代码： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768/** * AbstractList.Itr 的迭代器实现 */private class Itr implements Iterator&lt;E&gt; &#123; int cursor; // index of next element to return int lastRet = -1; // index of last element returned; -1 if no such //期望的modCount int expectedModCount = modCount; public boolean hasNext() &#123; return cursor != size; &#125; @SuppressWarnings("unchecked") public E next() &#123; checkForComodification(); int i = cursor; if (i &gt;= size) throw new NoSuchElementException(); Object[] elementData = ArrayList.this.elementData; if (i &gt;= elementData.length) throw new ConcurrentModificationException(); cursor = i + 1; return (E) elementData[lastRet = i]; &#125; public void remove() &#123; if (lastRet &lt; 0) throw new IllegalStateException(); checkForComodification(); try &#123; ArrayList.this.remove(lastRet); cursor = lastRet; lastRet = -1; expectedModCount = modCount; &#125; catch (IndexOutOfBoundsException ex) &#123; throw new ConcurrentModificationException(); &#125; &#125; @Override @SuppressWarnings("unchecked") public void forEachRemaining(Consumer&lt;? super E&gt; consumer) &#123; Objects.requireNonNull(consumer); final int size = ArrayList.this.size; int i = cursor; if (i &gt;= size) &#123; return; &#125; final Object[] elementData = ArrayList.this.elementData; if (i &gt;= elementData.length) &#123; throw new ConcurrentModificationException(); &#125; while (i != size &amp;&amp; modCount == expectedModCount) &#123; consumer.accept((E) elementData[i++]); &#125; // update once at end of iteration to reduce heap write traffic cursor = i; lastRet = i - 1; checkForComodification(); &#125; final void checkForComodification() &#123; if (modCount != expectedModCount) throw new ConcurrentModificationException(); &#125;&#125; 总结 如果在初始化的时候知道ArrayList的初始容量，请一开始就指定容量ArrayList&lt;String&gt; list = new ArrayList&lt;String&gt;(20);,如果一开始不知道容量，中途才得知，请调用list.ensureCapacity(20);来扩充容量，如果数据已经添加完毕，但仍需要保存在内存中一段时间，请调用list.trimToSize()将容器最小化到存储元素容量，进而消除这些存储空间浪费。 ArrayList是以1.5倍的容量去扩容的，如初始容量是10，则容量依次递增扩充为：15，22，33，49。扩容后把原始数据从旧数组复制至新数组中。 ArrayList访问元素速度较快，下标方式访问元素，时间复杂度为O(1)，添加与删除速度较慢，时间复杂度均为O(n)。 ArrayList不是线程安全的，但是在发生并发行为时，它会尽可能的抛出ConcurrentModificationException，此为fail-fast机制。]]></content>
      <categories>
        <category>ArrayList</category>
      </categories>
      <tags>
        <tag>基础数据类型</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ajax 请求]]></title>
    <url>%2F2018%2F06%2F03%2Fajax%20%E8%AF%B7%E6%B1%82%2F</url>
    <content type="text"><![CDATA[ajax 请求语法：在jq框架中使用jQuery.ajax(url,JSONSettings)方法来实现请求的发送和接收。 jQuery.ajax(url,json); $.ajax(url,json); 参数说明(1)请求类型：type 参数重要性：必要参数 参数值类型：字符串 参数可选值：get、post (2)请求地址：url 参数重要性：必要参数 参数值类型：字符串 参数说明：如果在json参数中写明本属性，则ajax函数的第一个参数就可以不写 (3)响应类型：dataType 参数重要性：必要参数 参数值类型：字符串 参数可选值：json、xml、html、jsonp等等..我们只用json (4)post数据包：data 参数重要性：可选参数，但post请求下一般为必要参数 参数值类型：json 参数说明：本参数是专门提供给post请求服务的， 因为post请求不会将数据直接拼接在url地址中， 因此通过本属性将数据添加到请求内部。 (5)回调函数：success、error、beforeSend 参数重要性：可选参数 参数值类型：函数 参数说明：success属性对应的函数会在请求完成后自动发生回调 error属性对应的函数会在请求发生错误后（发生错误指的是网络错误、链接失败或者url地址违 法）自动发生回调 beforeSend属性对应的函数会在请求发送出之前自动发生回调 get 无参请求12345678910111213141516171819202122232425262728293031323334353637383940414243444546&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head lang="en"&gt; &lt;meta charset="UTF-8"&gt; &lt;title&gt;lesson7&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;button&gt;get无参请求&lt;/button&gt;&lt;ul&gt;&lt;/ul&gt;&lt;script src="js/jquery-1.12.3.min.js"&gt;&lt;/script&gt;&lt;script&gt; var ul = document.querySelector('ul'); $('button').click(function () &#123; $.ajax(&#123; "type":"get", "url":"http://wwtliu.com/sxtstu/blueberrypai/getIndexBanner.php", "dataType":"json", "success":function (response) &#123; //console.log(response.banner); for(var i=0;i&lt;response.banner.length;i++)&#123; //console.log(response.banner[i]); var li = document.createElement('li'); ul.appendChild(li); var pTitle = document.createElement('p'); pTitle.innerHTML = response.banner[i].title; li.appendChild(pTitle); var pcont = document.createElement('p'); pcont.innerHTML = response.banner[i].content; li.appendChild(pcont); var img = document.createElement('img'); img.src = response.banner[i].img; li.appendChild(img); &#125; &#125; &#125;); &#125;);&lt;/script&gt;&lt;/body&gt;&lt;/html&gt; get 有参请求12345678910111213141516171819202122232425262728&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head lang="en"&gt; &lt;meta charset="UTF-8"&gt; &lt;title&gt;lesson7&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;button&gt;get无参请求&lt;/button&gt;&lt;ul&gt;&lt;/ul&gt;&lt;script src="js/jquery-1.12.3.min.js"&gt;&lt;/script&gt;&lt;script&gt; $('button').click(function () &#123; $.ajax(&#123; "type":"get", "url":"http://wwtliu.com/sxtstu/news/juhenews.php?type=yule&amp;count=10", "dataType":"json", "success":function (response) &#123; console.log(response); &#125; &#125;); &#125;);&lt;/script&gt;&lt;/body&gt;&lt;/html&gt; post 请求12345678910111213141516171819202122232425262728293031323334353637383940&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head lang="en"&gt; &lt;meta charset="UTF-8"&gt; &lt;title&gt;lesson7&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;button&gt;get无参请求&lt;/button&gt;&lt;ul&gt;&lt;/ul&gt;&lt;script src="js/jquery-1.12.3.min.js"&gt;&lt;/script&gt;&lt;script&gt; $('button').click(function () &#123; $.ajax(&#123; "type":"post", "url":"http://wwtliu.com/sxtstu/blueberrypai/login.php", "dataType":"json", "data":&#123; user_id:"iwen@qq.com", password:"iwen1232", verification_code:"crfvw" &#125;, "success":function (data) &#123; console.log(data); &#125;, "error":function (error) &#123; console.log(error); &#125;, // 在请求发送前调用 "beforeSend":function () &#123; console.log('这里是请求发送出去前的最后一步！'); &#125; &#125;); &#125;);&lt;/script&gt;&lt;/body&gt;&lt;/html&gt; 更多详细的参数参考：https://www.jquery123.com/jQuery.ajax/ ​​​​​​​]]></content>
      <categories>
        <category>jQuery</category>
      </categories>
      <tags>
        <tag>前端框架</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[10 款最好的 Python IDE]]></title>
    <url>%2F2018%2F06%2F03%2F10%E6%AC%BE%E6%9C%80%E5%A5%BD%E7%9A%84Python%20IDE%2F</url>
    <content type="text"><![CDATA[10 款最好的 Python IDE Vim Eclipse PyDev Sublime Text Emacs Komodo PyCharm Wing PyScripter The Eric Python IDE Interactive Editor for Python]]></content>
      <categories>
        <category>Python IDE</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(两到三年)Java 面试精髓]]></title>
    <url>%2F2018%2F06%2F03%2F(%E4%B8%A4%E5%88%B0%E4%B8%89%E5%B9%B4)Java%20%E9%9D%A2%E8%AF%95%E7%B2%BE%E9%AB%93%2F</url>
    <content type="text"><![CDATA[(两到三年)Java 面试精髓引言：工作两到三年。在面试的时候，其实会对我们平时所有用框架的源码和一些功能底层代码实现给出提问。 Struts2框架的执行流程 ?从客户端发送请求过来,先经过前端控制器（核心过滤器）过滤器中,执行一组拦截器（一组拦截器 就会完成部分功能代码）执行目标Action, 在Action中返回一个结果视图,根据Result的配置进行页面的跳转. Struts2和Struts1没有任何联系.Struts2内核是webwork的内核. hibernate框架的理解?定义: Hibernate是一个开放源代码的对象关系映射（ORM）框架，它对JDBC进行了非常轻量级的对象封装， 使得Java程序员可以随心所欲的使用对象编程思维来操纵数据库.可以通过对象保存到关系型数据库中,仅提供sava/get方法即可 Hibernate是一个持久层的ORM框架. Spring框架的理解?Spring是一个开源框架,核心是控制反转（IOC编程思想）和面向切面（AOP）。简单来说，Spring是一个分层的JavaSE/EEfull-stack(一站式) 轻量级开源框架 Spring的AOP的理解:通过预编译方式和运行期动态代理实现程序功能的统一维护的一种技术,利用AOP可以对业务逻辑的各个部分进行隔离，从而使得业务逻辑各部分之间的耦合度降低，提高程序的可重用性，同时提高了开发的效率.可以在不修改源代码的前提下，对程序进行增强. 例如:在以前配置事务的时候,进行事务的回滚,提交等操作,配置AOP 以后可以将事务的权限交给Spring框架去管理,自动管理 SpringMVC的理解?springMvc:是一个表现层框架,就是从请求中接收传入的参数, 将处理后的结果数据返回给页面展示 基本类型:string,double,float,integer,long.boolean Mybatis的理解?MyBatis是一个优秀的持久层框架，它对jdbc的操作数据库的过程进行封装，使开发者只需要关注 SQL 本身，而不需要花费精力去处理例如注册驱动、创建connection、创建statement、手动设置参数、结果集检索等jdbc繁杂的过程代码。 Mybatis通过xml或注解的方式将要执行的各种statement（statement、preparedStatement、CallableStatement）配置起来，并通过java对象和statement中的sql进行映射生成最终执行的sql语句，最后由mybatis框架执行sql并将结果映射成java对象并返回。 #{}可以有效防止sql注入 Servlet的理解?* GET和POST区别? * GET：请求参数会显示到地址栏.GET方式有大小的限制.GET方式没有请求体 * POST：请求参数不会显示到地址栏.在请求体中.POST没有大小限制.POST方式有请求体. * 只有表单设置为method=”post”才是post请求.其他的都是get请求 生命周期:客户端第一次访问该Servlet的时候才会创建一个Servlet的对象,那么Servlet中的init方法就会执行.任何一次从客户端发送的请求,那么服务器创建一个新的线程执行Servlet中service方法为这次请求服务. service方法的内部根据请求的方式的不同调用不同doXXX的方法.当Servlet从服务器中移除或者关闭服务器的时候Servlet对象就会被销毁.destroy的方法就会执行. Struts2与SpringMVC的区别?1)springmvc的入口是一个servlet即前端控制器，而struts2入口是一个filter过虑器。 2)springmvc是基于方法开发(一个url对应一个方法)，请求参数传递到方法的形参，可以设计为单例或多例(建议单例)，struts2是基于类开发，传递参数是通过类的属性，只能设计为多例。 3)Struts采用值栈存储请求和响应的数据，通过OGNL存取数据， springmvc通过参数解析器是将request请求内容解析，并给方法形参赋值，将数据和视图封装成ModelAndView对象，最后又将ModelAndView中的模型数据通过reques域传输到页面。Jsp视图解析器默认使用jstl。 Jsp的核心及核心标签?a) Servlet b) Core XML Database Funcations Redis什么情况下使用，redis持久化方案?a) 处理大数据量的时候 b) Redis的所有数据都是保存在内存中， Rdb：快照形式，定期把内存中当前时刻的数据保存到磁盘，redis默认支持的持久化方案 aof形式：append only file。把所有对redis数据库操作的命令，增删改操作命令，保存到文件中，数据库恢复是把所有命令执行一遍即可。 Hibernate和Mybatis的区别和优劣?a) Sql优化方面：hibernate的查询会将表中所有的字段查询出来，这一点会有性能的消耗 Mybatis的sql是手动编写的，所以可以按需求指定查询的字段，sql会更灵活，可控性更好 b) Hibernate是在JDBC上进行了一次封装 Mybatis是基于原生的JDBC，运行速度有优势 c) Mybatis mapper xml支持动态sql；Hibernate不支持 d) Hibernate与具体数据库的关联只需在xml文件中配置即可，所有hql语句与具体的数据库无关，移植性好 Mybatis项目所有的sql语句都是依赖所用的数据库的，所以不同数据库类型的支持不好 StringBuffer、StringBuilder的区别?StringBuffer、StringBuilder是容器，是可变的字符串序列，存放于堆内存。 StringBuffer是JDK1.0版本的，线程是安全的，效率比较低。StringBuilder是JDK1.5出现的，线程不安全，效率高。 说一下SOLR?solr就是一个中文搜索引擎,做完分词之后会做热度排名,核心是中文分词器,全文搜索支持,索引值指向对应的文档,相当于是一个字典,默认为collection的一个域对象,查询快,效率高. 可以在Redis里做分词之后的缓存,每次搜索一次就次数加一,里面还有一个投票容错机制,主机挂掉还有备份机,一般配置都为奇数态配置. Solr与Lucene的区别?Lucene是一个开放源代码的全文检索引擎工具包，它不是一个完整的全文检索引擎，Lucene提供了完整的查询引擎和索引引擎，目的是为软件开发人员提供一个简单易用的工具包，以方便的在目标系统中实现全文检索的功能，或者以Lucene为基础构建全文检索引擎。 Solr的目标是打造一款企业级的搜索引擎系统，它是一个搜索引擎服务，可以独立运行，通过Solr可以非常快速的构建企业的搜索引擎，通过Solr也可以高效的完成站内搜索功能。 什么是Redis?1) Redis的高性能是由于其将所有数据都存储在了内存中，为了使Redis在重启之后仍能保证数据不丢失，需要将数据从内存中同步到硬盘中，这一过程就是持久化。 Redis支持两种方式的持久化，一种是RDB方式，一种是AOF方式。可以单独使用其中一种或将二者结合使用。 1.1RDB持久化 RDB方式的持久化是通过快照（snapshotting）完成的，当符合一定条件时Redis会自动将内存中的数据进行快照并持久化到硬盘。 每次进行访问进行存储,如果服务器一旦崩溃,会导致数据丢失 RDB是Redis默认采用的持久化方式，在redis.conf配置文件中默认有此下配置： save 900 1 , save 300 10, save 60 10000 save 开头的一行就是持久化配置，可以配置多个条件（每行配置一个条件），每个条件之间是“或”的关系，“save 900 1”表示15分钟（900秒钟）内 至少 1个键被更改则进行快照，“save 300 10”表示5分钟（300秒）内至少10个键被更改则进行快照。 在redis.conf中： 配置dir指定 rdb快照文件的位置;配置dbfilenam指定rdb快照文件的名称 Redis启动后会读取RDB快照文件，将数据从硬盘载入到内存。根据数据量大小与结构和服务器性能不同，这个时间也不同。 通常将记录一千万个字符串类型键、大小为1GB的快照文件载入到内存中需要花费20～30秒钟。 问题总结： 通过RDB方式实现持久化，一旦Redis异常退出，就会丢失最后一次快照以后更改的所有数据。这就需要开发者根据具体的应用场合，通过组合设置自动快照条件的方式来将可能发生的数据损失控制在能够接受的范围。如果数据很重要以至于无法承受任何损失，则可以考虑使用AOF方式进行持久化。 1.2AOF持久化 默认情况下Redis没有开启AOF（append only file）方式的持久化，访问一段存储一段,效率高. 可以通过appendonly参数开启：appendonly yes开启AOF持久化后每执行一条会更改Redis中的数据的命令，Redis就会将该命令写入硬盘中的AOF文件 AOF文件的保存位置和RDB文件的位置相同，都是通过dir参数设置的，默认的文件名是appendonly.aof，可以通过appendfilename参数修改：appendfilename appendonly.aof 2)主从复制（了解） 2.1什么是主从复制 持久化保证了即使redis服务重启也会丢失数据，因为redis服务重启后会将硬盘上持久化的数据恢复到内存中，但是当redis服务器的硬盘损坏了可能会导致数据丢失，如果通过redis的主从复制机制就可以避免这种单点故障，如下图： 说明： n主redis中的数据有两个副本（replication）即从redis1和从redis2，即使一台redis服务器宕机其它两台redis服务也可以继续提供服务。 n主redis中的数据和从redis上的数据保持实时同步，当主redis写入数据时通过主从复制机制会复制到两个从redis服务上。 n只有一个主redis，可以有多个从redis。 n主从复制不会阻塞master，在同步数据时，master 可以继续处理client 请求 n一个redis可以即是主又是从，如下图： 2.2主从配置 2.2.1主redis配置 无需特殊配置。 2.2.2从redis配置 修改从redis服务器上的redis.conf文件，添加slaveof主redisip主redis端口 上边的配置说明当前该从redis服务器所对应的主redis是192.168.101.3，端口是6379 2.3主从复制过程 2.3.1完整复制 在redis2.8版本之前主从复制过程如下图： 复制过程说明： 1、slave 服务启动，slave 会建立和master 的连接，发送sync 命令。 2、master启动一个后台进程将数据库快照保存到RDB文件中 注意：此时如果生成RDB文件过程中存在写数据操作会导致RDB文件和当前主redis数据不一致，所以此时master 主进程会开始收集写命令并缓存起来。 3、master 就发送RDB文件给slave 4、slave 将文件保存到磁盘上，然后加载到内存恢复 5、master把缓存的命令转发给slave 注意：后续master 收到的写命令都会通过开始建立的连接发送给slave。 当master 和slave 的连接断开时slave 可以自动重新建立连接。如果master 同时收到多个slave 发来的同步连接命令，只会启动一个进程来写数据库镜像，然后发送给所有slave。 完整复制的问题： 在redis2.8之前从redis每次同步都会从主redis中复制全部的数据，如果从redis是新创建的从主redis中复制全部的数据这是没有问题的，但是，如果当从redis停止运行，再启动时可能只有少部分数据和主redis不同步，此时启动redis仍然会从主redis复制全部数据，这样的性能肯定没有只复制那一小部分不同步的数据高。 2.3.2部分复制 部分复制说明： 从机连接主机后，会主动发起 PSYNC 命令，从机会提供 master的runid(机器标识，随机生成的一个串) 和 offset（数据偏移量，如果offset主从不一致则说明数据不同步），主机验证 runid 和 offset 是否有效， runid 相当于主机身份验证码，用来验证从机上一次连接的主机，如果runid验证未通过则，则进行全同步，如果验证通过则说明曾经同步过，根据offset同步部分数据。 2)redis是一个nosql(not only sql不仅仅只有sql)数据库.翻译成中文叫做非关系型型数据库. 关系型数据库:以二维表形式存储数据 非关系型数据库: 以键值对形式存储数据(key, value形式) Redis是用C语言开发的一个开源的高性能键值对（key-value）数据库。它通过提供多种键值数据类型来适应不同场景下的存储需求， 目前为止Redis支持的键值数据类型如下： 字符串类型 散列类型 列表类型 集合类型 有序集合类型。 3)redis的应用场景 缓存（数据查询、短连接、新闻内容、商品内容等等）。（最多使用） 分布式集群架构中的session分离。 聊天室的在线好友列表。 任务队列。（秒杀、抢购、12306等等） 应用排行榜。 网站访问统计。 数据过期处理（可以精确到毫秒） redis是将数据存放到内存中,由于内容存取速度快所以redis被广泛应用在互联网项目中, redis有点:存取速度快,官方称读取速度会达到30万次每秒,写速度在10万次每秒最有,具体限制于硬件. 缺点:对持久化支持不够良好, 所以redis一般不作为数据的主数据库存储,一般配合传统的关系型数据库使用. 4) redis应用领域 分布式缓存 分布式session 保存博客或者论坛的留言回复等. 总之是用在数据量大,并发量高的情况下 谈下DUBBO?Dubbo就是资源调度和治理中心的管理工具。 调用关系说明： \0. 服务容器负责启动，加载，运行服务提供者。 \1. 服务提供者在启动时，向注册中心注册自己提供的服务。 \2. 服务消费者在启动时，向注册中心订阅自己所需的服务。 \3. 注册中心返回服务提供者地址列表给消费者，如果有变更，注册中心将基于长连接推送变更数据给消费者。 \4. 服务消费者，从提供者地址列表中，基于软负载均衡算法，选一台提供者进行调用，如果调用失败，再选另一台调用。 \5. 服务消费者和提供者，在内存中累计调用次数和调用时间，定时每分钟发送一次统计数据到监控中心。 解决跨域问题?JSONP–&gt;Script Tags 秒杀方案：1、把商品的数量放到redis中。 2、秒杀时使用decr命令对商品数量减一。如果不是负数说明抢到。 3、一旦返回数值变为0说明商品已售完。 ZOOKeeper?Zookeeper 作为一个分布式的服务框架，主要用来解决分布式集群中应用系统的一致性问题，它能提供基于类似于文件系统的目录节点树方式的数据存储，但是 Zookeeper 并不是用来专门存储数据的，它的作用主要是用来维护和监控你存储的数据的状态变化。 通过监控这些数据状态的变化，从而可以达到基于数据的集群管理 注册中心负责服务地址的注册与查找，相当于目录服务，服务提供者和消费者只在启动时与注册中心交互，注册中心不转发请求，压力小。使用dubbo-2.3.3以上版本，建议使用zookeeper注册中心。 Zookeeper是Apacahe Hadoop的子项目，是一个树型的目录服务，支持变更推送，适合作为Dubbo服务的注册中心，工业强度较高，可用于生产环境，并推荐使用 ActiveMQ的消息形式一种是点对点的，即一个生产者和一个消费者一一对应；可进行缓存,只允许单人登录查看 另一种是发布/订阅模式，即一个生产者产生消息并进行发送后，可以由多个消费者进行接收。无法进行缓存,支持多人访问. JMS定义了五种不同的消息正文格式，以及调用的消息类型，允许你发送并接收以一些不同形式的数据，提供现有消息格式的一些级别的兼容性。 StreamMessage – Java原始值的数据流 MapMessage–一套名称-值对 TextMessage–一个字符串对象 ObjectMessage–一个序列化的 Java对象 BytesMessage–一个字节的数据流 1.订单系统 1.1.功能分析 1、在购物车页面点击“去结算”按钮跳转到订单确认页面。 a)展示商品列表 b)配送地址列表 c)选择支付方式 2、展示订单确认页面之前，应该确认用户身份。 a)使用拦截器实现。 b)Cookie中取token c)取不到token跳转到登录页面 d)取到token，根据token查询用户信息。 e)如果没有用户信息，登录过期跳转到登录页面 f)取到用户信息，放行。 3、提交订单 a)生成订单 b)展示订单提交成功页面。 订单系统系统：订单确认页面、订单提交成功页面。 订单服务系统 1.1.展示订单确认页面 1.1.1.功能分析 1、在购物车页面点击“去结算”按钮跳转到订单确认页面。 2、请求的url： /order/order-cart 3、参数：没有参数。 4、购物车商品数据从cookie中取出来的。可以在订单系统中取到cookie中的购物车数据。 5、配送地址列表，需要用户登录。需要根据用户id查询收货地址列表。静态数据。 6、支付方式。静态数据。 7、返回值：逻辑视图String，展示订单确认页面。 1.1.2.Dao层、Service层（没有） 需要根据用户id查询收货地址列表。没有此功能。 1.1.3.表现层 请求的url：/order/order-cart 参数：无 业务逻辑： 从cookie中取商品列表展示到页面。 返回值：逻辑视图。 1.1.用户身份认证 在展示订单确认页面之前，需要对用户身份进行认证，要求用户必须登录。 1.1.1.功能分析 1、使用springmvc的拦截器实现。需要实现一个接口HandlerInterceptor接口。 2、业务逻辑 a)从cookie中取token。 b)没有token，需要跳转到登录页面。 c)有token。调用sso系统的服务，根据token查询用户信息。 d)如果查不到用户信息。用户登录已经过期。需要跳转到登录页面。 e)查询到用户信息。放行。 3、在springmvc.xml中配置拦截器。 1.1.2.拦截器实现 1.1.1.功能分析 1、在订单确认页面点击“提交订单”按钮生成订单。 2、请求的url：/order/create 3、参数：提交的是表单的数据。保存的数据：订单、订单明细、配送地址。 a)向tb_order中插入记录。 i.订单号需要手动生成。 要求订单号不能重复。 订单号可读性号。 可以使用redis的incr命令生成订单号。订单号需要一个初始值。 ii.Payment：表单数据 iii.payment_type：表单数据 iv.user_id：用户信息 v.buyer_nick：用户名 vi.其他字段null b)向tb_order_item订单明细表插入数据。 i.Id：使用incr生成 ii.order_id：生成的订单号 iii.其他的都是表单中的数据。 c)tb_order_shipping，订单配送信息 i.order_id：生成的订单号 ii.其他字段都是表单中的数据。 d)使用pojo接收表单的数据。 可以扩展TbOrder，在子类中添加两个属性一个是商品明细列表，一个是配送信息。 把pojo放到taotao-order-interface工程中。 业务逻辑： 1、接收表单的数据 2、生成订单id 3、向订单表插入数据。 4、向订单明细表插入数据 5、向订单物流表插入数据。 6、返回TaotaoResult。 返回值：TaotaoResult 1.1.1.Dao层 可以使用逆向工程。 1.1.2.Service层 参数：OrderInfo 单点登录单点登录就是我们是做了分布式，tomcat集群之后会有session复制的问题，影响利群数量。所以把注册登录拿出来单独做了一个单点登录系统。做的时候是用的redis，key是用uuid生成的一个token,类似于session id,是用户的唯一标识，value是用户的信息。设置了有效期是7天。然后把redis放到了cookie中，实现了cookie的二级跨域。当我们进行操作时，首先要从cookie里面取出token如果取不到，就跳到单点登录系统进行登录操作如果取到了，再看看token有没有过期，如果过期了，也是跳到单点登录系统登录一下，没过期就继续用户的操作。密码进行了加密，用Md5 HashMap 和 HashTable 的区别1）容器整体结构： HashMap的key和value都允许为null，HashMap遇到key为null的时候，调用putForNullKey方法进行处理，而对value没有处理。 Hashtable的key和value都不允许为null。Hashtable遇到null，直接返回NullPointerException。 2） 容量设定与扩容机制： HashMap默认初始化容量为 16，并且容器容量一定是2的n次方，扩容时，是以原容量 2倍 的方式 进行扩容。 Hashtable默认初始化容量为 11，扩容时，是以原容量 2倍 再加 1的方式进行扩容。即int newCapacity = (oldCapacity &lt;&lt; 1) + 1;。 3） 散列分布方式（计算存储位置）： HashMap是先将key键的hashCode经过扰动函数扰动后得到hash值，然后再利用 hash &amp; (length - 1)的方式代替取模，得到元素的存储位置。 Hashtable则是除留余数法进行计算存储位置的（因为其默认容量也不是2的n次方。所以也无法用位运算替代模运算），int index = (hash &amp; 0x7FFFFFFF) % tab.length;。 由于HashMap的容器容量一定是2的n次方，所以能使用hash &amp; (length - 1)的方式代替取模的方式计算元素的位置提高运算效率，但Hashtable的容器容量不一定是2的n次方，所以不能使用此运算方式代替。 4）线程安全（最重要）： HashMap 不是线程安全，如果想线程安全，可以通过调用synchronizedMap(Map&lt;K,V&gt; m)使其线程安全。但是使用时的运行效率会下降，所以建议使用ConcurrentHashMap容器以此达到线程安全。 Hashtable则是线程安全的，每个操作方法前都有synchronized修饰使其同步，但运行效率也不高，所以还是建议使用ConcurrentHashMap容器以此达到线程安全。 因此，Hashtable是一个遗留容器，如果我们不需要线程同步，则建议使用HashMap，如果需要线程同步，则建议使用ConcurrentHashMap。 ArrayList和LinkedList 的区别 LinkedList内部存储的是Node&lt;E&gt;，不仅要维护数据域，还要维护prev和next，如果LinkedList中的结点特别多，则LinkedList比ArrayList更占内存。 插入删除操作效率：LinkedList在做插入和删除操作时，插入或删除头部或尾部时是高效的，操作越靠近中间位置的元素时，需要遍历查找，速度相对慢一些，如果在数据量较大时，每次插入或删除时遍历查找比较费时。所以LinkedList插入与删除，慢在遍历查找，快在只需要更改相关结点的引用地址。ArrayList在做插入和删除操作时，插入或删除尾部时也一样是高效的，操作其他位置，则需要批量移动元素，所以ArrayList插入与删除，快在遍历查找，慢在需要批量移动元素。 循环遍历效率： 由于ArrayList实现了RandomAccess随机访问接口，所以使用for(int i = 0; i &lt; size; i++)遍历会比使用Iterator迭代器来遍历快 而由于LinkedList未实现RandomAccess接口，所以推荐使用Iterator迭代器来遍历数据。 因此，如果我们需要频繁在列表的中部改变插入或删除元素时，建议使用LinkedList，否则，建议使用ArrayList，因为ArrayList遍历查找元素较快，并且只需存储元素的数据域，不需要额外记录其他数据的位置信息，可以节省内存空间。]]></content>
      <categories>
        <category>基础面试题</category>
      </categories>
      <tags>
        <tag>基础面试题</tag>
      </tags>
  </entry>
</search>